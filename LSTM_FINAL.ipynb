{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import lxml\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import yfinance as yf\n",
    "import stockstats\n",
    "\n",
    "import unicodedata\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, TimeDistributed, RepeatVector, LSTM\n",
    "# from keras.layers import CuDNNLSTM as LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_without_absolute = pd.read_pickle('./data/stock_without_absolute.pkl')\n",
    "stock_with_absolute = pd.read_pickle('./data/stock_with_absolute.pkl')\n",
    "\n",
    "label_abs_1d = pd.read_pickle('./data/label_abs_1d.pkl')\n",
    "label_abs_7d = pd.read_pickle('./data/label_abs_7d.pkl')\n",
    "label_abs_30d = pd.read_pickle('./data/label_abs_30d.pkl')\n",
    "\n",
    "label_value_1d = pd.read_pickle('./data/label_value_1d.pkl')\n",
    "label_value_7d = pd.read_pickle('./data/label_value_7d.pkl')\n",
    "label_value_30d = pd.read_pickle('./data/label_value_30d.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stock_with_absolute.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data & Build Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_with_abs = MinMaxScaler()\n",
    "stock_with_abs_norm = scaler_with_abs.fit_transform(stock_with_absolute)\n",
    "\n",
    "scaler_without_abs = MinMaxScaler()\n",
    "stock_without_abs_norm = scaler_without_abs.fit_transform(stock_without_absolute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 10\n",
    "def build_batch(train, label, pastDay=30, futureDay=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train[i:i+pastDay]))\n",
    "        Y_train.append(np.array(label[i+pastDay:i+pastDay+futureDay]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "#     print('trim: ', no_of_rows_drop)\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(predicted_test, test_label, predicted_train, train_label, predicted_valid, valid_label,\n",
    "           file_name, decision_function, clf_name=\"LSTM\"):\n",
    "    print(\"Results for \", clf_name, \": \")\n",
    "    acc_train = accuracy_score(train_label, predicted_train)\n",
    "    acc_test = accuracy_score(test_label, predicted_test)\n",
    "    acc_valid = accuracy_score(valid_label, predicted_valid)\n",
    "    print(\"The Train Accuracy  %0.3f\" % (acc_train))\n",
    "    print(\"The Validation Accuracy  %0.3f\" % (acc_valid))\n",
    "    print(\"The Test Accuracy   %0.3f\" % (acc_test ))\n",
    "\n",
    "    print(\"AUC ROC : %0.3f\" %( roc_auc_score(test_label, predicted_test)))\n",
    "    # confusion matrix\n",
    "    print(\"confusion matrix / precision recall scores\")\n",
    "    print ( confusion_matrix(test_label, predicted_test) )\n",
    "    print ( classification_report(test_label, predicted_test))\n",
    "    \n",
    "    f = open(file_name+'.txt','w')\n",
    "    f.write(\"The Train Accuracy %0.3f\\n\" % (acc_train))\n",
    "    f.write(\"The Validation Accuracy %0.3f\\n\" % (acc_valid))\n",
    "    f.write(\"The Test Accuracy %0.3f\\n\" % (acc_test ))\n",
    "    f.write(\"AUC ROC : %0.3f\\n\" %( roc_auc_score(test_label, predicted_test) ))\n",
    "    f.write( str(confusion_matrix(test_label, predicted_test)) + \"\\n\")\n",
    "    f.write( str(classification_report(test_label, predicted_test)) + \"\\n\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Different Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_1stacks(shape, hidden_layer_size, batch_size):\n",
    "    print(shape)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size,  batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy']) # used for Binary classify\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_1stacks_true_value(shape, hidden_layer_size, batch_size):\n",
    "    print(shape)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size,  batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['mae']) # used for Binary classify\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_2stacks(shape, hidden_layer_size, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, \n",
    "    batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True, init='glorot_uniform'))\n",
    "    model.add(LSTM(hidden_layer_size, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_2stacks_true_value(shape, hidden_layer_size, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, batch_input_shape=(batch_size, shape[1], shape[2]), \n",
    "            stateful=True))\n",
    "    model.add(LSTM(hidden_layer_size, stateful=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['mae']) # or sgd\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_3stacks(shape, hidden_layer_size, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, \n",
    "    batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True, init='glorot_uniform'))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(hidden_layer_size, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy']) # used for Binary classify\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_3stacks_true_value(shape, hidden_layer_size, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(hidden_layer_size, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['mae']) # or adam\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_4stacks(shape, hidden_layer_size, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, \n",
    "    batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True, init='glorot_uniform'))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, stateful=True, init='glorot_uniform'))\n",
    "    model.add(LSTM(hidden_layer_size, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy']) # used for Binary classify\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(build_model_func, PAST_DAYS, stock_data, label, hidden_layer_size, batch_size, monitor, epochs):\n",
    "    X_train_batches, y_train_batches = build_batch(stock_data, label, PAST_DAYS, 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, shuffle = False, stratify = None)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle = False, stratify = None)\n",
    "    \n",
    "    X_train = trim_dataset(X_train, batch_size)\n",
    "    y_train = trim_dataset(y_train, batch_size)\n",
    "    X_valid = trim_dataset(X_valid, batch_size)\n",
    "    y_valid = trim_dataset(y_valid, batch_size)\n",
    "    X_test = trim_dataset(X_test, batch_size)\n",
    "    y_test = trim_dataset(y_test, batch_size)\n",
    "    \n",
    "    \n",
    "    model = build_model_func(X_train.shape, hidden_layer_size, batch_size)\n",
    "    callback = EarlyStopping(monitor=monitor, patience=200, verbose=1, mode=\"auto\")\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1,validation_data=(X_valid, y_valid), callbacks=[callback], shuffle=False)\n",
    "    return model, X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_loss(build_model_func, PAST_DAYS, stock_data, label, hidden_layer_size, batch_size):\n",
    "    X_train_batches, y_train_batches = build_batch(stock_data, label, PAST_DAYS, 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, shuffle = False, stratify = None)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle = False, stratify = None)\n",
    "    \n",
    "    X_train = trim_dataset(X_train, batch_size)\n",
    "    y_train = trim_dataset(y_train, batch_size)\n",
    "    X_valid = trim_dataset(X_valid, batch_size)\n",
    "    y_valid = trim_dataset(y_valid, batch_size)\n",
    "    X_test = trim_dataset(X_test, batch_size)\n",
    "    y_test = trim_dataset(y_test, batch_size)\n",
    "    \n",
    "    \n",
    "    model = build_model_func(X_train.shape, hidden_layer_size, batch_size)\n",
    "    callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "    model.fit(X_train, y_train, epochs=1000, batch_size=batch_size, verbose=1,validation_data=(X_valid, y_valid), callbacks=[callback], shuffle=False)\n",
    "    return model, X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Value Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1988 samples, validate on 217 samples\n",
      "Epoch 1/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 574.0231 - mae: 20.6357 - val_loss: 2590.3097 - val_mae: 46.6466\n",
      "Epoch 2/1000\n",
      "1988/1988 [==============================] - 1s 623us/step - loss: 1513.5479 - mae: 28.1820 - val_loss: 4213.6496 - val_mae: 61.6380\n",
      "Epoch 3/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 1399.1976 - mae: 28.5140 - val_loss: 4455.8929 - val_mae: 63.5727\n",
      "Epoch 4/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 1375.1698 - mae: 28.3734 - val_loss: 4586.4127 - val_mae: 64.5911\n",
      "Epoch 5/1000\n",
      "1988/1988 [==============================] - 1s 702us/step - loss: 1395.7849 - mae: 28.5535 - val_loss: 4617.7082 - val_mae: 64.8329\n",
      "Epoch 6/1000\n",
      "1988/1988 [==============================] - 1s 732us/step - loss: 1394.3392 - mae: 28.7477 - val_loss: 4625.4257 - val_mae: 64.8924\n",
      "Epoch 7/1000\n",
      "1988/1988 [==============================] - 1s 732us/step - loss: 1417.7693 - mae: 28.9666 - val_loss: 4647.6021 - val_mae: 65.0631\n",
      "Epoch 8/1000\n",
      "1988/1988 [==============================] - 1s 710us/step - loss: 1403.8417 - mae: 28.8710 - val_loss: 4780.7629 - val_mae: 66.0785\n",
      "Epoch 9/1000\n",
      "1988/1988 [==============================] - 1s 704us/step - loss: 1380.1557 - mae: 28.3715 - val_loss: 4672.3779 - val_mae: 65.2532\n",
      "Epoch 10/1000\n",
      "1988/1988 [==============================] - 1s 705us/step - loss: 1185.3160 - mae: 25.3159 - val_loss: 4469.7546 - val_mae: 63.6817\n",
      "Epoch 11/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 903.0526 - mae: 21.9635 - val_loss: 4126.1805 - val_mae: 60.9268\n",
      "Epoch 12/1000\n",
      "1988/1988 [==============================] - 1s 721us/step - loss: 635.8036 - mae: 18.6352 - val_loss: 2910.4467 - val_mae: 49.9725\n",
      "Epoch 13/1000\n",
      "1988/1988 [==============================] - 1s 706us/step - loss: 374.9786 - mae: 14.5320 - val_loss: 1935.7287 - val_mae: 39.0468\n",
      "Epoch 14/1000\n",
      "1988/1988 [==============================] - 1s 694us/step - loss: 312.7057 - mae: 13.9940 - val_loss: 1751.5915 - val_mae: 36.6565\n",
      "Epoch 15/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 271.1559 - mae: 13.0281 - val_loss: 1668.4898 - val_mae: 35.5671\n",
      "Epoch 16/1000\n",
      "1988/1988 [==============================] - 1s 653us/step - loss: 244.0574 - mae: 12.2435 - val_loss: 1536.1610 - val_mae: 33.7046\n",
      "Epoch 17/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 223.0078 - mae: 11.9612 - val_loss: 1539.8498 - val_mae: 33.8707\n",
      "Epoch 18/1000\n",
      "1988/1988 [==============================] - 1s 635us/step - loss: 195.9878 - mae: 10.9271 - val_loss: 1645.0022 - val_mae: 35.5560\n",
      "Epoch 19/1000\n",
      "1988/1988 [==============================] - 1s 633us/step - loss: 177.5370 - mae: 10.4770 - val_loss: 1477.3405 - val_mae: 33.2545\n",
      "Epoch 20/1000\n",
      "1988/1988 [==============================] - 1s 679us/step - loss: 184.8413 - mae: 10.4898 - val_loss: 1181.5885 - val_mae: 28.6611\n",
      "Epoch 21/1000\n",
      "1988/1988 [==============================] - 1s 679us/step - loss: 179.5362 - mae: 10.3931 - val_loss: 1306.9277 - val_mae: 30.9531\n",
      "Epoch 22/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 184.8206 - mae: 10.2917 - val_loss: 1291.6386 - val_mae: 30.7414\n",
      "Epoch 23/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 193.3437 - mae: 10.7308 - val_loss: 1302.1566 - val_mae: 31.2307\n",
      "Epoch 24/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 182.1613 - mae: 10.5749 - val_loss: 1216.8666 - val_mae: 29.7563\n",
      "Epoch 25/1000\n",
      "1988/1988 [==============================] - 1s 691us/step - loss: 186.7267 - mae: 10.6718 - val_loss: 1169.0713 - val_mae: 28.9328\n",
      "Epoch 26/1000\n",
      "1988/1988 [==============================] - 1s 724us/step - loss: 184.2959 - mae: 10.6695 - val_loss: 1407.6756 - val_mae: 33.2254\n",
      "Epoch 27/1000\n",
      "1988/1988 [==============================] - 1s 713us/step - loss: 166.3754 - mae: 10.0864 - val_loss: 1142.6239 - val_mae: 28.5734\n",
      "Epoch 28/1000\n",
      "1988/1988 [==============================] - 1s 707us/step - loss: 171.9863 - mae: 10.1211 - val_loss: 1301.8521 - val_mae: 31.3520\n",
      "Epoch 29/1000\n",
      "1988/1988 [==============================] - 1s 713us/step - loss: 171.8481 - mae: 10.0694 - val_loss: 1154.7208 - val_mae: 29.0024\n",
      "Epoch 30/1000\n",
      "1988/1988 [==============================] - 1s 741us/step - loss: 170.7389 - mae: 10.1202 - val_loss: 1590.8598 - val_mae: 35.7137\n",
      "Epoch 31/1000\n",
      "1988/1988 [==============================] - 1s 726us/step - loss: 202.3352 - mae: 10.7936 - val_loss: 1414.6938 - val_mae: 33.0241\n",
      "Epoch 32/1000\n",
      "1988/1988 [==============================] - 1s 713us/step - loss: 180.0558 - mae: 10.4364 - val_loss: 1076.0793 - val_mae: 27.2759\n",
      "Epoch 33/1000\n",
      "1988/1988 [==============================] - 1s 706us/step - loss: 178.0521 - mae: 10.1898 - val_loss: 1146.5139 - val_mae: 28.6889\n",
      "Epoch 34/1000\n",
      "1988/1988 [==============================] - 1s 671us/step - loss: 176.7084 - mae: 10.1105 - val_loss: 975.1857 - val_mae: 25.4654\n",
      "Epoch 35/1000\n",
      "1988/1988 [==============================] - 1s 695us/step - loss: 181.0195 - mae: 10.2593 - val_loss: 911.3317 - val_mae: 24.2951\n",
      "Epoch 36/1000\n",
      "1988/1988 [==============================] - 1s 696us/step - loss: 177.0108 - mae: 10.1584 - val_loss: 887.9662 - val_mae: 23.6176\n",
      "Epoch 37/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 167.8830 - mae: 9.9131 - val_loss: 913.3097 - val_mae: 24.5131\n",
      "Epoch 38/1000\n",
      "1988/1988 [==============================] - 1s 650us/step - loss: 168.0622 - mae: 9.9433 - val_loss: 1019.4685 - val_mae: 26.9060\n",
      "Epoch 39/1000\n",
      "1988/1988 [==============================] - 1s 648us/step - loss: 170.9046 - mae: 9.9037 - val_loss: 881.4903 - val_mae: 24.1876\n",
      "Epoch 40/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 150.5662 - mae: 9.3471 - val_loss: 929.1831 - val_mae: 25.2418\n",
      "Epoch 41/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 157.1698 - mae: 9.5588 - val_loss: 974.6116 - val_mae: 26.3439\n",
      "Epoch 42/1000\n",
      "1988/1988 [==============================] - 1s 641us/step - loss: 161.4182 - mae: 9.7191 - val_loss: 938.5474 - val_mae: 25.4638\n",
      "Epoch 43/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 155.8505 - mae: 9.5248 - val_loss: 964.0374 - val_mae: 25.9574\n",
      "Epoch 44/1000\n",
      "1988/1988 [==============================] - 1s 679us/step - loss: 155.4822 - mae: 9.3633 - val_loss: 818.5143 - val_mae: 23.3400\n",
      "Epoch 45/1000\n",
      "1988/1988 [==============================] - 1s 700us/step - loss: 149.7991 - mae: 9.2442 - val_loss: 657.1005 - val_mae: 19.5912\n",
      "Epoch 46/1000\n",
      "1988/1988 [==============================] - 1s 689us/step - loss: 147.5362 - mae: 9.0923 - val_loss: 867.6033 - val_mae: 24.3423\n",
      "Epoch 47/1000\n",
      "1988/1988 [==============================] - 1s 711us/step - loss: 162.8780 - mae: 9.5715 - val_loss: 727.6133 - val_mae: 21.5222\n",
      "Epoch 48/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 147.4448 - mae: 9.2639 - val_loss: 645.7626 - val_mae: 20.1396\n",
      "Epoch 49/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 136.2254 - mae: 8.8794 - val_loss: 517.0969 - val_mae: 16.9804\n",
      "Epoch 50/1000\n",
      "1988/1988 [==============================] - 1s 653us/step - loss: 132.5520 - mae: 8.7038 - val_loss: 660.4238 - val_mae: 20.3237\n",
      "Epoch 51/1000\n",
      "1988/1988 [==============================] - 1s 637us/step - loss: 140.2454 - mae: 8.8215 - val_loss: 685.3447 - val_mae: 21.3668\n",
      "Epoch 52/1000\n",
      "1988/1988 [==============================] - 1s 633us/step - loss: 131.6404 - mae: 8.7386 - val_loss: 759.3749 - val_mae: 22.9338\n",
      "Epoch 53/1000\n",
      "1988/1988 [==============================] - ETA: 0s - loss: 127.0868 - mae: 8.590 - 1s 650us/step - loss: 131.9179 - mae: 8.7310 - val_loss: 675.4772 - val_mae: 20.6749\n",
      "Epoch 54/1000\n",
      "1988/1988 [==============================] - 1s 605us/step - loss: 133.7560 - mae: 8.6573 - val_loss: 590.6259 - val_mae: 18.7994\n",
      "Epoch 55/1000\n",
      "1988/1988 [==============================] - 1s 611us/step - loss: 123.7218 - mae: 8.3563 - val_loss: 580.2536 - val_mae: 18.3320\n",
      "Epoch 56/1000\n",
      "1988/1988 [==============================] - 1s 611us/step - loss: 119.6081 - mae: 8.2251 - val_loss: 672.8650 - val_mae: 20.2088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000\n",
      "1988/1988 [==============================] - 1s 581us/step - loss: 117.3104 - mae: 8.0043 - val_loss: 535.7444 - val_mae: 17.1828\n",
      "Epoch 58/1000\n",
      "1988/1988 [==============================] - 1s 607us/step - loss: 110.1783 - mae: 7.9078 - val_loss: 664.6000 - val_mae: 20.6180\n",
      "Epoch 59/1000\n",
      "1988/1988 [==============================] - 1s 582us/step - loss: 114.2171 - mae: 7.7733 - val_loss: 676.7041 - val_mae: 21.5119\n",
      "Epoch 60/1000\n",
      "1988/1988 [==============================] - 1s 574us/step - loss: 108.6360 - mae: 7.7472 - val_loss: 610.7437 - val_mae: 19.5799\n",
      "Epoch 61/1000\n",
      "1988/1988 [==============================] - 1s 585us/step - loss: 115.2256 - mae: 7.9814 - val_loss: 457.7211 - val_mae: 15.8274\n",
      "Epoch 62/1000\n",
      "1988/1988 [==============================] - 1s 616us/step - loss: 102.5545 - mae: 7.5102 - val_loss: 571.4034 - val_mae: 18.4534\n",
      "Epoch 63/1000\n",
      "1988/1988 [==============================] - 1s 593us/step - loss: 102.8927 - mae: 7.5548 - val_loss: 567.2865 - val_mae: 18.3236\n",
      "Epoch 64/1000\n",
      "1988/1988 [==============================] - 1s 612us/step - loss: 107.2215 - mae: 7.6270 - val_loss: 774.0659 - val_mae: 23.3784\n",
      "Epoch 65/1000\n",
      "1988/1988 [==============================] - 1s 606us/step - loss: 103.9148 - mae: 7.5099 - val_loss: 456.0593 - val_mae: 16.0616\n",
      "Epoch 66/1000\n",
      "1988/1988 [==============================] - 1s 612us/step - loss: 102.6485 - mae: 7.4442 - val_loss: 542.4694 - val_mae: 18.3574\n",
      "Epoch 67/1000\n",
      "1988/1988 [==============================] - 1s 604us/step - loss: 105.1841 - mae: 7.5473 - val_loss: 378.5783 - val_mae: 14.3918\n",
      "Epoch 68/1000\n",
      "1988/1988 [==============================] - 1s 617us/step - loss: 97.6133 - mae: 7.2975 - val_loss: 488.4134 - val_mae: 16.6387\n",
      "Epoch 69/1000\n",
      "1988/1988 [==============================] - 1s 606us/step - loss: 94.5425 - mae: 7.2124 - val_loss: 659.3762 - val_mae: 21.0005\n",
      "Epoch 70/1000\n",
      "1988/1988 [==============================] - 1s 616us/step - loss: 88.1216 - mae: 6.8951 - val_loss: 612.2487 - val_mae: 20.4508\n",
      "Epoch 71/1000\n",
      "1988/1988 [==============================] - 1s 653us/step - loss: 85.9237 - mae: 6.8433 - val_loss: 447.9545 - val_mae: 16.2176\n",
      "Epoch 72/1000\n",
      "1988/1988 [==============================] - 2s 774us/step - loss: 89.4204 - mae: 7.0498 - val_loss: 429.8065 - val_mae: 16.1081\n",
      "Epoch 73/1000\n",
      "1988/1988 [==============================] - 1s 729us/step - loss: 75.1873 - mae: 6.3320 - val_loss: 296.4478 - val_mae: 12.7882\n",
      "Epoch 74/1000\n",
      "1988/1988 [==============================] - 1s 713us/step - loss: 84.0897 - mae: 6.7831 - val_loss: 445.4327 - val_mae: 16.4477\n",
      "Epoch 75/1000\n",
      "1988/1988 [==============================] - 1s 725us/step - loss: 84.8587 - mae: 6.8145 - val_loss: 487.8738 - val_mae: 18.1536\n",
      "Epoch 76/1000\n",
      "1988/1988 [==============================] - 1s 671us/step - loss: 85.9475 - mae: 7.0080 - val_loss: 480.7995 - val_mae: 17.9974\n",
      "Epoch 77/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 82.2757 - mae: 6.8829 - val_loss: 387.5357 - val_mae: 15.5461\n",
      "Epoch 78/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 82.4639 - mae: 6.9301 - val_loss: 298.6512 - val_mae: 12.9901\n",
      "Epoch 79/1000\n",
      "1988/1988 [==============================] - 1s 605us/step - loss: 70.0999 - mae: 6.2893 - val_loss: 466.5199 - val_mae: 17.7419\n",
      "Epoch 80/1000\n",
      "1988/1988 [==============================] - 1s 592us/step - loss: 72.3505 - mae: 6.4253 - val_loss: 405.2130 - val_mae: 15.8527\n",
      "Epoch 81/1000\n",
      "1988/1988 [==============================] - 1s 572us/step - loss: 77.9185 - mae: 6.5965 - val_loss: 352.1832 - val_mae: 14.3536\n",
      "Epoch 82/1000\n",
      "1988/1988 [==============================] - 1s 554us/step - loss: 76.4311 - mae: 6.5523 - val_loss: 363.5737 - val_mae: 14.8324\n",
      "Epoch 83/1000\n",
      "1988/1988 [==============================] - 1s 541us/step - loss: 73.5018 - mae: 6.3206 - val_loss: 525.5732 - val_mae: 19.2773\n",
      "Epoch 84/1000\n",
      "1988/1988 [==============================] - 1s 522us/step - loss: 70.8455 - mae: 6.3313 - val_loss: 462.8962 - val_mae: 17.6715\n",
      "Epoch 85/1000\n",
      "1988/1988 [==============================] - 1s 616us/step - loss: 66.0761 - mae: 6.0757 - val_loss: 288.1051 - val_mae: 12.9157\n",
      "Epoch 86/1000\n",
      "1988/1988 [==============================] - 1s 568us/step - loss: 66.1269 - mae: 6.0956 - val_loss: 443.4656 - val_mae: 17.2953\n",
      "Epoch 87/1000\n",
      "1988/1988 [==============================] - 1s 573us/step - loss: 64.9362 - mae: 5.9950 - val_loss: 309.9734 - val_mae: 13.5714\n",
      "Epoch 88/1000\n",
      "1988/1988 [==============================] - 1s 569us/step - loss: 64.3107 - mae: 5.9959 - val_loss: 313.7562 - val_mae: 13.6565\n",
      "Epoch 89/1000\n",
      "1988/1988 [==============================] - 1s 598us/step - loss: 61.5294 - mae: 5.9088 - val_loss: 411.7804 - val_mae: 16.1067\n",
      "Epoch 90/1000\n",
      "1988/1988 [==============================] - 1s 633us/step - loss: 63.9551 - mae: 6.0050 - val_loss: 433.0962 - val_mae: 16.8852\n",
      "Epoch 91/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 68.2098 - mae: 6.0518 - val_loss: 502.8151 - val_mae: 19.2424\n",
      "Epoch 92/1000\n",
      "1988/1988 [==============================] - 1s 661us/step - loss: 66.8710 - mae: 6.2734 - val_loss: 446.7923 - val_mae: 17.1954\n",
      "Epoch 93/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 57.9539 - mae: 5.8053 - val_loss: 392.8761 - val_mae: 15.8567\n",
      "Epoch 94/1000\n",
      "1988/1988 [==============================] - 1s 640us/step - loss: 59.9105 - mae: 5.6727 - val_loss: 385.7949 - val_mae: 15.6837\n",
      "Epoch 95/1000\n",
      "1988/1988 [==============================] - 1s 641us/step - loss: 60.9894 - mae: 5.7617 - val_loss: 490.3986 - val_mae: 18.5873\n",
      "Epoch 96/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 53.6629 - mae: 5.5540 - val_loss: 389.1644 - val_mae: 15.9825\n",
      "Epoch 97/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 59.6057 - mae: 5.7061 - val_loss: 564.3807 - val_mae: 19.6824\n",
      "Epoch 98/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 57.4431 - mae: 5.6580 - val_loss: 524.3894 - val_mae: 18.8119\n",
      "Epoch 99/1000\n",
      "1988/1988 [==============================] - 1s 651us/step - loss: 64.3765 - mae: 5.9564 - val_loss: 304.2765 - val_mae: 13.9283\n",
      "Epoch 100/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 57.9259 - mae: 5.6727 - val_loss: 238.5441 - val_mae: 11.9695\n",
      "Epoch 101/1000\n",
      "1988/1988 [==============================] - 1s 736us/step - loss: 55.3136 - mae: 5.5232 - val_loss: 387.4994 - val_mae: 16.2565\n",
      "Epoch 102/1000\n",
      "1988/1988 [==============================] - 2s 900us/step - loss: 57.4930 - mae: 5.6713 - val_loss: 392.6871 - val_mae: 16.0937\n",
      "Epoch 103/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 55.6618 - mae: 5.5979 - val_loss: 171.6628 - val_mae: 10.1362\n",
      "Epoch 104/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 51.9629 - mae: 5.3390 - val_loss: 418.3629 - val_mae: 16.9421\n",
      "Epoch 105/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 55.5197 - mae: 5.4030 - val_loss: 251.5244 - val_mae: 12.1453\n",
      "Epoch 106/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 53.2999 - mae: 5.4492 - val_loss: 391.5307 - val_mae: 15.5740\n",
      "Epoch 107/1000\n",
      "1988/1988 [==============================] - 1s 637us/step - loss: 55.4357 - mae: 5.3339 - val_loss: 360.9063 - val_mae: 14.7682\n",
      "Epoch 108/1000\n",
      "1988/1988 [==============================] - 1s 600us/step - loss: 50.3871 - mae: 5.2632 - val_loss: 455.4625 - val_mae: 17.2444\n",
      "Epoch 109/1000\n",
      "1988/1988 [==============================] - 1s 580us/step - loss: 52.2350 - mae: 5.2821 - val_loss: 393.8241 - val_mae: 15.3394\n",
      "Epoch 110/1000\n",
      "1988/1988 [==============================] - 1s 595us/step - loss: 54.2514 - mae: 5.4195 - val_loss: 514.9682 - val_mae: 17.6447\n",
      "Epoch 111/1000\n",
      "1988/1988 [==============================] - 1s 608us/step - loss: 59.6270 - mae: 5.6781 - val_loss: 405.5280 - val_mae: 15.2762\n",
      "Epoch 112/1000\n",
      "1988/1988 [==============================] - 1s 597us/step - loss: 52.2624 - mae: 5.3136 - val_loss: 372.5277 - val_mae: 14.7884\n",
      "Epoch 113/1000\n",
      "1988/1988 [==============================] - 1s 627us/step - loss: 53.1033 - mae: 5.2403 - val_loss: 383.2024 - val_mae: 14.9327\n",
      "Epoch 114/1000\n",
      "1988/1988 [==============================] - 1s 605us/step - loss: 48.8405 - mae: 5.1451 - val_loss: 466.7045 - val_mae: 16.8947\n",
      "Epoch 115/1000\n",
      "1988/1988 [==============================] - 1s 584us/step - loss: 56.2821 - mae: 5.4805 - val_loss: 576.6432 - val_mae: 19.6476\n",
      "Epoch 116/1000\n",
      "1988/1988 [==============================] - 1s 596us/step - loss: 49.7830 - mae: 5.1521 - val_loss: 466.5466 - val_mae: 16.8522\n",
      "Epoch 117/1000\n",
      "1988/1988 [==============================] - 1s 598us/step - loss: 50.3295 - mae: 5.2309 - val_loss: 380.4177 - val_mae: 14.7301\n",
      "Epoch 118/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 49.1418 - mae: 5.2491 - val_loss: 366.9516 - val_mae: 14.4065\n",
      "Epoch 119/1000\n",
      "1988/1988 [==============================] - 1s 614us/step - loss: 49.8331 - mae: 5.2238 - val_loss: 563.8678 - val_mae: 18.6198\n",
      "Epoch 120/1000\n",
      "1988/1988 [==============================] - 1s 612us/step - loss: 49.9939 - mae: 5.1772 - val_loss: 401.4500 - val_mae: 15.1033\n",
      "Epoch 121/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 52.2073 - mae: 5.2225 - val_loss: 305.0268 - val_mae: 13.0313\n",
      "Epoch 122/1000\n",
      "1988/1988 [==============================] - 1s 626us/step - loss: 49.6072 - mae: 5.1458 - val_loss: 357.9680 - val_mae: 14.7674\n",
      "Epoch 123/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 47.9523 - mae: 5.1241 - val_loss: 287.3115 - val_mae: 12.7534\n",
      "Epoch 124/1000\n",
      "1988/1988 [==============================] - 1s 628us/step - loss: 49.6558 - mae: 5.1449 - val_loss: 253.9197 - val_mae: 11.8628\n",
      "Epoch 125/1000\n",
      "1988/1988 [==============================] - 1s 620us/step - loss: 49.5131 - mae: 5.0270 - val_loss: 221.9303 - val_mae: 11.2267\n",
      "Epoch 126/1000\n",
      "1988/1988 [==============================] - 1s 635us/step - loss: 50.8887 - mae: 5.1974 - val_loss: 385.4023 - val_mae: 15.4870\n",
      "Epoch 127/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 48.8394 - mae: 5.1323 - val_loss: 384.2271 - val_mae: 15.1122\n",
      "Epoch 128/1000\n",
      "1988/1988 [==============================] - 1s 662us/step - loss: 47.4035 - mae: 4.9692 - val_loss: 402.2065 - val_mae: 15.5265\n",
      "Epoch 129/1000\n",
      "1988/1988 [==============================] - 1s 640us/step - loss: 46.3890 - mae: 4.9275 - val_loss: 496.2985 - val_mae: 17.9963\n",
      "Epoch 130/1000\n",
      "1988/1988 [==============================] - 1s 616us/step - loss: 47.7014 - mae: 5.0287 - val_loss: 378.4330 - val_mae: 15.3254\n",
      "Epoch 131/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 49.2423 - mae: 5.0097 - val_loss: 232.4266 - val_mae: 11.4365\n",
      "Epoch 132/1000\n",
      "1988/1988 [==============================] - 1s 618us/step - loss: 45.9587 - mae: 4.9459 - val_loss: 436.4894 - val_mae: 16.5031\n",
      "Epoch 133/1000\n",
      "1988/1988 [==============================] - 1s 615us/step - loss: 44.3233 - mae: 4.8116 - val_loss: 317.8222 - val_mae: 13.3589\n",
      "Epoch 134/1000\n",
      "1988/1988 [==============================] - 1s 628us/step - loss: 48.1610 - mae: 5.0934 - val_loss: 226.3398 - val_mae: 11.2153\n",
      "Epoch 135/1000\n",
      "1988/1988 [==============================] - 1s 651us/step - loss: 46.3567 - mae: 4.9085 - val_loss: 410.0466 - val_mae: 15.7910\n",
      "Epoch 136/1000\n",
      "1988/1988 [==============================] - 1s 648us/step - loss: 43.9101 - mae: 4.7982 - val_loss: 476.3135 - val_mae: 17.5307\n",
      "Epoch 137/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 45.8951 - mae: 4.8896 - val_loss: 313.3929 - val_mae: 13.1449\n",
      "Epoch 138/1000\n",
      "1988/1988 [==============================] - 1s 628us/step - loss: 45.0636 - mae: 4.9611 - val_loss: 266.8874 - val_mae: 12.1698\n",
      "Epoch 139/1000\n",
      "1988/1988 [==============================] - 1s 624us/step - loss: 49.0669 - mae: 5.0562 - val_loss: 332.8261 - val_mae: 14.0491\n",
      "Epoch 140/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 45.5568 - mae: 4.8505 - val_loss: 463.4437 - val_mae: 17.5582\n",
      "Epoch 141/1000\n",
      "1988/1988 [==============================] - 1s 733us/step - loss: 43.4934 - mae: 4.6800 - val_loss: 320.3838 - val_mae: 13.5642\n",
      "Epoch 142/1000\n",
      "1988/1988 [==============================] - 1s 723us/step - loss: 46.2829 - mae: 4.9591 - val_loss: 372.1930 - val_mae: 14.8224\n",
      "Epoch 143/1000\n",
      "1988/1988 [==============================] - 2s 764us/step - loss: 50.2953 - mae: 4.9934 - val_loss: 380.8497 - val_mae: 14.9818\n",
      "Epoch 144/1000\n",
      "1988/1988 [==============================] - 1s 716us/step - loss: 47.5085 - mae: 5.0706 - val_loss: 365.1260 - val_mae: 14.8688\n",
      "Epoch 145/1000\n",
      "1988/1988 [==============================] - 1s 725us/step - loss: 44.2259 - mae: 4.8529 - val_loss: 421.6879 - val_mae: 16.1994\n",
      "Epoch 146/1000\n",
      "1988/1988 [==============================] - 2s 762us/step - loss: 43.5472 - mae: 4.7231 - val_loss: 258.7748 - val_mae: 11.9248\n",
      "Epoch 147/1000\n",
      "1988/1988 [==============================] - 1s 711us/step - loss: 44.9623 - mae: 4.8928 - val_loss: 242.0168 - val_mae: 11.5676\n",
      "Epoch 148/1000\n",
      "1988/1988 [==============================] - 1s 705us/step - loss: 49.6716 - mae: 5.1070 - val_loss: 428.1842 - val_mae: 16.2904\n",
      "Epoch 149/1000\n",
      "1988/1988 [==============================] - 1s 700us/step - loss: 44.8839 - mae: 4.9076 - val_loss: 338.9700 - val_mae: 14.0386\n",
      "Epoch 150/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 42.7204 - mae: 4.6592 - val_loss: 562.1167 - val_mae: 19.7272\n",
      "Epoch 151/1000\n",
      "1988/1988 [==============================] - 1s 689us/step - loss: 44.5445 - mae: 4.7003 - val_loss: 292.9015 - val_mae: 12.8301\n",
      "Epoch 152/1000\n",
      "1988/1988 [==============================] - 1s 691us/step - loss: 50.3944 - mae: 5.1174 - val_loss: 372.9803 - val_mae: 14.7651\n",
      "Epoch 153/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 41.3915 - mae: 4.6398 - val_loss: 257.0615 - val_mae: 12.2955\n",
      "Epoch 154/1000\n",
      "1988/1988 [==============================] - 1s 692us/step - loss: 47.3844 - mae: 4.9435 - val_loss: 481.3123 - val_mae: 18.0473\n",
      "Epoch 155/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 51.2906 - mae: 5.3389 - val_loss: 264.2758 - val_mae: 12.1051\n",
      "Epoch 156/1000\n",
      "1988/1988 [==============================] - 1s 679us/step - loss: 49.4085 - mae: 5.1544 - val_loss: 254.9797 - val_mae: 12.2369\n",
      "Epoch 157/1000\n",
      "1988/1988 [==============================] - 1s 681us/step - loss: 46.7454 - mae: 4.8714 - val_loss: 285.7071 - val_mae: 13.9612\n",
      "Epoch 158/1000\n",
      "1988/1988 [==============================] - 1s 694us/step - loss: 41.4446 - mae: 4.6695 - val_loss: 129.9067 - val_mae: 8.5650\n",
      "Epoch 159/1000\n",
      "1988/1988 [==============================] - 1s 693us/step - loss: 45.7411 - mae: 5.0307 - val_loss: 89.4591 - val_mae: 7.6121\n",
      "Epoch 160/1000\n",
      "1988/1988 [==============================] - 1s 702us/step - loss: 40.3950 - mae: 4.5451 - val_loss: 147.0633 - val_mae: 8.9646\n",
      "Epoch 161/1000\n",
      "1988/1988 [==============================] - 1s 708us/step - loss: 44.1369 - mae: 4.7836 - val_loss: 266.1612 - val_mae: 13.1147\n",
      "Epoch 162/1000\n",
      "1988/1988 [==============================] - 1s 713us/step - loss: 43.5905 - mae: 4.7113 - val_loss: 260.4030 - val_mae: 12.5431\n",
      "Epoch 163/1000\n",
      "1988/1988 [==============================] - 1s 710us/step - loss: 41.7550 - mae: 4.6754 - val_loss: 179.4727 - val_mae: 10.0655\n",
      "Epoch 164/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 41.1055 - mae: 4.6457 - val_loss: 113.5222 - val_mae: 8.7627\n",
      "Epoch 165/1000\n",
      "1988/1988 [==============================] - 1s 703us/step - loss: 42.3418 - mae: 4.7489 - val_loss: 132.9653 - val_mae: 8.6481\n",
      "Epoch 166/1000\n",
      "1988/1988 [==============================] - 1s 679us/step - loss: 40.9380 - mae: 4.5833 - val_loss: 254.9497 - val_mae: 12.4801\n",
      "Epoch 167/1000\n",
      "1988/1988 [==============================] - 1s 672us/step - loss: 39.2871 - mae: 4.4885 - val_loss: 338.8485 - val_mae: 14.8045\n",
      "Epoch 168/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 42.4336 - mae: 4.5648 - val_loss: 218.4142 - val_mae: 11.2262\n",
      "Epoch 169/1000\n",
      "1988/1988 [==============================] - 1s 676us/step - loss: 41.3822 - mae: 4.6910 - val_loss: 296.2423 - val_mae: 12.9292\n",
      "Epoch 170/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 38.9925 - mae: 4.4968 - val_loss: 326.7817 - val_mae: 13.6109\n",
      "Epoch 171/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 1s 658us/step - loss: 44.7816 - mae: 4.7776 - val_loss: 374.1794 - val_mae: 14.6698\n",
      "Epoch 172/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 43.9915 - mae: 4.7338 - val_loss: 389.0807 - val_mae: 15.3585\n",
      "Epoch 173/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 44.3156 - mae: 4.7053 - val_loss: 381.5664 - val_mae: 15.8521\n",
      "Epoch 174/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 40.1544 - mae: 4.6616 - val_loss: 324.6167 - val_mae: 14.3729\n",
      "Epoch 175/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 42.1344 - mae: 4.5728 - val_loss: 338.1544 - val_mae: 13.9820\n",
      "Epoch 176/1000\n",
      "1988/1988 [==============================] - 1s 694us/step - loss: 41.1773 - mae: 4.6289 - val_loss: 335.6001 - val_mae: 13.9115\n",
      "Epoch 177/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 42.4080 - mae: 4.7982 - val_loss: 178.1245 - val_mae: 10.0371\n",
      "Epoch 178/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 40.1009 - mae: 4.5119 - val_loss: 325.7188 - val_mae: 14.7005\n",
      "Epoch 179/1000\n",
      "1988/1988 [==============================] - 1s 693us/step - loss: 41.2416 - mae: 4.6318 - val_loss: 207.2825 - val_mae: 10.7558\n",
      "Epoch 180/1000\n",
      "1988/1988 [==============================] - 1s 687us/step - loss: 39.7112 - mae: 4.5770 - val_loss: 311.6167 - val_mae: 13.4595\n",
      "Epoch 181/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 40.0043 - mae: 4.5357 - val_loss: 262.7060 - val_mae: 12.5528\n",
      "Epoch 182/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 35.5968 - mae: 4.3583 - val_loss: 283.1541 - val_mae: 13.0729\n",
      "Epoch 183/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 36.1531 - mae: 4.3691 - val_loss: 295.9611 - val_mae: 13.0332\n",
      "Epoch 184/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 39.7359 - mae: 4.5255 - val_loss: 260.9351 - val_mae: 12.3642\n",
      "Epoch 185/1000\n",
      "1988/1988 [==============================] - 1s 620us/step - loss: 40.2226 - mae: 4.6266 - val_loss: 438.5128 - val_mae: 17.7589\n",
      "Epoch 186/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 41.6456 - mae: 4.6655 - val_loss: 297.2924 - val_mae: 12.9338\n",
      "Epoch 187/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 45.6654 - mae: 5.0381 - val_loss: 314.3485 - val_mae: 13.8398\n",
      "Epoch 188/1000\n",
      "1988/1988 [==============================] - 1s 614us/step - loss: 41.7364 - mae: 4.6928 - val_loss: 246.2427 - val_mae: 11.9768\n",
      "Epoch 189/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 41.8654 - mae: 4.6801 - val_loss: 406.0091 - val_mae: 15.8905\n",
      "Epoch 190/1000\n",
      "1988/1988 [==============================] - 1s 604us/step - loss: 41.6753 - mae: 4.6525 - val_loss: 351.7696 - val_mae: 13.9903\n",
      "Epoch 191/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 38.1189 - mae: 4.5053 - val_loss: 332.9873 - val_mae: 13.5981\n",
      "Epoch 192/1000\n",
      "1988/1988 [==============================] - 1s 588us/step - loss: 41.1742 - mae: 4.5628 - val_loss: 316.9981 - val_mae: 13.2652\n",
      "Epoch 193/1000\n",
      "1988/1988 [==============================] - 1s 607us/step - loss: 41.8356 - mae: 4.6523 - val_loss: 365.0277 - val_mae: 14.4896\n",
      "Epoch 194/1000\n",
      "1988/1988 [==============================] - 1s 633us/step - loss: 41.0412 - mae: 4.6152 - val_loss: 396.0315 - val_mae: 15.4583\n",
      "Epoch 195/1000\n",
      "1988/1988 [==============================] - 1s 692us/step - loss: 42.0646 - mae: 4.5654 - val_loss: 485.4384 - val_mae: 17.5094\n",
      "Epoch 196/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 40.4283 - mae: 4.4922 - val_loss: 688.0255 - val_mae: 22.2847\n",
      "Epoch 197/1000\n",
      "1988/1988 [==============================] - 1s 753us/step - loss: 40.2578 - mae: 4.6169 - val_loss: 410.6593 - val_mae: 15.0851\n",
      "Epoch 198/1000\n",
      "1988/1988 [==============================] - 1s 729us/step - loss: 42.4510 - mae: 4.8005 - val_loss: 349.9191 - val_mae: 14.1058\n",
      "Epoch 199/1000\n",
      "1988/1988 [==============================] - 2s 766us/step - loss: 39.6380 - mae: 4.4869 - val_loss: 502.5511 - val_mae: 18.3773\n",
      "Epoch 200/1000\n",
      "1988/1988 [==============================] - 1s 748us/step - loss: 37.7510 - mae: 4.4331 - val_loss: 390.7206 - val_mae: 15.1680\n",
      "Epoch 201/1000\n",
      "1988/1988 [==============================] - 1s 709us/step - loss: 36.4633 - mae: 4.3477 - val_loss: 381.8639 - val_mae: 14.6353\n",
      "Epoch 202/1000\n",
      "1988/1988 [==============================] - 1s 711us/step - loss: 38.1034 - mae: 4.4465 - val_loss: 342.1149 - val_mae: 13.8294\n",
      "Epoch 203/1000\n",
      "1988/1988 [==============================] - 1s 711us/step - loss: 39.6852 - mae: 4.5343 - val_loss: 371.8219 - val_mae: 14.8297\n",
      "Epoch 204/1000\n",
      "1988/1988 [==============================] - 1s 721us/step - loss: 38.5583 - mae: 4.4979 - val_loss: 332.9112 - val_mae: 14.2537\n",
      "Epoch 205/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 39.6084 - mae: 4.4974 - val_loss: 301.0871 - val_mae: 13.3286\n",
      "Epoch 206/1000\n",
      "1988/1988 [==============================] - 1s 709us/step - loss: 39.8459 - mae: 4.5804 - val_loss: 253.9034 - val_mae: 12.0189\n",
      "Epoch 207/1000\n",
      "1988/1988 [==============================] - 1s 751us/step - loss: 40.4864 - mae: 4.5242 - val_loss: 339.6371 - val_mae: 13.6963\n",
      "Epoch 208/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 41.3733 - mae: 4.7554 - val_loss: 258.7765 - val_mae: 11.9230\n",
      "Epoch 209/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 37.3489 - mae: 4.4050 - val_loss: 314.7807 - val_mae: 13.4205\n",
      "Epoch 210/1000\n",
      "1988/1988 [==============================] - 1s 691us/step - loss: 39.1574 - mae: 4.5319 - val_loss: 314.8203 - val_mae: 13.1056\n",
      "Epoch 211/1000\n",
      "1988/1988 [==============================] - 2s 841us/step - loss: 46.5156 - mae: 4.9456 - val_loss: 273.5477 - val_mae: 12.4423\n",
      "Epoch 212/1000\n",
      "1988/1988 [==============================] - 1s 729us/step - loss: 36.7903 - mae: 4.4594 - val_loss: 200.5307 - val_mae: 10.5150\n",
      "Epoch 213/1000\n",
      "1988/1988 [==============================] - 1s 726us/step - loss: 41.8929 - mae: 4.6214 - val_loss: 234.1110 - val_mae: 11.5362\n",
      "Epoch 214/1000\n",
      "1988/1988 [==============================] - 1s 721us/step - loss: 38.0501 - mae: 4.5284 - val_loss: 386.4082 - val_mae: 14.9800\n",
      "Epoch 215/1000\n",
      "1988/1988 [==============================] - 1s 736us/step - loss: 38.7595 - mae: 4.5524 - val_loss: 420.3924 - val_mae: 16.0724\n",
      "Epoch 216/1000\n",
      "1988/1988 [==============================] - 1s 739us/step - loss: 41.9107 - mae: 4.7933 - val_loss: 278.4450 - val_mae: 12.4497\n",
      "Epoch 217/1000\n",
      "1988/1988 [==============================] - 1s 748us/step - loss: 40.6707 - mae: 4.7224 - val_loss: 227.1821 - val_mae: 11.3523\n",
      "Epoch 218/1000\n",
      "1988/1988 [==============================] - 1s 714us/step - loss: 36.8958 - mae: 4.3761 - val_loss: 462.8415 - val_mae: 17.0018\n",
      "Epoch 219/1000\n",
      "1988/1988 [==============================] - 1s 709us/step - loss: 36.5684 - mae: 4.4098 - val_loss: 295.2926 - val_mae: 12.7732\n",
      "Epoch 220/1000\n",
      "1988/1988 [==============================] - 1s 716us/step - loss: 41.8028 - mae: 4.6076 - val_loss: 358.2311 - val_mae: 14.6579\n",
      "Epoch 221/1000\n",
      "1988/1988 [==============================] - 1s 710us/step - loss: 39.3094 - mae: 4.5104 - val_loss: 262.6726 - val_mae: 11.9944\n",
      "Epoch 222/1000\n",
      "1988/1988 [==============================] - 1s 693us/step - loss: 37.2686 - mae: 4.3557 - val_loss: 380.0194 - val_mae: 15.0998\n",
      "Epoch 223/1000\n",
      "1988/1988 [==============================] - 1s 714us/step - loss: 38.8294 - mae: 4.4847 - val_loss: 273.9039 - val_mae: 12.3454\n",
      "Epoch 224/1000\n",
      "1988/1988 [==============================] - 1s 669us/step - loss: 36.9641 - mae: 4.3943 - val_loss: 228.2735 - val_mae: 11.2805\n",
      "Epoch 225/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 40.5017 - mae: 4.6359 - val_loss: 296.2012 - val_mae: 12.9022\n",
      "Epoch 226/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 36.9761 - mae: 4.3218 - val_loss: 521.8953 - val_mae: 19.2908\n",
      "Epoch 227/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 38.6886 - mae: 4.5051 - val_loss: 289.1708 - val_mae: 12.6879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 37.2277 - mae: 4.3790 - val_loss: 407.4132 - val_mae: 15.6201\n",
      "Epoch 229/1000\n",
      "1988/1988 [==============================] - 1s 636us/step - loss: 39.5836 - mae: 4.5147 - val_loss: 437.7494 - val_mae: 16.1212\n",
      "Epoch 230/1000\n",
      "1988/1988 [==============================] - 1s 671us/step - loss: 37.5236 - mae: 4.4855 - val_loss: 431.5472 - val_mae: 16.1239\n",
      "Epoch 231/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 39.0606 - mae: 4.4845 - val_loss: 446.2162 - val_mae: 16.0455\n",
      "Epoch 232/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 39.2671 - mae: 4.5319 - val_loss: 424.3704 - val_mae: 15.8941\n",
      "Epoch 233/1000\n",
      "1988/1988 [==============================] - 1s 710us/step - loss: 36.7581 - mae: 4.3978 - val_loss: 389.7585 - val_mae: 14.9829\n",
      "Epoch 234/1000\n",
      "1988/1988 [==============================] - 1s 725us/step - loss: 34.6914 - mae: 4.2238 - val_loss: 299.1130 - val_mae: 12.8246\n",
      "Epoch 235/1000\n",
      "1988/1988 [==============================] - 1s 729us/step - loss: 35.2328 - mae: 4.2739 - val_loss: 297.2679 - val_mae: 12.7640\n",
      "Epoch 236/1000\n",
      "1988/1988 [==============================] - 1s 733us/step - loss: 38.1789 - mae: 4.3785 - val_loss: 273.9551 - val_mae: 12.2471\n",
      "Epoch 237/1000\n",
      "1988/1988 [==============================] - 1s 696us/step - loss: 37.2888 - mae: 4.3490 - val_loss: 339.1544 - val_mae: 13.6676\n",
      "Epoch 238/1000\n",
      "1988/1988 [==============================] - 1s 665us/step - loss: 40.1492 - mae: 4.3902 - val_loss: 346.5872 - val_mae: 13.8587\n",
      "Epoch 239/1000\n",
      "1988/1988 [==============================] - 1s 633us/step - loss: 37.2816 - mae: 4.3476 - val_loss: 408.2047 - val_mae: 15.3327\n",
      "Epoch 240/1000\n",
      "1988/1988 [==============================] - 1s 627us/step - loss: 36.2519 - mae: 4.3031 - val_loss: 440.1964 - val_mae: 16.1101\n",
      "Epoch 241/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 36.9566 - mae: 4.3869 - val_loss: 579.5979 - val_mae: 19.3372\n",
      "Epoch 242/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 36.6369 - mae: 4.2977 - val_loss: 513.4038 - val_mae: 17.9554\n",
      "Epoch 243/1000\n",
      "1988/1988 [==============================] - 1s 599us/step - loss: 36.7898 - mae: 4.3201 - val_loss: 458.2713 - val_mae: 16.7564\n",
      "Epoch 244/1000\n",
      "1988/1988 [==============================] - 1s 598us/step - loss: 36.4541 - mae: 4.3478 - val_loss: 352.8846 - val_mae: 13.9364\n",
      "Epoch 245/1000\n",
      "1988/1988 [==============================] - 1s 582us/step - loss: 36.2419 - mae: 4.3593 - val_loss: 451.5155 - val_mae: 16.3063\n",
      "Epoch 246/1000\n",
      "1988/1988 [==============================] - 1s 582us/step - loss: 39.2137 - mae: 4.5522 - val_loss: 335.7426 - val_mae: 13.6102\n",
      "Epoch 247/1000\n",
      "1988/1988 [==============================] - 1s 581us/step - loss: 39.0161 - mae: 4.4947 - val_loss: 259.9964 - val_mae: 11.9684\n",
      "Epoch 248/1000\n",
      "1988/1988 [==============================] - 1s 580us/step - loss: 34.7065 - mae: 4.2037 - val_loss: 477.2505 - val_mae: 17.3753\n",
      "Epoch 249/1000\n",
      "1988/1988 [==============================] - 1s 586us/step - loss: 35.8234 - mae: 4.3790 - val_loss: 444.2896 - val_mae: 16.3038\n",
      "Epoch 250/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 36.4738 - mae: 4.3508 - val_loss: 300.6412 - val_mae: 12.8415\n",
      "Epoch 251/1000\n",
      "1988/1988 [==============================] - 1s 595us/step - loss: 40.0509 - mae: 4.5516 - val_loss: 280.3943 - val_mae: 12.3477\n",
      "Epoch 252/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 40.5242 - mae: 4.4642 - val_loss: 426.0844 - val_mae: 16.2196\n",
      "Epoch 253/1000\n",
      "1988/1988 [==============================] - 1s 618us/step - loss: 34.5252 - mae: 4.2007 - val_loss: 317.8415 - val_mae: 13.3570\n",
      "Epoch 254/1000\n",
      "1988/1988 [==============================] - 1s 711us/step - loss: 43.8555 - mae: 4.7903 - val_loss: 312.2248 - val_mae: 13.1081\n",
      "Epoch 255/1000\n",
      "1988/1988 [==============================] - 1s 700us/step - loss: 39.7820 - mae: 4.5030 - val_loss: 442.5059 - val_mae: 16.3584\n",
      "Epoch 256/1000\n",
      "1988/1988 [==============================] - 2s 758us/step - loss: 37.2276 - mae: 4.4313 - val_loss: 294.7714 - val_mae: 12.6669\n",
      "Epoch 257/1000\n",
      "1988/1988 [==============================] - 2s 759us/step - loss: 39.0473 - mae: 4.4648 - val_loss: 378.4692 - val_mae: 14.6189\n",
      "Epoch 258/1000\n",
      "1988/1988 [==============================] - 2s 791us/step - loss: 36.5090 - mae: 4.3178 - val_loss: 433.1971 - val_mae: 15.8327\n",
      "Epoch 259/1000\n",
      "1988/1988 [==============================] - 1s 732us/step - loss: 35.7720 - mae: 4.4500 - val_loss: 434.6650 - val_mae: 16.2776\n",
      "Epoch 260/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 38.3956 - mae: 4.4480 - val_loss: 341.2541 - val_mae: 13.7119\n",
      "Epoch 261/1000\n",
      "1988/1988 [==============================] - 1s 659us/step - loss: 37.0521 - mae: 4.3542 - val_loss: 497.4475 - val_mae: 18.0801\n",
      "Epoch 262/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 39.5378 - mae: 4.6216 - val_loss: 342.3418 - val_mae: 13.7221\n",
      "Epoch 263/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 38.9939 - mae: 4.5388 - val_loss: 455.7368 - val_mae: 17.0102\n",
      "Epoch 264/1000\n",
      "1988/1988 [==============================] - 1s 599us/step - loss: 34.2089 - mae: 4.1827 - val_loss: 424.7370 - val_mae: 16.2215\n",
      "Epoch 265/1000\n",
      "1988/1988 [==============================] - 1s 602us/step - loss: 33.0329 - mae: 4.2344 - val_loss: 289.4139 - val_mae: 12.5765\n",
      "Epoch 266/1000\n",
      "1988/1988 [==============================] - 1s 615us/step - loss: 36.9807 - mae: 4.4812 - val_loss: 347.8258 - val_mae: 14.1673\n",
      "Epoch 267/1000\n",
      "1988/1988 [==============================] - 1s 586us/step - loss: 38.6236 - mae: 4.4549 - val_loss: 325.0780 - val_mae: 13.6544\n",
      "Epoch 268/1000\n",
      "1988/1988 [==============================] - 1s 583us/step - loss: 39.4960 - mae: 4.4549 - val_loss: 303.5938 - val_mae: 13.0312\n",
      "Epoch 269/1000\n",
      "1988/1988 [==============================] - 1s 590us/step - loss: 38.2810 - mae: 4.4901 - val_loss: 237.8704 - val_mae: 11.3513\n",
      "Epoch 270/1000\n",
      "1988/1988 [==============================] - 1s 585us/step - loss: 38.6612 - mae: 4.4518 - val_loss: 365.3504 - val_mae: 14.8156\n",
      "Epoch 271/1000\n",
      "1988/1988 [==============================] - 1s 578us/step - loss: 37.1893 - mae: 4.3534 - val_loss: 289.7240 - val_mae: 12.6009\n",
      "Epoch 272/1000\n",
      "1988/1988 [==============================] - 1s 586us/step - loss: 41.2448 - mae: 4.6026 - val_loss: 338.2862 - val_mae: 14.0299\n",
      "Epoch 273/1000\n",
      "1988/1988 [==============================] - 1s 595us/step - loss: 37.0311 - mae: 4.4255 - val_loss: 272.5103 - val_mae: 12.1593\n",
      "Epoch 274/1000\n",
      "1988/1988 [==============================] - 1s 611us/step - loss: 37.5728 - mae: 4.4956 - val_loss: 283.9730 - val_mae: 12.4289\n",
      "Epoch 275/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 34.6565 - mae: 4.2829 - val_loss: 256.5470 - val_mae: 11.7296\n",
      "Epoch 276/1000\n",
      "1988/1988 [==============================] - 1s 607us/step - loss: 37.3359 - mae: 4.4808 - val_loss: 321.0109 - val_mae: 13.6010\n",
      "Epoch 277/1000\n",
      "1988/1988 [==============================] - 1s 620us/step - loss: 36.7297 - mae: 4.2070 - val_loss: 263.5348 - val_mae: 11.9545\n",
      "Epoch 278/1000\n",
      "1988/1988 [==============================] - 1s 624us/step - loss: 36.2367 - mae: 4.3442 - val_loss: 268.5665 - val_mae: 12.1123\n",
      "Epoch 279/1000\n",
      "1988/1988 [==============================] - 1s 623us/step - loss: 36.5137 - mae: 4.3939 - val_loss: 300.9163 - val_mae: 13.0873\n",
      "Epoch 280/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 35.9819 - mae: 4.3118 - val_loss: 435.0507 - val_mae: 16.7932\n",
      "Epoch 281/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 35.4595 - mae: 4.2065 - val_loss: 365.8707 - val_mae: 14.9271\n",
      "Epoch 282/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 36.7381 - mae: 4.3847 - val_loss: 307.9435 - val_mae: 13.2581\n",
      "Epoch 283/1000\n",
      "1988/1988 [==============================] - 1s 703us/step - loss: 34.4510 - mae: 4.2875 - val_loss: 307.0465 - val_mae: 13.2249\n",
      "Epoch 284/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 34.5159 - mae: 4.2982 - val_loss: 243.9070 - val_mae: 11.5682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 35.9979 - mae: 4.2407 - val_loss: 287.3794 - val_mae: 12.5991\n",
      "Epoch 286/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 34.3211 - mae: 4.2363 - val_loss: 277.0888 - val_mae: 12.3937\n",
      "Epoch 287/1000\n",
      "1988/1988 [==============================] - 1s 624us/step - loss: 35.9386 - mae: 4.3735 - val_loss: 210.5869 - val_mae: 10.8660\n",
      "Epoch 288/1000\n",
      "1988/1988 [==============================] - 1s 602us/step - loss: 36.5355 - mae: 4.3005 - val_loss: 250.7263 - val_mae: 11.7862\n",
      "Epoch 289/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 36.5280 - mae: 4.4063 - val_loss: 246.2625 - val_mae: 11.5469\n",
      "Epoch 290/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 35.0118 - mae: 4.2823 - val_loss: 291.6229 - val_mae: 12.6847\n",
      "Epoch 291/1000\n",
      "1988/1988 [==============================] - 1s 704us/step - loss: 37.4116 - mae: 4.4099 - val_loss: 312.5290 - val_mae: 13.1712\n",
      "Epoch 292/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 38.2512 - mae: 4.5145 - val_loss: 296.0532 - val_mae: 12.6807\n",
      "Epoch 293/1000\n",
      "1988/1988 [==============================] - 1s 648us/step - loss: 37.5277 - mae: 4.3427 - val_loss: 359.4547 - val_mae: 14.4395\n",
      "Epoch 294/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 33.8924 - mae: 4.2577 - val_loss: 274.5966 - val_mae: 12.0975\n",
      "Epoch 295/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 33.4170 - mae: 4.1473 - val_loss: 282.8760 - val_mae: 12.3321\n",
      "Epoch 296/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 36.8375 - mae: 4.4076 - val_loss: 296.3035 - val_mae: 12.8609\n",
      "Epoch 297/1000\n",
      "1988/1988 [==============================] - 1s 708us/step - loss: 32.7898 - mae: 4.1812 - val_loss: 292.8782 - val_mae: 12.9260\n",
      "Epoch 298/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 34.6256 - mae: 4.2006 - val_loss: 291.4356 - val_mae: 12.6441\n",
      "Epoch 299/1000\n",
      "1988/1988 [==============================] - 1s 738us/step - loss: 36.0394 - mae: 4.3333 - val_loss: 271.5419 - val_mae: 12.2877\n",
      "Epoch 300/1000\n",
      "1988/1988 [==============================] - 1s 714us/step - loss: 35.6381 - mae: 4.2258 - val_loss: 303.3339 - val_mae: 12.9988\n",
      "Epoch 301/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 34.9562 - mae: 4.2846 - val_loss: 300.8952 - val_mae: 12.8821\n",
      "Epoch 302/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 35.1687 - mae: 4.2692 - val_loss: 319.3973 - val_mae: 13.4380\n",
      "Epoch 303/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 34.4623 - mae: 4.2689 - val_loss: 270.4992 - val_mae: 12.1465\n",
      "Epoch 304/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 33.8333 - mae: 4.1234 - val_loss: 309.8785 - val_mae: 13.2487\n",
      "Epoch 305/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 36.5314 - mae: 4.2997 - val_loss: 199.5390 - val_mae: 10.6864\n",
      "Epoch 306/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 35.9103 - mae: 4.3434 - val_loss: 357.9582 - val_mae: 15.1201\n",
      "Epoch 307/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 34.0915 - mae: 4.2915 - val_loss: 266.3387 - val_mae: 12.1230\n",
      "Epoch 308/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 38.0377 - mae: 4.5467 - val_loss: 312.9544 - val_mae: 13.3828\n",
      "Epoch 309/1000\n",
      "1988/1988 [==============================] - 1s 633us/step - loss: 34.4066 - mae: 4.2282 - val_loss: 271.5914 - val_mae: 12.2966\n",
      "Epoch 310/1000\n",
      "1988/1988 [==============================] - 1s 631us/step - loss: 37.9355 - mae: 4.4602 - val_loss: 215.5055 - val_mae: 10.9511\n",
      "Epoch 311/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 35.9736 - mae: 4.4102 - val_loss: 354.0920 - val_mae: 15.0640\n",
      "Epoch 312/1000\n",
      "1988/1988 [==============================] - 1s 662us/step - loss: 35.6104 - mae: 4.3695 - val_loss: 258.7078 - val_mae: 12.1481\n",
      "Epoch 313/1000\n",
      "1988/1988 [==============================] - 1s 676us/step - loss: 36.3660 - mae: 4.3545 - val_loss: 224.2550 - val_mae: 11.2999\n",
      "Epoch 314/1000\n",
      "1988/1988 [==============================] - 1s 696us/step - loss: 36.7610 - mae: 4.4481 - val_loss: 197.1174 - val_mae: 10.4742\n",
      "Epoch 315/1000\n",
      "1988/1988 [==============================] - 1s 651us/step - loss: 35.5003 - mae: 4.2933 - val_loss: 272.2573 - val_mae: 12.5218\n",
      "Epoch 316/1000\n",
      "1988/1988 [==============================] - 1s 666us/step - loss: 34.6806 - mae: 4.2229 - val_loss: 351.9857 - val_mae: 14.6872\n",
      "Epoch 317/1000\n",
      "1988/1988 [==============================] - 1s 631us/step - loss: 33.7907 - mae: 4.3668 - val_loss: 337.5013 - val_mae: 14.3475\n",
      "Epoch 318/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 34.7956 - mae: 4.3587 - val_loss: 277.1238 - val_mae: 12.5728\n",
      "Epoch 319/1000\n",
      "1988/1988 [==============================] - 1s 648us/step - loss: 34.7395 - mae: 4.2511 - val_loss: 278.3825 - val_mae: 12.7250\n",
      "Epoch 320/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 32.0414 - mae: 4.0727 - val_loss: 313.2066 - val_mae: 13.7398\n",
      "Epoch 321/1000\n",
      "1988/1988 [==============================] - 1s 596us/step - loss: 31.7488 - mae: 4.0292 - val_loss: 249.3676 - val_mae: 11.8576\n",
      "Epoch 322/1000\n",
      "1988/1988 [==============================] - 1s 611us/step - loss: 35.8854 - mae: 4.3015 - val_loss: 251.9330 - val_mae: 12.1483\n",
      "Epoch 323/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 31.2633 - mae: 4.0645 - val_loss: 218.5853 - val_mae: 11.0878\n",
      "Epoch 324/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 32.8475 - mae: 4.2048 - val_loss: 315.0051 - val_mae: 13.7118\n",
      "Epoch 325/1000\n",
      "1988/1988 [==============================] - 1s 701us/step - loss: 35.1856 - mae: 4.3066 - val_loss: 367.4668 - val_mae: 15.2618\n",
      "Epoch 326/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 34.3291 - mae: 4.2229 - val_loss: 222.6592 - val_mae: 11.1665\n",
      "Epoch 327/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 34.4554 - mae: 4.2217 - val_loss: 299.0905 - val_mae: 13.1664\n",
      "Epoch 328/1000\n",
      "1988/1988 [==============================] - 1s 613us/step - loss: 36.6835 - mae: 4.4367 - val_loss: 230.7278 - val_mae: 11.3454\n",
      "Epoch 329/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 41.1475 - mae: 4.5644 - val_loss: 162.4344 - val_mae: 9.5935\n",
      "Epoch 330/1000\n",
      "1988/1988 [==============================] - 1s 714us/step - loss: 33.6717 - mae: 4.2182 - val_loss: 237.8098 - val_mae: 11.8320\n",
      "Epoch 331/1000\n",
      "1988/1988 [==============================] - 1s 662us/step - loss: 36.9119 - mae: 4.3745 - val_loss: 229.8859 - val_mae: 11.5021\n",
      "Epoch 332/1000\n",
      "1988/1988 [==============================] - 1s 673us/step - loss: 36.2397 - mae: 4.3881 - val_loss: 170.0213 - val_mae: 9.8864\n",
      "Epoch 333/1000\n",
      "1988/1988 [==============================] - 1s 751us/step - loss: 34.8201 - mae: 4.3334 - val_loss: 277.6718 - val_mae: 12.9723\n",
      "Epoch 334/1000\n",
      "1988/1988 [==============================] - 1s 739us/step - loss: 32.6313 - mae: 4.1794 - val_loss: 223.8569 - val_mae: 11.3402\n",
      "Epoch 335/1000\n",
      "1988/1988 [==============================] - 1s 695us/step - loss: 35.2981 - mae: 4.4429 - val_loss: 253.3465 - val_mae: 12.3261\n",
      "Epoch 336/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 34.1678 - mae: 4.1810 - val_loss: 167.1493 - val_mae: 9.8495\n",
      "Epoch 337/1000\n",
      "1988/1988 [==============================] - 1s 636us/step - loss: 32.1447 - mae: 4.1708 - val_loss: 203.3040 - val_mae: 10.9700\n",
      "Epoch 338/1000\n",
      "1988/1988 [==============================] - 1s 626us/step - loss: 34.3571 - mae: 4.2682 - val_loss: 249.1116 - val_mae: 12.3847\n",
      "Epoch 339/1000\n",
      "1988/1988 [==============================] - 1s 612us/step - loss: 33.3784 - mae: 4.1772 - val_loss: 230.2339 - val_mae: 11.5666\n",
      "Epoch 340/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 34.8962 - mae: 4.2852 - val_loss: 301.5767 - val_mae: 13.5970\n",
      "Epoch 341/1000\n",
      "1988/1988 [==============================] - 1s 602us/step - loss: 35.6721 - mae: 4.2102 - val_loss: 177.7661 - val_mae: 9.9942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 32.0807 - mae: 4.0970 - val_loss: 221.4397 - val_mae: 11.2201\n",
      "Epoch 343/1000\n",
      "1988/1988 [==============================] - 1s 650us/step - loss: 33.3683 - mae: 4.2046 - val_loss: 214.2859 - val_mae: 11.0828\n",
      "Epoch 344/1000\n",
      "1988/1988 [==============================] - 1s 662us/step - loss: 33.7070 - mae: 4.2587 - val_loss: 188.3798 - val_mae: 10.3103\n",
      "Epoch 345/1000\n",
      "1988/1988 [==============================] - 1s 662us/step - loss: 34.4561 - mae: 4.2017 - val_loss: 307.4061 - val_mae: 13.7654\n",
      "Epoch 346/1000\n",
      "1988/1988 [==============================] - 1s 672us/step - loss: 36.3345 - mae: 4.2679 - val_loss: 238.0948 - val_mae: 11.6015\n",
      "Epoch 347/1000\n",
      "1988/1988 [==============================] - 1s 673us/step - loss: 31.9399 - mae: 4.1045 - val_loss: 182.3743 - val_mae: 10.1585\n",
      "Epoch 348/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 32.4062 - mae: 4.0633 - val_loss: 234.0896 - val_mae: 11.6063\n",
      "Epoch 349/1000\n",
      "1988/1988 [==============================] - 1s 696us/step - loss: 32.6528 - mae: 4.2048 - val_loss: 134.6529 - val_mae: 9.1094\n",
      "Epoch 350/1000\n",
      "1988/1988 [==============================] - 1s 710us/step - loss: 29.8086 - mae: 3.9317 - val_loss: 150.3646 - val_mae: 9.2043\n",
      "Epoch 351/1000\n",
      "1988/1988 [==============================] - 1s 698us/step - loss: 33.4110 - mae: 4.2725 - val_loss: 243.2581 - val_mae: 11.9264\n",
      "Epoch 352/1000\n",
      "1988/1988 [==============================] - 1s 705us/step - loss: 31.7880 - mae: 4.2562 - val_loss: 199.1428 - val_mae: 10.5876\n",
      "Epoch 353/1000\n",
      "1988/1988 [==============================] - 1s 723us/step - loss: 35.5309 - mae: 4.3132 - val_loss: 280.5785 - val_mae: 13.2399\n",
      "Epoch 354/1000\n",
      "1988/1988 [==============================] - 1s 713us/step - loss: 34.5944 - mae: 4.2487 - val_loss: 191.1850 - val_mae: 10.3695\n",
      "Epoch 355/1000\n",
      "1988/1988 [==============================] - 1s 703us/step - loss: 33.5424 - mae: 4.2291 - val_loss: 170.0587 - val_mae: 9.8409\n",
      "Epoch 356/1000\n",
      "1988/1988 [==============================] - 1s 694us/step - loss: 34.9580 - mae: 4.2870 - val_loss: 177.5452 - val_mae: 10.0121\n",
      "Epoch 357/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 31.1321 - mae: 4.1121 - val_loss: 243.4741 - val_mae: 12.1145\n",
      "Epoch 358/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 32.8688 - mae: 4.2501 - val_loss: 272.9184 - val_mae: 12.6156\n",
      "Epoch 359/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 33.6274 - mae: 4.1303 - val_loss: 282.7459 - val_mae: 13.0378\n",
      "Epoch 360/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 31.6724 - mae: 4.0792 - val_loss: 195.5507 - val_mae: 10.5737\n",
      "Epoch 361/1000\n",
      "1988/1988 [==============================] - 1s 692us/step - loss: 34.3103 - mae: 4.1997 - val_loss: 248.7463 - val_mae: 12.0435\n",
      "Epoch 362/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 31.8553 - mae: 4.0555 - val_loss: 179.5539 - val_mae: 10.1817\n",
      "Epoch 363/1000\n",
      "1988/1988 [==============================] - 1s 669us/step - loss: 35.3564 - mae: 4.2933 - val_loss: 195.7554 - val_mae: 10.6341\n",
      "Epoch 364/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 33.0769 - mae: 4.1519 - val_loss: 220.4560 - val_mae: 11.2265\n",
      "Epoch 365/1000\n",
      "1988/1988 [==============================] - 1s 663us/step - loss: 32.2831 - mae: 4.0943 - val_loss: 215.3202 - val_mae: 11.0958\n",
      "Epoch 366/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 30.5815 - mae: 3.9093 - val_loss: 230.8415 - val_mae: 11.6057\n",
      "Epoch 367/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 33.5390 - mae: 4.0872 - val_loss: 153.4162 - val_mae: 9.5789\n",
      "Epoch 368/1000\n",
      "1988/1988 [==============================] - 1s 597us/step - loss: 32.5851 - mae: 4.1002 - val_loss: 223.5525 - val_mae: 11.2971\n",
      "Epoch 369/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 31.5376 - mae: 4.0527 - val_loss: 178.3654 - val_mae: 9.9714\n",
      "Epoch 370/1000\n",
      "1988/1988 [==============================] - 1s 608us/step - loss: 34.0590 - mae: 4.1686 - val_loss: 206.4376 - val_mae: 10.7406\n",
      "Epoch 371/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 32.3850 - mae: 4.1292 - val_loss: 215.0196 - val_mae: 10.9268\n",
      "Epoch 372/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 33.6864 - mae: 4.2236 - val_loss: 200.3553 - val_mae: 10.6585\n",
      "Epoch 373/1000\n",
      "1988/1988 [==============================] - 1s 625us/step - loss: 35.3043 - mae: 4.3334 - val_loss: 217.1360 - val_mae: 11.1074\n",
      "Epoch 374/1000\n",
      "1988/1988 [==============================] - 1s 627us/step - loss: 32.1481 - mae: 4.1253 - val_loss: 201.0125 - val_mae: 10.6552\n",
      "Epoch 375/1000\n",
      "1988/1988 [==============================] - 1s 623us/step - loss: 32.1662 - mae: 4.1650 - val_loss: 218.9649 - val_mae: 11.2836\n",
      "Epoch 376/1000\n",
      "1988/1988 [==============================] - 1s 611us/step - loss: 32.6168 - mae: 4.1628 - val_loss: 293.6130 - val_mae: 13.4544\n",
      "Epoch 377/1000\n",
      "1988/1988 [==============================] - 1s 612us/step - loss: 32.3418 - mae: 4.1213 - val_loss: 299.9859 - val_mae: 13.4427\n",
      "Epoch 378/1000\n",
      "1988/1988 [==============================] - 1s 605us/step - loss: 31.1775 - mae: 3.9900 - val_loss: 244.2933 - val_mae: 11.8440\n",
      "Epoch 379/1000\n",
      "1988/1988 [==============================] - 1s 604us/step - loss: 34.8053 - mae: 4.3441 - val_loss: 207.9026 - val_mae: 10.9258\n",
      "Epoch 380/1000\n",
      "1988/1988 [==============================] - 1s 588us/step - loss: 30.2177 - mae: 3.9597 - val_loss: 174.1474 - val_mae: 10.1033\n",
      "Epoch 381/1000\n",
      "1988/1988 [==============================] - 1s 575us/step - loss: 36.8363 - mae: 4.4121 - val_loss: 253.6934 - val_mae: 12.3249\n",
      "Epoch 382/1000\n",
      "1988/1988 [==============================] - 1s 582us/step - loss: 31.3974 - mae: 4.0889 - val_loss: 232.2192 - val_mae: 11.5360\n",
      "Epoch 383/1000\n",
      "1988/1988 [==============================] - 1s 585us/step - loss: 36.5045 - mae: 4.5201 - val_loss: 195.8369 - val_mae: 10.5340\n",
      "Epoch 384/1000\n",
      "1988/1988 [==============================] - 1s 582us/step - loss: 32.0582 - mae: 4.1118 - val_loss: 157.6389 - val_mae: 9.7213\n",
      "Epoch 385/1000\n",
      "1988/1988 [==============================] - 1s 578us/step - loss: 34.4120 - mae: 4.2624 - val_loss: 212.1073 - val_mae: 10.9942\n",
      "Epoch 386/1000\n",
      "1988/1988 [==============================] - 1s 596us/step - loss: 34.8894 - mae: 4.2510 - val_loss: 244.4829 - val_mae: 11.9844\n",
      "Epoch 387/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 32.9731 - mae: 4.1046 - val_loss: 189.3685 - val_mae: 10.3577\n",
      "Epoch 388/1000\n",
      "1988/1988 [==============================] - 1s 590us/step - loss: 29.8464 - mae: 3.9892 - val_loss: 178.3424 - val_mae: 10.0955\n",
      "Epoch 389/1000\n",
      "1988/1988 [==============================] - 1s 608us/step - loss: 31.4674 - mae: 4.0828 - val_loss: 262.3730 - val_mae: 12.4276\n",
      "Epoch 390/1000\n",
      "1988/1988 [==============================] - 1s 604us/step - loss: 30.9615 - mae: 3.9650 - val_loss: 233.2888 - val_mae: 11.3898\n",
      "Epoch 391/1000\n",
      "1988/1988 [==============================] - 1s 598us/step - loss: 34.7036 - mae: 4.2143 - val_loss: 229.1074 - val_mae: 11.2733\n",
      "Epoch 392/1000\n",
      "1988/1988 [==============================] - 1s 610us/step - loss: 32.7429 - mae: 4.1186 - val_loss: 183.5800 - val_mae: 10.2251\n",
      "Epoch 393/1000\n",
      "1988/1988 [==============================] - 1s 623us/step - loss: 32.2766 - mae: 4.1177 - val_loss: 184.5983 - val_mae: 10.3224\n",
      "Epoch 394/1000\n",
      "1988/1988 [==============================] - 1s 630us/step - loss: 33.0668 - mae: 4.2534 - val_loss: 218.5708 - val_mae: 10.9949\n",
      "Epoch 395/1000\n",
      "1988/1988 [==============================] - 1s 621us/step - loss: 34.4498 - mae: 4.2371 - val_loss: 207.5886 - val_mae: 10.8506\n",
      "Epoch 396/1000\n",
      "1988/1988 [==============================] - 1s 624us/step - loss: 32.4643 - mae: 4.3064 - val_loss: 243.1246 - val_mae: 11.5568\n",
      "Epoch 397/1000\n",
      "1988/1988 [==============================] - 1s 597us/step - loss: 32.8384 - mae: 4.1388 - val_loss: 299.1275 - val_mae: 13.0005\n",
      "Epoch 398/1000\n",
      "1988/1988 [==============================] - 1s 616us/step - loss: 32.6420 - mae: 4.0777 - val_loss: 276.9133 - val_mae: 12.4777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399/1000\n",
      "1988/1988 [==============================] - 1s 595us/step - loss: 29.4763 - mae: 3.9080 - val_loss: 283.6354 - val_mae: 12.8138\n",
      "Epoch 400/1000\n",
      "1988/1988 [==============================] - 1s 585us/step - loss: 28.7384 - mae: 3.8193 - val_loss: 334.9527 - val_mae: 14.4434\n",
      "Epoch 401/1000\n",
      "1988/1988 [==============================] - 1s 576us/step - loss: 29.5995 - mae: 3.8610 - val_loss: 224.0889 - val_mae: 11.0395\n",
      "Epoch 402/1000\n",
      "1988/1988 [==============================] - 1s 580us/step - loss: 29.6687 - mae: 3.9827 - val_loss: 286.8460 - val_mae: 12.8494\n",
      "Epoch 403/1000\n",
      "1988/1988 [==============================] - 1s 587us/step - loss: 30.8717 - mae: 4.0178 - val_loss: 281.1876 - val_mae: 12.6623\n",
      "Epoch 404/1000\n",
      "1988/1988 [==============================] - 1s 583us/step - loss: 31.1062 - mae: 4.0289 - val_loss: 349.0195 - val_mae: 14.7449\n",
      "Epoch 405/1000\n",
      "1988/1988 [==============================] - 1s 582us/step - loss: 32.1298 - mae: 4.1080 - val_loss: 209.9762 - val_mae: 10.6933\n",
      "Epoch 406/1000\n",
      "1988/1988 [==============================] - 1s 579us/step - loss: 30.9052 - mae: 4.0249 - val_loss: 254.5372 - val_mae: 12.0704\n",
      "Epoch 407/1000\n",
      "1988/1988 [==============================] - 1s 589us/step - loss: 30.9012 - mae: 4.0030 - val_loss: 208.9454 - val_mae: 10.5908\n",
      "Epoch 408/1000\n",
      "1988/1988 [==============================] - 1s 610us/step - loss: 30.4163 - mae: 3.9765 - val_loss: 199.0956 - val_mae: 10.3653\n",
      "Epoch 409/1000\n",
      "1988/1988 [==============================] - 1s 621us/step - loss: 30.4615 - mae: 4.0906 - val_loss: 195.3023 - val_mae: 10.3884\n",
      "Epoch 410/1000\n",
      "1988/1988 [==============================] - 1s 635us/step - loss: 29.7489 - mae: 3.9673 - val_loss: 197.1112 - val_mae: 10.4850\n",
      "Epoch 411/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 29.3200 - mae: 4.0364 - val_loss: 186.3822 - val_mae: 10.0630\n",
      "Epoch 412/1000\n",
      "1988/1988 [==============================] - 1s 652us/step - loss: 31.1560 - mae: 4.0172 - val_loss: 190.9275 - val_mae: 10.3152\n",
      "Epoch 413/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 29.0808 - mae: 3.9022 - val_loss: 186.0570 - val_mae: 10.2999\n",
      "Epoch 414/1000\n",
      "1988/1988 [==============================] - 1s 637us/step - loss: 32.6834 - mae: 4.1682 - val_loss: 199.3070 - val_mae: 10.6141\n",
      "Epoch 415/1000\n",
      "1988/1988 [==============================] - 1s 630us/step - loss: 33.4477 - mae: 4.3305 - val_loss: 223.1189 - val_mae: 11.0876\n",
      "Epoch 416/1000\n",
      "1988/1988 [==============================] - 1s 693us/step - loss: 31.8242 - mae: 4.0687 - val_loss: 166.7974 - val_mae: 9.6074\n",
      "Epoch 417/1000\n",
      "1988/1988 [==============================] - 2s 775us/step - loss: 29.0237 - mae: 3.8551 - val_loss: 241.3932 - val_mae: 11.8896\n",
      "Epoch 418/1000\n",
      "1988/1988 [==============================] - 1s 735us/step - loss: 31.8401 - mae: 4.0303 - val_loss: 257.2067 - val_mae: 12.2317\n",
      "Epoch 419/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 32.9946 - mae: 4.0960 - val_loss: 176.6236 - val_mae: 9.8970\n",
      "Epoch 420/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 31.4057 - mae: 4.1648 - val_loss: 276.0330 - val_mae: 12.7831\n",
      "Epoch 421/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 31.0561 - mae: 4.1004 - val_loss: 275.2691 - val_mae: 12.7231\n",
      "Epoch 422/1000\n",
      "1988/1988 [==============================] - 1s 682us/step - loss: 29.1949 - mae: 3.9703 - val_loss: 222.9370 - val_mae: 11.2012\n",
      "Epoch 423/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 30.4028 - mae: 3.8749 - val_loss: 199.8846 - val_mae: 10.5358\n",
      "Epoch 424/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 35.5948 - mae: 4.1879 - val_loss: 208.1869 - val_mae: 10.9743\n",
      "Epoch 425/1000\n",
      "1988/1988 [==============================] - 1s 713us/step - loss: 32.1719 - mae: 4.0230 - val_loss: 166.5978 - val_mae: 9.5460\n",
      "Epoch 426/1000\n",
      "1988/1988 [==============================] - 1s 677us/step - loss: 28.7697 - mae: 3.9651 - val_loss: 213.6481 - val_mae: 11.0665\n",
      "Epoch 427/1000\n",
      "1988/1988 [==============================] - 1s 677us/step - loss: 27.8612 - mae: 3.8665 - val_loss: 206.8294 - val_mae: 10.7165\n",
      "Epoch 428/1000\n",
      "1988/1988 [==============================] - 1s 666us/step - loss: 30.9377 - mae: 4.0114 - val_loss: 296.3672 - val_mae: 13.2363\n",
      "Epoch 429/1000\n",
      "1988/1988 [==============================] - 1s 672us/step - loss: 30.6100 - mae: 4.1627 - val_loss: 270.3784 - val_mae: 12.5163\n",
      "Epoch 430/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 31.2355 - mae: 4.1075 - val_loss: 249.7077 - val_mae: 11.8263\n",
      "Epoch 431/1000\n",
      "1988/1988 [==============================] - 1s 661us/step - loss: 30.9091 - mae: 4.0550 - val_loss: 183.5061 - val_mae: 9.9996\n",
      "Epoch 432/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 30.4281 - mae: 3.9582 - val_loss: 148.2039 - val_mae: 8.9999\n",
      "Epoch 433/1000\n",
      "1988/1988 [==============================] - 1s 673us/step - loss: 28.5757 - mae: 3.8516 - val_loss: 155.4813 - val_mae: 9.3157\n",
      "Epoch 434/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 29.7980 - mae: 3.9074 - val_loss: 181.5938 - val_mae: 9.9445\n",
      "Epoch 435/1000\n",
      "1988/1988 [==============================] - 1s 710us/step - loss: 33.3074 - mae: 4.2492 - val_loss: 156.0626 - val_mae: 9.2005\n",
      "Epoch 436/1000\n",
      "1988/1988 [==============================] - 1s 701us/step - loss: 31.2337 - mae: 3.9898 - val_loss: 230.8656 - val_mae: 11.4196\n",
      "Epoch 437/1000\n",
      "1988/1988 [==============================] - 1s 720us/step - loss: 31.9590 - mae: 4.0220 - val_loss: 237.0258 - val_mae: 11.6213\n",
      "Epoch 438/1000\n",
      "1988/1988 [==============================] - 1s 701us/step - loss: 29.3682 - mae: 3.9663 - val_loss: 158.0565 - val_mae: 9.3626\n",
      "Epoch 439/1000\n",
      "1988/1988 [==============================] - 1s 710us/step - loss: 34.2730 - mae: 4.2082 - val_loss: 220.3024 - val_mae: 11.1110\n",
      "Epoch 440/1000\n",
      "1988/1988 [==============================] - 1s 704us/step - loss: 30.2499 - mae: 3.9872 - val_loss: 126.0951 - val_mae: 8.5029\n",
      "Epoch 441/1000\n",
      "1988/1988 [==============================] - 1s 694us/step - loss: 29.7529 - mae: 3.9996 - val_loss: 163.3100 - val_mae: 9.4055\n",
      "Epoch 442/1000\n",
      "1988/1988 [==============================] - 1s 722us/step - loss: 30.9868 - mae: 4.1051 - val_loss: 208.9553 - val_mae: 10.8035\n",
      "Epoch 443/1000\n",
      "1988/1988 [==============================] - 1s 689us/step - loss: 29.1470 - mae: 3.8568 - val_loss: 201.3587 - val_mae: 10.6894\n",
      "Epoch 444/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 26.9996 - mae: 3.8045 - val_loss: 174.7437 - val_mae: 9.7657\n",
      "Epoch 445/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 29.4373 - mae: 3.9140 - val_loss: 252.8223 - val_mae: 12.0578\n",
      "Epoch 446/1000\n",
      "1988/1988 [==============================] - 1s 668us/step - loss: 28.1682 - mae: 3.8381 - val_loss: 262.1472 - val_mae: 12.3416\n",
      "Epoch 447/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 29.9321 - mae: 3.8802 - val_loss: 269.9547 - val_mae: 12.6511\n",
      "Epoch 448/1000\n",
      "1988/1988 [==============================] - 1s 687us/step - loss: 29.3275 - mae: 3.8484 - val_loss: 245.9204 - val_mae: 11.8745\n",
      "Epoch 449/1000\n",
      "1988/1988 [==============================] - 1s 677us/step - loss: 30.0866 - mae: 4.0705 - val_loss: 276.4359 - val_mae: 12.7437\n",
      "Epoch 450/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 29.9135 - mae: 4.0272 - val_loss: 269.6085 - val_mae: 12.3844\n",
      "Epoch 451/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 29.1357 - mae: 3.9012 - val_loss: 278.3368 - val_mae: 12.9911\n",
      "Epoch 452/1000\n",
      "1988/1988 [==============================] - 1s 683us/step - loss: 29.3157 - mae: 3.8733 - val_loss: 227.6983 - val_mae: 11.4318\n",
      "Epoch 453/1000\n",
      "1988/1988 [==============================] - 1s 671us/step - loss: 27.4470 - mae: 3.8072 - val_loss: 202.9960 - val_mae: 10.6888\n",
      "Epoch 454/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 31.0452 - mae: 4.0287 - val_loss: 189.2621 - val_mae: 10.2349\n",
      "Epoch 455/1000\n",
      "1988/1988 [==============================] - 1s 701us/step - loss: 29.7323 - mae: 3.8927 - val_loss: 256.3791 - val_mae: 12.4924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/1000\n",
      "1988/1988 [==============================] - 1s 725us/step - loss: 30.4160 - mae: 3.9768 - val_loss: 276.4047 - val_mae: 12.6531\n",
      "Epoch 457/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 34.6951 - mae: 4.4024 - val_loss: 199.3545 - val_mae: 10.4130\n",
      "Epoch 458/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 29.7777 - mae: 3.9401 - val_loss: 335.4303 - val_mae: 14.6287\n",
      "Epoch 459/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 30.4894 - mae: 3.9707 - val_loss: 261.6957 - val_mae: 12.1749\n",
      "Epoch 460/1000\n",
      "1988/1988 [==============================] - 1s 662us/step - loss: 29.9160 - mae: 4.0062 - val_loss: 285.0735 - val_mae: 12.5569\n",
      "Epoch 461/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 27.3702 - mae: 3.8137 - val_loss: 320.4440 - val_mae: 13.8461\n",
      "Epoch 462/1000\n",
      "1988/1988 [==============================] - 1s 625us/step - loss: 29.6947 - mae: 4.0098 - val_loss: 271.6111 - val_mae: 12.4960\n",
      "Epoch 463/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 32.1442 - mae: 4.0775 - val_loss: 294.6195 - val_mae: 13.1210\n",
      "Epoch 464/1000\n",
      "1988/1988 [==============================] - 1s 640us/step - loss: 30.3013 - mae: 3.9255 - val_loss: 208.8847 - val_mae: 10.7871\n",
      "Epoch 465/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 29.4143 - mae: 3.8475 - val_loss: 358.6714 - val_mae: 15.3764\n",
      "Epoch 466/1000\n",
      "1988/1988 [==============================] - 1s 695us/step - loss: 30.8915 - mae: 4.0234 - val_loss: 303.9499 - val_mae: 13.3275\n",
      "Epoch 467/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 32.5685 - mae: 4.1597 - val_loss: 285.6320 - val_mae: 12.8573\n",
      "Epoch 468/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 28.0509 - mae: 3.8721 - val_loss: 274.0023 - val_mae: 12.3065\n",
      "Epoch 469/1000\n",
      "1988/1988 [==============================] - 1s 646us/step - loss: 27.1934 - mae: 3.7325 - val_loss: 271.5886 - val_mae: 12.1424\n",
      "Epoch 470/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 28.7889 - mae: 3.8590 - val_loss: 408.2735 - val_mae: 16.0004\n",
      "Epoch 471/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 28.0206 - mae: 3.8530 - val_loss: 269.5609 - val_mae: 12.1183\n",
      "Epoch 472/1000\n",
      "1988/1988 [==============================] - 1s 620us/step - loss: 28.6784 - mae: 3.9584 - val_loss: 324.9218 - val_mae: 14.0327\n",
      "Epoch 473/1000\n",
      "1988/1988 [==============================] - 1s 620us/step - loss: 28.4826 - mae: 3.9390 - val_loss: 224.6102 - val_mae: 11.0433\n",
      "Epoch 474/1000\n",
      "1988/1988 [==============================] - 1s 663us/step - loss: 27.1521 - mae: 3.7689 - val_loss: 240.3663 - val_mae: 11.5089\n",
      "Epoch 475/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 26.4648 - mae: 3.6042 - val_loss: 214.2410 - val_mae: 10.9350\n",
      "Epoch 476/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 27.6642 - mae: 3.7835 - val_loss: 188.6077 - val_mae: 10.2359\n",
      "Epoch 477/1000\n",
      "1988/1988 [==============================] - 2s 772us/step - loss: 27.2006 - mae: 3.8125 - val_loss: 223.2750 - val_mae: 11.0042\n",
      "Epoch 478/1000\n",
      "1988/1988 [==============================] - 1s 717us/step - loss: 28.8330 - mae: 3.8959 - val_loss: 263.8009 - val_mae: 11.9571\n",
      "Epoch 479/1000\n",
      "1988/1988 [==============================] - 1s 691us/step - loss: 27.9425 - mae: 3.8288 - val_loss: 262.7011 - val_mae: 11.7780\n",
      "Epoch 480/1000\n",
      "1988/1988 [==============================] - 1s 690us/step - loss: 30.3163 - mae: 4.0361 - val_loss: 350.2125 - val_mae: 14.1848\n",
      "Epoch 481/1000\n",
      "1988/1988 [==============================] - 1s 696us/step - loss: 29.4417 - mae: 3.9221 - val_loss: 286.0992 - val_mae: 12.3155\n",
      "Epoch 482/1000\n",
      "1988/1988 [==============================] - 1s 704us/step - loss: 30.3770 - mae: 4.0305 - val_loss: 389.8850 - val_mae: 15.1896\n",
      "Epoch 483/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 26.3906 - mae: 3.8046 - val_loss: 473.3015 - val_mae: 17.8929\n",
      "Epoch 484/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 28.2614 - mae: 3.8597 - val_loss: 284.7173 - val_mae: 12.2282\n",
      "Epoch 485/1000\n",
      "1988/1988 [==============================] - 1s 623us/step - loss: 31.1853 - mae: 4.1459 - val_loss: 327.2162 - val_mae: 13.2253\n",
      "Epoch 486/1000\n",
      "1988/1988 [==============================] - 1s 652us/step - loss: 27.6391 - mae: 3.8439 - val_loss: 434.9443 - val_mae: 16.4866\n",
      "Epoch 487/1000\n",
      "1988/1988 [==============================] - 1s 739us/step - loss: 26.7620 - mae: 3.7485 - val_loss: 329.3821 - val_mae: 13.4045\n",
      "Epoch 488/1000\n",
      "1988/1988 [==============================] - 1s 676us/step - loss: 27.5346 - mae: 3.8555 - val_loss: 251.8645 - val_mae: 11.5948\n",
      "Epoch 489/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 26.5482 - mae: 3.7275 - val_loss: 329.7923 - val_mae: 13.5802\n",
      "Epoch 490/1000\n",
      "1988/1988 [==============================] - 1s 693us/step - loss: 26.8607 - mae: 3.7196 - val_loss: 295.6153 - val_mae: 12.5437\n",
      "Epoch 491/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 25.9656 - mae: 3.7473 - val_loss: 384.1567 - val_mae: 14.6539\n",
      "Epoch 492/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 28.7782 - mae: 3.8174 - val_loss: 424.2132 - val_mae: 15.8293\n",
      "Epoch 493/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 29.3270 - mae: 3.9126 - val_loss: 267.2831 - val_mae: 11.8558\n",
      "Epoch 494/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 29.3449 - mae: 3.9079 - val_loss: 340.3284 - val_mae: 13.8567\n",
      "Epoch 495/1000\n",
      "1988/1988 [==============================] - 1s 636us/step - loss: 27.3838 - mae: 3.8289 - val_loss: 438.4957 - val_mae: 16.2986\n",
      "Epoch 496/1000\n",
      "1988/1988 [==============================] - 1s 668us/step - loss: 30.7939 - mae: 4.0618 - val_loss: 388.2528 - val_mae: 15.1564\n",
      "Epoch 497/1000\n",
      "1988/1988 [==============================] - 1s 635us/step - loss: 29.4800 - mae: 3.8853 - val_loss: 351.5340 - val_mae: 14.2373\n",
      "Epoch 498/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 29.3814 - mae: 3.8813 - val_loss: 329.5987 - val_mae: 13.5462\n",
      "Epoch 499/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 28.7963 - mae: 3.8485 - val_loss: 361.8992 - val_mae: 14.5651\n",
      "Epoch 500/1000\n",
      "1988/1988 [==============================] - 1s 595us/step - loss: 27.7237 - mae: 3.8513 - val_loss: 415.7725 - val_mae: 15.6781\n",
      "Epoch 501/1000\n",
      "1988/1988 [==============================] - 1s 593us/step - loss: 29.6416 - mae: 3.9925 - val_loss: 363.0591 - val_mae: 14.7046\n",
      "Epoch 502/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 28.4011 - mae: 3.8962 - val_loss: 319.6857 - val_mae: 13.1927\n",
      "Epoch 503/1000\n",
      "1988/1988 [==============================] - 1s 626us/step - loss: 29.5845 - mae: 3.7954 - val_loss: 401.6826 - val_mae: 15.4601\n",
      "Epoch 504/1000\n",
      "1988/1988 [==============================] - 1s 630us/step - loss: 29.2624 - mae: 3.9966 - val_loss: 304.8657 - val_mae: 13.3643\n",
      "Epoch 505/1000\n",
      "1988/1988 [==============================] - 1s 690us/step - loss: 28.9655 - mae: 3.9490 - val_loss: 296.8487 - val_mae: 12.7343\n",
      "Epoch 506/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 28.8308 - mae: 3.9702 - val_loss: 241.8014 - val_mae: 11.4235\n",
      "Epoch 507/1000\n",
      "1988/1988 [==============================] - 1s 677us/step - loss: 26.9662 - mae: 3.7474 - val_loss: 200.2936 - val_mae: 10.1601\n",
      "Epoch 508/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 29.1930 - mae: 3.9026 - val_loss: 258.0888 - val_mae: 11.8739\n",
      "Epoch 509/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 26.9358 - mae: 3.7243 - val_loss: 342.5996 - val_mae: 14.7699\n",
      "Epoch 510/1000\n",
      "1988/1988 [==============================] - 1s 663us/step - loss: 28.2869 - mae: 3.9121 - val_loss: 320.1104 - val_mae: 13.3822\n",
      "Epoch 511/1000\n",
      "1988/1988 [==============================] - 1s 646us/step - loss: 28.6431 - mae: 3.9730 - val_loss: 274.3276 - val_mae: 12.1872\n",
      "Epoch 512/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 33.2186 - mae: 4.2366 - val_loss: 244.2331 - val_mae: 11.4004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 513/1000\n",
      "1988/1988 [==============================] - 1s 623us/step - loss: 30.3936 - mae: 4.0167 - val_loss: 312.7306 - val_mae: 13.1380\n",
      "Epoch 514/1000\n",
      "1988/1988 [==============================] - 1s 625us/step - loss: 29.9431 - mae: 3.9695 - val_loss: 230.6671 - val_mae: 11.0696\n",
      "Epoch 515/1000\n",
      "1988/1988 [==============================] - 1s 566us/step - loss: 32.4416 - mae: 4.0873 - val_loss: 209.5808 - val_mae: 10.5170\n",
      "Epoch 516/1000\n",
      "1988/1988 [==============================] - 1s 575us/step - loss: 27.7709 - mae: 3.7936 - val_loss: 330.7976 - val_mae: 13.7870\n",
      "Epoch 517/1000\n",
      "1988/1988 [==============================] - 1s 561us/step - loss: 27.1485 - mae: 3.8723 - val_loss: 325.4604 - val_mae: 13.5567\n",
      "Epoch 518/1000\n",
      "1988/1988 [==============================] - 1s 552us/step - loss: 30.0345 - mae: 4.1959 - val_loss: 256.1847 - val_mae: 11.8007\n",
      "Epoch 519/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 27.3618 - mae: 3.8153 - val_loss: 380.4157 - val_mae: 15.5882\n",
      "Epoch 520/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 26.4078 - mae: 3.6158 - val_loss: 285.7054 - val_mae: 12.5021\n",
      "Epoch 521/1000\n",
      "1988/1988 [==============================] - 1s 673us/step - loss: 29.2869 - mae: 3.9618 - val_loss: 301.6640 - val_mae: 12.8777\n",
      "Epoch 522/1000\n",
      "1988/1988 [==============================] - 1s 608us/step - loss: 27.9972 - mae: 3.9265 - val_loss: 277.8834 - val_mae: 12.3011\n",
      "Epoch 523/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 25.1167 - mae: 3.6354 - val_loss: 281.3399 - val_mae: 12.6673\n",
      "Epoch 524/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 28.4070 - mae: 3.8294 - val_loss: 189.6676 - val_mae: 10.0093\n",
      "Epoch 525/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 28.0036 - mae: 3.8193 - val_loss: 238.5120 - val_mae: 11.3360\n",
      "Epoch 526/1000\n",
      "1988/1988 [==============================] - 1s 646us/step - loss: 29.5036 - mae: 3.9849 - val_loss: 265.5804 - val_mae: 12.3480\n",
      "Epoch 527/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 27.3832 - mae: 3.7226 - val_loss: 241.3666 - val_mae: 11.2610\n",
      "Epoch 528/1000\n",
      "1988/1988 [==============================] - 1s 673us/step - loss: 27.3026 - mae: 3.8710 - val_loss: 262.2606 - val_mae: 12.1466\n",
      "Epoch 529/1000\n",
      "1988/1988 [==============================] - 1s 730us/step - loss: 26.6271 - mae: 3.7621 - val_loss: 323.2485 - val_mae: 13.7788\n",
      "Epoch 530/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 31.3883 - mae: 4.2147 - val_loss: 200.4686 - val_mae: 10.1982\n",
      "Epoch 531/1000\n",
      "1988/1988 [==============================] - 1s 730us/step - loss: 31.2272 - mae: 4.0536 - val_loss: 207.1377 - val_mae: 10.4285\n",
      "Epoch 532/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 29.4408 - mae: 4.0134 - val_loss: 321.5346 - val_mae: 14.2183\n",
      "Epoch 533/1000\n",
      "1988/1988 [==============================] - 1s 653us/step - loss: 27.6881 - mae: 3.8846 - val_loss: 197.5425 - val_mae: 10.3564\n",
      "Epoch 534/1000\n",
      "1988/1988 [==============================] - 1s 646us/step - loss: 28.1138 - mae: 3.9143 - val_loss: 284.3740 - val_mae: 12.8132\n",
      "Epoch 535/1000\n",
      "1988/1988 [==============================] - 1s 614us/step - loss: 28.2234 - mae: 3.8982 - val_loss: 300.7177 - val_mae: 13.3580\n",
      "Epoch 536/1000\n",
      "1988/1988 [==============================] - 1s 631us/step - loss: 28.5188 - mae: 3.8704 - val_loss: 218.4033 - val_mae: 10.7697\n",
      "Epoch 537/1000\n",
      "1988/1988 [==============================] - 1s 633us/step - loss: 28.8951 - mae: 3.8183 - val_loss: 322.9139 - val_mae: 14.1646\n",
      "Epoch 538/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 27.5575 - mae: 3.8540 - val_loss: 231.2883 - val_mae: 11.0007\n",
      "Epoch 539/1000\n",
      "1988/1988 [==============================] - 1s 621us/step - loss: 27.1428 - mae: 3.8360 - val_loss: 234.6305 - val_mae: 11.1889\n",
      "Epoch 540/1000\n",
      "1988/1988 [==============================] - 1s 623us/step - loss: 28.1105 - mae: 3.8592 - val_loss: 221.8124 - val_mae: 10.8171\n",
      "Epoch 541/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 30.4086 - mae: 3.9499 - val_loss: 250.5316 - val_mae: 11.4646\n",
      "Epoch 542/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 27.3823 - mae: 3.8823 - val_loss: 291.2007 - val_mae: 13.3743\n",
      "Epoch 543/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 30.8912 - mae: 4.1422 - val_loss: 300.3447 - val_mae: 13.0300\n",
      "Epoch 544/1000\n",
      "1988/1988 [==============================] - 1s 715us/step - loss: 30.6048 - mae: 4.0407 - val_loss: 295.9381 - val_mae: 12.6731\n",
      "Epoch 545/1000\n",
      "1988/1988 [==============================] - 1s 671us/step - loss: 28.4183 - mae: 3.8933 - val_loss: 275.7495 - val_mae: 12.2942\n",
      "Epoch 546/1000\n",
      "1988/1988 [==============================] - 1s 646us/step - loss: 28.1615 - mae: 3.7668 - val_loss: 341.1606 - val_mae: 14.4556\n",
      "Epoch 547/1000\n",
      "1988/1988 [==============================] - 1s 676us/step - loss: 29.1435 - mae: 3.8271 - val_loss: 243.9462 - val_mae: 11.4632\n",
      "Epoch 548/1000\n",
      "1988/1988 [==============================] - 1s 735us/step - loss: 27.6097 - mae: 3.8361 - val_loss: 300.5107 - val_mae: 12.9346\n",
      "Epoch 549/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 27.8941 - mae: 3.7766 - val_loss: 359.3579 - val_mae: 14.9009\n",
      "Epoch 550/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 28.5461 - mae: 3.8328 - val_loss: 310.1268 - val_mae: 13.4583\n",
      "Epoch 551/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 26.4792 - mae: 3.8053 - val_loss: 227.9245 - val_mae: 10.9733\n",
      "Epoch 552/1000\n",
      "1988/1988 [==============================] - 1s 659us/step - loss: 27.2913 - mae: 3.7495 - val_loss: 188.8564 - val_mae: 9.9178\n",
      "Epoch 553/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 29.4021 - mae: 3.9126 - val_loss: 200.5206 - val_mae: 10.3307\n",
      "Epoch 554/1000\n",
      "1988/1988 [==============================] - 1s 631us/step - loss: 27.3350 - mae: 3.7203 - val_loss: 263.6439 - val_mae: 12.5009\n",
      "Epoch 555/1000\n",
      "1988/1988 [==============================] - 1s 630us/step - loss: 24.6014 - mae: 3.6313 - val_loss: 194.9017 - val_mae: 10.0823\n",
      "Epoch 556/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 26.0234 - mae: 3.6825 - val_loss: 279.6971 - val_mae: 12.5100\n",
      "Epoch 557/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 28.2279 - mae: 3.8348 - val_loss: 329.6955 - val_mae: 13.8408\n",
      "Epoch 558/1000\n",
      "1988/1988 [==============================] - 1s 653us/step - loss: 27.3673 - mae: 3.7555 - val_loss: 285.5986 - val_mae: 12.5063\n",
      "Epoch 559/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 25.7987 - mae: 3.6731 - val_loss: 339.3373 - val_mae: 14.0552\n",
      "Epoch 560/1000\n",
      "1988/1988 [==============================] - 1s 650us/step - loss: 26.6912 - mae: 3.7151 - val_loss: 246.8978 - val_mae: 11.2772\n",
      "Epoch 561/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 24.8418 - mae: 3.6204 - val_loss: 282.9972 - val_mae: 12.3496\n",
      "Epoch 562/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 26.5865 - mae: 3.7390 - val_loss: 254.6862 - val_mae: 11.6495\n",
      "Epoch 563/1000\n",
      "1988/1988 [==============================] - 1s 631us/step - loss: 26.6302 - mae: 3.7407 - val_loss: 464.7340 - val_mae: 17.9168\n",
      "Epoch 564/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 26.3428 - mae: 3.7235 - val_loss: 230.3814 - val_mae: 11.1381\n",
      "Epoch 565/1000\n",
      "1988/1988 [==============================] - 1s 676us/step - loss: 25.9345 - mae: 3.7692 - val_loss: 252.4852 - val_mae: 11.6448\n",
      "Epoch 566/1000\n",
      "1988/1988 [==============================] - 1s 650us/step - loss: 26.5694 - mae: 3.8090 - val_loss: 394.4610 - val_mae: 15.6864\n",
      "Epoch 567/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 27.6589 - mae: 3.7669 - val_loss: 263.2812 - val_mae: 11.6328\n",
      "Epoch 568/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 26.5696 - mae: 3.7360 - val_loss: 323.6921 - val_mae: 13.4398\n",
      "Epoch 569/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 29.1305 - mae: 3.8074 - val_loss: 231.9597 - val_mae: 11.0542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 570/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 27.9676 - mae: 3.7778 - val_loss: 197.5306 - val_mae: 10.2634\n",
      "Epoch 571/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 29.3083 - mae: 3.8785 - val_loss: 265.3518 - val_mae: 12.5769\n",
      "Epoch 572/1000\n",
      "1988/1988 [==============================] - 1s 621us/step - loss: 31.7836 - mae: 3.9411 - val_loss: 178.3976 - val_mae: 9.7202\n",
      "Epoch 573/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 32.3121 - mae: 4.1189 - val_loss: 329.7991 - val_mae: 13.9377\n",
      "Epoch 574/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 30.2869 - mae: 4.0171 - val_loss: 365.8059 - val_mae: 15.3060\n",
      "Epoch 575/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 27.8951 - mae: 3.7628 - val_loss: 239.4371 - val_mae: 11.4221\n",
      "Epoch 576/1000\n",
      "1988/1988 [==============================] - 1s 601us/step - loss: 24.4939 - mae: 3.6617 - val_loss: 261.8342 - val_mae: 11.8813\n",
      "Epoch 577/1000\n",
      "1988/1988 [==============================] - 1s 617us/step - loss: 26.8781 - mae: 3.7578 - val_loss: 289.9309 - val_mae: 12.3623\n",
      "Epoch 578/1000\n",
      "1988/1988 [==============================] - 1s 617us/step - loss: 26.6096 - mae: 3.7890 - val_loss: 285.4465 - val_mae: 12.7448\n",
      "Epoch 579/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 26.9010 - mae: 3.7555 - val_loss: 344.7532 - val_mae: 14.6417\n",
      "Epoch 580/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 28.0837 - mae: 3.8869 - val_loss: 265.8959 - val_mae: 11.7716\n",
      "Epoch 581/1000\n",
      "1988/1988 [==============================] - 1s 615us/step - loss: 27.7871 - mae: 3.8538 - val_loss: 275.6853 - val_mae: 12.3912\n",
      "Epoch 582/1000\n",
      "1988/1988 [==============================] - 1s 624us/step - loss: 26.1931 - mae: 3.6433 - val_loss: 240.5877 - val_mae: 11.4237\n",
      "Epoch 583/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 27.3784 - mae: 3.7905 - val_loss: 261.4876 - val_mae: 12.2860\n",
      "Epoch 584/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 27.5095 - mae: 3.8090 - val_loss: 211.0139 - val_mae: 10.4615\n",
      "Epoch 585/1000\n",
      "1988/1988 [==============================] - 1s 663us/step - loss: 27.8317 - mae: 3.8671 - val_loss: 144.7127 - val_mae: 9.8953\n",
      "Epoch 586/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 25.8557 - mae: 3.6602 - val_loss: 183.4451 - val_mae: 9.7852\n",
      "Epoch 587/1000\n",
      "1988/1988 [==============================] - 1s 652us/step - loss: 28.3549 - mae: 3.8160 - val_loss: 197.6773 - val_mae: 10.1132\n",
      "Epoch 588/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 29.3262 - mae: 3.8885 - val_loss: 225.5438 - val_mae: 11.1362\n",
      "Epoch 589/1000\n",
      "1988/1988 [==============================] - 1s 673us/step - loss: 26.9579 - mae: 3.7188 - val_loss: 323.9916 - val_mae: 14.4715\n",
      "Epoch 590/1000\n",
      "1988/1988 [==============================] - 1s 648us/step - loss: 26.3609 - mae: 3.7510 - val_loss: 271.5825 - val_mae: 12.3579\n",
      "Epoch 591/1000\n",
      "1988/1988 [==============================] - 1s 701us/step - loss: 26.7748 - mae: 3.7611 - val_loss: 274.6466 - val_mae: 12.3985\n",
      "Epoch 592/1000\n",
      "1988/1988 [==============================] - 1s 683us/step - loss: 28.3017 - mae: 3.7983 - val_loss: 262.2944 - val_mae: 12.3779\n",
      "Epoch 593/1000\n",
      "1988/1988 [==============================] - 1s 704us/step - loss: 28.0123 - mae: 3.9344 - val_loss: 214.3921 - val_mae: 10.8543\n",
      "Epoch 594/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 27.0430 - mae: 3.8149 - val_loss: 193.2600 - val_mae: 10.0674\n",
      "Epoch 595/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 34.7923 - mae: 4.2731 - val_loss: 228.6682 - val_mae: 11.3108\n",
      "Epoch 596/1000\n",
      "1988/1988 [==============================] - 1s 672us/step - loss: 29.0184 - mae: 3.9291 - val_loss: 276.4258 - val_mae: 13.0541\n",
      "Epoch 597/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 26.4396 - mae: 3.8091 - val_loss: 261.5340 - val_mae: 12.0327\n",
      "Epoch 598/1000\n",
      "1988/1988 [==============================] - 1s 612us/step - loss: 29.9040 - mae: 3.9937 - val_loss: 224.1186 - val_mae: 10.9925\n",
      "Epoch 599/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 26.7674 - mae: 3.7360 - val_loss: 262.7975 - val_mae: 12.4547\n",
      "Epoch 600/1000\n",
      "1988/1988 [==============================] - 1s 605us/step - loss: 27.9082 - mae: 3.7929 - val_loss: 195.7019 - val_mae: 10.0743\n",
      "Epoch 601/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 26.9459 - mae: 3.7409 - val_loss: 181.0169 - val_mae: 9.7287\n",
      "Epoch 602/1000\n",
      "1988/1988 [==============================] - 1s 608us/step - loss: 27.5141 - mae: 3.6784 - val_loss: 353.4353 - val_mae: 15.3931\n",
      "Epoch 603/1000\n",
      "1988/1988 [==============================] - 1s 627us/step - loss: 25.5564 - mae: 3.6555 - val_loss: 187.4843 - val_mae: 9.8735\n",
      "Epoch 604/1000\n",
      "1988/1988 [==============================] - 1s 635us/step - loss: 26.4561 - mae: 3.6695 - val_loss: 154.1616 - val_mae: 9.1208\n",
      "Epoch 605/1000\n",
      "1988/1988 [==============================] - 1s 641us/step - loss: 27.4039 - mae: 3.7818 - val_loss: 267.2049 - val_mae: 12.6657\n",
      "Epoch 606/1000\n",
      "1988/1988 [==============================] - 1s 624us/step - loss: 26.4780 - mae: 3.7866 - val_loss: 294.4538 - val_mae: 12.9993\n",
      "Epoch 607/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 26.2264 - mae: 3.7039 - val_loss: 368.0381 - val_mae: 15.1715\n",
      "Epoch 608/1000\n",
      "1988/1988 [==============================] - 1s 626us/step - loss: 28.3778 - mae: 3.8657 - val_loss: 245.8403 - val_mae: 11.5649\n",
      "Epoch 609/1000\n",
      "1988/1988 [==============================] - 1s 641us/step - loss: 25.7234 - mae: 3.7609 - val_loss: 245.2407 - val_mae: 11.6554\n",
      "Epoch 610/1000\n",
      "1988/1988 [==============================] - 1s 665us/step - loss: 26.9201 - mae: 3.7748 - val_loss: 240.4362 - val_mae: 11.7229\n",
      "Epoch 611/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 27.0913 - mae: 3.7635 - val_loss: 281.5323 - val_mae: 13.1308\n",
      "Epoch 612/1000\n",
      "1988/1988 [==============================] - 1s 650us/step - loss: 26.2087 - mae: 3.6667 - val_loss: 186.5947 - val_mae: 9.9228\n",
      "Epoch 613/1000\n",
      "1988/1988 [==============================] - 1s 704us/step - loss: 25.4929 - mae: 3.6530 - val_loss: 237.6818 - val_mae: 11.6157\n",
      "Epoch 614/1000\n",
      "1988/1988 [==============================] - 1s 703us/step - loss: 25.6599 - mae: 3.6126 - val_loss: 198.1072 - val_mae: 10.4967\n",
      "Epoch 615/1000\n",
      "1988/1988 [==============================] - 1s 677us/step - loss: 26.3638 - mae: 3.6202 - val_loss: 296.5266 - val_mae: 13.8907\n",
      "Epoch 616/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 25.4731 - mae: 3.6488 - val_loss: 212.4798 - val_mae: 10.9000\n",
      "Epoch 617/1000\n",
      "1988/1988 [==============================] - 1s 666us/step - loss: 28.6470 - mae: 3.8546 - val_loss: 146.0877 - val_mae: 8.8754\n",
      "Epoch 618/1000\n",
      "1988/1988 [==============================] - 1s 676us/step - loss: 28.2026 - mae: 3.8249 - val_loss: 254.0259 - val_mae: 12.4677\n",
      "Epoch 619/1000\n",
      "1988/1988 [==============================] - 1s 672us/step - loss: 25.2796 - mae: 3.7176 - val_loss: 245.6827 - val_mae: 11.6826\n",
      "Epoch 620/1000\n",
      "1988/1988 [==============================] - 1s 673us/step - loss: 30.0006 - mae: 4.0049 - val_loss: 250.3924 - val_mae: 12.0080\n",
      "Epoch 621/1000\n",
      "1988/1988 [==============================] - 1s 666us/step - loss: 29.3707 - mae: 4.0678 - val_loss: 289.4747 - val_mae: 13.1311\n",
      "Epoch 622/1000\n",
      "1988/1988 [==============================] - 1s 616us/step - loss: 27.3816 - mae: 3.8161 - val_loss: 163.3813 - val_mae: 9.4075\n",
      "Epoch 623/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 26.7649 - mae: 3.7775 - val_loss: 173.1106 - val_mae: 9.5495\n",
      "Epoch 624/1000\n",
      "1988/1988 [==============================] - 1s 604us/step - loss: 26.2651 - mae: 3.7244 - val_loss: 283.0609 - val_mae: 13.4136\n",
      "Epoch 625/1000\n",
      "1988/1988 [==============================] - 1s 617us/step - loss: 28.1168 - mae: 3.8157 - val_loss: 212.4407 - val_mae: 10.5968\n",
      "Epoch 626/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 28.1960 - mae: 3.8293 - val_loss: 220.6687 - val_mae: 11.1938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 627/1000\n",
      "1988/1988 [==============================] - 1s 626us/step - loss: 26.6203 - mae: 3.7699 - val_loss: 243.9754 - val_mae: 11.6567\n",
      "Epoch 628/1000\n",
      "1988/1988 [==============================] - 1s 641us/step - loss: 28.4848 - mae: 3.8859 - val_loss: 210.3707 - val_mae: 10.6597\n",
      "Epoch 629/1000\n",
      "1988/1988 [==============================] - 1s 625us/step - loss: 28.1037 - mae: 3.7861 - val_loss: 230.3394 - val_mae: 11.3190\n",
      "Epoch 630/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 26.2178 - mae: 3.6709 - val_loss: 275.0716 - val_mae: 12.8052\n",
      "Epoch 631/1000\n",
      "1988/1988 [==============================] - 1s 626us/step - loss: 25.1496 - mae: 3.5703 - val_loss: 262.8252 - val_mae: 12.0633\n",
      "Epoch 632/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 25.7664 - mae: 3.7344 - val_loss: 344.7818 - val_mae: 14.7297\n",
      "Epoch 633/1000\n",
      "1988/1988 [==============================] - 1s 616us/step - loss: 27.7177 - mae: 3.7389 - val_loss: 250.7222 - val_mae: 11.7256\n",
      "Epoch 634/1000\n",
      "1988/1988 [==============================] - 1s 630us/step - loss: 27.0080 - mae: 3.7806 - val_loss: 187.1509 - val_mae: 9.8965\n",
      "Epoch 635/1000\n",
      "1988/1988 [==============================] - 1s 612us/step - loss: 25.2243 - mae: 3.6371 - val_loss: 215.8756 - val_mae: 10.8982\n",
      "Epoch 636/1000\n",
      "1988/1988 [==============================] - 1s 615us/step - loss: 25.0866 - mae: 3.6634 - val_loss: 240.1260 - val_mae: 11.5322\n",
      "Epoch 637/1000\n",
      "1988/1988 [==============================] - 1s 631us/step - loss: 29.2348 - mae: 3.9231 - val_loss: 302.3601 - val_mae: 13.6032\n",
      "Epoch 638/1000\n",
      "1988/1988 [==============================] - 1s 627us/step - loss: 28.2535 - mae: 3.9014 - val_loss: 228.2713 - val_mae: 11.0889\n",
      "Epoch 639/1000\n",
      "1988/1988 [==============================] - 1s 640us/step - loss: 26.6622 - mae: 3.7366 - val_loss: 230.3306 - val_mae: 11.3871\n",
      "Epoch 640/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 27.2854 - mae: 3.8001 - val_loss: 352.9819 - val_mae: 14.9105\n",
      "Epoch 641/1000\n",
      "1988/1988 [==============================] - 1s 652us/step - loss: 27.6706 - mae: 3.7899 - val_loss: 339.8302 - val_mae: 14.6937\n",
      "Epoch 642/1000\n",
      "1988/1988 [==============================] - 1s 662us/step - loss: 26.2578 - mae: 3.6953 - val_loss: 234.8298 - val_mae: 11.7010\n",
      "Epoch 643/1000\n",
      "1988/1988 [==============================] - 1s 646us/step - loss: 26.3904 - mae: 3.7371 - val_loss: 384.1081 - val_mae: 15.8188\n",
      "Epoch 644/1000\n",
      "1988/1988 [==============================] - 1s 679us/step - loss: 26.4583 - mae: 3.7574 - val_loss: 188.4885 - val_mae: 9.8894\n",
      "Epoch 645/1000\n",
      "1988/1988 [==============================] - 1s 652us/step - loss: 26.9303 - mae: 3.7539 - val_loss: 204.7325 - val_mae: 10.4543\n",
      "Epoch 646/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 25.1892 - mae: 3.6458 - val_loss: 266.2848 - val_mae: 12.1038\n",
      "Epoch 647/1000\n",
      "1988/1988 [==============================] - 1s 635us/step - loss: 28.7088 - mae: 3.9793 - val_loss: 274.8598 - val_mae: 12.7873\n",
      "Epoch 648/1000\n",
      "1988/1988 [==============================] - 1s 625us/step - loss: 28.2798 - mae: 3.9148 - val_loss: 347.1639 - val_mae: 14.2336\n",
      "Epoch 649/1000\n",
      "1988/1988 [==============================] - 1s 606us/step - loss: 30.5843 - mae: 4.0044 - val_loss: 259.1791 - val_mae: 12.0963\n",
      "Epoch 650/1000\n",
      "1988/1988 [==============================] - 1s 600us/step - loss: 27.1181 - mae: 3.6771 - val_loss: 296.4531 - val_mae: 13.5337\n",
      "Epoch 651/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 28.8894 - mae: 3.7814 - val_loss: 194.1761 - val_mae: 10.0555\n",
      "Epoch 652/1000\n",
      "1988/1988 [==============================] - 1s 627us/step - loss: 27.8702 - mae: 3.8048 - val_loss: 186.6428 - val_mae: 10.2363\n",
      "Epoch 653/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 25.6367 - mae: 3.6495 - val_loss: 247.5527 - val_mae: 12.0275\n",
      "Epoch 654/1000\n",
      "1988/1988 [==============================] - 1s 616us/step - loss: 24.6006 - mae: 3.6328 - val_loss: 178.6180 - val_mae: 9.6433\n",
      "Epoch 655/1000\n",
      "1988/1988 [==============================] - 1s 627us/step - loss: 27.5918 - mae: 3.7480 - val_loss: 204.9423 - val_mae: 10.6738\n",
      "Epoch 656/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 28.0162 - mae: 3.7439 - val_loss: 261.9329 - val_mae: 12.7755\n",
      "Epoch 657/1000\n",
      "1988/1988 [==============================] - 1s 618us/step - loss: 26.1747 - mae: 3.6716 - val_loss: 312.7743 - val_mae: 13.8283\n",
      "Epoch 658/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 26.8976 - mae: 3.8230 - val_loss: 232.4993 - val_mae: 11.1317\n",
      "Epoch 659/1000\n",
      "1988/1988 [==============================] - 1s 608us/step - loss: 26.8686 - mae: 3.7120 - val_loss: 229.2972 - val_mae: 11.0237\n",
      "Epoch 660/1000\n",
      "1988/1988 [==============================] - 1s 614us/step - loss: 28.0799 - mae: 3.7740 - val_loss: 193.3307 - val_mae: 10.1425\n",
      "Epoch 661/1000\n",
      "1988/1988 [==============================] - 1s 615us/step - loss: 27.6753 - mae: 3.8446 - val_loss: 184.0404 - val_mae: 9.9847\n",
      "Epoch 662/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 28.0394 - mae: 3.7560 - val_loss: 184.1860 - val_mae: 9.8270\n",
      "Epoch 663/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 28.1463 - mae: 3.7927 - val_loss: 240.2446 - val_mae: 11.4877\n",
      "Epoch 664/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 27.8604 - mae: 3.7882 - val_loss: 199.5110 - val_mae: 10.2590\n",
      "Epoch 665/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 25.7011 - mae: 3.6737 - val_loss: 281.7366 - val_mae: 12.5075\n",
      "Epoch 666/1000\n",
      "1988/1988 [==============================] - 1s 705us/step - loss: 23.8429 - mae: 3.5191 - val_loss: 290.9013 - val_mae: 13.1770\n",
      "Epoch 667/1000\n",
      "1988/1988 [==============================] - 1s 711us/step - loss: 25.9584 - mae: 3.6441 - val_loss: 214.3056 - val_mae: 10.6134\n",
      "Epoch 668/1000\n",
      "1988/1988 [==============================] - 1s 691us/step - loss: 29.3884 - mae: 3.8683 - val_loss: 176.1760 - val_mae: 9.6331\n",
      "Epoch 669/1000\n",
      "1988/1988 [==============================] - 1s 702us/step - loss: 30.1339 - mae: 3.9277 - val_loss: 251.1389 - val_mae: 12.3198\n",
      "Epoch 670/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 26.1280 - mae: 3.7331 - val_loss: 188.4722 - val_mae: 9.8464\n",
      "Epoch 671/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 25.8294 - mae: 3.6583 - val_loss: 218.9173 - val_mae: 10.6539\n",
      "Epoch 672/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 25.1450 - mae: 3.6151 - val_loss: 156.5658 - val_mae: 9.4785\n",
      "Epoch 673/1000\n",
      "1988/1988 [==============================] - 1s 671us/step - loss: 27.2809 - mae: 3.7826 - val_loss: 229.4427 - val_mae: 11.5575\n",
      "Epoch 674/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 23.7563 - mae: 3.5927 - val_loss: 179.8188 - val_mae: 10.0284\n",
      "Epoch 675/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 25.0301 - mae: 3.6387 - val_loss: 172.0230 - val_mae: 9.6841\n",
      "Epoch 676/1000\n",
      "1988/1988 [==============================] - 1s 665us/step - loss: 26.7485 - mae: 3.7888 - val_loss: 183.3351 - val_mae: 10.3228\n",
      "Epoch 677/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 24.4166 - mae: 3.5713 - val_loss: 265.1086 - val_mae: 13.0426\n",
      "Epoch 678/1000\n",
      "1988/1988 [==============================] - 1s 608us/step - loss: 24.2081 - mae: 3.6134 - val_loss: 190.7434 - val_mae: 10.1659\n",
      "Epoch 679/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 27.9080 - mae: 3.8518 - val_loss: 189.2624 - val_mae: 10.4289\n",
      "Epoch 680/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 27.2714 - mae: 3.7764 - val_loss: 126.3284 - val_mae: 8.3145\n",
      "Epoch 681/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 28.1147 - mae: 3.8107 - val_loss: 208.9568 - val_mae: 10.8706\n",
      "Epoch 682/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 26.3372 - mae: 3.6717 - val_loss: 125.3335 - val_mae: 8.4085\n",
      "Epoch 683/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 29.5842 - mae: 3.7619 - val_loss: 173.9875 - val_mae: 9.8391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 684/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 26.6159 - mae: 3.6651 - val_loss: 149.5970 - val_mae: 8.9232\n",
      "Epoch 685/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 25.9707 - mae: 3.7009 - val_loss: 158.9217 - val_mae: 9.1634\n",
      "Epoch 686/1000\n",
      "1988/1988 [==============================] - 1s 652us/step - loss: 26.7483 - mae: 3.7806 - val_loss: 217.6429 - val_mae: 11.1078\n",
      "Epoch 687/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 28.1072 - mae: 3.9464 - val_loss: 207.0962 - val_mae: 10.4377\n",
      "Epoch 688/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 27.3781 - mae: 3.8692 - val_loss: 333.5310 - val_mae: 14.3123\n",
      "Epoch 689/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 29.3261 - mae: 3.9257 - val_loss: 262.2235 - val_mae: 12.2799\n",
      "Epoch 690/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 25.2972 - mae: 3.6425 - val_loss: 316.1226 - val_mae: 14.3123\n",
      "Epoch 691/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 24.9059 - mae: 3.5854 - val_loss: 213.5397 - val_mae: 10.8931\n",
      "Epoch 692/1000\n",
      "1988/1988 [==============================] - 1s 620us/step - loss: 26.8703 - mae: 3.7141 - val_loss: 194.3877 - val_mae: 10.1518\n",
      "Epoch 693/1000\n",
      "1988/1988 [==============================] - 1s 621us/step - loss: 23.9411 - mae: 3.6296 - val_loss: 145.8136 - val_mae: 8.7712\n",
      "Epoch 694/1000\n",
      "1988/1988 [==============================] - 1s 609us/step - loss: 27.8654 - mae: 3.6930 - val_loss: 150.1169 - val_mae: 8.8112\n",
      "Epoch 695/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 28.1667 - mae: 3.7667 - val_loss: 254.3240 - val_mae: 11.9672\n",
      "Epoch 696/1000\n",
      "1988/1988 [==============================] - 1s 623us/step - loss: 25.7218 - mae: 3.5652 - val_loss: 228.6013 - val_mae: 11.1212\n",
      "Epoch 697/1000\n",
      "1988/1988 [==============================] - 1s 620us/step - loss: 29.2929 - mae: 3.9267 - val_loss: 303.1878 - val_mae: 14.1964\n",
      "Epoch 698/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 23.7716 - mae: 3.5291 - val_loss: 191.4400 - val_mae: 10.1812\n",
      "Epoch 699/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 26.3175 - mae: 3.6582 - val_loss: 123.9775 - val_mae: 8.1699\n",
      "Epoch 700/1000\n",
      "1988/1988 [==============================] - 1s 624us/step - loss: 26.9362 - mae: 3.7349 - val_loss: 250.6885 - val_mae: 12.1902\n",
      "Epoch 701/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 27.4730 - mae: 3.7456 - val_loss: 268.9590 - val_mae: 12.1800\n",
      "Epoch 702/1000\n",
      "1988/1988 [==============================] - 1s 614us/step - loss: 26.6661 - mae: 3.7717 - val_loss: 344.1067 - val_mae: 14.4687\n",
      "Epoch 703/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 28.3006 - mae: 3.9229 - val_loss: 363.7994 - val_mae: 14.8603\n",
      "Epoch 704/1000\n",
      "1988/1988 [==============================] - 1s 631us/step - loss: 30.7319 - mae: 4.1438 - val_loss: 215.9346 - val_mae: 10.8465\n",
      "Epoch 705/1000\n",
      "1988/1988 [==============================] - 1s 620us/step - loss: 27.3750 - mae: 3.7865 - val_loss: 192.0732 - val_mae: 9.9497\n",
      "Epoch 706/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 26.3124 - mae: 3.7222 - val_loss: 298.4466 - val_mae: 12.8676\n",
      "Epoch 707/1000\n",
      "1988/1988 [==============================] - 1s 679us/step - loss: 25.8011 - mae: 3.6134 - val_loss: 240.3068 - val_mae: 11.2455\n",
      "Epoch 708/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 25.9949 - mae: 3.6088 - val_loss: 204.6989 - val_mae: 10.2828\n",
      "Epoch 709/1000\n",
      "1988/1988 [==============================] - 1s 669us/step - loss: 26.1731 - mae: 3.6686 - val_loss: 269.7412 - val_mae: 12.6014\n",
      "Epoch 710/1000\n",
      "1988/1988 [==============================] - 1s 668us/step - loss: 25.4102 - mae: 3.6042 - val_loss: 236.2902 - val_mae: 11.5134\n",
      "Epoch 711/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 26.3413 - mae: 3.6366 - val_loss: 144.2459 - val_mae: 8.7325\n",
      "Epoch 712/1000\n",
      "1988/1988 [==============================] - 1s 681us/step - loss: 24.1352 - mae: 3.5601 - val_loss: 180.8851 - val_mae: 9.9111\n",
      "Epoch 713/1000\n",
      "1988/1988 [==============================] - 1s 668us/step - loss: 28.3235 - mae: 3.8082 - val_loss: 106.9046 - val_mae: 7.7675\n",
      "Epoch 714/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 26.4193 - mae: 3.7455 - val_loss: 161.6866 - val_mae: 9.4293\n",
      "Epoch 715/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 27.4382 - mae: 3.7429 - val_loss: 210.6188 - val_mae: 10.6440\n",
      "Epoch 716/1000\n",
      "1988/1988 [==============================] - 1s 672us/step - loss: 26.9375 - mae: 3.6809 - val_loss: 201.8884 - val_mae: 10.0586\n",
      "Epoch 717/1000\n",
      "1988/1988 [==============================] - 1s 651us/step - loss: 25.9698 - mae: 3.6767 - val_loss: 202.5686 - val_mae: 10.8314\n",
      "Epoch 718/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 26.0014 - mae: 3.6467 - val_loss: 111.2715 - val_mae: 7.7693\n",
      "Epoch 719/1000\n",
      "1988/1988 [==============================] - 1s 636us/step - loss: 25.2046 - mae: 3.5902 - val_loss: 241.4490 - val_mae: 12.6507\n",
      "Epoch 720/1000\n",
      "1988/1988 [==============================] - 1s 648us/step - loss: 27.7399 - mae: 3.7587 - val_loss: 141.4146 - val_mae: 8.8589\n",
      "Epoch 721/1000\n",
      "1988/1988 [==============================] - 1s 708us/step - loss: 29.2938 - mae: 3.8934 - val_loss: 168.5364 - val_mae: 9.7102\n",
      "Epoch 722/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 29.2959 - mae: 3.9846 - val_loss: 247.6690 - val_mae: 11.7131\n",
      "Epoch 723/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 28.2134 - mae: 3.9036 - val_loss: 137.4361 - val_mae: 8.5324\n",
      "Epoch 724/1000\n",
      "1988/1988 [==============================] - 1s 652us/step - loss: 27.3483 - mae: 3.7113 - val_loss: 167.2863 - val_mae: 9.6591\n",
      "Epoch 725/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 25.0164 - mae: 3.6277 - val_loss: 157.6999 - val_mae: 9.1122\n",
      "Epoch 726/1000\n",
      "1988/1988 [==============================] - 1s 661us/step - loss: 25.7459 - mae: 3.6671 - val_loss: 191.2246 - val_mae: 10.4977\n",
      "Epoch 727/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 27.6423 - mae: 3.8167 - val_loss: 124.9250 - val_mae: 8.1028\n",
      "Epoch 728/1000\n",
      "1988/1988 [==============================] - 1s 727us/step - loss: 29.1573 - mae: 3.8183 - val_loss: 150.5256 - val_mae: 8.9550\n",
      "Epoch 729/1000\n",
      "1988/1988 [==============================] - 1s 721us/step - loss: 27.0311 - mae: 3.7663 - val_loss: 199.6900 - val_mae: 10.6255\n",
      "Epoch 730/1000\n",
      "1988/1988 [==============================] - 1s 718us/step - loss: 26.4386 - mae: 3.7286 - val_loss: 144.8833 - val_mae: 8.9190\n",
      "Epoch 731/1000\n",
      "1988/1988 [==============================] - 1s 697us/step - loss: 28.1548 - mae: 3.7574 - val_loss: 139.8490 - val_mae: 8.7185\n",
      "Epoch 732/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 27.1253 - mae: 3.7308 - val_loss: 211.3456 - val_mae: 11.3969\n",
      "Epoch 733/1000\n",
      "1988/1988 [==============================] - 1s 679us/step - loss: 25.0887 - mae: 3.5725 - val_loss: 215.5010 - val_mae: 11.1665\n",
      "Epoch 734/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 28.6258 - mae: 3.8398 - val_loss: 212.5422 - val_mae: 10.8152\n",
      "Epoch 735/1000\n",
      "1988/1988 [==============================] - 1s 661us/step - loss: 25.9802 - mae: 3.6226 - val_loss: 243.4230 - val_mae: 12.1593\n",
      "Epoch 736/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 26.5903 - mae: 3.7269 - val_loss: 174.2113 - val_mae: 9.7298\n",
      "Epoch 737/1000\n",
      "1988/1988 [==============================] - 1s 636us/step - loss: 24.0193 - mae: 3.5017 - val_loss: 225.4907 - val_mae: 11.4493\n",
      "Epoch 738/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 25.0416 - mae: 3.5742 - val_loss: 176.9259 - val_mae: 9.6611\n",
      "Epoch 739/1000\n",
      "1988/1988 [==============================] - 1s 623us/step - loss: 25.7718 - mae: 3.6374 - val_loss: 170.8169 - val_mae: 9.6093\n",
      "Epoch 740/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 25.2017 - mae: 3.6494 - val_loss: 195.2722 - val_mae: 10.8336\n",
      "Epoch 741/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 1s 626us/step - loss: 25.8893 - mae: 3.5868 - val_loss: 176.8480 - val_mae: 9.9405\n",
      "Epoch 742/1000\n",
      "1988/1988 [==============================] - 1s 631us/step - loss: 25.4525 - mae: 3.6955 - val_loss: 124.8511 - val_mae: 8.1426\n",
      "Epoch 743/1000\n",
      "1988/1988 [==============================] - 1s 634us/step - loss: 27.5383 - mae: 3.7044 - val_loss: 168.3976 - val_mae: 9.1726\n",
      "Epoch 744/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 26.6829 - mae: 3.6639 - val_loss: 212.1690 - val_mae: 10.3890\n",
      "Epoch 745/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 25.9446 - mae: 3.7545 - val_loss: 237.5346 - val_mae: 11.6676\n",
      "Epoch 746/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 30.5973 - mae: 3.8940 - val_loss: 298.8003 - val_mae: 14.2950\n",
      "Epoch 747/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 27.9672 - mae: 3.7716 - val_loss: 174.4250 - val_mae: 9.5961\n",
      "Epoch 748/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 29.8743 - mae: 3.9525 - val_loss: 192.8977 - val_mae: 10.4651\n",
      "Epoch 749/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 27.5588 - mae: 3.7712 - val_loss: 140.6117 - val_mae: 8.5530\n",
      "Epoch 750/1000\n",
      "1988/1988 [==============================] - 1s 653us/step - loss: 30.5343 - mae: 4.0535 - val_loss: 238.8291 - val_mae: 12.2345\n",
      "Epoch 751/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 27.7016 - mae: 3.8885 - val_loss: 275.5417 - val_mae: 12.7046\n",
      "Epoch 752/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 33.0653 - mae: 4.1583 - val_loss: 228.0776 - val_mae: 11.5006\n",
      "Epoch 753/1000\n",
      "1988/1988 [==============================] - 1s 700us/step - loss: 27.4111 - mae: 3.7607 - val_loss: 192.6000 - val_mae: 10.2760\n",
      "Epoch 754/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 27.1318 - mae: 3.7611 - val_loss: 159.2071 - val_mae: 9.0611\n",
      "Epoch 755/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 28.1874 - mae: 3.7613 - val_loss: 149.0690 - val_mae: 8.8527\n",
      "Epoch 756/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 29.8801 - mae: 3.8743 - val_loss: 186.0063 - val_mae: 10.1380\n",
      "Epoch 757/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 25.3124 - mae: 3.6497 - val_loss: 319.2456 - val_mae: 14.4331\n",
      "Epoch 758/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 27.9271 - mae: 3.7865 - val_loss: 127.3389 - val_mae: 8.0764\n",
      "Epoch 759/1000\n",
      "1988/1988 [==============================] - 1s 640us/step - loss: 25.2158 - mae: 3.6430 - val_loss: 173.2671 - val_mae: 9.5487\n",
      "Epoch 760/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 26.7638 - mae: 3.6962 - val_loss: 118.3790 - val_mae: 7.7691\n",
      "Epoch 761/1000\n",
      "1988/1988 [==============================] - 1s 660us/step - loss: 26.7869 - mae: 3.6202 - val_loss: 116.2617 - val_mae: 7.7141\n",
      "Epoch 762/1000\n",
      "1988/1988 [==============================] - 1s 662us/step - loss: 24.6613 - mae: 3.5475 - val_loss: 180.5750 - val_mae: 10.0745\n",
      "Epoch 763/1000\n",
      "1988/1988 [==============================] - 1s 653us/step - loss: 27.0359 - mae: 3.6464 - val_loss: 115.7332 - val_mae: 7.7970\n",
      "Epoch 764/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 25.7041 - mae: 3.7061 - val_loss: 102.4982 - val_mae: 7.4539\n",
      "Epoch 765/1000\n",
      "1988/1988 [==============================] - 1s 693us/step - loss: 26.4508 - mae: 3.6906 - val_loss: 135.6793 - val_mae: 8.2613\n",
      "Epoch 766/1000\n",
      "1988/1988 [==============================] - 1s 668us/step - loss: 26.9132 - mae: 3.6720 - val_loss: 170.7303 - val_mae: 9.1583\n",
      "Epoch 767/1000\n",
      "1988/1988 [==============================] - 1s 672us/step - loss: 24.3392 - mae: 3.5177 - val_loss: 306.7845 - val_mae: 12.9934\n",
      "Epoch 768/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 23.7654 - mae: 3.5700 - val_loss: 274.4656 - val_mae: 12.4894\n",
      "Epoch 769/1000\n",
      "1988/1988 [==============================] - 1s 693us/step - loss: 27.1011 - mae: 3.7021 - val_loss: 219.7852 - val_mae: 11.3194\n",
      "Epoch 770/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 26.0602 - mae: 3.7593 - val_loss: 221.4706 - val_mae: 10.8083\n",
      "Epoch 771/1000\n",
      "1988/1988 [==============================] - 1s 665us/step - loss: 26.2812 - mae: 3.7440 - val_loss: 227.0796 - val_mae: 11.2751\n",
      "Epoch 772/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 25.1959 - mae: 3.6012 - val_loss: 245.8629 - val_mae: 11.5926\n",
      "Epoch 773/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 29.5316 - mae: 3.8743 - val_loss: 200.6175 - val_mae: 10.1846\n",
      "Epoch 774/1000\n",
      "1988/1988 [==============================] - 1s 630us/step - loss: 25.0069 - mae: 3.6194 - val_loss: 212.9676 - val_mae: 10.2436\n",
      "Epoch 775/1000\n",
      "1988/1988 [==============================] - 1s 621us/step - loss: 25.8267 - mae: 3.5905 - val_loss: 293.1438 - val_mae: 13.1612\n",
      "Epoch 776/1000\n",
      "1988/1988 [==============================] - 1s 713us/step - loss: 25.8259 - mae: 3.6249 - val_loss: 323.0283 - val_mae: 14.2128\n",
      "Epoch 777/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 27.7234 - mae: 3.7231 - val_loss: 248.6241 - val_mae: 11.3047\n",
      "Epoch 778/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 27.6173 - mae: 3.8424 - val_loss: 369.5728 - val_mae: 15.0085\n",
      "Epoch 779/1000\n",
      "1988/1988 [==============================] - 1s 641us/step - loss: 24.4586 - mae: 3.6642 - val_loss: 180.7246 - val_mae: 9.4204\n",
      "Epoch 780/1000\n",
      "1988/1988 [==============================] - 1s 648us/step - loss: 25.7212 - mae: 3.6872 - val_loss: 216.5409 - val_mae: 10.5855\n",
      "Epoch 781/1000\n",
      "1988/1988 [==============================] - 1s 640us/step - loss: 26.2403 - mae: 3.6131 - val_loss: 259.5523 - val_mae: 11.6584\n",
      "Epoch 782/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 27.4974 - mae: 3.8797 - val_loss: 219.2244 - val_mae: 11.2274\n",
      "Epoch 783/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 24.9789 - mae: 3.6280 - val_loss: 289.9200 - val_mae: 13.0367\n",
      "Epoch 784/1000\n",
      "1988/1988 [==============================] - 1s 659us/step - loss: 27.2224 - mae: 3.7082 - val_loss: 252.7393 - val_mae: 12.0050\n",
      "Epoch 785/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 25.5947 - mae: 3.6369 - val_loss: 203.2284 - val_mae: 10.2398\n",
      "Epoch 786/1000\n",
      "1988/1988 [==============================] - 1s 673us/step - loss: 24.0236 - mae: 3.4637 - val_loss: 178.1109 - val_mae: 9.4924\n",
      "Epoch 787/1000\n",
      "1988/1988 [==============================] - 1s 710us/step - loss: 25.4685 - mae: 3.6799 - val_loss: 237.3739 - val_mae: 11.5502\n",
      "Epoch 788/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 27.7307 - mae: 3.7676 - val_loss: 232.0536 - val_mae: 10.7182\n",
      "Epoch 789/1000\n",
      "1988/1988 [==============================] - 1s 691us/step - loss: 27.0040 - mae: 3.7386 - val_loss: 216.3587 - val_mae: 10.1689\n",
      "Epoch 790/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 27.5934 - mae: 3.6143 - val_loss: 292.0286 - val_mae: 12.8741\n",
      "Epoch 791/1000\n",
      "1988/1988 [==============================] - 1s 669us/step - loss: 26.3498 - mae: 3.5760 - val_loss: 191.0677 - val_mae: 9.5829\n",
      "Epoch 792/1000\n",
      "1988/1988 [==============================] - 1s 669us/step - loss: 25.5842 - mae: 3.6049 - val_loss: 119.4443 - val_mae: 7.7906\n",
      "Epoch 793/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 26.6001 - mae: 3.6970 - val_loss: 181.1594 - val_mae: 9.8550\n",
      "Epoch 794/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 28.1311 - mae: 3.7860 - val_loss: 113.6056 - val_mae: 7.2394\n",
      "Epoch 795/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 24.3641 - mae: 3.5208 - val_loss: 170.4088 - val_mae: 9.2146\n",
      "Epoch 796/1000\n",
      "1988/1988 [==============================] - 1s 632us/step - loss: 24.8980 - mae: 3.5942 - val_loss: 182.0907 - val_mae: 9.0826\n",
      "Epoch 797/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 24.0149 - mae: 3.5214 - val_loss: 163.6720 - val_mae: 9.0070\n",
      "Epoch 798/1000\n",
      "1988/1988 [==============================] - 1s 671us/step - loss: 26.0545 - mae: 3.6814 - val_loss: 283.7836 - val_mae: 13.3421\n",
      "Epoch 799/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 25.1606 - mae: 3.6137 - val_loss: 148.0303 - val_mae: 8.3280\n",
      "Epoch 800/1000\n",
      "1988/1988 [==============================] - 1s 695us/step - loss: 26.4028 - mae: 3.6580 - val_loss: 156.3310 - val_mae: 8.7109\n",
      "Epoch 801/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 25.6126 - mae: 3.5277 - val_loss: 173.7213 - val_mae: 9.4971\n",
      "Epoch 802/1000\n",
      "1988/1988 [==============================] - 1s 683us/step - loss: 24.1327 - mae: 3.5150 - val_loss: 159.7370 - val_mae: 9.1805\n",
      "Epoch 803/1000\n",
      "1988/1988 [==============================] - 1s 703us/step - loss: 27.2260 - mae: 3.7537 - val_loss: 240.1932 - val_mae: 11.7058\n",
      "Epoch 804/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 24.3517 - mae: 3.5565 - val_loss: 207.1879 - val_mae: 10.5916\n",
      "Epoch 805/1000\n",
      "1988/1988 [==============================] - 1s 650us/step - loss: 22.4017 - mae: 3.4319 - val_loss: 180.0732 - val_mae: 9.4909\n",
      "Epoch 806/1000\n",
      "1988/1988 [==============================] - 1s 614us/step - loss: 28.0116 - mae: 3.8293 - val_loss: 223.0784 - val_mae: 11.0814\n",
      "Epoch 807/1000\n",
      "1988/1988 [==============================] - 1s 600us/step - loss: 25.0517 - mae: 3.5361 - val_loss: 207.5283 - val_mae: 10.2707\n",
      "Epoch 808/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 26.3297 - mae: 3.6774 - val_loss: 206.5233 - val_mae: 10.3473\n",
      "Epoch 809/1000\n",
      "1988/1988 [==============================] - 1s 614us/step - loss: 26.3308 - mae: 3.7068 - val_loss: 297.7128 - val_mae: 13.2062\n",
      "Epoch 810/1000\n",
      "1988/1988 [==============================] - 1s 615us/step - loss: 25.4424 - mae: 3.5833 - val_loss: 253.5066 - val_mae: 11.3210\n",
      "Epoch 811/1000\n",
      "1988/1988 [==============================] - 1s 636us/step - loss: 25.6867 - mae: 3.6814 - val_loss: 253.5566 - val_mae: 10.9654\n",
      "Epoch 812/1000\n",
      "1988/1988 [==============================] - 1s 637us/step - loss: 25.4566 - mae: 3.6010 - val_loss: 294.7591 - val_mae: 13.0418\n",
      "Epoch 813/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 26.6267 - mae: 3.6168 - val_loss: 246.4805 - val_mae: 10.9412\n",
      "Epoch 814/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 28.4043 - mae: 3.7927 - val_loss: 251.0104 - val_mae: 11.9087\n",
      "Epoch 815/1000\n",
      "1988/1988 [==============================] - 1s 630us/step - loss: 26.5298 - mae: 3.6016 - val_loss: 180.7361 - val_mae: 9.2131\n",
      "Epoch 816/1000\n",
      "1988/1988 [==============================] - 1s 626us/step - loss: 26.7326 - mae: 3.8003 - val_loss: 148.4354 - val_mae: 8.2382\n",
      "Epoch 817/1000\n",
      "1988/1988 [==============================] - 1s 615us/step - loss: 25.9947 - mae: 3.6030 - val_loss: 212.2567 - val_mae: 10.0092\n",
      "Epoch 818/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 25.3973 - mae: 3.6641 - val_loss: 181.7787 - val_mae: 9.3979\n",
      "Epoch 819/1000\n",
      "1988/1988 [==============================] - 1s 640us/step - loss: 25.9582 - mae: 3.7065 - val_loss: 234.3086 - val_mae: 11.8733\n",
      "Epoch 820/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 26.8500 - mae: 3.7476 - val_loss: 215.0562 - val_mae: 11.0604\n",
      "Epoch 821/1000\n",
      "1988/1988 [==============================] - 1s 672us/step - loss: 29.2304 - mae: 3.7784 - val_loss: 330.1878 - val_mae: 13.8699\n",
      "Epoch 822/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 23.2503 - mae: 3.4432 - val_loss: 309.4483 - val_mae: 13.6771\n",
      "Epoch 823/1000\n",
      "1988/1988 [==============================] - 1s 628us/step - loss: 24.5299 - mae: 3.5998 - val_loss: 175.3487 - val_mae: 9.0587\n",
      "Epoch 824/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 25.1508 - mae: 3.6719 - val_loss: 197.5910 - val_mae: 9.7908\n",
      "Epoch 825/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 25.4599 - mae: 3.6249 - val_loss: 266.4651 - val_mae: 11.5449\n",
      "Epoch 826/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 27.5177 - mae: 3.7655 - val_loss: 198.1542 - val_mae: 10.1075\n",
      "Epoch 827/1000\n",
      "1988/1988 [==============================] - 1s 617us/step - loss: 23.4997 - mae: 3.5111 - val_loss: 257.5953 - val_mae: 11.7354\n",
      "Epoch 828/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 25.4637 - mae: 3.6176 - val_loss: 299.2994 - val_mae: 13.1012\n",
      "Epoch 829/1000\n",
      "1988/1988 [==============================] - 1s 668us/step - loss: 27.8211 - mae: 3.7050 - val_loss: 267.5067 - val_mae: 12.9231\n",
      "Epoch 830/1000\n",
      "1988/1988 [==============================] - 1s 666us/step - loss: 25.8751 - mae: 3.6788 - val_loss: 240.4179 - val_mae: 10.8883\n",
      "Epoch 831/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 25.7909 - mae: 3.6572 - val_loss: 184.3696 - val_mae: 9.4904\n",
      "Epoch 832/1000\n",
      "1988/1988 [==============================] - 1s 677us/step - loss: 25.8928 - mae: 3.6379 - val_loss: 295.6016 - val_mae: 12.8601\n",
      "Epoch 833/1000\n",
      "1988/1988 [==============================] - 1s 617us/step - loss: 24.9541 - mae: 3.5521 - val_loss: 178.8383 - val_mae: 9.4488\n",
      "Epoch 834/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 26.5509 - mae: 3.6823 - val_loss: 179.6956 - val_mae: 9.8331\n",
      "Epoch 835/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 25.9099 - mae: 3.7148 - val_loss: 154.0087 - val_mae: 8.7414\n",
      "Epoch 836/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 24.4761 - mae: 3.5499 - val_loss: 176.7433 - val_mae: 9.2331\n",
      "Epoch 837/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 24.6696 - mae: 3.5715 - val_loss: 237.6604 - val_mae: 10.9862\n",
      "Epoch 838/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 24.4861 - mae: 3.4991 - val_loss: 334.6240 - val_mae: 15.0540\n",
      "Epoch 839/1000\n",
      "1988/1988 [==============================] - 1s 651us/step - loss: 26.2475 - mae: 3.6541 - val_loss: 259.3143 - val_mae: 12.0060\n",
      "Epoch 840/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 27.9425 - mae: 3.8816 - val_loss: 224.1666 - val_mae: 10.7949\n",
      "Epoch 841/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 26.4584 - mae: 3.7804 - val_loss: 284.6735 - val_mae: 12.8060\n",
      "Epoch 842/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 28.7755 - mae: 3.7688 - val_loss: 213.2819 - val_mae: 10.1882\n",
      "Epoch 843/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 25.4064 - mae: 3.5024 - val_loss: 274.2087 - val_mae: 11.7313\n",
      "Epoch 844/1000\n",
      "1988/1988 [==============================] - 1s 653us/step - loss: 24.9556 - mae: 3.6050 - val_loss: 288.9922 - val_mae: 13.2958\n",
      "Epoch 845/1000\n",
      "1988/1988 [==============================] - 1s 646us/step - loss: 24.2055 - mae: 3.4810 - val_loss: 156.8309 - val_mae: 8.9153\n",
      "Epoch 846/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 24.5288 - mae: 3.5779 - val_loss: 253.8400 - val_mae: 11.1778\n",
      "Epoch 847/1000\n",
      "1988/1988 [==============================] - 1s 677us/step - loss: 25.3809 - mae: 3.5256 - val_loss: 173.5224 - val_mae: 9.1722\n",
      "Epoch 848/1000\n",
      "1988/1988 [==============================] - 1s 701us/step - loss: 27.0919 - mae: 3.6388 - val_loss: 197.0472 - val_mae: 10.2227\n",
      "Epoch 849/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 24.3324 - mae: 3.5844 - val_loss: 208.4886 - val_mae: 10.1438\n",
      "Epoch 850/1000\n",
      "1988/1988 [==============================] - 1s 698us/step - loss: 23.9107 - mae: 3.4701 - val_loss: 239.0059 - val_mae: 11.0259\n",
      "Epoch 851/1000\n",
      "1988/1988 [==============================] - 1s 702us/step - loss: 28.1985 - mae: 3.8313 - val_loss: 184.2843 - val_mae: 9.5707\n",
      "Epoch 852/1000\n",
      "1988/1988 [==============================] - 1s 738us/step - loss: 26.1472 - mae: 3.6515 - val_loss: 210.9595 - val_mae: 10.2453\n",
      "Epoch 853/1000\n",
      "1988/1988 [==============================] - 1s 706us/step - loss: 26.5374 - mae: 3.6795 - val_loss: 307.4209 - val_mae: 13.3456\n",
      "Epoch 854/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 25.8169 - mae: 3.6614 - val_loss: 272.1176 - val_mae: 11.8792\n",
      "Epoch 855/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 1s 641us/step - loss: 26.1272 - mae: 3.7049 - val_loss: 180.6084 - val_mae: 9.4895\n",
      "Epoch 856/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 26.2754 - mae: 3.6797 - val_loss: 166.1318 - val_mae: 9.2047\n",
      "Epoch 857/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 27.7547 - mae: 3.7360 - val_loss: 177.7677 - val_mae: 9.4763\n",
      "Epoch 858/1000\n",
      "1988/1988 [==============================] - 1s 626us/step - loss: 26.7524 - mae: 3.7862 - val_loss: 264.8794 - val_mae: 11.5394\n",
      "Epoch 859/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 26.9875 - mae: 3.7282 - val_loss: 252.4124 - val_mae: 11.1755\n",
      "Epoch 860/1000\n",
      "1988/1988 [==============================] - 1s 635us/step - loss: 26.8818 - mae: 3.7302 - val_loss: 181.4444 - val_mae: 9.5766\n",
      "Epoch 861/1000\n",
      "1988/1988 [==============================] - 1s 648us/step - loss: 25.8626 - mae: 3.6069 - val_loss: 237.8332 - val_mae: 11.5356\n",
      "Epoch 862/1000\n",
      "1988/1988 [==============================] - 1s 658us/step - loss: 28.6088 - mae: 3.8398 - val_loss: 270.3069 - val_mae: 12.4555\n",
      "Epoch 863/1000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 27.5886 - mae: 3.6556 - val_loss: 153.7732 - val_mae: 8.6813\n",
      "Epoch 864/1000\n",
      "1988/1988 [==============================] - 1s 691us/step - loss: 26.3208 - mae: 3.7744 - val_loss: 111.1334 - val_mae: 7.4844\n",
      "Epoch 865/1000\n",
      "1988/1988 [==============================] - 1s 645us/step - loss: 24.9336 - mae: 3.6054 - val_loss: 137.8643 - val_mae: 8.0832\n",
      "Epoch 866/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 25.8803 - mae: 3.5608 - val_loss: 172.1533 - val_mae: 9.3443\n",
      "Epoch 867/1000\n",
      "1988/1988 [==============================] - 1s 714us/step - loss: 25.8311 - mae: 3.5251 - val_loss: 145.2331 - val_mae: 8.3576\n",
      "Epoch 868/1000\n",
      "1988/1988 [==============================] - 1s 682us/step - loss: 27.8636 - mae: 3.7134 - val_loss: 160.5179 - val_mae: 8.6728\n",
      "Epoch 869/1000\n",
      "1988/1988 [==============================] - 1s 671us/step - loss: 25.7869 - mae: 3.6395 - val_loss: 188.2422 - val_mae: 9.5359\n",
      "Epoch 870/1000\n",
      "1988/1988 [==============================] - 1s 699us/step - loss: 25.5939 - mae: 3.5644 - val_loss: 266.5761 - val_mae: 12.5407\n",
      "Epoch 871/1000\n",
      "1988/1988 [==============================] - 1s 697us/step - loss: 25.3942 - mae: 3.5870 - val_loss: 185.3607 - val_mae: 9.5716\n",
      "Epoch 872/1000\n",
      "1988/1988 [==============================] - 1s 687us/step - loss: 26.3050 - mae: 3.7035 - val_loss: 200.9457 - val_mae: 10.0684\n",
      "Epoch 873/1000\n",
      "1988/1988 [==============================] - 1s 689us/step - loss: 27.9728 - mae: 3.8835 - val_loss: 216.0028 - val_mae: 10.3255\n",
      "Epoch 874/1000\n",
      "1988/1988 [==============================] - 1s 628us/step - loss: 26.3215 - mae: 3.6926 - val_loss: 232.7460 - val_mae: 11.0719\n",
      "Epoch 875/1000\n",
      "1988/1988 [==============================] - 1s 620us/step - loss: 25.2756 - mae: 3.5211 - val_loss: 257.2215 - val_mae: 12.1137\n",
      "Epoch 876/1000\n",
      "1988/1988 [==============================] - 1s 651us/step - loss: 24.0946 - mae: 3.4954 - val_loss: 168.8101 - val_mae: 9.0869\n",
      "Epoch 877/1000\n",
      "1988/1988 [==============================] - 1s 635us/step - loss: 24.2678 - mae: 3.5692 - val_loss: 169.3123 - val_mae: 9.3957\n",
      "Epoch 878/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 24.0831 - mae: 3.5404 - val_loss: 164.4843 - val_mae: 8.7668\n",
      "Epoch 879/1000\n",
      "1988/1988 [==============================] - 1s 646us/step - loss: 22.6857 - mae: 3.4357 - val_loss: 225.9875 - val_mae: 11.3080\n",
      "Epoch 880/1000\n",
      "1988/1988 [==============================] - 1s 688us/step - loss: 27.7942 - mae: 3.6794 - val_loss: 219.0397 - val_mae: 10.8577\n",
      "Epoch 881/1000\n",
      "1988/1988 [==============================] - 1s 682us/step - loss: 27.5372 - mae: 3.6319 - val_loss: 179.4561 - val_mae: 9.7766\n",
      "Epoch 882/1000\n",
      "1988/1988 [==============================] - 1s 710us/step - loss: 24.8718 - mae: 3.4602 - val_loss: 242.4386 - val_mae: 11.4394\n",
      "Epoch 883/1000\n",
      "1988/1988 [==============================] - 1s 698us/step - loss: 26.5672 - mae: 3.6773 - val_loss: 270.8825 - val_mae: 12.0917\n",
      "Epoch 884/1000\n",
      "1988/1988 [==============================] - 1s 724us/step - loss: 30.6169 - mae: 4.0898 - val_loss: 224.5664 - val_mae: 10.7090\n",
      "Epoch 885/1000\n",
      "1988/1988 [==============================] - 1s 677us/step - loss: 26.3328 - mae: 3.7403 - val_loss: 340.0334 - val_mae: 14.2651\n",
      "Epoch 886/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 24.0193 - mae: 3.5422 - val_loss: 242.0367 - val_mae: 11.1577\n",
      "Epoch 887/1000\n",
      "1988/1988 [==============================] - 1s 663us/step - loss: 27.6286 - mae: 3.7563 - val_loss: 170.7953 - val_mae: 9.1896\n",
      "Epoch 888/1000\n",
      "1988/1988 [==============================] - 1s 717us/step - loss: 24.8166 - mae: 3.5348 - val_loss: 214.5418 - val_mae: 10.3731\n",
      "Epoch 889/1000\n",
      "1988/1988 [==============================] - 1s 694us/step - loss: 25.1259 - mae: 3.6065 - val_loss: 208.6002 - val_mae: 10.3340\n",
      "Epoch 890/1000\n",
      "1988/1988 [==============================] - 1s 701us/step - loss: 23.6520 - mae: 3.4709 - val_loss: 219.0012 - val_mae: 10.7494\n",
      "Epoch 891/1000\n",
      "1988/1988 [==============================] - 1s 695us/step - loss: 27.1740 - mae: 3.6763 - val_loss: 228.4688 - val_mae: 11.0265\n",
      "Epoch 892/1000\n",
      "1988/1988 [==============================] - 1s 679us/step - loss: 23.6430 - mae: 3.5046 - val_loss: 221.2639 - val_mae: 10.8016\n",
      "Epoch 893/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 26.2856 - mae: 3.5571 - val_loss: 266.8319 - val_mae: 12.2382\n",
      "Epoch 894/1000\n",
      "1988/1988 [==============================] - 1s 692us/step - loss: 24.0551 - mae: 3.5311 - val_loss: 299.6703 - val_mae: 13.2044\n",
      "Epoch 895/1000\n",
      "1988/1988 [==============================] - 1s 712us/step - loss: 26.0779 - mae: 3.6949 - val_loss: 267.2227 - val_mae: 12.1295\n",
      "Epoch 896/1000\n",
      "1988/1988 [==============================] - 1s 708us/step - loss: 28.3559 - mae: 3.7815 - val_loss: 146.3946 - val_mae: 9.0139\n",
      "Epoch 897/1000\n",
      "1988/1988 [==============================] - 1s 700us/step - loss: 26.3848 - mae: 3.6613 - val_loss: 228.2819 - val_mae: 10.6095\n",
      "Epoch 898/1000\n",
      "1988/1988 [==============================] - 1s 696us/step - loss: 28.2755 - mae: 3.7810 - val_loss: 234.9361 - val_mae: 11.1598\n",
      "Epoch 899/1000\n",
      "1988/1988 [==============================] - 1s 707us/step - loss: 27.4680 - mae: 3.7428 - val_loss: 202.8433 - val_mae: 10.0603\n",
      "Epoch 900/1000\n",
      "1988/1988 [==============================] - 1s 717us/step - loss: 26.0918 - mae: 3.7249 - val_loss: 195.7614 - val_mae: 10.0216\n",
      "Epoch 901/1000\n",
      "1988/1988 [==============================] - 1s 723us/step - loss: 25.4052 - mae: 3.6434 - val_loss: 214.3366 - val_mae: 10.1193\n",
      "Epoch 902/1000\n",
      "1988/1988 [==============================] - 1s 714us/step - loss: 25.8071 - mae: 3.6430 - val_loss: 276.8361 - val_mae: 12.1572\n",
      "Epoch 903/1000\n",
      "1988/1988 [==============================] - 1s 708us/step - loss: 26.0735 - mae: 3.6003 - val_loss: 234.5028 - val_mae: 11.1794\n",
      "Epoch 904/1000\n",
      "1988/1988 [==============================] - 1s 686us/step - loss: 25.0830 - mae: 3.5543 - val_loss: 214.9788 - val_mae: 10.3093\n",
      "Epoch 905/1000\n",
      "1988/1988 [==============================] - 1s 683us/step - loss: 26.6096 - mae: 3.6386 - val_loss: 328.7418 - val_mae: 13.5870\n",
      "Epoch 906/1000\n",
      "1988/1988 [==============================] - 1s 687us/step - loss: 26.5535 - mae: 3.5805 - val_loss: 192.1677 - val_mae: 9.5711\n",
      "Epoch 907/1000\n",
      "1988/1988 [==============================] - 1s 676us/step - loss: 25.2673 - mae: 3.5662 - val_loss: 226.9149 - val_mae: 10.9728\n",
      "Epoch 908/1000\n",
      "1988/1988 [==============================] - 1s 663us/step - loss: 24.1168 - mae: 3.5370 - val_loss: 157.9987 - val_mae: 8.9585\n",
      "Epoch 909/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 23.8249 - mae: 3.5304 - val_loss: 169.5799 - val_mae: 9.1289\n",
      "Epoch 910/1000\n",
      "1988/1988 [==============================] - 1s 651us/step - loss: 27.2606 - mae: 3.6534 - val_loss: 160.4901 - val_mae: 8.8090\n",
      "Epoch 911/1000\n",
      "1988/1988 [==============================] - 1s 664us/step - loss: 25.2985 - mae: 3.4889 - val_loss: 230.3279 - val_mae: 10.6256\n",
      "Epoch 912/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 24.8317 - mae: 3.5279 - val_loss: 194.0810 - val_mae: 9.7938\n",
      "Epoch 913/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 23.0955 - mae: 3.3983 - val_loss: 223.9497 - val_mae: 10.8722\n",
      "Epoch 914/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 25.3584 - mae: 3.5899 - val_loss: 159.9009 - val_mae: 8.9135\n",
      "Epoch 915/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 26.9482 - mae: 3.6360 - val_loss: 254.8313 - val_mae: 11.2522\n",
      "Epoch 916/1000\n",
      "1988/1988 [==============================] - 1s 654us/step - loss: 27.6864 - mae: 3.7985 - val_loss: 148.1595 - val_mae: 8.6921\n",
      "Epoch 917/1000\n",
      "1988/1988 [==============================] - 1s 695us/step - loss: 26.6821 - mae: 3.6151 - val_loss: 216.7640 - val_mae: 10.6424\n",
      "Epoch 918/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 24.3277 - mae: 3.5971 - val_loss: 178.9378 - val_mae: 9.8476\n",
      "Epoch 919/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 24.1770 - mae: 3.4850 - val_loss: 146.9582 - val_mae: 8.6214\n",
      "Epoch 920/1000\n",
      "1988/1988 [==============================] - 1s 713us/step - loss: 25.3118 - mae: 3.6003 - val_loss: 148.2939 - val_mae: 8.4723\n",
      "Epoch 921/1000\n",
      "1988/1988 [==============================] - 1s 693us/step - loss: 26.7204 - mae: 3.7429 - val_loss: 275.5494 - val_mae: 12.7937\n",
      "Epoch 922/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 24.3311 - mae: 3.6260 - val_loss: 267.3372 - val_mae: 11.9564\n",
      "Epoch 923/1000\n",
      "1988/1988 [==============================] - 1s 669us/step - loss: 28.6426 - mae: 3.8090 - val_loss: 176.1404 - val_mae: 9.1657\n",
      "Epoch 924/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 26.7006 - mae: 3.6583 - val_loss: 282.1422 - val_mae: 12.5578\n",
      "Epoch 925/1000\n",
      "1988/1988 [==============================] - 1s 670us/step - loss: 27.9601 - mae: 3.8014 - val_loss: 280.4901 - val_mae: 13.0874\n",
      "Epoch 926/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 25.9443 - mae: 3.6638 - val_loss: 180.9348 - val_mae: 9.2638\n",
      "Epoch 927/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 28.9964 - mae: 3.8070 - val_loss: 208.1996 - val_mae: 10.2749\n",
      "Epoch 928/1000\n",
      "1988/1988 [==============================] - 1s 637us/step - loss: 25.4870 - mae: 3.6636 - val_loss: 186.5847 - val_mae: 9.6130\n",
      "Epoch 929/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 27.3098 - mae: 3.7738 - val_loss: 195.1762 - val_mae: 10.0838\n",
      "Epoch 930/1000\n",
      "1988/1988 [==============================] - 1s 633us/step - loss: 29.1569 - mae: 3.8993 - val_loss: 257.0658 - val_mae: 11.7053\n",
      "Epoch 931/1000\n",
      "1988/1988 [==============================] - 1s 657us/step - loss: 29.7919 - mae: 3.8829 - val_loss: 235.4313 - val_mae: 11.6673\n",
      "Epoch 932/1000\n",
      "1988/1988 [==============================] - 1s 692us/step - loss: 27.7083 - mae: 3.7635 - val_loss: 168.1890 - val_mae: 8.9554\n",
      "Epoch 933/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 27.5062 - mae: 3.8438 - val_loss: 229.4151 - val_mae: 10.4627\n",
      "Epoch 934/1000\n",
      "1988/1988 [==============================] - 1s 692us/step - loss: 27.5615 - mae: 3.8625 - val_loss: 222.4151 - val_mae: 10.2610\n",
      "Epoch 935/1000\n",
      "1988/1988 [==============================] - 1s 673us/step - loss: 28.3837 - mae: 3.8799 - val_loss: 308.5972 - val_mae: 14.3014\n",
      "Epoch 936/1000\n",
      "1988/1988 [==============================] - 1s 689us/step - loss: 26.9071 - mae: 3.8556 - val_loss: 237.7125 - val_mae: 10.9250\n",
      "Epoch 937/1000\n",
      "1988/1988 [==============================] - 1s 695us/step - loss: 30.5652 - mae: 3.9993 - val_loss: 294.4383 - val_mae: 11.9150\n",
      "Epoch 938/1000\n",
      "1988/1988 [==============================] - 1s 675us/step - loss: 28.6068 - mae: 3.8383 - val_loss: 147.2989 - val_mae: 8.5947\n",
      "Epoch 939/1000\n",
      "1988/1988 [==============================] - 1s 691us/step - loss: 25.7118 - mae: 3.6472 - val_loss: 163.3013 - val_mae: 9.1932\n",
      "Epoch 940/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 24.4881 - mae: 3.5919 - val_loss: 341.0824 - val_mae: 14.2337\n",
      "Epoch 941/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 26.4164 - mae: 3.7236 - val_loss: 306.6897 - val_mae: 13.3463\n",
      "Epoch 942/1000\n",
      "1988/1988 [==============================] - 1s 618us/step - loss: 25.3728 - mae: 3.6632 - val_loss: 250.8262 - val_mae: 12.0011\n",
      "Epoch 943/1000\n",
      "1988/1988 [==============================] - 1s 619us/step - loss: 26.5492 - mae: 3.6820 - val_loss: 355.5107 - val_mae: 14.5442\n",
      "Epoch 944/1000\n",
      "1988/1988 [==============================] - 1s 617us/step - loss: 26.1111 - mae: 3.6574 - val_loss: 202.3654 - val_mae: 9.9596\n",
      "Epoch 945/1000\n",
      "1988/1988 [==============================] - 1s 669us/step - loss: 25.7487 - mae: 3.6430 - val_loss: 279.9816 - val_mae: 12.6931\n",
      "Epoch 946/1000\n",
      "1988/1988 [==============================] - 1s 649us/step - loss: 27.1480 - mae: 3.8147 - val_loss: 175.7815 - val_mae: 9.6755\n",
      "Epoch 947/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 26.6880 - mae: 3.7244 - val_loss: 176.7936 - val_mae: 9.2937\n",
      "Epoch 948/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 26.7840 - mae: 3.7098 - val_loss: 146.0297 - val_mae: 8.4021\n",
      "Epoch 949/1000\n",
      "1988/1988 [==============================] - 1s 612us/step - loss: 23.7338 - mae: 3.4846 - val_loss: 198.7605 - val_mae: 10.1047\n",
      "Epoch 950/1000\n",
      "1988/1988 [==============================] - 1s 609us/step - loss: 25.0334 - mae: 3.5627 - val_loss: 236.8074 - val_mae: 10.8900\n",
      "Epoch 951/1000\n",
      "1988/1988 [==============================] - 1s 636us/step - loss: 26.3625 - mae: 3.7430 - val_loss: 242.0314 - val_mae: 12.4132\n",
      "Epoch 952/1000\n",
      "1988/1988 [==============================] - 1s 609us/step - loss: 23.2004 - mae: 3.4269 - val_loss: 195.0747 - val_mae: 10.4557\n",
      "Epoch 953/1000\n",
      "1988/1988 [==============================] - 1s 610us/step - loss: 23.0416 - mae: 3.3845 - val_loss: 253.0067 - val_mae: 11.7866\n",
      "Epoch 954/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 25.6275 - mae: 3.5272 - val_loss: 276.8311 - val_mae: 13.3572\n",
      "Epoch 955/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 26.6523 - mae: 3.7960 - val_loss: 203.9458 - val_mae: 10.0699\n",
      "Epoch 956/1000\n",
      "1988/1988 [==============================] - 1s 663us/step - loss: 25.7725 - mae: 3.7575 - val_loss: 200.8112 - val_mae: 10.0330\n",
      "Epoch 957/1000\n",
      "1988/1988 [==============================] - 1s 630us/step - loss: 28.9116 - mae: 3.7707 - val_loss: 285.9781 - val_mae: 12.4216\n",
      "Epoch 958/1000\n",
      "1988/1988 [==============================] - 1s 651us/step - loss: 25.1332 - mae: 3.6008 - val_loss: 371.0340 - val_mae: 15.5069\n",
      "Epoch 959/1000\n",
      "1988/1988 [==============================] - 1s 642us/step - loss: 24.8200 - mae: 3.5315 - val_loss: 373.2058 - val_mae: 15.5701\n",
      "Epoch 960/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 24.9320 - mae: 3.6267 - val_loss: 175.8595 - val_mae: 9.4290\n",
      "Epoch 961/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 26.6713 - mae: 3.6937 - val_loss: 135.3639 - val_mae: 8.3148\n",
      "Epoch 962/1000\n",
      "1988/1988 [==============================] - 1s 625us/step - loss: 27.7714 - mae: 3.7108 - val_loss: 341.9085 - val_mae: 15.1257\n",
      "Epoch 963/1000\n",
      "1988/1988 [==============================] - 1s 622us/step - loss: 24.5077 - mae: 3.5808 - val_loss: 268.7791 - val_mae: 12.2092\n",
      "Epoch 964/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 25.8746 - mae: 3.6532 - val_loss: 230.0017 - val_mae: 11.0971\n",
      "Epoch 965/1000\n",
      "1988/1988 [==============================] - 1s 652us/step - loss: 27.2333 - mae: 3.6903 - val_loss: 225.6037 - val_mae: 10.6185\n",
      "Epoch 966/1000\n",
      "1988/1988 [==============================] - 1s 651us/step - loss: 24.7008 - mae: 3.6057 - val_loss: 162.1641 - val_mae: 9.2204\n",
      "Epoch 967/1000\n",
      "1988/1988 [==============================] - 1s 599us/step - loss: 25.9302 - mae: 3.7032 - val_loss: 179.3990 - val_mae: 9.5373\n",
      "Epoch 968/1000\n",
      "1988/1988 [==============================] - 1s 639us/step - loss: 24.9752 - mae: 3.5658 - val_loss: 157.2122 - val_mae: 9.0487\n",
      "Epoch 969/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 1s 649us/step - loss: 24.5455 - mae: 3.5782 - val_loss: 166.6593 - val_mae: 9.3393\n",
      "Epoch 970/1000\n",
      "1988/1988 [==============================] - 1s 644us/step - loss: 26.8490 - mae: 3.6512 - val_loss: 293.5001 - val_mae: 13.2824\n",
      "Epoch 971/1000\n",
      "1988/1988 [==============================] - 1s 656us/step - loss: 25.6852 - mae: 3.5566 - val_loss: 216.6001 - val_mae: 10.8849\n",
      "Epoch 972/1000\n",
      "1988/1988 [==============================] - 1s 684us/step - loss: 25.4197 - mae: 3.6130 - val_loss: 173.8107 - val_mae: 9.3701\n",
      "Epoch 973/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 28.3222 - mae: 3.9122 - val_loss: 230.4433 - val_mae: 11.7183\n",
      "Epoch 974/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 26.4097 - mae: 3.7726 - val_loss: 253.7423 - val_mae: 11.6553\n",
      "Epoch 975/1000\n",
      "1988/1988 [==============================] - 1s 662us/step - loss: 27.8410 - mae: 3.7832 - val_loss: 224.8481 - val_mae: 11.0957\n",
      "Epoch 976/1000\n",
      "1988/1988 [==============================] - 1s 695us/step - loss: 24.9953 - mae: 3.5669 - val_loss: 317.3293 - val_mae: 14.0700\n",
      "Epoch 977/1000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 26.1662 - mae: 3.5440 - val_loss: 300.0052 - val_mae: 13.2166\n",
      "Epoch 978/1000\n",
      "1988/1988 [==============================] - 1s 694us/step - loss: 26.6120 - mae: 3.7082 - val_loss: 317.5938 - val_mae: 14.5294\n",
      "Epoch 979/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 26.3622 - mae: 3.6964 - val_loss: 166.2785 - val_mae: 9.1382\n",
      "Epoch 980/1000\n",
      "1988/1988 [==============================] - 1s 685us/step - loss: 26.2153 - mae: 3.6164 - val_loss: 206.3819 - val_mae: 10.3790\n",
      "Epoch 981/1000\n",
      "1988/1988 [==============================] - 1s 655us/step - loss: 25.6026 - mae: 3.5536 - val_loss: 196.5830 - val_mae: 10.2188\n",
      "Epoch 982/1000\n",
      "1988/1988 [==============================] - 1s 607us/step - loss: 24.8450 - mae: 3.5368 - val_loss: 300.1508 - val_mae: 13.6710\n",
      "Epoch 983/1000\n",
      "1988/1988 [==============================] - 1s 630us/step - loss: 22.9467 - mae: 3.4603 - val_loss: 239.5385 - val_mae: 11.6902\n",
      "Epoch 984/1000\n",
      "1988/1988 [==============================] - 1s 616us/step - loss: 25.8142 - mae: 3.5438 - val_loss: 251.2317 - val_mae: 11.7471\n",
      "Epoch 985/1000\n",
      "1988/1988 [==============================] - 1s 612us/step - loss: 24.2264 - mae: 3.5087 - val_loss: 214.1193 - val_mae: 11.0178\n",
      "Epoch 986/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 25.9521 - mae: 3.5546 - val_loss: 242.5827 - val_mae: 11.6764\n",
      "Epoch 987/1000\n",
      "1988/1988 [==============================] - 1s 648us/step - loss: 23.3672 - mae: 3.4076 - val_loss: 263.1077 - val_mae: 12.3246\n",
      "Epoch 988/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 24.2022 - mae: 3.5154 - val_loss: 230.5092 - val_mae: 11.2865\n",
      "Epoch 989/1000\n",
      "1988/1988 [==============================] - 1s 629us/step - loss: 23.7121 - mae: 3.4481 - val_loss: 247.1583 - val_mae: 11.7172\n",
      "Epoch 990/1000\n",
      "1988/1988 [==============================] - 1s 624us/step - loss: 26.5540 - mae: 3.6747 - val_loss: 315.1727 - val_mae: 13.7860\n",
      "Epoch 991/1000\n",
      "1988/1988 [==============================] - 1s 638us/step - loss: 27.7509 - mae: 3.7138 - val_loss: 188.2204 - val_mae: 9.7646\n",
      "Epoch 992/1000\n",
      "1988/1988 [==============================] - 1s 667us/step - loss: 25.2401 - mae: 3.5635 - val_loss: 232.6913 - val_mae: 11.4289\n",
      "Epoch 993/1000\n",
      "1988/1988 [==============================] - 1s 659us/step - loss: 25.5566 - mae: 3.6442 - val_loss: 249.5133 - val_mae: 11.8189\n",
      "Epoch 994/1000\n",
      "1988/1988 [==============================] - 1s 678us/step - loss: 28.5385 - mae: 3.7459 - val_loss: 207.2553 - val_mae: 10.2331\n",
      "Epoch 995/1000\n",
      "1988/1988 [==============================] - 1s 680us/step - loss: 25.5009 - mae: 3.6823 - val_loss: 184.9987 - val_mae: 9.8621\n",
      "Epoch 996/1000\n",
      "1988/1988 [==============================] - 1s 738us/step - loss: 26.9728 - mae: 3.6249 - val_loss: 293.9354 - val_mae: 12.8024\n",
      "Epoch 997/1000\n",
      "1988/1988 [==============================] - 1s 683us/step - loss: 26.9579 - mae: 3.6307 - val_loss: 148.2186 - val_mae: 8.6455\n",
      "Epoch 998/1000\n",
      "1988/1988 [==============================] - 1s 672us/step - loss: 24.9379 - mae: 3.4894 - val_loss: 415.9942 - val_mae: 15.5449\n",
      "Epoch 999/1000\n",
      "1988/1988 [==============================] - 1s 643us/step - loss: 24.1270 - mae: 3.4381 - val_loss: 302.6127 - val_mae: 12.6144\n",
      "Epoch 1000/1000\n",
      "1988/1988 [==============================] - 1s 671us/step - loss: 25.5572 - mae: 3.5393 - val_loss: 258.1202 - val_mae: 12.0735\n",
      "\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 228us/step\n",
      "test loss, test acc: [241.17076233455114, 11.515603065490723]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 15.257412144449212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>true_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171.568954</td>\n",
       "      <td>185.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181.986511</td>\n",
       "      <td>176.979996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169.149536</td>\n",
       "      <td>176.779999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.831696</td>\n",
       "      <td>172.289993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>162.193817</td>\n",
       "      <td>174.240005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>219.126175</td>\n",
       "      <td>259.429993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>218.255234</td>\n",
       "      <td>260.140015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>221.194626</td>\n",
       "      <td>262.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>219.814133</td>\n",
       "      <td>261.959991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>222.475754</td>\n",
       "      <td>264.470001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  true_value\n",
       "0    171.568954  185.860001\n",
       "1    181.986511  176.979996\n",
       "2    169.149536  176.779999\n",
       "3    165.831696  172.289993\n",
       "4    162.193817  174.240005\n",
       "..          ...         ...\n",
       "240  219.126175  259.429993\n",
       "241  218.255234  260.140015\n",
       "242  221.194626  262.200012\n",
       "243  219.814133  261.959991\n",
       "244  222.475754  264.470001\n",
       "\n",
       "[245 rows x 2 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_2stacks_true_value, 3, \n",
    "                                                    stock_with_abs_norm, label_value_1d, 64, batch_size, \"loss\")\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=7)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=7)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)\n",
    "predictFrame = pd.DataFrame({'prediction': predictions.reshape(X_test.shape[0]), 'true_value': y_test.reshape(X_test.shape[0])})\n",
    "predictFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = predicted_train.reshape(X_train.shape[0]).tolist()\n",
    "b = predicted_valid.reshape(X_valid.shape[0]).tolist()\n",
    "c = predicted_test.reshape(X_test.shape[0]).tolist()\n",
    "d = y_train.reshape(X_train.shape[0]).tolist()\n",
    "e = y_valid.reshape(X_valid.shape[0]).tolist()\n",
    "f = y_test.reshape(X_test.shape[0]).tolist()\n",
    "a.extend(b)\n",
    "a.extend(c)\n",
    "d.extend(e)\n",
    "d.extend(f)\n",
    "predictFrame = pd.DataFrame({'prediction': a, 'true_value': d})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1981 samples, validate on 217 samples\n",
      "Epoch 1/1000\n",
      "1981/1981 [==============================] - 3s 2ms/step - loss: 663.2203 - mae: 22.2123 - val_loss: 2850.8781 - val_mae: 49.3780\n",
      "Epoch 2/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1475.5207 - mae: 28.4040 - val_loss: 4097.5241 - val_mae: 60.7029\n",
      "Epoch 3/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1411.2855 - mae: 28.2517 - val_loss: 4630.1013 - val_mae: 64.9416\n",
      "Epoch 4/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1379.0412 - mae: 28.3599 - val_loss: 4764.7541 - val_mae: 65.9702\n",
      "Epoch 5/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1400.8009 - mae: 28.6247 - val_loss: 4812.9401 - val_mae: 66.3344\n",
      "Epoch 6/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1391.5443 - mae: 28.6669 - val_loss: 4885.1469 - val_mae: 66.8764\n",
      "Epoch 7/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1402.1494 - mae: 28.7266 - val_loss: 4859.3853 - val_mae: 66.6835\n",
      "Epoch 8/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1385.1389 - mae: 28.4368 - val_loss: 4864.2716 - val_mae: 66.7201\n",
      "Epoch 9/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1381.8914 - mae: 28.5655 - val_loss: 4867.1187 - val_mae: 66.7415\n",
      "Epoch 10/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1343.6220 - mae: 28.2225 - val_loss: 4765.6846 - val_mae: 65.9774\n",
      "Epoch 11/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1310.0408 - mae: 27.0295 - val_loss: 5134.4870 - val_mae: 68.7154\n",
      "Epoch 12/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1258.9613 - mae: 26.5420 - val_loss: 5241.7323 - val_mae: 69.4915\n",
      "Epoch 13/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 780.7076 - mae: 20.4508 - val_loss: 3689.7223 - val_mae: 57.2589\n",
      "Epoch 14/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 730.9872 - mae: 19.4260 - val_loss: 3059.0508 - val_mae: 51.4567\n",
      "Epoch 15/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 575.7724 - mae: 17.8858 - val_loss: 2584.2421 - val_mae: 46.6527\n",
      "Epoch 16/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 448.0240 - mae: 16.6108 - val_loss: 1872.7418 - val_mae: 38.2734\n",
      "Epoch 17/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 339.7461 - mae: 14.6749 - val_loss: 1678.8229 - val_mae: 35.7083\n",
      "Epoch 18/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 293.2661 - mae: 13.6901 - val_loss: 1564.6088 - val_mae: 34.0929\n",
      "Epoch 19/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 260.4006 - mae: 12.8471 - val_loss: 1347.4174 - val_mae: 30.9068\n",
      "Epoch 20/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 260.5784 - mae: 12.7132 - val_loss: 1409.3212 - val_mae: 32.0797\n",
      "Epoch 21/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 227.4908 - mae: 11.6555 - val_loss: 1405.7935 - val_mae: 32.1754\n",
      "Epoch 22/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 226.9258 - mae: 11.6472 - val_loss: 1616.3182 - val_mae: 35.3487\n",
      "Epoch 23/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 219.2250 - mae: 11.4587 - val_loss: 1236.1681 - val_mae: 29.5640\n",
      "Epoch 24/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 194.2354 - mae: 10.8646 - val_loss: 1399.7344 - val_mae: 32.3486\n",
      "Epoch 25/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 178.4663 - mae: 10.2337 - val_loss: 1185.6711 - val_mae: 29.0644\n",
      "Epoch 26/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 183.1813 - mae: 10.3684 - val_loss: 1197.5828 - val_mae: 29.5145\n",
      "Epoch 27/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 199.4049 - mae: 10.6699 - val_loss: 1340.8694 - val_mae: 31.9331\n",
      "Epoch 28/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 180.8240 - mae: 10.3368 - val_loss: 1139.2044 - val_mae: 28.5544\n",
      "Epoch 29/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 196.9745 - mae: 10.6545 - val_loss: 878.2887 - val_mae: 23.6370\n",
      "Epoch 30/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 174.9947 - mae: 10.2356 - val_loss: 1021.2513 - val_mae: 26.6202\n",
      "Epoch 31/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 166.6119 - mae: 9.8277 - val_loss: 891.4651 - val_mae: 24.2611\n",
      "Epoch 32/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 174.8434 - mae: 9.9361 - val_loss: 984.0067 - val_mae: 26.2440\n",
      "Epoch 33/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 181.4201 - mae: 10.2318 - val_loss: 1286.1068 - val_mae: 31.7714\n",
      "Epoch 34/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 187.0441 - mae: 10.4912 - val_loss: 1016.1466 - val_mae: 27.0658\n",
      "Epoch 35/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 186.8843 - mae: 10.4439 - val_loss: 1077.1819 - val_mae: 28.0889\n",
      "Epoch 36/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 174.7915 - mae: 10.0311 - val_loss: 634.5673 - val_mae: 19.2786\n",
      "Epoch 37/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 164.8730 - mae: 9.7545 - val_loss: 689.3436 - val_mae: 20.4719\n",
      "Epoch 38/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 169.4574 - mae: 9.7228 - val_loss: 961.1062 - val_mae: 26.1960\n",
      "Epoch 39/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 166.0744 - mae: 9.6994 - val_loss: 827.3240 - val_mae: 23.9525\n",
      "Epoch 40/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 182.4073 - mae: 10.3428 - val_loss: 587.6028 - val_mae: 18.6828\n",
      "Epoch 41/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 160.0041 - mae: 9.5801 - val_loss: 1075.8329 - val_mae: 29.0424\n",
      "Epoch 42/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 171.6032 - mae: 9.8571 - val_loss: 763.0780 - val_mae: 22.9045\n",
      "Epoch 43/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 165.1181 - mae: 9.7391 - val_loss: 659.6758 - val_mae: 20.3220\n",
      "Epoch 44/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 161.5886 - mae: 9.5070 - val_loss: 821.0189 - val_mae: 23.6591\n",
      "Epoch 45/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 155.8482 - mae: 9.3185 - val_loss: 1024.8044 - val_mae: 27.7176\n",
      "Epoch 46/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 160.6579 - mae: 9.5235 - val_loss: 921.5247 - val_mae: 25.6516\n",
      "Epoch 47/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 151.4036 - mae: 9.3033 - val_loss: 798.2112 - val_mae: 23.2964\n",
      "Epoch 48/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 146.5737 - mae: 9.0394 - val_loss: 791.1412 - val_mae: 23.1433\n",
      "Epoch 49/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 140.7444 - mae: 8.9398 - val_loss: 736.2145 - val_mae: 21.9789\n",
      "Epoch 50/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 139.0053 - mae: 8.7340 - val_loss: 720.1614 - val_mae: 21.4536\n",
      "Epoch 51/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 131.7686 - mae: 8.6530 - val_loss: 688.7124 - val_mae: 21.4118\n",
      "Epoch 52/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 128.4561 - mae: 8.4492 - val_loss: 604.3473 - val_mae: 19.8433\n",
      "Epoch 53/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 130.0932 - mae: 8.5134 - val_loss: 365.2299 - val_mae: 14.3668\n",
      "Epoch 54/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 127.0531 - mae: 8.2799 - val_loss: 668.1539 - val_mae: 20.7312\n",
      "Epoch 55/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 127.3873 - mae: 8.3284 - val_loss: 521.0019 - val_mae: 17.5554\n",
      "Epoch 56/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 115.6437 - mae: 8.1677 - val_loss: 531.5700 - val_mae: 18.0844\n",
      "Epoch 57/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 124.2847 - mae: 8.2609 - val_loss: 749.5982 - val_mae: 23.1031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 116.5768 - mae: 8.0689 - val_loss: 537.4819 - val_mae: 18.0628\n",
      "Epoch 59/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 115.8295 - mae: 8.0119 - val_loss: 465.6203 - val_mae: 16.3227\n",
      "Epoch 60/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 106.2283 - mae: 7.5754 - val_loss: 517.8536 - val_mae: 18.0438\n",
      "Epoch 61/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 104.6462 - mae: 7.4519 - val_loss: 349.5784 - val_mae: 14.0000\n",
      "Epoch 62/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 100.8286 - mae: 7.4681 - val_loss: 435.0175 - val_mae: 15.7608\n",
      "Epoch 63/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 101.4110 - mae: 7.5554 - val_loss: 385.0962 - val_mae: 14.7579\n",
      "Epoch 64/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 98.3292 - mae: 7.5070 - val_loss: 401.8431 - val_mae: 15.1663\n",
      "Epoch 65/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 99.1222 - mae: 7.3300 - val_loss: 465.1540 - val_mae: 16.8523\n",
      "Epoch 66/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 91.0344 - mae: 7.0659 - val_loss: 409.6870 - val_mae: 15.5512\n",
      "Epoch 67/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 93.7336 - mae: 7.2638 - val_loss: 369.3403 - val_mae: 14.5467\n",
      "Epoch 68/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 88.1319 - mae: 7.2664 - val_loss: 450.4491 - val_mae: 16.4779\n",
      "Epoch 69/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 90.9779 - mae: 7.0685 - val_loss: 416.9545 - val_mae: 15.5966\n",
      "Epoch 70/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 82.6212 - mae: 6.7388 - val_loss: 376.4871 - val_mae: 14.6099\n",
      "Epoch 71/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 76.6344 - mae: 6.5400 - val_loss: 498.0274 - val_mae: 17.7946\n",
      "Epoch 72/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 84.7624 - mae: 6.8561 - val_loss: 334.6342 - val_mae: 13.6790\n",
      "Epoch 73/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 76.7009 - mae: 6.6606 - val_loss: 485.9218 - val_mae: 17.9815\n",
      "Epoch 74/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 78.8666 - mae: 6.5039 - val_loss: 369.3459 - val_mae: 14.8537\n",
      "Epoch 75/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 71.4302 - mae: 6.3678 - val_loss: 295.6374 - val_mae: 12.8826\n",
      "Epoch 76/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 79.9987 - mae: 6.6022 - val_loss: 438.6755 - val_mae: 16.5367\n",
      "Epoch 77/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 75.4208 - mae: 6.5692 - val_loss: 375.6457 - val_mae: 14.8754\n",
      "Epoch 78/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 70.1020 - mae: 6.2586 - val_loss: 453.6375 - val_mae: 17.0038\n",
      "Epoch 79/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 71.8577 - mae: 6.3495 - val_loss: 330.7166 - val_mae: 13.6944\n",
      "Epoch 80/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 63.9725 - mae: 6.0354 - val_loss: 373.8333 - val_mae: 14.8328\n",
      "Epoch 81/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 73.6106 - mae: 6.3974 - val_loss: 367.4241 - val_mae: 14.7093\n",
      "Epoch 82/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 66.0200 - mae: 6.1266 - val_loss: 582.4137 - val_mae: 18.4711\n",
      "Epoch 83/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 63.8457 - mae: 6.0425 - val_loss: 532.3873 - val_mae: 17.4062\n",
      "Epoch 84/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 63.9233 - mae: 5.9476 - val_loss: 530.9562 - val_mae: 17.4488\n",
      "Epoch 85/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 66.8253 - mae: 6.0920 - val_loss: 525.9534 - val_mae: 17.3542\n",
      "Epoch 86/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 62.1759 - mae: 5.9290 - val_loss: 735.4891 - val_mae: 22.5540\n",
      "Epoch 87/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 68.9475 - mae: 6.3301 - val_loss: 485.9271 - val_mae: 16.5672\n",
      "Epoch 88/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 57.1979 - mae: 5.8174 - val_loss: 591.0736 - val_mae: 19.1450\n",
      "Epoch 89/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 61.1658 - mae: 5.8560 - val_loss: 458.9277 - val_mae: 15.9625\n",
      "Epoch 90/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 58.4976 - mae: 5.8464 - val_loss: 408.4078 - val_mae: 14.8781\n",
      "Epoch 91/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 57.7497 - mae: 5.6997 - val_loss: 582.4361 - val_mae: 19.1262\n",
      "Epoch 92/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 51.7890 - mae: 5.4951 - val_loss: 465.8413 - val_mae: 16.2491\n",
      "Epoch 93/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 52.9298 - mae: 5.5604 - val_loss: 438.2352 - val_mae: 15.5885\n",
      "Epoch 94/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 57.1059 - mae: 5.6649 - val_loss: 532.7740 - val_mae: 17.8240\n",
      "Epoch 95/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 50.9916 - mae: 5.3941 - val_loss: 389.5541 - val_mae: 14.4665\n",
      "Epoch 96/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 56.3988 - mae: 5.6703 - val_loss: 417.1611 - val_mae: 15.2791\n",
      "Epoch 97/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 53.7428 - mae: 5.5221 - val_loss: 428.7403 - val_mae: 15.6209\n",
      "Epoch 98/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 54.1467 - mae: 5.3858 - val_loss: 514.4858 - val_mae: 18.0227\n",
      "Epoch 99/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 48.9238 - mae: 5.2493 - val_loss: 450.3494 - val_mae: 16.1332\n",
      "Epoch 100/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 51.1794 - mae: 5.4963 - val_loss: 379.2181 - val_mae: 14.3890\n",
      "Epoch 101/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 49.8169 - mae: 5.3536 - val_loss: 457.3692 - val_mae: 16.5569\n",
      "Epoch 102/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 50.5378 - mae: 5.4467 - val_loss: 429.9411 - val_mae: 15.6013\n",
      "Epoch 103/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 48.2347 - mae: 5.3505 - val_loss: 601.3893 - val_mae: 19.9754\n",
      "Epoch 104/1000\n",
      "1981/1981 [==============================] - 2s 986us/step - loss: 53.0879 - mae: 5.6287 - val_loss: 549.3391 - val_mae: 18.3589\n",
      "Epoch 105/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 53.1443 - mae: 5.4979 - val_loss: 397.5549 - val_mae: 14.7129\n",
      "Epoch 106/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 53.8087 - mae: 5.4933 - val_loss: 462.6688 - val_mae: 16.4862\n",
      "Epoch 107/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 49.6827 - mae: 5.2556 - val_loss: 557.3834 - val_mae: 19.0205\n",
      "Epoch 108/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 50.3244 - mae: 5.2795 - val_loss: 418.9532 - val_mae: 15.4441\n",
      "Epoch 109/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 47.4192 - mae: 5.1494 - val_loss: 327.4473 - val_mae: 13.1689\n",
      "Epoch 110/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 44.2392 - mae: 5.0433 - val_loss: 402.4136 - val_mae: 15.0322\n",
      "Epoch 111/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 47.3881 - mae: 5.1793 - val_loss: 292.1177 - val_mae: 12.4615\n",
      "Epoch 112/1000\n",
      "1981/1981 [==============================] - 2s 996us/step - loss: 47.0227 - mae: 5.0834 - val_loss: 336.6799 - val_mae: 13.4166\n",
      "Epoch 113/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 44.9226 - mae: 4.9740 - val_loss: 395.7554 - val_mae: 14.9426\n",
      "Epoch 114/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 44.3266 - mae: 4.9159 - val_loss: 385.9468 - val_mae: 14.5336\n",
      "Epoch 115/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.4164 - mae: 4.9920 - val_loss: 466.5053 - val_mae: 16.3712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.5786 - mae: 4.9850 - val_loss: 276.7429 - val_mae: 12.2578\n",
      "Epoch 117/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 45.6370 - mae: 5.1023 - val_loss: 247.8916 - val_mae: 11.5401\n",
      "Epoch 118/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.6824 - mae: 4.8490 - val_loss: 386.8147 - val_mae: 14.9932\n",
      "Epoch 119/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 42.8442 - mae: 4.8960 - val_loss: 262.3700 - val_mae: 11.8716\n",
      "Epoch 120/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 45.5441 - mae: 5.0368 - val_loss: 412.0987 - val_mae: 15.2882\n",
      "Epoch 121/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 46.1828 - mae: 5.0032 - val_loss: 410.5527 - val_mae: 15.0239\n",
      "Epoch 122/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 47.7101 - mae: 5.0860 - val_loss: 496.7518 - val_mae: 17.7634\n",
      "Epoch 123/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 45.2390 - mae: 4.9252 - val_loss: 373.9288 - val_mae: 14.4951\n",
      "Epoch 124/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 44.1905 - mae: 4.9659 - val_loss: 417.6836 - val_mae: 15.9874\n",
      "Epoch 125/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.7857 - mae: 4.6482 - val_loss: 269.5142 - val_mae: 12.0222\n",
      "Epoch 126/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 48.3681 - mae: 5.0619 - val_loss: 310.7384 - val_mae: 13.1005\n",
      "Epoch 127/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.7547 - mae: 4.9021 - val_loss: 298.9250 - val_mae: 13.1977\n",
      "Epoch 128/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 42.0257 - mae: 4.8868 - val_loss: 394.4213 - val_mae: 15.7977\n",
      "Epoch 129/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 45.6132 - mae: 4.9822 - val_loss: 259.0257 - val_mae: 11.9356\n",
      "Epoch 130/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.0994 - mae: 4.8387 - val_loss: 291.9303 - val_mae: 12.8364\n",
      "Epoch 131/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.1233 - mae: 4.8839 - val_loss: 530.7045 - val_mae: 19.3694\n",
      "Epoch 132/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 47.8246 - mae: 5.0040 - val_loss: 507.2914 - val_mae: 17.9866\n",
      "Epoch 133/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 63.4433 - mae: 6.3004 - val_loss: 267.0089 - val_mae: 12.0153\n",
      "Epoch 134/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 44.2278 - mae: 4.8391 - val_loss: 278.0071 - val_mae: 12.3026\n",
      "Epoch 135/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.0645 - mae: 4.6504 - val_loss: 313.8219 - val_mae: 13.4317\n",
      "Epoch 136/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.4152 - mae: 4.8019 - val_loss: 279.7955 - val_mae: 12.4695\n",
      "Epoch 137/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.6811 - mae: 4.8852 - val_loss: 276.3646 - val_mae: 12.5220\n",
      "Epoch 138/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.4734 - mae: 4.4844 - val_loss: 239.6514 - val_mae: 11.4452\n",
      "Epoch 139/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.2080 - mae: 4.6990 - val_loss: 479.6481 - val_mae: 18.0722\n",
      "Epoch 140/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.8067 - mae: 4.6189 - val_loss: 376.9061 - val_mae: 15.1486\n",
      "Epoch 141/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.5846 - mae: 4.8771 - val_loss: 218.2845 - val_mae: 10.9228\n",
      "Epoch 142/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 46.1718 - mae: 4.9103 - val_loss: 520.6249 - val_mae: 19.7195\n",
      "Epoch 143/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 44.7159 - mae: 5.0030 - val_loss: 393.7996 - val_mae: 14.9682\n",
      "Epoch 144/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.8724 - mae: 4.7966 - val_loss: 473.8197 - val_mae: 16.8840\n",
      "Epoch 145/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 45.3983 - mae: 4.9166 - val_loss: 453.4732 - val_mae: 16.5018\n",
      "Epoch 146/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.6393 - mae: 4.6999 - val_loss: 475.5571 - val_mae: 17.2430\n",
      "Epoch 147/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.6020 - mae: 4.5336 - val_loss: 352.5670 - val_mae: 14.1945\n",
      "Epoch 148/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.3298 - mae: 4.8048 - val_loss: 275.3534 - val_mae: 12.7389\n",
      "Epoch 149/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.5047 - mae: 4.6500 - val_loss: 249.9078 - val_mae: 11.6814\n",
      "Epoch 150/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.8408 - mae: 4.5808 - val_loss: 255.1371 - val_mae: 11.9927\n",
      "Epoch 151/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.1987 - mae: 4.7503 - val_loss: 321.9257 - val_mae: 14.1346\n",
      "Epoch 152/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.5138 - mae: 4.6553 - val_loss: 294.8488 - val_mae: 13.1396\n",
      "Epoch 153/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.6144 - mae: 4.6742 - val_loss: 274.3251 - val_mae: 12.5029\n",
      "Epoch 154/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.1577 - mae: 4.4315 - val_loss: 349.5729 - val_mae: 14.4134\n",
      "Epoch 155/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.3430 - mae: 4.5176 - val_loss: 292.4922 - val_mae: 12.7921\n",
      "Epoch 156/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.3021 - mae: 4.4313 - val_loss: 546.2306 - val_mae: 19.2589\n",
      "Epoch 157/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.4422 - mae: 4.3479 - val_loss: 383.3576 - val_mae: 14.9025\n",
      "Epoch 158/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.5299 - mae: 4.5412 - val_loss: 268.9565 - val_mae: 12.2299\n",
      "Epoch 159/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 42.5058 - mae: 4.6690 - val_loss: 282.6002 - val_mae: 12.5207\n",
      "Epoch 160/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.8775 - mae: 4.6053 - val_loss: 307.5622 - val_mae: 13.2233\n",
      "Epoch 161/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.9799 - mae: 4.5018 - val_loss: 361.3385 - val_mae: 14.6695\n",
      "Epoch 162/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.0567 - mae: 4.4356 - val_loss: 238.5800 - val_mae: 11.4849\n",
      "Epoch 163/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.7609 - mae: 4.5033 - val_loss: 321.8249 - val_mae: 13.8416\n",
      "Epoch 164/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.8438 - mae: 4.5465 - val_loss: 198.1520 - val_mae: 10.4413\n",
      "Epoch 165/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.7113 - mae: 4.5023 - val_loss: 433.7330 - val_mae: 16.9220\n",
      "Epoch 166/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.3038 - mae: 4.4300 - val_loss: 206.9011 - val_mae: 10.6194\n",
      "Epoch 167/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.9369 - mae: 4.6559 - val_loss: 179.6927 - val_mae: 9.9911\n",
      "Epoch 168/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.3246 - mae: 4.4306 - val_loss: 327.1939 - val_mae: 14.2428\n",
      "Epoch 169/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.7813 - mae: 4.4477 - val_loss: 341.7282 - val_mae: 14.5188\n",
      "Epoch 170/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 42.4771 - mae: 4.6934 - val_loss: 288.8653 - val_mae: 13.1074\n",
      "Epoch 171/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.5919 - mae: 4.4825 - val_loss: 205.3629 - val_mae: 10.9597\n",
      "Epoch 172/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.4168 - mae: 4.3971 - val_loss: 423.0796 - val_mae: 17.2445\n",
      "Epoch 173/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.8768 - mae: 4.5830 - val_loss: 228.7204 - val_mae: 11.3652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.6547 - mae: 4.7156 - val_loss: 216.4921 - val_mae: 10.9605\n",
      "Epoch 175/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.6439 - mae: 4.4905 - val_loss: 284.2475 - val_mae: 12.8064\n",
      "Epoch 176/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.7099 - mae: 4.5414 - val_loss: 485.0941 - val_mae: 17.9609\n",
      "Epoch 177/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.3058 - mae: 4.6726 - val_loss: 422.4604 - val_mae: 15.9226\n",
      "Epoch 178/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.6386 - mae: 4.6037 - val_loss: 393.5735 - val_mae: 15.4002\n",
      "Epoch 179/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.4355 - mae: 4.3714 - val_loss: 233.4951 - val_mae: 11.3509\n",
      "Epoch 180/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1346 - mae: 4.3593 - val_loss: 256.9872 - val_mae: 11.9162\n",
      "Epoch 181/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.4404 - mae: 4.5517 - val_loss: 304.9338 - val_mae: 13.4312\n",
      "Epoch 182/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.6649 - mae: 4.6257 - val_loss: 338.6774 - val_mae: 14.0062\n",
      "Epoch 183/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.5450 - mae: 4.6195 - val_loss: 224.0650 - val_mae: 11.1088\n",
      "Epoch 184/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.1182 - mae: 4.3686 - val_loss: 378.2743 - val_mae: 14.9504\n",
      "Epoch 185/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 37.7227 - mae: 4.4533 - val_loss: 449.2301 - val_mae: 17.4450\n",
      "Epoch 186/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.6273 - mae: 4.4943 - val_loss: 231.6364 - val_mae: 11.3258\n",
      "Epoch 187/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.6915 - mae: 4.5276 - val_loss: 300.8650 - val_mae: 13.1717\n",
      "Epoch 188/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8155 - mae: 4.1962 - val_loss: 196.8638 - val_mae: 10.4806\n",
      "Epoch 189/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.2595 - mae: 4.5473 - val_loss: 305.6609 - val_mae: 13.4420\n",
      "Epoch 190/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.5607 - mae: 4.5490 - val_loss: 315.6316 - val_mae: 13.4570\n",
      "Epoch 191/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.3275 - mae: 4.5558 - val_loss: 264.1353 - val_mae: 12.1946\n",
      "Epoch 192/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.2704 - mae: 4.4706 - val_loss: 287.7545 - val_mae: 12.7081\n",
      "Epoch 193/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.7314 - mae: 4.6248 - val_loss: 396.8815 - val_mae: 15.8035\n",
      "Epoch 194/1000\n",
      "1981/1981 [==============================] - 2s 999us/step - loss: 38.6680 - mae: 4.6102 - val_loss: 278.5421 - val_mae: 12.4959\n",
      "Epoch 195/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0534 - mae: 4.3867 - val_loss: 438.3065 - val_mae: 17.0662\n",
      "Epoch 196/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.1503 - mae: 4.6393 - val_loss: 283.1966 - val_mae: 12.5245\n",
      "Epoch 197/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.1998 - mae: 4.5607 - val_loss: 357.0177 - val_mae: 14.3960\n",
      "Epoch 198/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0956 - mae: 4.4916 - val_loss: 325.4653 - val_mae: 13.5920\n",
      "Epoch 199/1000\n",
      "1981/1981 [==============================] - 2s 989us/step - loss: 39.8673 - mae: 4.5125 - val_loss: 385.7492 - val_mae: 15.6030\n",
      "Epoch 200/1000\n",
      "1981/1981 [==============================] - 2s 988us/step - loss: 37.8480 - mae: 4.4648 - val_loss: 377.7286 - val_mae: 15.1648\n",
      "Epoch 201/1000\n",
      "1981/1981 [==============================] - 2s 937us/step - loss: 40.0956 - mae: 4.5585 - val_loss: 261.4534 - val_mae: 12.1861\n",
      "Epoch 202/1000\n",
      "1981/1981 [==============================] - 2s 937us/step - loss: 39.5903 - mae: 4.5489 - val_loss: 314.1367 - val_mae: 13.4597\n",
      "Epoch 203/1000\n",
      "1981/1981 [==============================] - 2s 978us/step - loss: 39.0328 - mae: 4.5149 - val_loss: 275.6670 - val_mae: 12.8920\n",
      "Epoch 204/1000\n",
      "1981/1981 [==============================] - 2s 969us/step - loss: 37.6537 - mae: 4.5445 - val_loss: 265.9132 - val_mae: 12.2292\n",
      "Epoch 205/1000\n",
      "1981/1981 [==============================] - 2s 996us/step - loss: 41.3269 - mae: 4.8352 - val_loss: 485.6409 - val_mae: 18.4375\n",
      "Epoch 206/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.7336 - mae: 4.6484 - val_loss: 311.7621 - val_mae: 13.2819\n",
      "Epoch 207/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.7341 - mae: 4.6294 - val_loss: 243.0670 - val_mae: 11.9015\n",
      "Epoch 208/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.7036 - mae: 4.4942 - val_loss: 231.5906 - val_mae: 11.3137\n",
      "Epoch 209/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8129 - mae: 4.2995 - val_loss: 353.1448 - val_mae: 14.6631\n",
      "Epoch 210/1000\n",
      "1981/1981 [==============================] - 2s 998us/step - loss: 37.2259 - mae: 4.4096 - val_loss: 285.1465 - val_mae: 12.5957\n",
      "Epoch 211/1000\n",
      "1981/1981 [==============================] - 2s 973us/step - loss: 39.6696 - mae: 4.5653 - val_loss: 285.2647 - val_mae: 12.7435\n",
      "Epoch 212/1000\n",
      "1981/1981 [==============================] - 2s 979us/step - loss: 36.8056 - mae: 4.3669 - val_loss: 417.9008 - val_mae: 16.3542\n",
      "Epoch 213/1000\n",
      "1981/1981 [==============================] - 2s 977us/step - loss: 35.4496 - mae: 4.3298 - val_loss: 452.4874 - val_mae: 17.2627\n",
      "Epoch 214/1000\n",
      "1981/1981 [==============================] - 2s 977us/step - loss: 36.7582 - mae: 4.4397 - val_loss: 199.5936 - val_mae: 10.5723\n",
      "Epoch 215/1000\n",
      "1981/1981 [==============================] - 2s 970us/step - loss: 36.2601 - mae: 4.3658 - val_loss: 321.6209 - val_mae: 13.7647\n",
      "Epoch 216/1000\n",
      "1981/1981 [==============================] - 2s 980us/step - loss: 38.7141 - mae: 4.4363 - val_loss: 351.8016 - val_mae: 14.4580\n",
      "Epoch 217/1000\n",
      "1981/1981 [==============================] - 2s 967us/step - loss: 38.0589 - mae: 4.4506 - val_loss: 284.9121 - val_mae: 12.7402\n",
      "Epoch 218/1000\n",
      "1981/1981 [==============================] - 2s 976us/step - loss: 38.7682 - mae: 4.5561 - val_loss: 254.1103 - val_mae: 12.2370\n",
      "Epoch 219/1000\n",
      "1981/1981 [==============================] - 2s 999us/step - loss: 38.2802 - mae: 4.4096 - val_loss: 206.4514 - val_mae: 10.8319\n",
      "Epoch 220/1000\n",
      "1981/1981 [==============================] - 2s 987us/step - loss: 35.9042 - mae: 4.4229 - val_loss: 360.0423 - val_mae: 15.0782\n",
      "Epoch 221/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.7819 - mae: 4.4222 - val_loss: 268.8320 - val_mae: 12.3840\n",
      "Epoch 222/1000\n",
      "1981/1981 [==============================] - 2s 998us/step - loss: 37.1125 - mae: 4.3543 - val_loss: 310.5165 - val_mae: 13.4933\n",
      "Epoch 223/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9469 - mae: 4.4204 - val_loss: 355.2951 - val_mae: 14.8734\n",
      "Epoch 224/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.4261 - mae: 4.4862 - val_loss: 333.5280 - val_mae: 13.9982\n",
      "Epoch 225/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0671 - mae: 4.4119 - val_loss: 329.7818 - val_mae: 14.3670\n",
      "Epoch 226/1000\n",
      "1981/1981 [==============================] - 2s 997us/step - loss: 38.6908 - mae: 4.5742 - val_loss: 269.4435 - val_mae: 12.4427\n",
      "Epoch 227/1000\n",
      "1981/1981 [==============================] - 2s 958us/step - loss: 37.8696 - mae: 4.4571 - val_loss: 388.3526 - val_mae: 15.8257\n",
      "Epoch 228/1000\n",
      "1981/1981 [==============================] - 2s 936us/step - loss: 36.7420 - mae: 4.4329 - val_loss: 470.7289 - val_mae: 17.6636\n",
      "Epoch 229/1000\n",
      "1981/1981 [==============================] - 2s 964us/step - loss: 35.1489 - mae: 4.3142 - val_loss: 387.1089 - val_mae: 15.4804\n",
      "Epoch 230/1000\n",
      "1981/1981 [==============================] - 2s 987us/step - loss: 38.0011 - mae: 4.4048 - val_loss: 192.1732 - val_mae: 10.4646\n",
      "Epoch 231/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.0058 - mae: 4.6743 - val_loss: 212.1365 - val_mae: 11.0332\n",
      "Epoch 232/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.1341 - mae: 4.4535 - val_loss: 368.6375 - val_mae: 14.8014\n",
      "Epoch 233/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.4670 - mae: 4.4191 - val_loss: 389.8811 - val_mae: 15.7201\n",
      "Epoch 234/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1643 - mae: 4.4107 - val_loss: 536.0640 - val_mae: 19.6444\n",
      "Epoch 235/1000\n",
      "1981/1981 [==============================] - 2s 997us/step - loss: 38.1821 - mae: 4.4265 - val_loss: 223.7821 - val_mae: 11.2847\n",
      "Epoch 236/1000\n",
      "1981/1981 [==============================] - 2s 984us/step - loss: 39.1251 - mae: 4.6553 - val_loss: 258.7624 - val_mae: 12.7825\n",
      "Epoch 237/1000\n",
      "1981/1981 [==============================] - 2s 980us/step - loss: 39.7938 - mae: 4.7616 - val_loss: 318.7959 - val_mae: 13.7679\n",
      "Epoch 238/1000\n",
      "1981/1981 [==============================] - 2s 961us/step - loss: 39.5907 - mae: 4.5656 - val_loss: 410.9822 - val_mae: 15.7884\n",
      "Epoch 239/1000\n",
      "1981/1981 [==============================] - 2s 936us/step - loss: 34.9905 - mae: 4.3533 - val_loss: 438.3496 - val_mae: 16.8303\n",
      "Epoch 240/1000\n",
      "1981/1981 [==============================] - 2s 943us/step - loss: 34.8918 - mae: 4.2419 - val_loss: 422.6587 - val_mae: 16.6155\n",
      "Epoch 241/1000\n",
      "1981/1981 [==============================] - 2s 972us/step - loss: 35.3115 - mae: 4.2908 - val_loss: 419.5077 - val_mae: 16.2124\n",
      "Epoch 242/1000\n",
      "1981/1981 [==============================] - 2s 993us/step - loss: 36.0855 - mae: 4.4468 - val_loss: 373.2393 - val_mae: 15.2507\n",
      "Epoch 243/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.6201 - mae: 4.4828 - val_loss: 378.8369 - val_mae: 15.1849\n",
      "Epoch 244/1000\n",
      "1981/1981 [==============================] - 2s 995us/step - loss: 35.0492 - mae: 4.2829 - val_loss: 456.3919 - val_mae: 17.4330\n",
      "Epoch 245/1000\n",
      "1981/1981 [==============================] - 2s 995us/step - loss: 33.1252 - mae: 4.2098 - val_loss: 461.1980 - val_mae: 17.3652\n",
      "Epoch 246/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6409 - mae: 4.2354 - val_loss: 328.0662 - val_mae: 13.6667\n",
      "Epoch 247/1000\n",
      "1981/1981 [==============================] - 2s 998us/step - loss: 36.8837 - mae: 4.4742 - val_loss: 395.4543 - val_mae: 16.5200\n",
      "Epoch 248/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2147 - mae: 4.2543 - val_loss: 365.8549 - val_mae: 14.7749\n",
      "Epoch 249/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.4699 - mae: 4.4230 - val_loss: 407.4968 - val_mae: 15.4339\n",
      "Epoch 250/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.1298 - mae: 4.4222 - val_loss: 395.9276 - val_mae: 15.3447\n",
      "Epoch 251/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4104 - mae: 4.3384 - val_loss: 459.8008 - val_mae: 16.6560\n",
      "Epoch 252/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1072 - mae: 4.2840 - val_loss: 233.8841 - val_mae: 11.7439\n",
      "Epoch 253/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.7316 - mae: 4.5114 - val_loss: 455.8403 - val_mae: 17.5256\n",
      "Epoch 254/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.2631 - mae: 4.5125 - val_loss: 251.6201 - val_mae: 12.0402\n",
      "Epoch 255/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.0525 - mae: 4.2604 - val_loss: 366.1174 - val_mae: 14.6920\n",
      "Epoch 256/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9549 - mae: 4.2799 - val_loss: 451.9012 - val_mae: 16.9438\n",
      "Epoch 257/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.0186 - mae: 4.1130 - val_loss: 312.2162 - val_mae: 13.6078\n",
      "Epoch 258/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.2405 - mae: 4.2809 - val_loss: 195.5003 - val_mae: 10.6273\n",
      "Epoch 259/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.9632 - mae: 4.5648 - val_loss: 208.8093 - val_mae: 11.0526\n",
      "Epoch 260/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.8485 - mae: 4.1453 - val_loss: 246.2155 - val_mae: 12.1406\n",
      "Epoch 261/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.3316 - mae: 4.3548 - val_loss: 234.1701 - val_mae: 11.5691\n",
      "Epoch 262/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.1685 - mae: 4.4398 - val_loss: 447.4818 - val_mae: 16.1025\n",
      "Epoch 263/1000\n",
      "1981/1981 [==============================] - 2s 994us/step - loss: 37.2543 - mae: 4.3915 - val_loss: 328.0727 - val_mae: 13.6984\n",
      "Epoch 264/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.0666 - mae: 4.3455 - val_loss: 413.9332 - val_mae: 15.5530\n",
      "Epoch 265/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.9807 - mae: 4.4955 - val_loss: 504.5648 - val_mae: 17.5207\n",
      "Epoch 266/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0081 - mae: 4.2475 - val_loss: 352.0278 - val_mae: 14.0082\n",
      "Epoch 267/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.5340 - mae: 4.2411 - val_loss: 327.9828 - val_mae: 13.6442\n",
      "Epoch 268/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.1190 - mae: 4.4055 - val_loss: 296.2957 - val_mae: 13.5338\n",
      "Epoch 269/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.6370 - mae: 4.5019 - val_loss: 276.2385 - val_mae: 12.4364\n",
      "Epoch 270/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6067 - mae: 4.2571 - val_loss: 257.5756 - val_mae: 12.4359\n",
      "Epoch 271/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.3814 - mae: 4.3407 - val_loss: 275.6211 - val_mae: 12.4918\n",
      "Epoch 272/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.1463 - mae: 4.2329 - val_loss: 238.7883 - val_mae: 11.7374\n",
      "Epoch 273/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1632 - mae: 4.3775 - val_loss: 278.6821 - val_mae: 12.8863\n",
      "Epoch 274/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.7754 - mae: 4.3675 - val_loss: 229.9024 - val_mae: 11.4249\n",
      "Epoch 275/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0715 - mae: 4.4051 - val_loss: 260.7599 - val_mae: 12.0849\n",
      "Epoch 276/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2264 - mae: 4.2587 - val_loss: 336.6198 - val_mae: 14.3768\n",
      "Epoch 277/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0485 - mae: 4.3327 - val_loss: 453.2896 - val_mae: 17.2861\n",
      "Epoch 278/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7741 - mae: 4.2789 - val_loss: 283.5280 - val_mae: 12.6463\n",
      "Epoch 279/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.3778 - mae: 4.2913 - val_loss: 303.3563 - val_mae: 13.4518\n",
      "Epoch 280/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.2146 - mae: 4.3240 - val_loss: 391.7988 - val_mae: 15.9269\n",
      "Epoch 281/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7409 - mae: 4.3030 - val_loss: 212.1664 - val_mae: 11.0301\n",
      "Epoch 282/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.6031 - mae: 4.2104 - val_loss: 234.7429 - val_mae: 11.7230\n",
      "Epoch 283/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.1806 - mae: 4.4635 - val_loss: 352.7881 - val_mae: 15.1539\n",
      "Epoch 284/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.3491 - mae: 4.5169 - val_loss: 198.9138 - val_mae: 10.7474\n",
      "Epoch 285/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9234 - mae: 4.5492 - val_loss: 192.2559 - val_mae: 10.5176\n",
      "Epoch 286/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0850 - mae: 4.4873 - val_loss: 156.0285 - val_mae: 9.4669\n",
      "Epoch 287/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2087 - mae: 4.2591 - val_loss: 214.2352 - val_mae: 11.1243\n",
      "Epoch 288/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.3262 - mae: 4.3245 - val_loss: 290.2183 - val_mae: 12.8530\n",
      "Epoch 289/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5855 - mae: 4.3780 - val_loss: 494.7708 - val_mae: 17.3920\n",
      "Epoch 290/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.4575 - mae: 4.3796 - val_loss: 322.8421 - val_mae: 13.4547\n",
      "Epoch 291/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6770 - mae: 4.1552 - val_loss: 461.6010 - val_mae: 17.1871\n",
      "Epoch 292/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.2328 - mae: 4.3713 - val_loss: 354.7174 - val_mae: 14.0846\n",
      "Epoch 293/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9582 - mae: 4.3057 - val_loss: 390.9382 - val_mae: 15.2480\n",
      "Epoch 294/1000\n",
      "1981/1981 [==============================] - 2s 980us/step - loss: 38.0396 - mae: 4.4293 - val_loss: 283.4578 - val_mae: 12.7231\n",
      "Epoch 295/1000\n",
      "1981/1981 [==============================] - 2s 986us/step - loss: 35.5825 - mae: 4.3115 - val_loss: 343.2317 - val_mae: 14.2360\n",
      "Epoch 296/1000\n",
      "1981/1981 [==============================] - 2s 961us/step - loss: 33.7024 - mae: 4.2001 - val_loss: 363.9513 - val_mae: 14.4001\n",
      "Epoch 297/1000\n",
      "1981/1981 [==============================] - 2s 946us/step - loss: 36.4205 - mae: 4.3565 - val_loss: 371.2351 - val_mae: 14.7685\n",
      "Epoch 298/1000\n",
      "1981/1981 [==============================] - 2s 940us/step - loss: 34.5786 - mae: 4.2022 - val_loss: 404.2729 - val_mae: 16.2869\n",
      "Epoch 299/1000\n",
      "1981/1981 [==============================] - 2s 971us/step - loss: 32.6279 - mae: 4.1370 - val_loss: 262.0654 - val_mae: 12.1432\n",
      "Epoch 300/1000\n",
      "1981/1981 [==============================] - 2s 975us/step - loss: 35.4520 - mae: 4.3342 - val_loss: 258.8342 - val_mae: 12.2875\n",
      "Epoch 301/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2775 - mae: 4.1387 - val_loss: 179.3209 - val_mae: 10.1956\n",
      "Epoch 302/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6531 - mae: 4.3213 - val_loss: 293.4962 - val_mae: 14.4270\n",
      "Epoch 303/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6638 - mae: 4.2348 - val_loss: 147.4645 - val_mae: 9.3008\n",
      "Epoch 304/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.9574 - mae: 4.5262 - val_loss: 327.3765 - val_mae: 14.2542\n",
      "Epoch 305/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3500 - mae: 4.2863 - val_loss: 283.4669 - val_mae: 12.5655\n",
      "Epoch 306/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0081 - mae: 4.4501 - val_loss: 346.7551 - val_mae: 14.5220\n",
      "Epoch 307/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.2801 - mae: 4.5412 - val_loss: 337.0013 - val_mae: 13.8326\n",
      "Epoch 308/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.5924 - mae: 4.3665 - val_loss: 366.5326 - val_mae: 14.3947\n",
      "Epoch 309/1000\n",
      "1981/1981 [==============================] - 2s 975us/step - loss: 35.1715 - mae: 4.2939 - val_loss: 345.6415 - val_mae: 14.2382\n",
      "Epoch 310/1000\n",
      "1981/1981 [==============================] - 2s 960us/step - loss: 33.2651 - mae: 4.1862 - val_loss: 403.0223 - val_mae: 15.7488\n",
      "Epoch 311/1000\n",
      "1981/1981 [==============================] - 2s 958us/step - loss: 37.2661 - mae: 4.4152 - val_loss: 356.3085 - val_mae: 14.4835\n",
      "Epoch 312/1000\n",
      "1981/1981 [==============================] - 2s 907us/step - loss: 36.4820 - mae: 4.3766 - val_loss: 397.3590 - val_mae: 15.0302\n",
      "Epoch 313/1000\n",
      "1981/1981 [==============================] - 2s 931us/step - loss: 36.6603 - mae: 4.3127 - val_loss: 244.3228 - val_mae: 11.7577\n",
      "Epoch 314/1000\n",
      "1981/1981 [==============================] - 2s 971us/step - loss: 36.2360 - mae: 4.3309 - val_loss: 216.7035 - val_mae: 11.0282\n",
      "Epoch 315/1000\n",
      "1981/1981 [==============================] - 2s 937us/step - loss: 37.7098 - mae: 4.3932 - val_loss: 469.4529 - val_mae: 17.0039\n",
      "Epoch 316/1000\n",
      "1981/1981 [==============================] - 2s 956us/step - loss: 33.4479 - mae: 4.1881 - val_loss: 395.2326 - val_mae: 14.7494\n",
      "Epoch 317/1000\n",
      "1981/1981 [==============================] - 2s 964us/step - loss: 35.1655 - mae: 4.3830 - val_loss: 331.5827 - val_mae: 13.5493\n",
      "Epoch 318/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6523 - mae: 4.3936 - val_loss: 342.7074 - val_mae: 14.1086\n",
      "Epoch 319/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.4388 - mae: 4.4955 - val_loss: 410.2824 - val_mae: 15.1174\n",
      "Epoch 320/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.7106 - mae: 4.4079 - val_loss: 469.2610 - val_mae: 16.0785\n",
      "Epoch 321/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6928 - mae: 4.3478 - val_loss: 553.2877 - val_mae: 18.6122\n",
      "Epoch 322/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.1157 - mae: 4.5040 - val_loss: 501.2047 - val_mae: 16.8500\n",
      "Epoch 323/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.3185 - mae: 4.4658 - val_loss: 398.4075 - val_mae: 14.9120\n",
      "Epoch 324/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.4293 - mae: 4.3493 - val_loss: 296.9294 - val_mae: 12.8144\n",
      "Epoch 325/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.8439 - mae: 4.5057 - val_loss: 346.6765 - val_mae: 13.6484\n",
      "Epoch 326/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.0353 - mae: 4.6638 - val_loss: 290.8502 - val_mae: 12.8002\n",
      "Epoch 327/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.1192 - mae: 4.3710 - val_loss: 390.4729 - val_mae: 14.6986\n",
      "Epoch 328/1000\n",
      "1981/1981 [==============================] - 2s 984us/step - loss: 37.2449 - mae: 4.4623 - val_loss: 319.7155 - val_mae: 13.3030\n",
      "Epoch 329/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.5492 - mae: 4.3978 - val_loss: 291.0446 - val_mae: 12.7314\n",
      "Epoch 330/1000\n",
      "1981/1981 [==============================] - 2s 998us/step - loss: 40.3011 - mae: 4.5224 - val_loss: 359.1380 - val_mae: 14.0602\n",
      "Epoch 331/1000\n",
      "1981/1981 [==============================] - 2s 992us/step - loss: 36.4557 - mae: 4.3869 - val_loss: 371.7608 - val_mae: 15.3128\n",
      "Epoch 332/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.6982 - mae: 4.6748 - val_loss: 351.3142 - val_mae: 13.9484\n",
      "Epoch 333/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 36.6472 - mae: 4.3882 - val_loss: 398.1047 - val_mae: 15.0537\n",
      "Epoch 334/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.7747 - mae: 4.3871 - val_loss: 313.4314 - val_mae: 13.1631\n",
      "Epoch 335/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9347 - mae: 4.2579 - val_loss: 495.5239 - val_mae: 16.8439\n",
      "Epoch 336/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.3087 - mae: 4.2996 - val_loss: 364.8215 - val_mae: 14.0817\n",
      "Epoch 337/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6783 - mae: 4.2410 - val_loss: 405.1694 - val_mae: 14.8525\n",
      "Epoch 338/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2957 - mae: 4.2595 - val_loss: 576.0411 - val_mae: 18.3997\n",
      "Epoch 339/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.2376 - mae: 4.3982 - val_loss: 472.4562 - val_mae: 16.5158\n",
      "Epoch 340/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4789 - mae: 4.2356 - val_loss: 485.6774 - val_mae: 16.5103\n",
      "Epoch 341/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2039 - mae: 4.2046 - val_loss: 420.2744 - val_mae: 15.1721\n",
      "Epoch 342/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.5841 - mae: 4.4337 - val_loss: 573.9348 - val_mae: 18.9145\n",
      "Epoch 343/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7978 - mae: 4.2907 - val_loss: 411.8702 - val_mae: 14.8331\n",
      "Epoch 344/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6941 - mae: 4.3370 - val_loss: 340.4009 - val_mae: 13.7110\n",
      "Epoch 345/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0030 - mae: 4.3350 - val_loss: 409.0408 - val_mae: 14.9494\n",
      "Epoch 346/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.0398 - mae: 4.3388 - val_loss: 357.3092 - val_mae: 13.9403\n",
      "Epoch 347/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.7278 - mae: 4.3361 - val_loss: 399.4480 - val_mae: 14.6772\n",
      "Epoch 348/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2759 - mae: 4.3049 - val_loss: 427.3430 - val_mae: 15.2542\n",
      "Epoch 349/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.5047 - mae: 4.2664 - val_loss: 480.6009 - val_mae: 16.8123\n",
      "Epoch 350/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8462 - mae: 4.2718 - val_loss: 360.4074 - val_mae: 13.9648\n",
      "Epoch 351/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.7679 - mae: 4.4393 - val_loss: 363.1774 - val_mae: 14.8152\n",
      "Epoch 352/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.9267 - mae: 4.5977 - val_loss: 324.9111 - val_mae: 13.2479\n",
      "Epoch 353/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.3498 - mae: 4.3514 - val_loss: 320.2892 - val_mae: 13.1588\n",
      "Epoch 354/1000\n",
      "1981/1981 [==============================] - 2s 986us/step - loss: 36.8658 - mae: 4.4552 - val_loss: 430.3857 - val_mae: 15.6967\n",
      "Epoch 355/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.3458 - mae: 4.3758 - val_loss: 340.8728 - val_mae: 13.6633\n",
      "Epoch 356/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2423 - mae: 4.2472 - val_loss: 311.3479 - val_mae: 12.8843\n",
      "Epoch 357/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8395 - mae: 4.1922 - val_loss: 264.4941 - val_mae: 11.9746\n",
      "Epoch 358/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6191 - mae: 4.3068 - val_loss: 538.7526 - val_mae: 18.3001\n",
      "Epoch 359/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.2450 - mae: 4.4365 - val_loss: 349.5563 - val_mae: 13.6190\n",
      "Epoch 360/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.8755 - mae: 4.4755 - val_loss: 441.3919 - val_mae: 15.5326\n",
      "Epoch 361/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.6415 - mae: 4.5195 - val_loss: 426.2875 - val_mae: 15.1750\n",
      "Epoch 362/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.0736 - mae: 4.5896 - val_loss: 420.0358 - val_mae: 15.1867\n",
      "Epoch 363/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.7865 - mae: 4.4343 - val_loss: 293.2651 - val_mae: 12.6650\n",
      "Epoch 364/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.6333 - mae: 4.5880 - val_loss: 256.7346 - val_mae: 11.7927\n",
      "Epoch 365/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.1213 - mae: 4.5914 - val_loss: 268.0859 - val_mae: 12.2593\n",
      "Epoch 366/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.4876 - mae: 4.6032 - val_loss: 352.2851 - val_mae: 14.3184\n",
      "Epoch 367/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1125 - mae: 4.4200 - val_loss: 246.8019 - val_mae: 11.6021\n",
      "Epoch 368/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2839 - mae: 4.3928 - val_loss: 220.2566 - val_mae: 11.1841\n",
      "Epoch 369/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.7840 - mae: 4.6503 - val_loss: 253.5934 - val_mae: 11.6546\n",
      "Epoch 370/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.4291 - mae: 4.2658 - val_loss: 272.2495 - val_mae: 11.9909\n",
      "Epoch 371/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.4505 - mae: 4.4351 - val_loss: 288.0931 - val_mae: 12.4590\n",
      "Epoch 372/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.8347 - mae: 4.4043 - val_loss: 254.0374 - val_mae: 11.6173\n",
      "Epoch 373/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4313 - mae: 4.3398 - val_loss: 317.2321 - val_mae: 13.3113\n",
      "Epoch 374/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.9441 - mae: 4.2358 - val_loss: 260.8183 - val_mae: 11.6542\n",
      "Epoch 375/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.0000 - mae: 4.2865 - val_loss: 255.2050 - val_mae: 11.4525\n",
      "Epoch 376/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.1162 - mae: 4.6205 - val_loss: 224.2499 - val_mae: 10.8743\n",
      "Epoch 377/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.1469 - mae: 4.4904 - val_loss: 211.4490 - val_mae: 10.4721\n",
      "Epoch 378/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.5378 - mae: 4.4306 - val_loss: 243.5732 - val_mae: 11.1909\n",
      "Epoch 379/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.5798 - mae: 4.1934 - val_loss: 320.9556 - val_mae: 13.3377\n",
      "Epoch 380/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1631 - mae: 4.2942 - val_loss: 206.2263 - val_mae: 10.4275\n",
      "Epoch 381/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.3321 - mae: 4.3340 - val_loss: 290.8693 - val_mae: 12.5067\n",
      "Epoch 382/1000\n",
      "1981/1981 [==============================] - 2s 983us/step - loss: 38.8969 - mae: 4.5483 - val_loss: 213.7385 - val_mae: 10.5561\n",
      "Epoch 383/1000\n",
      "1981/1981 [==============================] - 2s 988us/step - loss: 38.0775 - mae: 4.4979 - val_loss: 262.8431 - val_mae: 11.9371\n",
      "Epoch 384/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2755 - mae: 4.2905 - val_loss: 188.9564 - val_mae: 9.8701\n",
      "Epoch 385/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1815 - mae: 4.1223 - val_loss: 280.6300 - val_mae: 12.2735\n",
      "Epoch 386/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8486 - mae: 4.2861 - val_loss: 176.2657 - val_mae: 10.0651\n",
      "Epoch 387/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.8228 - mae: 4.2935 - val_loss: 403.4391 - val_mae: 15.5396\n",
      "Epoch 388/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.1703 - mae: 4.3785 - val_loss: 360.9771 - val_mae: 14.1943\n",
      "Epoch 389/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.2126 - mae: 4.4654 - val_loss: 340.1702 - val_mae: 14.4222\n",
      "Epoch 390/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.2242 - mae: 4.4972 - val_loss: 412.1481 - val_mae: 15.5736\n",
      "Epoch 391/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 41.9987 - mae: 4.7281 - val_loss: 277.0699 - val_mae: 12.1251\n",
      "Epoch 392/1000\n",
      "1981/1981 [==============================] - 2s 993us/step - loss: 35.9476 - mae: 4.3458 - val_loss: 375.4637 - val_mae: 15.6305\n",
      "Epoch 393/1000\n",
      "1981/1981 [==============================] - 2s 978us/step - loss: 35.3146 - mae: 4.4206 - val_loss: 352.7493 - val_mae: 14.3147\n",
      "Epoch 394/1000\n",
      "1981/1981 [==============================] - 2s 978us/step - loss: 36.4225 - mae: 4.4455 - val_loss: 415.9543 - val_mae: 15.5814\n",
      "Epoch 395/1000\n",
      "1981/1981 [==============================] - 2s 985us/step - loss: 35.2866 - mae: 4.2761 - val_loss: 374.6533 - val_mae: 14.6431\n",
      "Epoch 396/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.7758 - mae: 4.4986 - val_loss: 369.0952 - val_mae: 14.4513\n",
      "Epoch 397/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 34.7876 - mae: 4.2956 - val_loss: 483.2503 - val_mae: 17.7949\n",
      "Epoch 398/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 33.1288 - mae: 4.1298 - val_loss: 404.7115 - val_mae: 15.4688\n",
      "Epoch 399/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1821 - mae: 4.3074 - val_loss: 385.1985 - val_mae: 15.0883\n",
      "Epoch 400/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4167 - mae: 4.0095 - val_loss: 288.1910 - val_mae: 12.4130\n",
      "Epoch 401/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.8879 - mae: 4.4643 - val_loss: 223.3769 - val_mae: 10.8621\n",
      "Epoch 402/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9899 - mae: 4.1940 - val_loss: 279.7791 - val_mae: 12.4106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 403/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.3617 - mae: 4.3058 - val_loss: 212.7248 - val_mae: 10.5594\n",
      "Epoch 404/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5373 - mae: 4.3034 - val_loss: 248.6838 - val_mae: 11.3344\n",
      "Epoch 405/1000\n",
      "1981/1981 [==============================] - 2s 987us/step - loss: 36.6467 - mae: 4.2736 - val_loss: 204.2700 - val_mae: 10.2954\n",
      "Epoch 406/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5513 - mae: 4.2228 - val_loss: 172.8259 - val_mae: 9.5822\n",
      "Epoch 407/1000\n",
      "1981/1981 [==============================] - 2s 993us/step - loss: 37.3467 - mae: 4.2445 - val_loss: 224.1168 - val_mae: 10.8392\n",
      "Epoch 408/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4352 - mae: 4.1244 - val_loss: 172.3049 - val_mae: 9.4493\n",
      "Epoch 409/1000\n",
      "1981/1981 [==============================] - 2s 985us/step - loss: 35.4966 - mae: 4.3058 - val_loss: 241.8210 - val_mae: 11.4467\n",
      "Epoch 410/1000\n",
      "1981/1981 [==============================] - 2s 974us/step - loss: 34.5097 - mae: 4.1643 - val_loss: 249.0963 - val_mae: 11.7170\n",
      "Epoch 411/1000\n",
      "1981/1981 [==============================] - 2s 983us/step - loss: 32.8496 - mae: 4.0927 - val_loss: 185.4744 - val_mae: 9.8485\n",
      "Epoch 412/1000\n",
      "1981/1981 [==============================] - 2s 982us/step - loss: 33.7102 - mae: 4.1455 - val_loss: 155.7493 - val_mae: 9.1832\n",
      "Epoch 413/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0870 - mae: 4.2724 - val_loss: 161.8855 - val_mae: 9.2029\n",
      "Epoch 414/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8680 - mae: 4.3078 - val_loss: 135.0165 - val_mae: 8.8895\n",
      "Epoch 415/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.8563 - mae: 4.5383 - val_loss: 120.9537 - val_mae: 8.6284\n",
      "Epoch 416/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 43.5531 - mae: 4.5593 - val_loss: 136.0804 - val_mae: 8.7718\n",
      "Epoch 417/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0442 - mae: 4.4689 - val_loss: 279.2946 - val_mae: 12.2402\n",
      "Epoch 418/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.8473 - mae: 4.5067 - val_loss: 349.2430 - val_mae: 14.5683\n",
      "Epoch 419/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.6904 - mae: 4.3291 - val_loss: 245.6655 - val_mae: 11.6210\n",
      "Epoch 420/1000\n",
      "1981/1981 [==============================] - 2s 976us/step - loss: 38.4836 - mae: 4.3966 - val_loss: 154.4092 - val_mae: 9.2326\n",
      "Epoch 421/1000\n",
      "1981/1981 [==============================] - 2s 994us/step - loss: 35.4815 - mae: 4.2534 - val_loss: 269.2749 - val_mae: 12.1779\n",
      "Epoch 422/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3763 - mae: 4.1684 - val_loss: 208.9674 - val_mae: 10.4625\n",
      "Epoch 423/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.5255 - mae: 4.2620 - val_loss: 193.5574 - val_mae: 10.0768\n",
      "Epoch 424/1000\n",
      "1981/1981 [==============================] - 2s 999us/step - loss: 33.7215 - mae: 4.1388 - val_loss: 183.4321 - val_mae: 9.8556\n",
      "Epoch 425/1000\n",
      "1981/1981 [==============================] - 2s 997us/step - loss: 36.7082 - mae: 4.2966 - val_loss: 283.4285 - val_mae: 12.7349\n",
      "Epoch 426/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6753 - mae: 4.2745 - val_loss: 274.6069 - val_mae: 12.2726\n",
      "Epoch 427/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.5424 - mae: 4.2585 - val_loss: 175.0896 - val_mae: 9.6490\n",
      "Epoch 428/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.3839 - mae: 4.2793 - val_loss: 176.6113 - val_mae: 9.7160\n",
      "Epoch 429/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3970 - mae: 4.1317 - val_loss: 192.9551 - val_mae: 10.1258\n",
      "Epoch 430/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.1160 - mae: 4.1226 - val_loss: 203.1426 - val_mae: 10.4321\n",
      "Epoch 431/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8139 - mae: 4.2016 - val_loss: 200.1619 - val_mae: 10.3567\n",
      "Epoch 432/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7295 - mae: 4.1371 - val_loss: 282.5003 - val_mae: 12.8421\n",
      "Epoch 433/1000\n",
      "1981/1981 [==============================] - 2s 991us/step - loss: 34.5176 - mae: 4.2199 - val_loss: 134.5869 - val_mae: 8.5717\n",
      "Epoch 434/1000\n",
      "1981/1981 [==============================] - 2s 939us/step - loss: 34.0652 - mae: 4.1929 - val_loss: 149.6522 - val_mae: 9.1219\n",
      "Epoch 435/1000\n",
      "1981/1981 [==============================] - 2s 975us/step - loss: 33.6343 - mae: 4.1486 - val_loss: 209.3415 - val_mae: 10.4676\n",
      "Epoch 436/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.6917 - mae: 4.4694 - val_loss: 106.9526 - val_mae: 8.0024\n",
      "Epoch 437/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5755 - mae: 4.2555 - val_loss: 162.8696 - val_mae: 9.4145\n",
      "Epoch 438/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.3709 - mae: 4.3405 - val_loss: 166.7455 - val_mae: 9.4041\n",
      "Epoch 439/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.0315 - mae: 4.8238 - val_loss: 127.9965 - val_mae: 8.4485\n",
      "Epoch 440/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8067 - mae: 4.2668 - val_loss: 160.9846 - val_mae: 9.1901\n",
      "Epoch 441/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5958 - mae: 4.1732 - val_loss: 241.9347 - val_mae: 11.3000\n",
      "Epoch 442/1000\n",
      "1981/1981 [==============================] - 2s 996us/step - loss: 36.8326 - mae: 4.3993 - val_loss: 178.6394 - val_mae: 9.7523\n",
      "Epoch 443/1000\n",
      "1981/1981 [==============================] - 2s 969us/step - loss: 37.5708 - mae: 4.3238 - val_loss: 250.8514 - val_mae: 11.6892\n",
      "Epoch 444/1000\n",
      "1981/1981 [==============================] - 2s 981us/step - loss: 32.7178 - mae: 4.0751 - val_loss: 179.9729 - val_mae: 9.7917\n",
      "Epoch 445/1000\n",
      "1981/1981 [==============================] - 2s 945us/step - loss: 36.7032 - mae: 4.3045 - val_loss: 239.0258 - val_mae: 11.6242\n",
      "Epoch 446/1000\n",
      "1981/1981 [==============================] - 2s 944us/step - loss: 35.5908 - mae: 4.2226 - val_loss: 278.5254 - val_mae: 12.8398\n",
      "Epoch 447/1000\n",
      "1981/1981 [==============================] - 2s 982us/step - loss: 33.7286 - mae: 4.1418 - val_loss: 127.6426 - val_mae: 8.4427\n",
      "Epoch 448/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9640 - mae: 4.2840 - val_loss: 170.9206 - val_mae: 9.5059\n",
      "Epoch 449/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.9382 - mae: 4.1888 - val_loss: 347.1980 - val_mae: 14.8136\n",
      "Epoch 450/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3913 - mae: 4.2633 - val_loss: 160.8439 - val_mae: 9.2949\n",
      "Epoch 451/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3425 - mae: 4.1107 - val_loss: 220.7126 - val_mae: 10.9731\n",
      "Epoch 452/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3885 - mae: 4.1485 - val_loss: 181.5687 - val_mae: 9.7790\n",
      "Epoch 453/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0075 - mae: 4.3171 - val_loss: 165.8988 - val_mae: 9.3457\n",
      "Epoch 454/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7225 - mae: 4.2766 - val_loss: 126.3119 - val_mae: 8.2563\n",
      "Epoch 455/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5990 - mae: 4.4044 - val_loss: 136.4976 - val_mae: 8.6757\n",
      "Epoch 456/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.3621 - mae: 4.3385 - val_loss: 112.2238 - val_mae: 7.9973\n",
      "Epoch 457/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.8898 - mae: 4.5048 - val_loss: 227.4815 - val_mae: 12.2581\n",
      "Epoch 458/1000\n",
      "1981/1981 [==============================] - 2s 999us/step - loss: 35.5144 - mae: 4.3529 - val_loss: 238.3346 - val_mae: 11.8736\n",
      "Epoch 459/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 40.3820 - mae: 4.7156 - val_loss: 197.3350 - val_mae: 10.5711\n",
      "Epoch 460/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.5495 - mae: 4.3577 - val_loss: 269.1959 - val_mae: 12.5011\n",
      "Epoch 461/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1050 - mae: 4.3330 - val_loss: 353.9023 - val_mae: 14.7861\n",
      "Epoch 462/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.8769 - mae: 4.4903 - val_loss: 136.3213 - val_mae: 8.6095\n",
      "Epoch 463/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4402 - mae: 4.3204 - val_loss: 171.0977 - val_mae: 9.5708\n",
      "Epoch 464/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5423 - mae: 4.3024 - val_loss: 121.2038 - val_mae: 8.1340\n",
      "Epoch 465/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.3298 - mae: 4.1043 - val_loss: 192.6708 - val_mae: 10.4392\n",
      "Epoch 466/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.9675 - mae: 4.4680 - val_loss: 216.5482 - val_mae: 10.9913\n",
      "Epoch 467/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.8011 - mae: 4.4326 - val_loss: 176.0126 - val_mae: 9.5798\n",
      "Epoch 468/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.5881 - mae: 4.3745 - val_loss: 99.0010 - val_mae: 7.4324\n",
      "Epoch 469/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7756 - mae: 4.3113 - val_loss: 95.1872 - val_mae: 7.4340\n",
      "Epoch 470/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8961 - mae: 4.2681 - val_loss: 241.4135 - val_mae: 12.0077\n",
      "Epoch 471/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1016 - mae: 4.4154 - val_loss: 381.6555 - val_mae: 15.4955\n",
      "Epoch 472/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0714 - mae: 4.5358 - val_loss: 253.8935 - val_mae: 11.8870\n",
      "Epoch 473/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.6731 - mae: 4.2310 - val_loss: 157.9564 - val_mae: 9.0210\n",
      "Epoch 474/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.1271 - mae: 4.2162 - val_loss: 248.4565 - val_mae: 11.7664\n",
      "Epoch 475/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.3803 - mae: 4.3670 - val_loss: 505.7789 - val_mae: 17.4688\n",
      "Epoch 476/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.7821 - mae: 4.4618 - val_loss: 355.7169 - val_mae: 14.7292\n",
      "Epoch 477/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.7811 - mae: 4.4833 - val_loss: 417.1918 - val_mae: 16.0145\n",
      "Epoch 478/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7086 - mae: 4.3439 - val_loss: 331.2661 - val_mae: 14.0562\n",
      "Epoch 479/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8270 - mae: 4.1503 - val_loss: 337.1577 - val_mae: 14.1235\n",
      "Epoch 480/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3989 - mae: 4.1588 - val_loss: 375.7658 - val_mae: 14.8477\n",
      "Epoch 481/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7275 - mae: 4.2388 - val_loss: 282.4779 - val_mae: 12.5413\n",
      "Epoch 482/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.1754 - mae: 3.9924 - val_loss: 408.3562 - val_mae: 15.9926\n",
      "Epoch 483/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7637 - mae: 4.2094 - val_loss: 278.4727 - val_mae: 12.4782\n",
      "Epoch 484/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.8474 - mae: 4.2919 - val_loss: 286.2256 - val_mae: 13.3219\n",
      "Epoch 485/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.9517 - mae: 4.5000 - val_loss: 394.6591 - val_mae: 15.0183\n",
      "Epoch 486/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.2572 - mae: 4.5207 - val_loss: 320.3532 - val_mae: 13.3246\n",
      "Epoch 487/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.4079 - mae: 4.4663 - val_loss: 319.1341 - val_mae: 13.5706\n",
      "Epoch 488/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.0132 - mae: 4.2754 - val_loss: 249.8309 - val_mae: 11.8822\n",
      "Epoch 489/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.9425 - mae: 4.0774 - val_loss: 325.5444 - val_mae: 13.8663\n",
      "Epoch 490/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.5207 - mae: 4.4117 - val_loss: 284.6927 - val_mae: 12.8281\n",
      "Epoch 491/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6827 - mae: 4.2750 - val_loss: 360.1570 - val_mae: 14.1505\n",
      "Epoch 492/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3620 - mae: 4.2711 - val_loss: 349.9298 - val_mae: 14.7570\n",
      "Epoch 493/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3957 - mae: 4.2357 - val_loss: 329.2646 - val_mae: 14.1700\n",
      "Epoch 494/1000\n",
      "1981/1981 [==============================] - 2s 995us/step - loss: 33.3803 - mae: 4.1739 - val_loss: 244.3278 - val_mae: 11.6249\n",
      "Epoch 495/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7823 - mae: 4.1554 - val_loss: 333.4673 - val_mae: 13.9530\n",
      "Epoch 496/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.0960 - mae: 4.1588 - val_loss: 296.7928 - val_mae: 13.0923\n",
      "Epoch 497/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.0843 - mae: 4.0973 - val_loss: 317.4207 - val_mae: 14.5130\n",
      "Epoch 498/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.9084 - mae: 4.4054 - val_loss: 296.2666 - val_mae: 12.7851\n",
      "Epoch 499/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.2279 - mae: 4.5167 - val_loss: 173.0914 - val_mae: 9.6294\n",
      "Epoch 500/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4936 - mae: 4.1616 - val_loss: 228.7788 - val_mae: 11.1564\n",
      "Epoch 501/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2281 - mae: 4.2471 - val_loss: 205.9153 - val_mae: 10.8447\n",
      "Epoch 502/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.1317 - mae: 4.2940 - val_loss: 288.0953 - val_mae: 12.7913\n",
      "Epoch 503/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2841 - mae: 4.3049 - val_loss: 360.5411 - val_mae: 15.1037\n",
      "Epoch 504/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.8139 - mae: 4.1309 - val_loss: 185.1106 - val_mae: 9.9448\n",
      "Epoch 505/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.6846 - mae: 4.0227 - val_loss: 229.2362 - val_mae: 11.4971\n",
      "Epoch 506/1000\n",
      "1981/1981 [==============================] - 2s 991us/step - loss: 32.2899 - mae: 4.0468 - val_loss: 169.3752 - val_mae: 9.4939\n",
      "Epoch 507/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.9387 - mae: 4.2458 - val_loss: 188.8839 - val_mae: 10.0008\n",
      "Epoch 508/1000\n",
      "1981/1981 [==============================] - 2s 995us/step - loss: 33.9300 - mae: 4.1071 - val_loss: 167.5215 - val_mae: 9.3705\n",
      "Epoch 509/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.8532 - mae: 4.3231 - val_loss: 79.1903 - val_mae: 7.2066\n",
      "Epoch 510/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.3319 - mae: 4.1856 - val_loss: 170.0509 - val_mae: 9.4706\n",
      "Epoch 511/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1901 - mae: 4.1683 - val_loss: 190.2715 - val_mae: 9.9783\n",
      "Epoch 512/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0593 - mae: 3.9987 - val_loss: 206.1053 - val_mae: 10.4692\n",
      "Epoch 513/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2200 - mae: 4.1730 - val_loss: 285.0148 - val_mae: 12.7936\n",
      "Epoch 514/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.9647 - mae: 4.0688 - val_loss: 244.1795 - val_mae: 11.4111\n",
      "Epoch 515/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7016 - mae: 4.2038 - val_loss: 512.0066 - val_mae: 19.0966\n",
      "Epoch 516/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9713 - mae: 4.1397 - val_loss: 425.4516 - val_mae: 15.7417\n",
      "Epoch 517/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3259 - mae: 4.1797 - val_loss: 595.4890 - val_mae: 20.4219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 518/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.8980 - mae: 4.3186 - val_loss: 337.9158 - val_mae: 13.4454\n",
      "Epoch 519/1000\n",
      "1981/1981 [==============================] - 2s 988us/step - loss: 34.9688 - mae: 4.2160 - val_loss: 382.5559 - val_mae: 14.7854\n",
      "Epoch 520/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9186 - mae: 4.1492 - val_loss: 558.7147 - val_mae: 19.3015\n",
      "Epoch 521/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.5449 - mae: 4.1185 - val_loss: 167.6564 - val_mae: 9.6957\n",
      "Epoch 522/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.5443 - mae: 4.3796 - val_loss: 265.6246 - val_mae: 12.9141\n",
      "Epoch 523/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3993 - mae: 4.2125 - val_loss: 126.6142 - val_mae: 8.3168\n",
      "Epoch 524/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6953 - mae: 4.2607 - val_loss: 137.7938 - val_mae: 8.5919\n",
      "Epoch 525/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.0615 - mae: 4.2046 - val_loss: 327.4027 - val_mae: 13.9883\n",
      "Epoch 526/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.9383 - mae: 4.2773 - val_loss: 227.2281 - val_mae: 10.9660\n",
      "Epoch 527/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9289 - mae: 4.3249 - val_loss: 263.5183 - val_mae: 12.2068\n",
      "Epoch 528/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7315 - mae: 4.2391 - val_loss: 187.4930 - val_mae: 9.9510\n",
      "Epoch 529/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.9008 - mae: 4.2676 - val_loss: 222.2477 - val_mae: 10.7974\n",
      "Epoch 530/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.5252 - mae: 4.2377 - val_loss: 222.7943 - val_mae: 11.0794\n",
      "Epoch 531/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.0819 - mae: 4.1302 - val_loss: 149.5918 - val_mae: 8.8766\n",
      "Epoch 532/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.9254 - mae: 4.1043 - val_loss: 312.9243 - val_mae: 13.6222\n",
      "Epoch 533/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6263 - mae: 4.3098 - val_loss: 135.8406 - val_mae: 9.0969\n",
      "Epoch 534/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.3596 - mae: 4.2565 - val_loss: 191.3500 - val_mae: 10.3058\n",
      "Epoch 535/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7116 - mae: 4.2836 - val_loss: 256.1257 - val_mae: 11.8935\n",
      "Epoch 536/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6878 - mae: 4.1202 - val_loss: 271.1892 - val_mae: 12.5596\n",
      "Epoch 537/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3294 - mae: 4.1731 - val_loss: 160.1183 - val_mae: 9.2690\n",
      "Epoch 538/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2209 - mae: 4.2881 - val_loss: 143.1194 - val_mae: 8.7749\n",
      "Epoch 539/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9156 - mae: 4.1717 - val_loss: 100.5779 - val_mae: 7.4997\n",
      "Epoch 540/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6975 - mae: 4.2579 - val_loss: 171.5307 - val_mae: 9.5716\n",
      "Epoch 541/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.6119 - mae: 4.3064 - val_loss: 251.2068 - val_mae: 11.8950\n",
      "Epoch 542/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.5355 - mae: 4.5087 - val_loss: 251.0797 - val_mae: 11.7249\n",
      "Epoch 543/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0379 - mae: 4.2582 - val_loss: 217.6087 - val_mae: 10.7035\n",
      "Epoch 544/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6583 - mae: 4.1375 - val_loss: 213.2270 - val_mae: 10.5662\n",
      "Epoch 545/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4496 - mae: 4.1525 - val_loss: 153.6362 - val_mae: 8.9807\n",
      "Epoch 546/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.4505 - mae: 4.1922 - val_loss: 237.8728 - val_mae: 11.2710\n",
      "Epoch 547/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.7098 - mae: 4.3235 - val_loss: 300.9269 - val_mae: 12.9041\n",
      "Epoch 548/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.7140 - mae: 4.4451 - val_loss: 254.4025 - val_mae: 11.5681\n",
      "Epoch 549/1000\n",
      "1981/1981 [==============================] - 2s 996us/step - loss: 34.6744 - mae: 4.1613 - val_loss: 272.1954 - val_mae: 12.3082\n",
      "Epoch 550/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.9991 - mae: 4.0123 - val_loss: 266.1147 - val_mae: 12.1692\n",
      "Epoch 551/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6559 - mae: 4.2301 - val_loss: 211.4892 - val_mae: 10.7320\n",
      "Epoch 552/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9939 - mae: 4.1391 - val_loss: 120.1772 - val_mae: 8.5726\n",
      "Epoch 553/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.5129 - mae: 4.2028 - val_loss: 315.1971 - val_mae: 13.7472\n",
      "Epoch 554/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.8969 - mae: 4.0772 - val_loss: 297.9475 - val_mae: 13.4018\n",
      "Epoch 555/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.0078 - mae: 4.0043 - val_loss: 239.0974 - val_mae: 11.4010\n",
      "Epoch 556/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3969 - mae: 4.1881 - val_loss: 169.5185 - val_mae: 9.4162\n",
      "Epoch 557/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5412 - mae: 4.1155 - val_loss: 266.2475 - val_mae: 12.4048\n",
      "Epoch 558/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9648 - mae: 4.1092 - val_loss: 112.3318 - val_mae: 9.0502\n",
      "Epoch 559/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.2087 - mae: 4.3030 - val_loss: 141.6142 - val_mae: 8.8574\n",
      "Epoch 560/1000\n",
      "1981/1981 [==============================] - 2s 995us/step - loss: 32.2727 - mae: 4.0707 - val_loss: 138.9713 - val_mae: 8.5124\n",
      "Epoch 561/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3123 - mae: 4.2313 - val_loss: 152.8297 - val_mae: 9.0182\n",
      "Epoch 562/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.4762 - mae: 4.2040 - val_loss: 132.7821 - val_mae: 8.4084\n",
      "Epoch 563/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9407 - mae: 4.2337 - val_loss: 202.3630 - val_mae: 10.2533\n",
      "Epoch 564/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.0810 - mae: 4.1630 - val_loss: 115.6435 - val_mae: 9.0655\n",
      "Epoch 565/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3049 - mae: 4.0950 - val_loss: 129.0407 - val_mae: 8.3783\n",
      "Epoch 566/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.3334 - mae: 3.9607 - val_loss: 188.7893 - val_mae: 9.7483\n",
      "Epoch 567/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6251 - mae: 4.2213 - val_loss: 218.7210 - val_mae: 11.0643\n",
      "Epoch 568/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.0319 - mae: 4.1093 - val_loss: 126.9824 - val_mae: 8.7133\n",
      "Epoch 569/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.3120 - mae: 4.6460 - val_loss: 177.4452 - val_mae: 10.1448\n",
      "Epoch 570/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.6425 - mae: 4.7556 - val_loss: 102.1867 - val_mae: 7.8579\n",
      "Epoch 571/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.7090 - mae: 4.0388 - val_loss: 142.0164 - val_mae: 8.6651\n",
      "Epoch 572/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.7505 - mae: 4.0547 - val_loss: 111.8328 - val_mae: 7.7480\n",
      "Epoch 573/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.8680 - mae: 4.1659 - val_loss: 180.6492 - val_mae: 9.8462\n",
      "Epoch 574/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.7336 - mae: 4.1650 - val_loss: 238.2237 - val_mae: 11.5323\n",
      "Epoch 575/1000\n",
      "1981/1981 [==============================] - 2s 998us/step - loss: 32.7189 - mae: 4.1160 - val_loss: 110.1913 - val_mae: 8.1112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 576/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.5247 - mae: 4.2735 - val_loss: 163.5121 - val_mae: 9.4371\n",
      "Epoch 577/1000\n",
      "1981/1981 [==============================] - 2s 994us/step - loss: 35.2806 - mae: 4.2938 - val_loss: 223.5424 - val_mae: 10.8511\n",
      "Epoch 578/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.9471 - mae: 3.9625 - val_loss: 184.1665 - val_mae: 9.8185\n",
      "Epoch 579/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.8732 - mae: 4.0515 - val_loss: 195.6420 - val_mae: 10.0815\n",
      "Epoch 580/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2634 - mae: 4.2040 - val_loss: 217.3971 - val_mae: 10.6510\n",
      "Epoch 581/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9890 - mae: 4.1627 - val_loss: 217.5071 - val_mae: 10.7527\n",
      "Epoch 582/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.6040 - mae: 4.5562 - val_loss: 293.7643 - val_mae: 12.8064\n",
      "Epoch 583/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.6210 - mae: 4.1861 - val_loss: 145.8891 - val_mae: 9.0972\n",
      "Epoch 584/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.5709 - mae: 4.2626 - val_loss: 207.3839 - val_mae: 10.4269\n",
      "Epoch 585/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.9528 - mae: 3.9532 - val_loss: 265.2733 - val_mae: 11.9654\n",
      "Epoch 586/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8151 - mae: 4.2039 - val_loss: 268.6693 - val_mae: 12.2395\n",
      "Epoch 587/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3051 - mae: 4.1782 - val_loss: 299.5999 - val_mae: 13.0012\n",
      "Epoch 588/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2429 - mae: 4.1574 - val_loss: 250.7078 - val_mae: 11.4712\n",
      "Epoch 589/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9312 - mae: 4.1409 - val_loss: 398.1889 - val_mae: 15.6737\n",
      "Epoch 590/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7739 - mae: 4.1380 - val_loss: 214.8585 - val_mae: 10.5656\n",
      "Epoch 591/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6598 - mae: 4.2237 - val_loss: 312.1391 - val_mae: 13.6661\n",
      "Epoch 592/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.5479 - mae: 4.2165 - val_loss: 285.3922 - val_mae: 12.4078\n",
      "Epoch 593/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0834 - mae: 4.3421 - val_loss: 221.4704 - val_mae: 10.6711\n",
      "Epoch 594/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7716 - mae: 4.1350 - val_loss: 177.9162 - val_mae: 9.5824\n",
      "Epoch 595/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3988 - mae: 4.1810 - val_loss: 144.4293 - val_mae: 9.2040\n",
      "Epoch 596/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7593 - mae: 4.2062 - val_loss: 229.9879 - val_mae: 10.9112\n",
      "Epoch 597/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3793 - mae: 4.0799 - val_loss: 250.9349 - val_mae: 11.4889\n",
      "Epoch 598/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5869 - mae: 4.1627 - val_loss: 168.7299 - val_mae: 9.3406\n",
      "Epoch 599/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6628 - mae: 4.0040 - val_loss: 326.2844 - val_mae: 13.9458\n",
      "Epoch 600/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7049 - mae: 4.2163 - val_loss: 341.5629 - val_mae: 14.0797\n",
      "Epoch 601/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8122 - mae: 4.2053 - val_loss: 398.1417 - val_mae: 15.7676\n",
      "Epoch 602/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1406 - mae: 4.0774 - val_loss: 169.2150 - val_mae: 9.4410\n",
      "Epoch 603/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4780 - mae: 4.2623 - val_loss: 142.2450 - val_mae: 8.7193\n",
      "Epoch 604/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0677 - mae: 4.1694 - val_loss: 200.0055 - val_mae: 10.2117\n",
      "Epoch 605/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7873 - mae: 4.1694 - val_loss: 197.0140 - val_mae: 10.0611\n",
      "Epoch 606/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.6879 - mae: 3.9884 - val_loss: 237.8231 - val_mae: 11.3916\n",
      "Epoch 607/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.1235 - mae: 4.0017 - val_loss: 221.9362 - val_mae: 11.0423\n",
      "Epoch 608/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5433 - mae: 4.1050 - val_loss: 149.6547 - val_mae: 8.6211\n",
      "Epoch 609/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1546 - mae: 4.0225 - val_loss: 183.8632 - val_mae: 9.6915\n",
      "Epoch 610/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.0744 - mae: 4.1731 - val_loss: 187.7152 - val_mae: 10.1408\n",
      "Epoch 611/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.5756 - mae: 4.0623 - val_loss: 167.0196 - val_mae: 9.3352\n",
      "Epoch 612/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9077 - mae: 4.2339 - val_loss: 192.8547 - val_mae: 10.1710\n",
      "Epoch 613/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.8403 - mae: 4.3301 - val_loss: 159.0466 - val_mae: 9.1968\n",
      "Epoch 614/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0758 - mae: 4.0337 - val_loss: 232.2631 - val_mae: 11.4520\n",
      "Epoch 615/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0004 - mae: 4.2123 - val_loss: 142.5797 - val_mae: 8.7559\n",
      "Epoch 616/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2411 - mae: 4.2024 - val_loss: 240.4806 - val_mae: 11.6729\n",
      "Epoch 617/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4140 - mae: 4.2507 - val_loss: 202.9119 - val_mae: 10.5456\n",
      "Epoch 618/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.6518 - mae: 4.4095 - val_loss: 176.9024 - val_mae: 9.7307\n",
      "Epoch 619/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.7225 - mae: 4.0837 - val_loss: 175.9518 - val_mae: 9.6573\n",
      "Epoch 620/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.7227 - mae: 4.0910 - val_loss: 121.3218 - val_mae: 8.0485\n",
      "Epoch 621/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3549 - mae: 4.1818 - val_loss: 243.2214 - val_mae: 12.0725\n",
      "Epoch 622/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4077 - mae: 4.1036 - val_loss: 199.9519 - val_mae: 10.3193\n",
      "Epoch 623/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.7597 - mae: 3.8876 - val_loss: 308.3269 - val_mae: 14.2859\n",
      "Epoch 624/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.7504 - mae: 4.3272 - val_loss: 153.8516 - val_mae: 8.8336\n",
      "Epoch 625/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.9147 - mae: 4.3904 - val_loss: 89.6628 - val_mae: 7.4731\n",
      "Epoch 626/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.6983 - mae: 3.8814 - val_loss: 108.8973 - val_mae: 7.5776\n",
      "Epoch 627/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.9345 - mae: 4.1517 - val_loss: 160.1797 - val_mae: 9.0874\n",
      "Epoch 628/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.0756 - mae: 3.9551 - val_loss: 167.5197 - val_mae: 9.5253\n",
      "Epoch 629/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.2044 - mae: 3.9463 - val_loss: 184.2820 - val_mae: 9.9094\n",
      "Epoch 630/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6448 - mae: 4.2523 - val_loss: 179.9345 - val_mae: 9.7460\n",
      "Epoch 631/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.0308 - mae: 4.2166 - val_loss: 172.3632 - val_mae: 9.5007\n",
      "Epoch 632/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.1114 - mae: 4.1238 - val_loss: 175.5962 - val_mae: 9.4256\n",
      "Epoch 633/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5579 - mae: 4.1950 - val_loss: 176.9515 - val_mae: 9.4907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 634/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.7875 - mae: 3.9345 - val_loss: 122.9543 - val_mae: 8.2711\n",
      "Epoch 635/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.9201 - mae: 4.4007 - val_loss: 109.3797 - val_mae: 7.5824\n",
      "Epoch 636/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.9394 - mae: 4.0103 - val_loss: 97.4276 - val_mae: 7.6336\n",
      "Epoch 637/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2329 - mae: 4.1605 - val_loss: 161.3770 - val_mae: 9.1968\n",
      "Epoch 638/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5233 - mae: 4.1350 - val_loss: 112.8177 - val_mae: 7.6126\n",
      "Epoch 639/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.3109 - mae: 4.3101 - val_loss: 144.5538 - val_mae: 8.6668\n",
      "Epoch 640/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9431 - mae: 4.2486 - val_loss: 167.8291 - val_mae: 9.7680\n",
      "Epoch 641/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.9976 - mae: 4.0079 - val_loss: 175.2361 - val_mae: 9.9132\n",
      "Epoch 642/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2845 - mae: 4.2640 - val_loss: 110.2305 - val_mae: 7.6111\n",
      "Epoch 643/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.2420 - mae: 4.0963 - val_loss: 109.6553 - val_mae: 7.6362\n",
      "Epoch 644/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3094 - mae: 4.1100 - val_loss: 140.2591 - val_mae: 8.4608\n",
      "Epoch 645/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8217 - mae: 4.1175 - val_loss: 227.6481 - val_mae: 11.3243\n",
      "Epoch 646/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.1822 - mae: 4.1398 - val_loss: 98.4713 - val_mae: 7.4469\n",
      "Epoch 647/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.1868 - mae: 4.0302 - val_loss: 93.3944 - val_mae: 7.7963\n",
      "Epoch 648/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0727 - mae: 4.0271 - val_loss: 96.0824 - val_mae: 8.1092\n",
      "Epoch 649/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4015 - mae: 4.0022 - val_loss: 90.6888 - val_mae: 7.4651\n",
      "Epoch 650/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.4035 - mae: 4.0820 - val_loss: 107.2125 - val_mae: 7.8395\n",
      "Epoch 651/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5169 - mae: 4.2933 - val_loss: 302.0087 - val_mae: 13.5919\n",
      "Epoch 652/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.8918 - mae: 4.4247 - val_loss: 136.2223 - val_mae: 8.9999\n",
      "Epoch 653/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.2274 - mae: 4.3630 - val_loss: 146.9081 - val_mae: 8.8984\n",
      "Epoch 654/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.5262 - mae: 4.1995 - val_loss: 171.0809 - val_mae: 9.3337\n",
      "Epoch 655/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0650 - mae: 4.1150 - val_loss: 142.6258 - val_mae: 8.5417\n",
      "Epoch 656/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8866 - mae: 4.1692 - val_loss: 205.6326 - val_mae: 10.4992\n",
      "Epoch 657/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9959 - mae: 4.2984 - val_loss: 139.5126 - val_mae: 8.6126\n",
      "Epoch 658/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.8645 - mae: 4.1749 - val_loss: 180.5097 - val_mae: 10.0142\n",
      "Epoch 659/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6071 - mae: 4.1151 - val_loss: 190.1925 - val_mae: 10.2038\n",
      "Epoch 660/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6637 - mae: 4.0612 - val_loss: 138.6754 - val_mae: 8.4482\n",
      "Epoch 661/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.5033 - mae: 4.1514 - val_loss: 119.6364 - val_mae: 8.4443\n",
      "Epoch 662/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5040 - mae: 4.1077 - val_loss: 100.6942 - val_mae: 8.5008\n",
      "Epoch 663/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1803 - mae: 4.1281 - val_loss: 120.5391 - val_mae: 7.7982\n",
      "Epoch 664/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4550 - mae: 4.2197 - val_loss: 257.1324 - val_mae: 12.2874\n",
      "Epoch 665/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.0631 - mae: 4.2413 - val_loss: 135.2103 - val_mae: 8.6029\n",
      "Epoch 666/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.5174 - mae: 4.2381 - val_loss: 305.9878 - val_mae: 13.8986\n",
      "Epoch 667/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4760 - mae: 4.0921 - val_loss: 271.1217 - val_mae: 12.7077\n",
      "Epoch 668/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.8338 - mae: 3.9394 - val_loss: 268.0902 - val_mae: 12.8222\n",
      "Epoch 669/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.5262 - mae: 4.0219 - val_loss: 138.1412 - val_mae: 8.6167\n",
      "Epoch 670/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.4384 - mae: 4.3733 - val_loss: 119.0296 - val_mae: 8.0963\n",
      "Epoch 671/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1919 - mae: 4.0713 - val_loss: 186.5629 - val_mae: 10.1381\n",
      "Epoch 672/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6639 - mae: 4.0833 - val_loss: 203.5879 - val_mae: 10.6262\n",
      "Epoch 673/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.7927 - mae: 4.0782 - val_loss: 129.7273 - val_mae: 8.5396\n",
      "Epoch 674/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2026 - mae: 4.0518 - val_loss: 135.2836 - val_mae: 8.5096\n",
      "Epoch 675/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3125 - mae: 4.1307 - val_loss: 124.4524 - val_mae: 8.2149\n",
      "Epoch 676/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.1599 - mae: 4.0156 - val_loss: 154.9931 - val_mae: 8.9373\n",
      "Epoch 677/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2136 - mae: 4.1191 - val_loss: 210.8788 - val_mae: 10.5868\n",
      "Epoch 678/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.2589 - mae: 3.9074 - val_loss: 151.8757 - val_mae: 8.8815\n",
      "Epoch 679/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.4171 - mae: 4.0135 - val_loss: 237.8352 - val_mae: 11.6896\n",
      "Epoch 680/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3326 - mae: 4.1855 - val_loss: 207.2557 - val_mae: 10.9222\n",
      "Epoch 681/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.1763 - mae: 4.0896 - val_loss: 111.2975 - val_mae: 8.2938\n",
      "Epoch 682/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8075 - mae: 4.1985 - val_loss: 114.6412 - val_mae: 7.7479\n",
      "Epoch 683/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0101 - mae: 4.2200 - val_loss: 108.9520 - val_mae: 7.6955\n",
      "Epoch 684/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.6541 - mae: 4.1684 - val_loss: 131.1122 - val_mae: 8.2558\n",
      "Epoch 685/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2683 - mae: 4.1797 - val_loss: 114.8980 - val_mae: 7.9425\n",
      "Epoch 686/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.6513 - mae: 4.0377 - val_loss: 220.3352 - val_mae: 11.3559\n",
      "Epoch 687/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6218 - mae: 4.1785 - val_loss: 199.9069 - val_mae: 10.3040\n",
      "Epoch 688/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.2676 - mae: 4.3719 - val_loss: 219.2118 - val_mae: 10.5802\n",
      "Epoch 689/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2008 - mae: 4.1996 - val_loss: 304.4847 - val_mae: 13.5413\n",
      "Epoch 690/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9636 - mae: 4.0501 - val_loss: 368.2618 - val_mae: 15.4564\n",
      "Epoch 691/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.2052 - mae: 4.1655 - val_loss: 172.9458 - val_mae: 9.3136\n",
      "Epoch 692/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.4349 - mae: 4.1234 - val_loss: 208.7803 - val_mae: 10.5184\n",
      "Epoch 693/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.7651 - mae: 3.9509 - val_loss: 137.8606 - val_mae: 8.3307\n",
      "Epoch 694/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9670 - mae: 4.2163 - val_loss: 158.3119 - val_mae: 9.0703\n",
      "Epoch 695/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0587 - mae: 4.0519 - val_loss: 179.8629 - val_mae: 9.6603\n",
      "Epoch 696/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.7176 - mae: 4.5618 - val_loss: 214.6739 - val_mae: 10.8218\n",
      "Epoch 697/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.1649 - mae: 4.0464 - val_loss: 286.8417 - val_mae: 12.9223\n",
      "Epoch 698/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.4310 - mae: 4.2380 - val_loss: 159.4181 - val_mae: 9.1209\n",
      "Epoch 699/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7772 - mae: 4.1855 - val_loss: 131.0778 - val_mae: 8.3885\n",
      "Epoch 700/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.1070 - mae: 4.1459 - val_loss: 229.0240 - val_mae: 12.2968\n",
      "Epoch 701/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.1808 - mae: 4.1946 - val_loss: 133.4392 - val_mae: 8.3913\n",
      "Epoch 702/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.1321 - mae: 4.3695 - val_loss: 122.0481 - val_mae: 8.2256\n",
      "Epoch 703/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9908 - mae: 4.4162 - val_loss: 126.4190 - val_mae: 8.1494\n",
      "Epoch 704/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.0118 - mae: 4.3726 - val_loss: 128.1558 - val_mae: 8.1163\n",
      "Epoch 705/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3255 - mae: 4.1665 - val_loss: 133.8714 - val_mae: 8.2675\n",
      "Epoch 706/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.6123 - mae: 4.1467 - val_loss: 146.0569 - val_mae: 8.6979\n",
      "Epoch 707/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5123 - mae: 4.0073 - val_loss: 128.5256 - val_mae: 8.2430\n",
      "Epoch 708/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8029 - mae: 4.1898 - val_loss: 227.7661 - val_mae: 11.8594\n",
      "Epoch 709/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.2110 - mae: 4.0711 - val_loss: 147.8620 - val_mae: 8.8455\n",
      "Epoch 710/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7728 - mae: 4.1549 - val_loss: 151.1165 - val_mae: 9.1695\n",
      "Epoch 711/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.8335 - mae: 3.9678 - val_loss: 195.5682 - val_mae: 10.4552\n",
      "Epoch 712/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.6957 - mae: 4.0495 - val_loss: 137.0364 - val_mae: 8.5053\n",
      "Epoch 713/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7534 - mae: 4.2795 - val_loss: 155.9783 - val_mae: 8.9324\n",
      "Epoch 714/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.4680 - mae: 4.3186 - val_loss: 119.1393 - val_mae: 7.7798\n",
      "Epoch 715/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8686 - mae: 4.1619 - val_loss: 127.9582 - val_mae: 8.0883\n",
      "Epoch 716/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8909 - mae: 4.0726 - val_loss: 154.7422 - val_mae: 8.9842\n",
      "Epoch 717/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.9548 - mae: 4.2581 - val_loss: 94.8105 - val_mae: 7.1863\n",
      "Epoch 718/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.3574 - mae: 4.0947 - val_loss: 142.8976 - val_mae: 8.6141\n",
      "Epoch 719/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.5613 - mae: 4.1996 - val_loss: 239.0175 - val_mae: 12.4952\n",
      "Epoch 720/1000\n",
      "1981/1981 [==============================] - 2s 993us/step - loss: 33.1900 - mae: 3.9441 - val_loss: 113.8507 - val_mae: 9.2040\n",
      "Epoch 721/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8766 - mae: 4.1897 - val_loss: 128.4470 - val_mae: 8.2317\n",
      "Epoch 722/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.8395 - mae: 4.0948 - val_loss: 137.3126 - val_mae: 8.3458\n",
      "Epoch 723/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7029 - mae: 4.1855 - val_loss: 134.2729 - val_mae: 8.5153\n",
      "Epoch 724/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.0282 - mae: 4.0435 - val_loss: 211.4654 - val_mae: 11.0328\n",
      "Epoch 725/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.2842 - mae: 4.2916 - val_loss: 142.7826 - val_mae: 8.5526\n",
      "Epoch 726/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2182 - mae: 4.3353 - val_loss: 140.1316 - val_mae: 8.4017\n",
      "Epoch 727/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2310 - mae: 4.1036 - val_loss: 117.4103 - val_mae: 7.7265\n",
      "Epoch 728/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8013 - mae: 4.0540 - val_loss: 194.3061 - val_mae: 10.5924\n",
      "Epoch 729/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.9399 - mae: 4.1005 - val_loss: 140.8979 - val_mae: 8.5447\n",
      "Epoch 730/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.1404 - mae: 4.2910 - val_loss: 192.2056 - val_mae: 11.1951\n",
      "Epoch 731/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3058 - mae: 4.2407 - val_loss: 223.2948 - val_mae: 12.1906\n",
      "Epoch 732/1000\n",
      "1981/1981 [==============================] - 2s 995us/step - loss: 32.7470 - mae: 4.1682 - val_loss: 170.9794 - val_mae: 10.0369\n",
      "Epoch 733/1000\n",
      "1981/1981 [==============================] - 2s 965us/step - loss: 32.3870 - mae: 4.0683 - val_loss: 142.4548 - val_mae: 8.5960\n",
      "Epoch 734/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.9801 - mae: 4.1708 - val_loss: 113.1675 - val_mae: 7.7092\n",
      "Epoch 735/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7356 - mae: 4.0746 - val_loss: 102.8388 - val_mae: 7.2519\n",
      "Epoch 736/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.1243 - mae: 4.1894 - val_loss: 84.4781 - val_mae: 7.4364\n",
      "Epoch 737/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.9112 - mae: 3.9133 - val_loss: 76.2327 - val_mae: 7.1879\n",
      "Epoch 738/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.8475 - mae: 3.9460 - val_loss: 74.4103 - val_mae: 6.5451\n",
      "Epoch 739/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.3722 - mae: 4.3150 - val_loss: 77.0230 - val_mae: 7.1296\n",
      "Epoch 740/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1855 - mae: 4.0406 - val_loss: 81.4163 - val_mae: 7.4065\n",
      "Epoch 741/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.2761 - mae: 3.9507 - val_loss: 74.7538 - val_mae: 6.0948\n",
      "Epoch 742/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.9100 - mae: 4.0323 - val_loss: 93.8175 - val_mae: 6.8610\n",
      "Epoch 743/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6123 - mae: 4.1428 - val_loss: 138.1412 - val_mae: 9.0876\n",
      "Epoch 744/1000\n",
      "1981/1981 [==============================] - 2s 987us/step - loss: 33.8209 - mae: 4.2209 - val_loss: 120.3533 - val_mae: 7.7259\n",
      "Epoch 745/1000\n",
      "1981/1981 [==============================] - 2s 968us/step - loss: 35.6150 - mae: 4.1827 - val_loss: 177.9079 - val_mae: 10.1915\n",
      "Epoch 746/1000\n",
      "1981/1981 [==============================] - 2s 975us/step - loss: 30.9387 - mae: 4.0548 - val_loss: 191.7103 - val_mae: 10.9091\n",
      "Epoch 747/1000\n",
      "1981/1981 [==============================] - 2s 970us/step - loss: 35.1942 - mae: 4.2166 - val_loss: 108.8340 - val_mae: 7.5066\n",
      "Epoch 748/1000\n",
      "1981/1981 [==============================] - 2s 974us/step - loss: 32.9478 - mae: 4.0186 - val_loss: 116.7636 - val_mae: 7.6179\n",
      "Epoch 749/1000\n",
      "1981/1981 [==============================] - 2s 974us/step - loss: 28.0076 - mae: 3.8113 - val_loss: 163.9148 - val_mae: 9.4139\n",
      "Epoch 750/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 983us/step - loss: 37.7450 - mae: 4.4960 - val_loss: 151.4381 - val_mae: 8.9960\n",
      "Epoch 751/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8020 - mae: 4.3230 - val_loss: 223.3942 - val_mae: 11.5817\n",
      "Epoch 752/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7543 - mae: 4.2804 - val_loss: 208.6873 - val_mae: 11.4997\n",
      "Epoch 753/1000\n",
      "1981/1981 [==============================] - 2s 998us/step - loss: 32.8942 - mae: 4.1778 - val_loss: 180.0347 - val_mae: 10.0678\n",
      "Epoch 754/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.5575 - mae: 4.1168 - val_loss: 180.6347 - val_mae: 10.0711\n",
      "Epoch 755/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.5107 - mae: 4.5468 - val_loss: 163.8928 - val_mae: 10.0613\n",
      "Epoch 756/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.3827 - mae: 4.2709 - val_loss: 153.6592 - val_mae: 9.3197\n",
      "Epoch 757/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.7175 - mae: 4.2311 - val_loss: 84.3529 - val_mae: 6.4755\n",
      "Epoch 758/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.0411 - mae: 4.0500 - val_loss: 72.4656 - val_mae: 6.2067\n",
      "Epoch 759/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9347 - mae: 4.1167 - val_loss: 141.2877 - val_mae: 9.2042\n",
      "Epoch 760/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.5815 - mae: 4.0045 - val_loss: 70.4074 - val_mae: 6.0412\n",
      "Epoch 761/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.0169 - mae: 4.2373 - val_loss: 82.9147 - val_mae: 7.5188\n",
      "Epoch 762/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.6179 - mae: 4.3803 - val_loss: 94.5257 - val_mae: 8.1008\n",
      "Epoch 763/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5452 - mae: 4.1253 - val_loss: 63.1854 - val_mae: 5.8214\n",
      "Epoch 764/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.7671 - mae: 4.1184 - val_loss: 128.7822 - val_mae: 8.4060\n",
      "Epoch 765/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.6202 - mae: 3.7786 - val_loss: 126.2342 - val_mae: 8.2819\n",
      "Epoch 766/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.8673 - mae: 4.0363 - val_loss: 153.1706 - val_mae: 9.5883\n",
      "Epoch 767/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.5756 - mae: 3.9064 - val_loss: 204.0452 - val_mae: 11.5066\n",
      "Epoch 768/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.1987 - mae: 4.0202 - val_loss: 174.0135 - val_mae: 10.2326\n",
      "Epoch 769/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5733 - mae: 4.0234 - val_loss: 105.5150 - val_mae: 7.5162\n",
      "Epoch 770/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9792 - mae: 4.1336 - val_loss: 87.3058 - val_mae: 6.6691\n",
      "Epoch 771/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.4663 - mae: 3.9382 - val_loss: 138.7422 - val_mae: 8.9390\n",
      "Epoch 772/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.1281 - mae: 3.9366 - val_loss: 109.3864 - val_mae: 7.3540\n",
      "Epoch 773/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.3541 - mae: 3.9068 - val_loss: 136.9870 - val_mae: 8.6824\n",
      "Epoch 774/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.5052 - mae: 4.2307 - val_loss: 70.5105 - val_mae: 5.8855\n",
      "Epoch 775/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7704 - mae: 4.2483 - val_loss: 95.1771 - val_mae: 6.8240\n",
      "Epoch 776/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1852 - mae: 4.1927 - val_loss: 107.0773 - val_mae: 7.5017\n",
      "Epoch 777/1000\n",
      "1981/1981 [==============================] - 2s 996us/step - loss: 37.0958 - mae: 4.4169 - val_loss: 165.5356 - val_mae: 10.0428\n",
      "Epoch 778/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.8039 - mae: 4.3854 - val_loss: 136.2109 - val_mae: 8.5620\n",
      "Epoch 779/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 37.0757 - mae: 4.4121 - val_loss: 111.9826 - val_mae: 7.7763\n",
      "Epoch 780/1000\n",
      "1981/1981 [==============================] - 2s 986us/step - loss: 33.6105 - mae: 4.1875 - val_loss: 80.0785 - val_mae: 6.4125\n",
      "Epoch 781/1000\n",
      "1981/1981 [==============================] - 2s 968us/step - loss: 34.8649 - mae: 4.2092 - val_loss: 120.1445 - val_mae: 7.9757\n",
      "Epoch 782/1000\n",
      "1981/1981 [==============================] - 2s 973us/step - loss: 30.2148 - mae: 3.8990 - val_loss: 147.6491 - val_mae: 9.2460\n",
      "Epoch 783/1000\n",
      "1981/1981 [==============================] - 2s 940us/step - loss: 31.4023 - mae: 4.0037 - val_loss: 100.9497 - val_mae: 7.1560\n",
      "Epoch 784/1000\n",
      "1981/1981 [==============================] - 2s 945us/step - loss: 32.4677 - mae: 4.1007 - val_loss: 76.8968 - val_mae: 6.2138\n",
      "Epoch 785/1000\n",
      "1981/1981 [==============================] - 2s 940us/step - loss: 33.7563 - mae: 4.2001 - val_loss: 69.8291 - val_mae: 6.0266\n",
      "Epoch 786/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.4883 - mae: 4.5072 - val_loss: 119.3634 - val_mae: 8.3287\n",
      "Epoch 787/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5345 - mae: 4.0994 - val_loss: 123.9789 - val_mae: 8.1178\n",
      "Epoch 788/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.7506 - mae: 4.1320 - val_loss: 76.9211 - val_mae: 6.1981\n",
      "Epoch 789/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.8427 - mae: 4.0113 - val_loss: 64.5715 - val_mae: 5.8091\n",
      "Epoch 790/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7213 - mae: 4.0765 - val_loss: 65.1678 - val_mae: 5.6519\n",
      "Epoch 791/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.8853 - mae: 3.9424 - val_loss: 116.9171 - val_mae: 8.1741\n",
      "Epoch 792/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.2518 - mae: 4.0174 - val_loss: 111.7524 - val_mae: 7.5683\n",
      "Epoch 793/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7606 - mae: 4.2359 - val_loss: 114.1016 - val_mae: 7.5426\n",
      "Epoch 794/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.2204 - mae: 4.1547 - val_loss: 115.9452 - val_mae: 7.7690\n",
      "Epoch 795/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.8233 - mae: 4.1771 - val_loss: 117.6217 - val_mae: 7.9033\n",
      "Epoch 796/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.0131 - mae: 3.8884 - val_loss: 97.7753 - val_mae: 7.0208\n",
      "Epoch 797/1000\n",
      "1981/1981 [==============================] - 2s 990us/step - loss: 33.9215 - mae: 4.0376 - val_loss: 195.0747 - val_mae: 10.9848\n",
      "Epoch 798/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.4098 - mae: 4.2858 - val_loss: 75.3186 - val_mae: 6.6190\n",
      "Epoch 799/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.0609 - mae: 3.9960 - val_loss: 156.0809 - val_mae: 9.6134\n",
      "Epoch 800/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7099 - mae: 4.1478 - val_loss: 163.3781 - val_mae: 10.1590\n",
      "Epoch 801/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9638 - mae: 4.1686 - val_loss: 56.1885 - val_mae: 5.6593\n",
      "Epoch 802/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.6534 - mae: 4.0839 - val_loss: 81.2638 - val_mae: 6.5783\n",
      "Epoch 803/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.5009 - mae: 3.8707 - val_loss: 87.1223 - val_mae: 6.8147\n",
      "Epoch 804/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.1325 - mae: 4.0521 - val_loss: 83.1671 - val_mae: 6.6446\n",
      "Epoch 805/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9983 - mae: 4.1791 - val_loss: 173.8521 - val_mae: 10.4803\n",
      "Epoch 806/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.4474 - mae: 4.1939 - val_loss: 161.8939 - val_mae: 9.4733\n",
      "Epoch 807/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.8485 - mae: 4.3721 - val_loss: 180.4140 - val_mae: 10.5800\n",
      "Epoch 808/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.9763 - mae: 4.1593 - val_loss: 102.8581 - val_mae: 7.2681\n",
      "Epoch 809/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.9137 - mae: 4.0625 - val_loss: 98.2036 - val_mae: 7.0745\n",
      "Epoch 810/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.5310 - mae: 3.9207 - val_loss: 73.7374 - val_mae: 6.1355\n",
      "Epoch 811/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.9656 - mae: 3.9475 - val_loss: 84.9093 - val_mae: 6.5108\n",
      "Epoch 812/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0821 - mae: 3.9139 - val_loss: 63.3312 - val_mae: 6.0109\n",
      "Epoch 813/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.5380 - mae: 4.2962 - val_loss: 87.6563 - val_mae: 6.5158\n",
      "Epoch 814/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.5321 - mae: 3.9305 - val_loss: 192.3763 - val_mae: 10.7378\n",
      "Epoch 815/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1416 - mae: 4.1349 - val_loss: 155.9821 - val_mae: 9.4953\n",
      "Epoch 816/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.1386 - mae: 4.2813 - val_loss: 95.0688 - val_mae: 6.7883\n",
      "Epoch 817/1000\n",
      "1981/1981 [==============================] - 2s 993us/step - loss: 32.3888 - mae: 4.0879 - val_loss: 131.2248 - val_mae: 8.3708\n",
      "Epoch 818/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9408 - mae: 4.2000 - val_loss: 153.3970 - val_mae: 9.5450\n",
      "Epoch 819/1000\n",
      "1981/1981 [==============================] - 2s 995us/step - loss: 34.6278 - mae: 4.1857 - val_loss: 169.4367 - val_mae: 9.7119\n",
      "Epoch 820/1000\n",
      "1981/1981 [==============================] - 2s 989us/step - loss: 36.0082 - mae: 4.4504 - val_loss: 94.2999 - val_mae: 6.7945\n",
      "Epoch 821/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.0624 - mae: 4.0606 - val_loss: 85.0776 - val_mae: 6.5145\n",
      "Epoch 822/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.9403 - mae: 3.9766 - val_loss: 86.9403 - val_mae: 6.5492\n",
      "Epoch 823/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.8818 - mae: 3.8908 - val_loss: 82.4800 - val_mae: 6.8841\n",
      "Epoch 824/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6140 - mae: 4.0407 - val_loss: 119.9044 - val_mae: 7.7508\n",
      "Epoch 825/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.2054 - mae: 3.9679 - val_loss: 91.1932 - val_mae: 6.4786\n",
      "Epoch 826/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6814 - mae: 4.2999 - val_loss: 160.7822 - val_mae: 9.7535\n",
      "Epoch 827/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.0226 - mae: 4.2131 - val_loss: 155.3839 - val_mae: 9.2995\n",
      "Epoch 828/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4560 - mae: 4.2191 - val_loss: 138.4722 - val_mae: 8.8633\n",
      "Epoch 829/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.2344 - mae: 4.4164 - val_loss: 159.4557 - val_mae: 9.4892\n",
      "Epoch 830/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4024 - mae: 4.3515 - val_loss: 138.4545 - val_mae: 8.6432\n",
      "Epoch 831/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.4624 - mae: 4.3096 - val_loss: 71.0801 - val_mae: 6.1619\n",
      "Epoch 832/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.7130 - mae: 3.9675 - val_loss: 70.4203 - val_mae: 5.9013\n",
      "Epoch 833/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 28.9414 - mae: 3.8772 - val_loss: 93.6827 - val_mae: 6.7943\n",
      "Epoch 834/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.8415 - mae: 3.9645 - val_loss: 98.5122 - val_mae: 7.1912\n",
      "Epoch 835/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 38.6573 - mae: 4.4797 - val_loss: 72.4341 - val_mae: 5.9078\n",
      "Epoch 836/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.8204 - mae: 4.4502 - val_loss: 110.5706 - val_mae: 7.4439\n",
      "Epoch 837/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4104 - mae: 4.0054 - val_loss: 138.2318 - val_mae: 8.4415\n",
      "Epoch 838/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4325 - mae: 4.2542 - val_loss: 133.3865 - val_mae: 8.2475\n",
      "Epoch 839/1000\n",
      "1981/1981 [==============================] - 2s 999us/step - loss: 30.6388 - mae: 3.9153 - val_loss: 141.4114 - val_mae: 8.7173\n",
      "Epoch 840/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.1254 - mae: 4.1537 - val_loss: 92.0284 - val_mae: 6.6325\n",
      "Epoch 841/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.2475 - mae: 3.8349 - val_loss: 167.6646 - val_mae: 9.8053\n",
      "Epoch 842/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2593 - mae: 4.0974 - val_loss: 118.0009 - val_mae: 7.8892\n",
      "Epoch 843/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.8903 - mae: 3.9355 - val_loss: 97.4316 - val_mae: 7.0677\n",
      "Epoch 844/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.3182 - mae: 4.0770 - val_loss: 87.6664 - val_mae: 6.4056\n",
      "Epoch 845/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0910 - mae: 4.0549 - val_loss: 76.7275 - val_mae: 6.6072\n",
      "Epoch 846/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.2575 - mae: 3.8972 - val_loss: 94.0406 - val_mae: 6.6828\n",
      "Epoch 847/1000\n",
      "1981/1981 [==============================] - 2s 999us/step - loss: 34.2733 - mae: 4.1744 - val_loss: 79.3836 - val_mae: 6.5206\n",
      "Epoch 848/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.4995 - mae: 4.0951 - val_loss: 104.9541 - val_mae: 7.5508\n",
      "Epoch 849/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.7146 - mae: 3.9817 - val_loss: 183.0283 - val_mae: 10.4350\n",
      "Epoch 850/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.4474 - mae: 3.9536 - val_loss: 140.8378 - val_mae: 8.8773\n",
      "Epoch 851/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4757 - mae: 4.0305 - val_loss: 116.9355 - val_mae: 7.8357\n",
      "Epoch 852/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2316 - mae: 4.0884 - val_loss: 150.9880 - val_mae: 9.2795\n",
      "Epoch 853/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.5346 - mae: 3.8853 - val_loss: 124.7282 - val_mae: 7.9089\n",
      "Epoch 854/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.2193 - mae: 4.1053 - val_loss: 129.0537 - val_mae: 7.9383\n",
      "Epoch 855/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.9367 - mae: 4.1216 - val_loss: 142.7457 - val_mae: 8.5623\n",
      "Epoch 856/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.2126 - mae: 4.0042 - val_loss: 106.4591 - val_mae: 7.3066\n",
      "Epoch 857/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6693 - mae: 4.1866 - val_loss: 161.1821 - val_mae: 9.9179\n",
      "Epoch 858/1000\n",
      "1981/1981 [==============================] - 2s 994us/step - loss: 34.2033 - mae: 4.1620 - val_loss: 131.9786 - val_mae: 8.6447\n",
      "Epoch 859/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3361 - mae: 4.3108 - val_loss: 132.7359 - val_mae: 8.1361\n",
      "Epoch 860/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.5578 - mae: 4.1423 - val_loss: 127.9103 - val_mae: 7.9423\n",
      "Epoch 861/1000\n",
      "1981/1981 [==============================] - 2s 991us/step - loss: 33.6299 - mae: 4.2086 - val_loss: 128.8364 - val_mae: 8.1722\n",
      "Epoch 862/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.1997 - mae: 3.9832 - val_loss: 163.1641 - val_mae: 9.4541\n",
      "Epoch 863/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.7974 - mae: 3.9406 - val_loss: 157.4912 - val_mae: 9.3858\n",
      "Epoch 864/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.2879 - mae: 3.9166 - val_loss: 248.8146 - val_mae: 12.6252\n",
      "Epoch 865/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 33.1524 - mae: 4.0478 - val_loss: 89.6575 - val_mae: 6.5186\n",
      "Epoch 866/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3390 - mae: 4.1226 - val_loss: 112.4111 - val_mae: 7.6919\n",
      "Epoch 867/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.3996 - mae: 4.0055 - val_loss: 92.4343 - val_mae: 6.6401\n",
      "Epoch 868/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3874 - mae: 4.1204 - val_loss: 174.8901 - val_mae: 10.0814\n",
      "Epoch 869/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.6800 - mae: 4.1175 - val_loss: 105.6737 - val_mae: 7.1599\n",
      "Epoch 870/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.2197 - mae: 3.9686 - val_loss: 117.3653 - val_mae: 7.4572\n",
      "Epoch 871/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6255 - mae: 4.1280 - val_loss: 120.5674 - val_mae: 7.5243\n",
      "Epoch 872/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.3120 - mae: 4.0909 - val_loss: 130.1264 - val_mae: 7.9422\n",
      "Epoch 873/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9071 - mae: 4.0770 - val_loss: 129.5410 - val_mae: 7.9785\n",
      "Epoch 874/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.9383 - mae: 4.1026 - val_loss: 132.1904 - val_mae: 8.7557\n",
      "Epoch 875/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3622 - mae: 4.0803 - val_loss: 102.7211 - val_mae: 6.8945\n",
      "Epoch 876/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.6376 - mae: 4.2658 - val_loss: 127.4434 - val_mae: 8.0660\n",
      "Epoch 877/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.8079 - mae: 4.3629 - val_loss: 92.4421 - val_mae: 6.7151\n",
      "Epoch 878/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.6894 - mae: 3.9666 - val_loss: 101.6901 - val_mae: 6.8499\n",
      "Epoch 879/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.4197 - mae: 4.2641 - val_loss: 104.9617 - val_mae: 7.9226\n",
      "Epoch 880/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3762 - mae: 4.1011 - val_loss: 98.4137 - val_mae: 6.9236\n",
      "Epoch 881/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 36.6037 - mae: 4.2284 - val_loss: 127.7734 - val_mae: 8.9248\n",
      "Epoch 882/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.4547 - mae: 3.9979 - val_loss: 117.6844 - val_mae: 7.6008\n",
      "Epoch 883/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.6634 - mae: 4.0098 - val_loss: 96.2092 - val_mae: 7.0819\n",
      "Epoch 884/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.5334 - mae: 3.9579 - val_loss: 103.7201 - val_mae: 7.4668\n",
      "Epoch 885/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.3988 - mae: 3.9832 - val_loss: 132.6381 - val_mae: 8.7191\n",
      "Epoch 886/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.0541 - mae: 4.0899 - val_loss: 286.9513 - val_mae: 14.3071\n",
      "Epoch 887/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2014 - mae: 3.9969 - val_loss: 113.5132 - val_mae: 7.9821\n",
      "Epoch 888/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.9079 - mae: 3.9756 - val_loss: 102.2657 - val_mae: 7.2405\n",
      "Epoch 889/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.3850 - mae: 4.0796 - val_loss: 99.2112 - val_mae: 7.1274\n",
      "Epoch 890/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.1738 - mae: 4.1066 - val_loss: 99.8838 - val_mae: 7.3224\n",
      "Epoch 891/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.3366 - mae: 4.0007 - val_loss: 123.3469 - val_mae: 8.4074\n",
      "Epoch 892/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.3662 - mae: 3.9413 - val_loss: 105.8107 - val_mae: 7.1597\n",
      "Epoch 893/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.4687 - mae: 3.9588 - val_loss: 92.4206 - val_mae: 6.6352\n",
      "Epoch 894/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2405 - mae: 4.1628 - val_loss: 121.6659 - val_mae: 7.7717\n",
      "Epoch 895/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.0291 - mae: 4.0107 - val_loss: 100.4161 - val_mae: 7.0390\n",
      "Epoch 896/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.9450 - mae: 4.0680 - val_loss: 89.7496 - val_mae: 6.6591\n",
      "Epoch 897/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0640 - mae: 3.9933 - val_loss: 88.2885 - val_mae: 6.5725\n",
      "Epoch 898/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.9617 - mae: 3.9756 - val_loss: 113.2492 - val_mae: 7.4882\n",
      "Epoch 899/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.8832 - mae: 3.9522 - val_loss: 157.9221 - val_mae: 10.0593\n",
      "Epoch 900/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.2690 - mae: 3.9167 - val_loss: 120.2664 - val_mae: 7.7790\n",
      "Epoch 901/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 30.3448 - mae: 3.9455 - val_loss: 109.5330 - val_mae: 7.3635\n",
      "Epoch 902/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.5588 - mae: 4.0197 - val_loss: 92.2265 - val_mae: 6.8354\n",
      "Epoch 903/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.0679 - mae: 4.0604 - val_loss: 105.5897 - val_mae: 7.6295\n",
      "Epoch 904/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.9044 - mae: 4.0829 - val_loss: 98.5429 - val_mae: 6.9154\n",
      "Epoch 905/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.0514 - mae: 4.2868 - val_loss: 94.6759 - val_mae: 6.5479\n",
      "Epoch 906/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.2882 - mae: 4.3703 - val_loss: 125.2836 - val_mae: 7.6812\n",
      "Epoch 907/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0878 - mae: 4.3384 - val_loss: 130.8494 - val_mae: 7.9810\n",
      "Epoch 908/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.9259 - mae: 4.0588 - val_loss: 128.1641 - val_mae: 7.6603\n",
      "Epoch 909/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.6077 - mae: 3.9088 - val_loss: 120.1177 - val_mae: 7.9113\n",
      "Epoch 910/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.9604 - mae: 3.9145 - val_loss: 127.6102 - val_mae: 7.6249\n",
      "Epoch 911/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.9853 - mae: 4.1182 - val_loss: 186.1540 - val_mae: 9.8264\n",
      "Epoch 912/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.7774 - mae: 4.1636 - val_loss: 162.5886 - val_mae: 9.0506\n",
      "Epoch 913/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 29.2903 - mae: 3.9274 - val_loss: 235.1900 - val_mae: 11.9012\n",
      "Epoch 914/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.3617 - mae: 4.0919 - val_loss: 124.7239 - val_mae: 7.7481\n",
      "Epoch 915/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.1693 - mae: 4.0672 - val_loss: 138.3087 - val_mae: 8.2827\n",
      "Epoch 916/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.2830 - mae: 4.0546 - val_loss: 102.7278 - val_mae: 6.8734\n",
      "Epoch 917/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.7632 - mae: 3.9709 - val_loss: 117.9996 - val_mae: 7.5322\n",
      "Epoch 918/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.1032 - mae: 4.0511 - val_loss: 82.6226 - val_mae: 6.2959\n",
      "Epoch 919/1000\n",
      "1981/1981 [==============================] - 2s 999us/step - loss: 34.0734 - mae: 4.1515 - val_loss: 90.2708 - val_mae: 7.2633\n",
      "Epoch 920/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 28.3825 - mae: 3.8374 - val_loss: 129.2632 - val_mae: 8.1642\n",
      "Epoch 921/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.6228 - mae: 4.1649 - val_loss: 90.6905 - val_mae: 6.3279\n",
      "Epoch 922/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.2983 - mae: 4.0326 - val_loss: 102.5830 - val_mae: 6.6916\n",
      "Epoch 923/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.3150 - mae: 3.9253 - val_loss: 197.7278 - val_mae: 11.1468\n",
      "Epoch 924/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.8176 - mae: 4.0821 - val_loss: 126.7998 - val_mae: 8.0960\n",
      "Epoch 925/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.3459 - mae: 4.0018 - val_loss: 83.8185 - val_mae: 6.2157\n",
      "Epoch 926/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0360 - mae: 3.9870 - val_loss: 112.4825 - val_mae: 7.8111\n",
      "Epoch 927/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.8179 - mae: 4.0158 - val_loss: 86.7602 - val_mae: 6.3158\n",
      "Epoch 928/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.7776 - mae: 4.1556 - val_loss: 92.1888 - val_mae: 6.4969\n",
      "Epoch 929/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.4665 - mae: 4.1057 - val_loss: 156.8880 - val_mae: 9.7624\n",
      "Epoch 930/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.6156 - mae: 3.9473 - val_loss: 152.2255 - val_mae: 9.6238\n",
      "Epoch 931/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0849 - mae: 3.9691 - val_loss: 102.3748 - val_mae: 7.0082\n",
      "Epoch 932/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.6537 - mae: 3.9948 - val_loss: 83.9202 - val_mae: 6.3523\n",
      "Epoch 933/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.4096 - mae: 4.1816 - val_loss: 79.8509 - val_mae: 6.3985\n",
      "Epoch 934/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 39.1573 - mae: 4.6781 - val_loss: 124.0569 - val_mae: 7.7676\n",
      "Epoch 935/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.0814 - mae: 4.2386 - val_loss: 102.5424 - val_mae: 6.6836\n",
      "Epoch 936/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.7282 - mae: 4.1660 - val_loss: 103.6389 - val_mae: 6.7222\n",
      "Epoch 937/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.5194 - mae: 4.1558 - val_loss: 118.2825 - val_mae: 7.5235\n",
      "Epoch 938/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6956 - mae: 4.0989 - val_loss: 94.6729 - val_mae: 6.7178\n",
      "Epoch 939/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.6307 - mae: 3.9342 - val_loss: 115.1091 - val_mae: 7.1603\n",
      "Epoch 940/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.5166 - mae: 4.0435 - val_loss: 137.6984 - val_mae: 8.1264\n",
      "Epoch 941/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.6015 - mae: 4.0824 - val_loss: 123.0943 - val_mae: 7.4953\n",
      "Epoch 942/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.8578 - mae: 4.0009 - val_loss: 131.4805 - val_mae: 8.1875\n",
      "Epoch 943/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.4400 - mae: 4.1554 - val_loss: 105.3989 - val_mae: 7.2055\n",
      "Epoch 944/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 35.4934 - mae: 4.3716 - val_loss: 121.3752 - val_mae: 7.8099\n",
      "Epoch 945/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 34.3232 - mae: 4.0789 - val_loss: 120.2629 - val_mae: 7.5946\n",
      "Epoch 946/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 31.3385 - mae: 3.9889 - val_loss: 110.3831 - val_mae: 7.1206\n",
      "Epoch 947/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 32.0124 - mae: 4.0003 - val_loss: 101.9046 - val_mae: 6.9361\n",
      "Epoch 948/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 30.6961 - mae: 3.8880 - val_loss: 237.5275 - val_mae: 12.4300\n",
      "Epoch 949/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 33.4033 - mae: 4.0861 - val_loss: 180.8527 - val_mae: 10.4406\n",
      "Epoch 00949: early stopping\n",
      "\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 305us/step\n",
      "test loss, test acc: [242.07929908207484, 11.751020431518555]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 15.726132362129597\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>true_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>204.541183</td>\n",
       "      <td>185.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211.480499</td>\n",
       "      <td>176.979996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189.951614</td>\n",
       "      <td>176.779999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185.403137</td>\n",
       "      <td>172.289993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>193.979935</td>\n",
       "      <td>174.240005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>225.095215</td>\n",
       "      <td>259.429993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>250.173492</td>\n",
       "      <td>260.140015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>256.476410</td>\n",
       "      <td>262.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>231.364120</td>\n",
       "      <td>261.959991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>245.134384</td>\n",
       "      <td>264.470001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  true_value\n",
       "0    204.541183  185.860001\n",
       "1    211.480499  176.979996\n",
       "2    189.951614  176.779999\n",
       "3    185.403137  172.289993\n",
       "4    193.979935  174.240005\n",
       "..          ...         ...\n",
       "240  225.095215  259.429993\n",
       "241  250.173492  260.140015\n",
       "242  256.476410  262.200012\n",
       "243  231.364120  261.959991\n",
       "244  245.134384  264.470001\n",
       "\n",
       "[245 rows x 2 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_2stacks_true_value, 7, \n",
    "                                                    stock_with_abs_norm, label_value_1d, 64, batch_size, \"loss\")\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=7)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test, batch_size=7)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)\n",
    "predictFrame = pd.DataFrame({'prediction': predictions.reshape(X_test.shape[0]), 'true_value': y_test.reshape(X_test.shape[0])})\n",
    "predictFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = predicted_train.reshape(X_train.shape[0]).tolist()\n",
    "b = predicted_valid.reshape(X_valid.shape[0]).tolist()\n",
    "c = predicted_test.reshape(X_test.shape[0]).tolist()\n",
    "d = y_train.reshape(X_train.shape[0]).tolist()\n",
    "e = y_valid.reshape(X_valid.shape[0]).tolist()\n",
    "f = y_test.reshape(X_test.shape[0]).tolist()\n",
    "a.extend(b)\n",
    "a.extend(c)\n",
    "d.extend(e)\n",
    "d.extend(f)\n",
    "predictFrame = pd.DataFrame({'prediction': a, 'true_value': d})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1988 samples, validate on 217 samples\n",
      "Epoch 1/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 648.6874 - mae: 22.0370 - val_loss: 2830.4064 - val_mae: 49.1528\n",
      "Epoch 2/1000\n",
      "1988/1988 [==============================] - 1s 460us/step - loss: 1455.2868 - mae: 28.2063 - val_loss: 4331.6485 - val_mae: 62.5879\n",
      "Epoch 3/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 1373.5670 - mae: 28.3823 - val_loss: 4561.5846 - val_mae: 64.3986\n",
      "Epoch 4/1000\n",
      "1988/1988 [==============================] - 1s 447us/step - loss: 1351.4389 - mae: 27.9814 - val_loss: 4717.6737 - val_mae: 65.5993\n",
      "Epoch 5/1000\n",
      "1988/1988 [==============================] - 1s 449us/step - loss: 1361.0586 - mae: 28.2912 - val_loss: 4719.5657 - val_mae: 65.6137\n",
      "Epoch 6/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 1397.0091 - mae: 28.5858 - val_loss: 4923.1604 - val_mae: 67.1472\n",
      "Epoch 7/1000\n",
      "1988/1988 [==============================] - 1s 479us/step - loss: 1357.1133 - mae: 28.2824 - val_loss: 4667.5400 - val_mae: 65.2161\n",
      "Epoch 8/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 1404.5923 - mae: 28.9783 - val_loss: 4751.6599 - val_mae: 65.8578\n",
      "Epoch 9/1000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 1371.4458 - mae: 28.4709 - val_loss: 4925.4672 - val_mae: 67.1644\n",
      "Epoch 10/1000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 1387.7820 - mae: 28.5357 - val_loss: 4838.4396 - val_mae: 66.5134\n",
      "Epoch 11/1000\n",
      "1988/1988 [==============================] - 1s 489us/step - loss: 1258.4680 - mae: 26.4595 - val_loss: 4481.7361 - val_mae: 63.7756\n",
      "Epoch 12/1000\n",
      "1988/1988 [==============================] - 1s 488us/step - loss: 960.7205 - mae: 23.1163 - val_loss: 3580.7492 - val_mae: 56.2759\n",
      "Epoch 13/1000\n",
      "1988/1988 [==============================] - 1s 507us/step - loss: 649.4066 - mae: 18.7194 - val_loss: 3186.4247 - val_mae: 52.6564\n",
      "Epoch 14/1000\n",
      "1988/1988 [==============================] - 1s 501us/step - loss: 425.0478 - mae: 15.7247 - val_loss: 2418.4212 - val_mae: 44.7887\n",
      "Epoch 15/1000\n",
      "1988/1988 [==============================] - 1s 490us/step - loss: 342.4574 - mae: 14.4477 - val_loss: 1828.3947 - val_mae: 37.6607\n",
      "Epoch 16/1000\n",
      "1988/1988 [==============================] - 1s 508us/step - loss: 257.9242 - mae: 12.7335 - val_loss: 1736.4051 - val_mae: 36.4823\n",
      "Epoch 17/1000\n",
      "1988/1988 [==============================] - 1s 505us/step - loss: 232.7181 - mae: 12.1777 - val_loss: 1664.2365 - val_mae: 35.5251\n",
      "Epoch 18/1000\n",
      "1988/1988 [==============================] - 1s 506us/step - loss: 203.2030 - mae: 11.2691 - val_loss: 1552.2751 - val_mae: 34.0079\n",
      "Epoch 19/1000\n",
      "1988/1988 [==============================] - 1s 490us/step - loss: 194.4900 - mae: 10.9000 - val_loss: 1300.1917 - val_mae: 30.2330\n",
      "Epoch 20/1000\n",
      "1988/1988 [==============================] - 1s 455us/step - loss: 196.0219 - mae: 10.8042 - val_loss: 1632.2554 - val_mae: 35.3202\n",
      "Epoch 21/1000\n",
      "1988/1988 [==============================] - 1s 458us/step - loss: 198.1145 - mae: 10.8777 - val_loss: 1396.6257 - val_mae: 31.9132\n",
      "Epoch 22/1000\n",
      "1988/1988 [==============================] - 1s 516us/step - loss: 181.3787 - mae: 10.3984 - val_loss: 1347.7278 - val_mae: 31.1399\n",
      "Epoch 23/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 171.0052 - mae: 10.0309 - val_loss: 1472.3395 - val_mae: 33.1994\n",
      "Epoch 24/1000\n",
      "1988/1988 [==============================] - 1s 505us/step - loss: 186.5442 - mae: 10.5126 - val_loss: 1268.9305 - val_mae: 30.0632\n",
      "Epoch 25/1000\n",
      "1988/1988 [==============================] - 1s 457us/step - loss: 192.8658 - mae: 10.5566 - val_loss: 1208.7526 - val_mae: 29.1580\n",
      "Epoch 26/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 170.9729 - mae: 10.1442 - val_loss: 1347.3022 - val_mae: 31.6152\n",
      "Epoch 27/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 170.7733 - mae: 10.0279 - val_loss: 1255.8190 - val_mae: 30.2151\n",
      "Epoch 28/1000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 177.2359 - mae: 10.3924 - val_loss: 1243.5722 - val_mae: 29.9071\n",
      "Epoch 29/1000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 176.0152 - mae: 10.1123 - val_loss: 1071.6796 - val_mae: 26.9976\n",
      "Epoch 30/1000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 183.1191 - mae: 10.5615 - val_loss: 1000.4740 - val_mae: 25.9693\n",
      "Epoch 31/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 190.1444 - mae: 10.4679 - val_loss: 1206.0009 - val_mae: 29.8270\n",
      "Epoch 32/1000\n",
      "1988/1988 [==============================] - 1s 471us/step - loss: 173.3494 - mae: 10.1150 - val_loss: 954.2549 - val_mae: 25.2755\n",
      "Epoch 33/1000\n",
      "1988/1988 [==============================] - 1s 478us/step - loss: 185.0568 - mae: 10.3723 - val_loss: 1073.9539 - val_mae: 27.6283\n",
      "Epoch 34/1000\n",
      "1988/1988 [==============================] - 1s 486us/step - loss: 182.8349 - mae: 10.2922 - val_loss: 1025.3836 - val_mae: 26.8034\n",
      "Epoch 35/1000\n",
      "1988/1988 [==============================] - 1s 503us/step - loss: 176.6427 - mae: 10.1834 - val_loss: 996.4436 - val_mae: 26.4529\n",
      "Epoch 36/1000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 178.5919 - mae: 10.1753 - val_loss: 827.9834 - val_mae: 22.7388\n",
      "Epoch 37/1000\n",
      "1988/1988 [==============================] - 1s 483us/step - loss: 177.8313 - mae: 10.3415 - val_loss: 1461.9643 - val_mae: 33.9317\n",
      "Epoch 38/1000\n",
      "1988/1988 [==============================] - 1s 491us/step - loss: 174.6689 - mae: 10.1180 - val_loss: 1025.8528 - val_mae: 26.8343\n",
      "Epoch 39/1000\n",
      "1988/1988 [==============================] - 1s 467us/step - loss: 166.1123 - mae: 9.8213 - val_loss: 955.8811 - val_mae: 25.2760\n",
      "Epoch 40/1000\n",
      "1988/1988 [==============================] - 1s 485us/step - loss: 157.7512 - mae: 9.6603 - val_loss: 1184.3736 - val_mae: 29.2855\n",
      "Epoch 41/1000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 164.9127 - mae: 9.7026 - val_loss: 1046.3133 - val_mae: 26.8496\n",
      "Epoch 42/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 170.6010 - mae: 9.9593 - val_loss: 1004.3309 - val_mae: 26.0958\n",
      "Epoch 43/1000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 153.6836 - mae: 9.3860 - val_loss: 1117.9860 - val_mae: 28.5931\n",
      "Epoch 44/1000\n",
      "1988/1988 [==============================] - 1s 502us/step - loss: 164.5364 - mae: 9.7401 - val_loss: 933.1910 - val_mae: 25.5378\n",
      "Epoch 45/1000\n",
      "1988/1988 [==============================] - 1s 478us/step - loss: 159.5556 - mae: 9.6643 - val_loss: 932.1543 - val_mae: 25.3787\n",
      "Epoch 46/1000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 163.0340 - mae: 9.6923 - val_loss: 848.0880 - val_mae: 23.2065\n",
      "Epoch 47/1000\n",
      "1988/1988 [==============================] - 1s 504us/step - loss: 153.0945 - mae: 9.4248 - val_loss: 997.9738 - val_mae: 26.7059\n",
      "Epoch 48/1000\n",
      "1988/1988 [==============================] - 1s 478us/step - loss: 158.7922 - mae: 9.6682 - val_loss: 923.4425 - val_mae: 25.1888\n",
      "Epoch 49/1000\n",
      "1988/1988 [==============================] - 1s 483us/step - loss: 147.4030 - mae: 9.1342 - val_loss: 921.8365 - val_mae: 25.2541\n",
      "Epoch 50/1000\n",
      "1988/1988 [==============================] - 1s 503us/step - loss: 156.0828 - mae: 9.4153 - val_loss: 834.0677 - val_mae: 23.0879\n",
      "Epoch 51/1000\n",
      "1988/1988 [==============================] - 1s 485us/step - loss: 150.2333 - mae: 9.1963 - val_loss: 1010.9768 - val_mae: 26.6325\n",
      "Epoch 52/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 150.3272 - mae: 9.1806 - val_loss: 1008.4158 - val_mae: 26.3043\n",
      "Epoch 53/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 150.7528 - mae: 9.2167 - val_loss: 789.2721 - val_mae: 22.0610\n",
      "Epoch 54/1000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 141.3114 - mae: 8.9806 - val_loss: 1079.9216 - val_mae: 27.9931\n",
      "Epoch 55/1000\n",
      "1988/1988 [==============================] - 1s 476us/step - loss: 147.8053 - mae: 9.1212 - val_loss: 904.8106 - val_mae: 24.7717\n",
      "Epoch 56/1000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 139.9248 - mae: 8.9553 - val_loss: 1075.5625 - val_mae: 28.2189\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 1s 470us/step - loss: 141.6541 - mae: 8.7845 - val_loss: 847.4994 - val_mae: 23.5252\n",
      "Epoch 58/1000\n",
      "1988/1988 [==============================] - 1s 453us/step - loss: 132.5476 - mae: 8.6533 - val_loss: 890.2791 - val_mae: 24.7583\n",
      "Epoch 59/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 138.7303 - mae: 8.8871 - val_loss: 1262.5097 - val_mae: 31.7390\n",
      "Epoch 60/1000\n",
      "1988/1988 [==============================] - 1s 468us/step - loss: 136.3729 - mae: 8.7283 - val_loss: 732.6111 - val_mae: 21.7962\n",
      "Epoch 61/1000\n",
      "1988/1988 [==============================] - 1s 461us/step - loss: 137.4399 - mae: 8.7194 - val_loss: 747.0145 - val_mae: 21.8391\n",
      "Epoch 62/1000\n",
      "1988/1988 [==============================] - 1s 451us/step - loss: 124.2813 - mae: 8.2855 - val_loss: 720.4638 - val_mae: 20.7551\n",
      "Epoch 63/1000\n",
      "1988/1988 [==============================] - 1s 458us/step - loss: 138.2635 - mae: 8.7561 - val_loss: 824.5376 - val_mae: 22.5819\n",
      "Epoch 64/1000\n",
      "1988/1988 [==============================] - 1s 468us/step - loss: 135.2194 - mae: 8.6745 - val_loss: 806.1205 - val_mae: 22.0365\n",
      "Epoch 65/1000\n",
      "1988/1988 [==============================] - 1s 457us/step - loss: 120.0892 - mae: 8.1328 - val_loss: 762.8055 - val_mae: 21.4515\n",
      "Epoch 66/1000\n",
      "1988/1988 [==============================] - 1s 458us/step - loss: 123.5703 - mae: 8.2495 - val_loss: 845.4754 - val_mae: 23.5454\n",
      "Epoch 67/1000\n",
      "1988/1988 [==============================] - 1s 457us/step - loss: 122.7032 - mae: 8.1482 - val_loss: 812.6754 - val_mae: 23.1605\n",
      "Epoch 68/1000\n",
      "1988/1988 [==============================] - 1s 475us/step - loss: 119.6645 - mae: 8.0940 - val_loss: 771.5517 - val_mae: 21.5878\n",
      "Epoch 69/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 118.7808 - mae: 7.9860 - val_loss: 889.4353 - val_mae: 24.3089\n",
      "Epoch 70/1000\n",
      "1988/1988 [==============================] - 1s 470us/step - loss: 114.9672 - mae: 7.8486 - val_loss: 673.1840 - val_mae: 19.9220\n",
      "Epoch 71/1000\n",
      "1988/1988 [==============================] - 1s 466us/step - loss: 114.4496 - mae: 7.9545 - val_loss: 839.7382 - val_mae: 23.9491\n",
      "Epoch 72/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 104.5067 - mae: 7.5667 - val_loss: 765.4527 - val_mae: 22.2013\n",
      "Epoch 73/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 107.3092 - mae: 7.7021 - val_loss: 731.4548 - val_mae: 21.8040\n",
      "Epoch 74/1000\n",
      "1988/1988 [==============================] - 1s 480us/step - loss: 94.2396 - mae: 7.3260 - val_loss: 694.5701 - val_mae: 20.4360\n",
      "Epoch 75/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 96.1818 - mae: 7.1814 - val_loss: 762.0517 - val_mae: 21.7048\n",
      "Epoch 76/1000\n",
      "1988/1988 [==============================] - 1s 480us/step - loss: 101.7599 - mae: 7.4847 - val_loss: 989.0624 - val_mae: 26.8528\n",
      "Epoch 77/1000\n",
      "1988/1988 [==============================] - 1s 487us/step - loss: 89.3833 - mae: 7.0267 - val_loss: 817.3788 - val_mae: 23.1774\n",
      "Epoch 78/1000\n",
      "1988/1988 [==============================] - 1s 479us/step - loss: 86.9669 - mae: 6.9647 - val_loss: 604.8177 - val_mae: 18.6392\n",
      "Epoch 79/1000\n",
      "1988/1988 [==============================] - 1s 485us/step - loss: 84.8292 - mae: 6.8337 - val_loss: 870.7895 - val_mae: 24.1410\n",
      "Epoch 80/1000\n",
      "1988/1988 [==============================] - 1s 480us/step - loss: 96.1149 - mae: 7.2447 - val_loss: 587.7202 - val_mae: 18.3354\n",
      "Epoch 81/1000\n",
      "1988/1988 [==============================] - 1s 489us/step - loss: 81.9452 - mae: 6.8568 - val_loss: 576.2879 - val_mae: 17.8226\n",
      "Epoch 82/1000\n",
      "1988/1988 [==============================] - 1s 498us/step - loss: 84.6901 - mae: 6.8106 - val_loss: 773.2582 - val_mae: 21.6197\n",
      "Epoch 83/1000\n",
      "1988/1988 [==============================] - 1s 478us/step - loss: 98.3349 - mae: 7.6036 - val_loss: 874.7989 - val_mae: 23.6517\n",
      "Epoch 84/1000\n",
      "1988/1988 [==============================] - 1s 489us/step - loss: 89.2102 - mae: 7.1556 - val_loss: 988.9820 - val_mae: 25.8731\n",
      "Epoch 85/1000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 85.0099 - mae: 7.0270 - val_loss: 863.9862 - val_mae: 23.5719\n",
      "Epoch 86/1000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 77.7270 - mae: 6.5808 - val_loss: 816.2155 - val_mae: 22.3999\n",
      "Epoch 87/1000\n",
      "1988/1988 [==============================] - 1s 456us/step - loss: 85.2137 - mae: 6.7815 - val_loss: 1187.9599 - val_mae: 29.6175\n",
      "Epoch 88/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 79.7032 - mae: 6.6865 - val_loss: 1112.3572 - val_mae: 28.2347\n",
      "Epoch 89/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 85.1800 - mae: 6.8002 - val_loss: 967.2969 - val_mae: 25.5534\n",
      "Epoch 90/1000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 79.0657 - mae: 6.6115 - val_loss: 857.3429 - val_mae: 23.2487\n",
      "Epoch 91/1000\n",
      "1988/1988 [==============================] - 1s 455us/step - loss: 74.6985 - mae: 6.4730 - val_loss: 725.6060 - val_mae: 20.7523\n",
      "Epoch 92/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 77.6794 - mae: 6.5744 - val_loss: 727.8655 - val_mae: 20.7318\n",
      "Epoch 93/1000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 77.9665 - mae: 6.6148 - val_loss: 770.7992 - val_mae: 21.5210\n",
      "Epoch 94/1000\n",
      "1988/1988 [==============================] - 1s 478us/step - loss: 74.9763 - mae: 6.5354 - val_loss: 804.2672 - val_mae: 23.0180\n",
      "Epoch 95/1000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 71.1344 - mae: 6.3045 - val_loss: 690.7609 - val_mae: 20.2534\n",
      "Epoch 96/1000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 72.9480 - mae: 6.3200 - val_loss: 660.8815 - val_mae: 19.4839\n",
      "Epoch 97/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 73.5933 - mae: 6.1424 - val_loss: 619.9062 - val_mae: 18.6171\n",
      "Epoch 98/1000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 81.7059 - mae: 6.6140 - val_loss: 771.2331 - val_mae: 21.3819\n",
      "Epoch 99/1000\n",
      "1988/1988 [==============================] - 1s 461us/step - loss: 77.5114 - mae: 6.5084 - val_loss: 776.4714 - val_mae: 22.5269\n",
      "Epoch 100/1000\n",
      "1988/1988 [==============================] - 1s 449us/step - loss: 71.9579 - mae: 6.1977 - val_loss: 706.0448 - val_mae: 20.4164\n",
      "Epoch 101/1000\n",
      "1988/1988 [==============================] - 1s 450us/step - loss: 66.6089 - mae: 5.9544 - val_loss: 743.8671 - val_mae: 20.9630\n",
      "Epoch 102/1000\n",
      "1988/1988 [==============================] - 1s 459us/step - loss: 68.5054 - mae: 6.1442 - val_loss: 854.6059 - val_mae: 22.9815\n",
      "Epoch 103/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 67.9370 - mae: 6.1252 - val_loss: 774.6836 - val_mae: 21.6826\n",
      "Epoch 104/1000\n",
      "1988/1988 [==============================] - 1s 480us/step - loss: 70.8218 - mae: 6.1982 - val_loss: 694.4625 - val_mae: 20.3225\n",
      "Epoch 105/1000\n",
      "1988/1988 [==============================] - 1s 491us/step - loss: 70.4174 - mae: 6.0622 - val_loss: 872.9246 - val_mae: 25.4196\n",
      "Epoch 106/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 69.4370 - mae: 6.0612 - val_loss: 530.2144 - val_mae: 17.3079\n",
      "Epoch 107/1000\n",
      "1988/1988 [==============================] - 1s 460us/step - loss: 69.8424 - mae: 6.1932 - val_loss: 628.0486 - val_mae: 18.9789\n",
      "Epoch 108/1000\n",
      "1988/1988 [==============================] - 1s 460us/step - loss: 68.2650 - mae: 6.0752 - val_loss: 695.1407 - val_mae: 20.0033\n",
      "Epoch 109/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 74.5955 - mae: 6.3370 - val_loss: 664.9830 - val_mae: 19.3334\n",
      "Epoch 110/1000\n",
      "1988/1988 [==============================] - 1s 459us/step - loss: 72.3788 - mae: 6.2059 - val_loss: 683.6795 - val_mae: 19.7004\n",
      "Epoch 111/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 65.7959 - mae: 5.9042 - val_loss: 794.6657 - val_mae: 22.7285\n",
      "Epoch 112/1000\n",
      "1988/1988 [==============================] - 1s 494us/step - loss: 69.8952 - mae: 6.1519 - val_loss: 678.8635 - val_mae: 20.2348\n",
      "Epoch 113/1000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 67.2956 - mae: 6.1181 - val_loss: 839.9332 - val_mae: 24.4961\n",
      "Epoch 114/1000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 68.2270 - mae: 6.0291 - val_loss: 576.9066 - val_mae: 18.1784\n",
      "Epoch 115/1000\n",
      "1988/1988 [==============================] - 1s 483us/step - loss: 79.7998 - mae: 6.7035 - val_loss: 754.3609 - val_mae: 21.3892\n",
      "Epoch 116/1000\n",
      "1988/1988 [==============================] - 1s 486us/step - loss: 70.0461 - mae: 6.0808 - val_loss: 671.7870 - val_mae: 19.5048\n",
      "Epoch 117/1000\n",
      "1988/1988 [==============================] - 1s 487us/step - loss: 70.7089 - mae: 6.0372 - val_loss: 669.4510 - val_mae: 19.5851\n",
      "Epoch 118/1000\n",
      "1988/1988 [==============================] - 1s 494us/step - loss: 67.9780 - mae: 5.9363 - val_loss: 823.0424 - val_mae: 22.6259\n",
      "Epoch 119/1000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 69.4371 - mae: 5.8880 - val_loss: 707.0090 - val_mae: 20.4863\n",
      "Epoch 120/1000\n",
      "1988/1988 [==============================] - 1s 452us/step - loss: 66.5066 - mae: 5.8846 - val_loss: 799.3574 - val_mae: 22.2418\n",
      "Epoch 121/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 65.0093 - mae: 5.8292 - val_loss: 905.3390 - val_mae: 24.5772\n",
      "Epoch 122/1000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 74.9219 - mae: 6.1161 - val_loss: 795.2332 - val_mae: 21.9434\n",
      "Epoch 123/1000\n",
      "1988/1988 [==============================] - 1s 448us/step - loss: 69.1960 - mae: 6.0438 - val_loss: 757.8890 - val_mae: 21.2226\n",
      "Epoch 124/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 63.3143 - mae: 5.8533 - val_loss: 722.8534 - val_mae: 20.9631\n",
      "Epoch 125/1000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 68.8259 - mae: 5.8862 - val_loss: 717.0583 - val_mae: 20.8009\n",
      "Epoch 126/1000\n",
      "1988/1988 [==============================] - 1s 446us/step - loss: 63.1627 - mae: 5.7535 - val_loss: 725.5860 - val_mae: 20.8359\n",
      "Epoch 127/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 67.6652 - mae: 5.8394 - val_loss: 646.9872 - val_mae: 19.3097\n",
      "Epoch 128/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 67.5605 - mae: 5.8722 - val_loss: 571.8744 - val_mae: 17.7553\n",
      "Epoch 129/1000\n",
      "1988/1988 [==============================] - 1s 475us/step - loss: 71.2006 - mae: 6.1144 - val_loss: 681.0565 - val_mae: 19.6169\n",
      "Epoch 130/1000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 71.3929 - mae: 6.2810 - val_loss: 840.0505 - val_mae: 22.7559\n",
      "Epoch 131/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 77.9482 - mae: 6.4893 - val_loss: 893.1097 - val_mae: 24.0280\n",
      "Epoch 132/1000\n",
      "1988/1988 [==============================] - 1s 451us/step - loss: 70.4580 - mae: 6.0819 - val_loss: 612.7675 - val_mae: 18.4878\n",
      "Epoch 133/1000\n",
      "1988/1988 [==============================] - 1s 469us/step - loss: 66.0635 - mae: 5.8351 - val_loss: 652.8757 - val_mae: 19.2288\n",
      "Epoch 134/1000\n",
      "1988/1988 [==============================] - 1s 497us/step - loss: 68.0992 - mae: 5.9490 - val_loss: 661.8299 - val_mae: 19.2592\n",
      "Epoch 135/1000\n",
      "1988/1988 [==============================] - 1s 478us/step - loss: 66.9944 - mae: 5.9661 - val_loss: 852.2120 - val_mae: 22.7204\n",
      "Epoch 136/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 63.1665 - mae: 5.7742 - val_loss: 906.6746 - val_mae: 23.8627\n",
      "Epoch 137/1000\n",
      "1988/1988 [==============================] - 1s 456us/step - loss: 69.5629 - mae: 5.9566 - val_loss: 915.0917 - val_mae: 24.0407\n",
      "Epoch 138/1000\n",
      "1988/1988 [==============================] - 1s 453us/step - loss: 65.2677 - mae: 5.8699 - val_loss: 709.6281 - val_mae: 20.5158\n",
      "Epoch 139/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 59.6147 - mae: 5.4921 - val_loss: 676.9048 - val_mae: 19.6886\n",
      "Epoch 140/1000\n",
      "1988/1988 [==============================] - 1s 451us/step - loss: 73.1218 - mae: 6.1546 - val_loss: 801.4375 - val_mae: 22.2983\n",
      "Epoch 141/1000\n",
      "1988/1988 [==============================] - 1s 460us/step - loss: 68.3883 - mae: 6.0031 - val_loss: 696.1285 - val_mae: 20.0877\n",
      "Epoch 142/1000\n",
      "1988/1988 [==============================] - 1s 540us/step - loss: 66.9806 - mae: 5.8992 - val_loss: 732.8030 - val_mae: 20.6051\n",
      "Epoch 143/1000\n",
      "1988/1988 [==============================] - 1s 492us/step - loss: 65.0781 - mae: 5.7433 - val_loss: 666.6954 - val_mae: 19.2886\n",
      "Epoch 144/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 69.5261 - mae: 5.8787 - val_loss: 695.6482 - val_mae: 19.8170\n",
      "Epoch 145/1000\n",
      "1988/1988 [==============================] - 1s 467us/step - loss: 67.4185 - mae: 5.8381 - val_loss: 780.6145 - val_mae: 21.3674\n",
      "Epoch 146/1000\n",
      "1988/1988 [==============================] - 1s 466us/step - loss: 63.5454 - mae: 5.7293 - val_loss: 830.0690 - val_mae: 22.7798\n",
      "Epoch 147/1000\n",
      "1988/1988 [==============================] - 1s 461us/step - loss: 64.8861 - mae: 5.7224 - val_loss: 507.1529 - val_mae: 16.5379\n",
      "Epoch 148/1000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 69.2236 - mae: 5.9489 - val_loss: 689.2318 - val_mae: 20.3109\n",
      "Epoch 149/1000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 62.5789 - mae: 5.6521 - val_loss: 899.1371 - val_mae: 25.3362\n",
      "Epoch 150/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 64.8974 - mae: 5.7692 - val_loss: 761.5485 - val_mae: 21.4293\n",
      "Epoch 151/1000\n",
      "1988/1988 [==============================] - 1s 450us/step - loss: 67.8652 - mae: 5.9045 - val_loss: 890.1506 - val_mae: 23.4179\n",
      "Epoch 152/1000\n",
      "1988/1988 [==============================] - 1s 439us/step - loss: 64.7794 - mae: 5.8137 - val_loss: 993.5402 - val_mae: 25.2551\n",
      "Epoch 153/1000\n",
      "1988/1988 [==============================] - 1s 446us/step - loss: 68.1515 - mae: 5.8460 - val_loss: 727.3071 - val_mae: 20.5552\n",
      "Epoch 154/1000\n",
      "1988/1988 [==============================] - 1s 448us/step - loss: 62.9636 - mae: 5.7083 - val_loss: 687.2306 - val_mae: 20.0734\n",
      "Epoch 155/1000\n",
      "1988/1988 [==============================] - 1s 444us/step - loss: 57.9302 - mae: 5.4408 - val_loss: 559.2767 - val_mae: 17.8145\n",
      "Epoch 156/1000\n",
      "1988/1988 [==============================] - 1s 456us/step - loss: 69.7051 - mae: 6.1158 - val_loss: 579.2439 - val_mae: 17.8634\n",
      "Epoch 157/1000\n",
      "1988/1988 [==============================] - 1s 450us/step - loss: 60.6060 - mae: 5.6076 - val_loss: 870.2689 - val_mae: 23.5854\n",
      "Epoch 158/1000\n",
      "1988/1988 [==============================] - 1s 442us/step - loss: 63.3705 - mae: 5.6755 - val_loss: 776.7684 - val_mae: 21.3278\n",
      "Epoch 159/1000\n",
      "1988/1988 [==============================] - 1s 453us/step - loss: 66.0180 - mae: 5.7517 - val_loss: 1044.7503 - val_mae: 26.4193\n",
      "Epoch 160/1000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 60.5222 - mae: 5.6468 - val_loss: 927.0335 - val_mae: 24.2017\n",
      "Epoch 161/1000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 61.7478 - mae: 5.6099 - val_loss: 790.7958 - val_mae: 21.7377\n",
      "Epoch 162/1000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 63.4303 - mae: 5.5829 - val_loss: 658.0778 - val_mae: 19.6565\n",
      "Epoch 163/1000\n",
      "1988/1988 [==============================] - 1s 461us/step - loss: 62.7987 - mae: 5.6235 - val_loss: 825.5702 - val_mae: 23.3509\n",
      "Epoch 164/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 64.3865 - mae: 5.6211 - val_loss: 681.3698 - val_mae: 20.3730\n",
      "Epoch 165/1000\n",
      "1988/1988 [==============================] - 1s 471us/step - loss: 65.1470 - mae: 5.8333 - val_loss: 779.4335 - val_mae: 21.8454\n",
      "Epoch 166/1000\n",
      "1988/1988 [==============================] - 1s 511us/step - loss: 65.1004 - mae: 5.7157 - val_loss: 640.7768 - val_mae: 19.3516\n",
      "Epoch 167/1000\n",
      "1988/1988 [==============================] - 1s 553us/step - loss: 63.9255 - mae: 5.6528 - val_loss: 506.6329 - val_mae: 16.8369\n",
      "Epoch 168/1000\n",
      "1988/1988 [==============================] - 1s 492us/step - loss: 61.8451 - mae: 5.5286 - val_loss: 713.1597 - val_mae: 20.2772\n",
      "Epoch 169/1000\n",
      "1988/1988 [==============================] - 1s 501us/step - loss: 60.8946 - mae: 5.5046 - val_loss: 841.6993 - val_mae: 22.9931\n",
      "Epoch 170/1000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 58.7376 - mae: 5.3719 - val_loss: 703.3300 - val_mae: 20.3864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/1000\n",
      "1988/1988 [==============================] - 1s 488us/step - loss: 67.6920 - mae: 5.8800 - val_loss: 660.8508 - val_mae: 19.2952\n",
      "Epoch 172/1000\n",
      "1988/1988 [==============================] - 1s 511us/step - loss: 59.9643 - mae: 5.5231 - val_loss: 562.0155 - val_mae: 17.8541\n",
      "Epoch 173/1000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 70.1369 - mae: 5.9575 - val_loss: 1115.3901 - val_mae: 27.8900\n",
      "Epoch 174/1000\n",
      "1988/1988 [==============================] - 1s 476us/step - loss: 62.3477 - mae: 5.4861 - val_loss: 707.8180 - val_mae: 20.1642\n",
      "Epoch 175/1000\n",
      "1988/1988 [==============================] - 1s 470us/step - loss: 65.7882 - mae: 5.9574 - val_loss: 898.2169 - val_mae: 24.8479\n",
      "Epoch 176/1000\n",
      "1988/1988 [==============================] - 1s 475us/step - loss: 62.7835 - mae: 5.6042 - val_loss: 589.9952 - val_mae: 18.1219\n",
      "Epoch 177/1000\n",
      "1988/1988 [==============================] - 1s 449us/step - loss: 60.3059 - mae: 5.6517 - val_loss: 866.9529 - val_mae: 23.0680\n",
      "Epoch 178/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 63.7465 - mae: 5.6992 - val_loss: 909.0025 - val_mae: 24.1967\n",
      "Epoch 179/1000\n",
      "1988/1988 [==============================] - 1s 444us/step - loss: 61.3268 - mae: 5.4954 - val_loss: 670.5877 - val_mae: 19.5270\n",
      "Epoch 180/1000\n",
      "1988/1988 [==============================] - 1s 424us/step - loss: 64.5667 - mae: 5.7118 - val_loss: 547.3996 - val_mae: 17.5747\n",
      "Epoch 181/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 65.3390 - mae: 5.7299 - val_loss: 861.5706 - val_mae: 23.5519\n",
      "Epoch 182/1000\n",
      "1988/1988 [==============================] - 1s 427us/step - loss: 64.7681 - mae: 5.6070 - val_loss: 702.9183 - val_mae: 20.3759\n",
      "Epoch 183/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 62.0582 - mae: 5.6943 - val_loss: 574.4540 - val_mae: 17.8079\n",
      "Epoch 184/1000\n",
      "1988/1988 [==============================] - 1s 450us/step - loss: 69.0623 - mae: 5.8024 - val_loss: 808.1439 - val_mae: 21.9568\n",
      "Epoch 185/1000\n",
      "1988/1988 [==============================] - 1s 443us/step - loss: 62.1493 - mae: 5.5664 - val_loss: 679.3537 - val_mae: 19.6606\n",
      "Epoch 186/1000\n",
      "1988/1988 [==============================] - 1s 424us/step - loss: 61.8070 - mae: 5.5818 - val_loss: 641.4996 - val_mae: 19.2595\n",
      "Epoch 187/1000\n",
      "1988/1988 [==============================] - 1s 420us/step - loss: 62.3604 - mae: 5.5957 - val_loss: 554.6223 - val_mae: 17.3941\n",
      "Epoch 188/1000\n",
      "1988/1988 [==============================] - 1s 430us/step - loss: 66.6920 - mae: 5.8157 - val_loss: 657.9128 - val_mae: 19.2750\n",
      "Epoch 189/1000\n",
      "1988/1988 [==============================] - 1s 437us/step - loss: 63.8809 - mae: 5.7306 - val_loss: 771.9941 - val_mae: 22.2178\n",
      "Epoch 190/1000\n",
      "1988/1988 [==============================] - 1s 456us/step - loss: 63.6784 - mae: 5.7896 - val_loss: 612.5455 - val_mae: 18.4287\n",
      "Epoch 191/1000\n",
      "1988/1988 [==============================] - 1s 419us/step - loss: 70.3605 - mae: 6.1481 - val_loss: 635.8482 - val_mae: 18.7694\n",
      "Epoch 192/1000\n",
      "1988/1988 [==============================] - 1s 436us/step - loss: 63.8676 - mae: 5.6434 - val_loss: 1094.0714 - val_mae: 27.3507\n",
      "Epoch 193/1000\n",
      "1988/1988 [==============================] - 1s 459us/step - loss: 63.2224 - mae: 5.5878 - val_loss: 677.0243 - val_mae: 19.5803\n",
      "Epoch 194/1000\n",
      "1988/1988 [==============================] - 1s 460us/step - loss: 67.1303 - mae: 5.8785 - val_loss: 814.8633 - val_mae: 22.0828\n",
      "Epoch 195/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 61.9561 - mae: 5.7357 - val_loss: 852.5610 - val_mae: 22.8680\n",
      "Epoch 196/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 69.1127 - mae: 6.0149 - val_loss: 874.8287 - val_mae: 23.3948\n",
      "Epoch 197/1000\n",
      "1988/1988 [==============================] - 1s 459us/step - loss: 66.7987 - mae: 6.1124 - val_loss: 958.1535 - val_mae: 25.2841\n",
      "Epoch 198/1000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 69.6778 - mae: 5.7677 - val_loss: 684.5416 - val_mae: 19.8477\n",
      "Epoch 199/1000\n",
      "1988/1988 [==============================] - 1s 467us/step - loss: 66.4974 - mae: 5.9720 - val_loss: 617.2826 - val_mae: 18.4181\n",
      "Epoch 200/1000\n",
      "1988/1988 [==============================] - 1s 503us/step - loss: 65.1778 - mae: 5.6247 - val_loss: 728.6097 - val_mae: 20.8424\n",
      "Epoch 201/1000\n",
      "1988/1988 [==============================] - 1s 507us/step - loss: 63.3048 - mae: 5.6455 - val_loss: 691.6132 - val_mae: 20.2226\n",
      "Epoch 202/1000\n",
      "1988/1988 [==============================] - 1s 483us/step - loss: 56.7753 - mae: 5.3928 - val_loss: 566.3495 - val_mae: 17.6124\n",
      "Epoch 203/1000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 63.6162 - mae: 5.6372 - val_loss: 627.6885 - val_mae: 18.7563\n",
      "Epoch 204/1000\n",
      "1988/1988 [==============================] - 1s 479us/step - loss: 62.9739 - mae: 5.6932 - val_loss: 661.1172 - val_mae: 19.1964\n",
      "Epoch 205/1000\n",
      "1988/1988 [==============================] - 1s 471us/step - loss: 66.3600 - mae: 5.7901 - val_loss: 722.7007 - val_mae: 20.6642\n",
      "Epoch 206/1000\n",
      "1988/1988 [==============================] - 1s 458us/step - loss: 61.5600 - mae: 5.5147 - val_loss: 942.0089 - val_mae: 26.0659\n",
      "Epoch 207/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 60.4908 - mae: 5.5517 - val_loss: 548.3254 - val_mae: 17.3611\n",
      "Epoch 208/1000\n",
      "1988/1988 [==============================] - 1s 459us/step - loss: 66.5369 - mae: 5.9812 - val_loss: 644.6294 - val_mae: 19.3501\n",
      "Epoch 209/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 65.2676 - mae: 5.7722 - val_loss: 567.4997 - val_mae: 17.7094\n",
      "Epoch 210/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 69.7560 - mae: 5.9187 - val_loss: 626.5796 - val_mae: 18.5534\n",
      "Epoch 211/1000\n",
      "1988/1988 [==============================] - 1s 432us/step - loss: 68.5369 - mae: 5.7781 - val_loss: 789.3957 - val_mae: 22.1530\n",
      "Epoch 212/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 59.2778 - mae: 5.5800 - val_loss: 774.5646 - val_mae: 22.1280\n",
      "Epoch 213/1000\n",
      "1988/1988 [==============================] - 1s 436us/step - loss: 64.5441 - mae: 5.8007 - val_loss: 660.7553 - val_mae: 19.5863\n",
      "Epoch 214/1000\n",
      "1988/1988 [==============================] - 1s 451us/step - loss: 69.9731 - mae: 6.0283 - val_loss: 617.6047 - val_mae: 18.6837\n",
      "Epoch 215/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 70.1455 - mae: 6.0846 - val_loss: 579.2951 - val_mae: 17.9687\n",
      "Epoch 216/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 60.7490 - mae: 5.5104 - val_loss: 569.2879 - val_mae: 17.7113\n",
      "Epoch 217/1000\n",
      "1988/1988 [==============================] - 1s 455us/step - loss: 63.3084 - mae: 5.7559 - val_loss: 646.9927 - val_mae: 19.1798\n",
      "Epoch 218/1000\n",
      "1988/1988 [==============================] - 1s 442us/step - loss: 67.5714 - mae: 5.8239 - val_loss: 670.6093 - val_mae: 19.4450\n",
      "Epoch 219/1000\n",
      "1988/1988 [==============================] - 1s 445us/step - loss: 61.0826 - mae: 5.4830 - val_loss: 876.1019 - val_mae: 23.4143\n",
      "Epoch 220/1000\n",
      "1988/1988 [==============================] - 1s 455us/step - loss: 64.6114 - mae: 5.7082 - val_loss: 563.8740 - val_mae: 17.5410\n",
      "Epoch 221/1000\n",
      "1988/1988 [==============================] - 1s 442us/step - loss: 58.4087 - mae: 5.5153 - val_loss: 1051.1201 - val_mae: 26.9034\n",
      "Epoch 222/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 58.6903 - mae: 5.4521 - val_loss: 966.3089 - val_mae: 25.1368\n",
      "Epoch 223/1000\n",
      "1988/1988 [==============================] - 1s 452us/step - loss: 66.2365 - mae: 5.8088 - val_loss: 700.9738 - val_mae: 19.7958\n",
      "Epoch 224/1000\n",
      "1988/1988 [==============================] - 1s 442us/step - loss: 63.2241 - mae: 5.5468 - val_loss: 1146.4187 - val_mae: 28.3690\n",
      "Epoch 225/1000\n",
      "1988/1988 [==============================] - 1s 436us/step - loss: 64.8974 - mae: 5.6526 - val_loss: 1105.2151 - val_mae: 28.2624\n",
      "Epoch 226/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 60.8954 - mae: 5.4767 - val_loss: 764.8370 - val_mae: 21.5085\n",
      "Epoch 227/1000\n",
      "1988/1988 [==============================] - 1s 469us/step - loss: 61.9842 - mae: 5.6723 - val_loss: 588.3000 - val_mae: 18.0425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/1000\n",
      "1988/1988 [==============================] - 1s 449us/step - loss: 61.6077 - mae: 5.5119 - val_loss: 729.3030 - val_mae: 20.5446\n",
      "Epoch 229/1000\n",
      "1988/1988 [==============================] - 1s 452us/step - loss: 64.9567 - mae: 5.6790 - val_loss: 744.2554 - val_mae: 20.8046\n",
      "Epoch 230/1000\n",
      "1988/1988 [==============================] - 1s 461us/step - loss: 60.6135 - mae: 5.5253 - val_loss: 665.9471 - val_mae: 19.4163\n",
      "Epoch 231/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 69.7083 - mae: 6.1121 - val_loss: 978.6871 - val_mae: 25.1763\n",
      "Epoch 232/1000\n",
      "1988/1988 [==============================] - 1s 453us/step - loss: 68.0917 - mae: 5.7745 - val_loss: 615.7997 - val_mae: 18.5423\n",
      "Epoch 233/1000\n",
      "1988/1988 [==============================] - 1s 456us/step - loss: 65.0224 - mae: 5.7489 - val_loss: 679.0058 - val_mae: 20.0774\n",
      "Epoch 234/1000\n",
      "1988/1988 [==============================] - 1s 504us/step - loss: 75.2694 - mae: 6.4577 - val_loss: 446.3560 - val_mae: 15.7635\n",
      "Epoch 235/1000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 76.1915 - mae: 6.4122 - val_loss: 616.3942 - val_mae: 18.5065\n",
      "Epoch 236/1000\n",
      "1988/1988 [==============================] - 1s 446us/step - loss: 67.1922 - mae: 6.0147 - val_loss: 1196.8008 - val_mae: 28.9466\n",
      "Epoch 237/1000\n",
      "1988/1988 [==============================] - 1s 488us/step - loss: 67.5647 - mae: 6.0636 - val_loss: 672.1695 - val_mae: 19.4284\n",
      "Epoch 238/1000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 66.1066 - mae: 5.9877 - val_loss: 893.9258 - val_mae: 23.3526\n",
      "Epoch 239/1000\n",
      "1988/1988 [==============================] - 1s 471us/step - loss: 63.2747 - mae: 5.8191 - val_loss: 855.2718 - val_mae: 23.2032\n",
      "Epoch 240/1000\n",
      "1988/1988 [==============================] - 1s 492us/step - loss: 64.4694 - mae: 5.7604 - val_loss: 674.7750 - val_mae: 19.6124\n",
      "Epoch 241/1000\n",
      "1988/1988 [==============================] - 1s 452us/step - loss: 59.9918 - mae: 5.7132 - val_loss: 777.7876 - val_mae: 21.2080\n",
      "Epoch 242/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 58.7701 - mae: 5.5547 - val_loss: 679.8410 - val_mae: 19.4955\n",
      "Epoch 243/1000\n",
      "1988/1988 [==============================] - 1s 470us/step - loss: 62.0361 - mae: 5.6965 - val_loss: 701.1941 - val_mae: 20.0033\n",
      "Epoch 244/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 64.7268 - mae: 5.7276 - val_loss: 607.9617 - val_mae: 18.3650\n",
      "Epoch 245/1000\n",
      "1988/1988 [==============================] - 1s 452us/step - loss: 60.4982 - mae: 5.4793 - val_loss: 744.9910 - val_mae: 20.7325\n",
      "Epoch 246/1000\n",
      "1988/1988 [==============================] - 1s 496us/step - loss: 63.0555 - mae: 5.6493 - val_loss: 763.4493 - val_mae: 21.0470\n",
      "Epoch 247/1000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 66.7126 - mae: 5.7135 - val_loss: 807.0987 - val_mae: 21.9690\n",
      "Epoch 248/1000\n",
      "1988/1988 [==============================] - 1s 466us/step - loss: 63.1340 - mae: 5.6814 - val_loss: 637.6798 - val_mae: 18.9113\n",
      "Epoch 249/1000\n",
      "1988/1988 [==============================] - 1s 457us/step - loss: 59.0427 - mae: 5.4763 - val_loss: 698.8200 - val_mae: 19.8025\n",
      "Epoch 250/1000\n",
      "1988/1988 [==============================] - 1s 451us/step - loss: 69.0427 - mae: 5.8209 - val_loss: 721.5489 - val_mae: 20.2725\n",
      "Epoch 251/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 66.1657 - mae: 5.8048 - val_loss: 1058.9858 - val_mae: 26.7490\n",
      "Epoch 252/1000\n",
      "1988/1988 [==============================] - 1s 427us/step - loss: 68.1842 - mae: 5.8479 - val_loss: 696.8380 - val_mae: 19.9571\n",
      "Epoch 253/1000\n",
      "1988/1988 [==============================] - 1s 449us/step - loss: 65.3650 - mae: 5.7543 - val_loss: 854.8383 - val_mae: 22.8498\n",
      "Epoch 254/1000\n",
      "1988/1988 [==============================] - 1s 411us/step - loss: 63.2537 - mae: 5.6420 - val_loss: 821.8663 - val_mae: 22.1147\n",
      "Epoch 255/1000\n",
      "1988/1988 [==============================] - 1s 410us/step - loss: 63.0079 - mae: 5.6546 - val_loss: 702.4086 - val_mae: 19.9712\n",
      "Epoch 256/1000\n",
      "1988/1988 [==============================] - 1s 386us/step - loss: 65.1527 - mae: 5.8033 - val_loss: 610.0952 - val_mae: 18.4985\n",
      "Epoch 257/1000\n",
      "1988/1988 [==============================] - 1s 391us/step - loss: 62.3288 - mae: 5.6765 - val_loss: 734.4786 - val_mae: 20.5836\n",
      "Epoch 258/1000\n",
      "1988/1988 [==============================] - 1s 445us/step - loss: 61.7840 - mae: 5.5014 - val_loss: 556.4639 - val_mae: 17.7325\n",
      "Epoch 259/1000\n",
      "1988/1988 [==============================] - 1s 384us/step - loss: 63.1199 - mae: 5.6521 - val_loss: 691.9374 - val_mae: 20.2159\n",
      "Epoch 260/1000\n",
      "1988/1988 [==============================] - 1s 378us/step - loss: 61.8338 - mae: 5.4877 - val_loss: 667.3603 - val_mae: 20.1835\n",
      "Epoch 261/1000\n",
      "1988/1988 [==============================] - 1s 370us/step - loss: 69.7332 - mae: 5.9696 - val_loss: 548.0077 - val_mae: 17.5988\n",
      "Epoch 262/1000\n",
      "1988/1988 [==============================] - 1s 373us/step - loss: 77.5504 - mae: 6.5516 - val_loss: 554.3657 - val_mae: 17.6094\n",
      "Epoch 263/1000\n",
      "1988/1988 [==============================] - 1s 408us/step - loss: 69.2037 - mae: 6.0940 - val_loss: 1354.3692 - val_mae: 31.7762\n",
      "Epoch 264/1000\n",
      "1988/1988 [==============================] - 1s 452us/step - loss: 62.8756 - mae: 5.6044 - val_loss: 713.2022 - val_mae: 20.4201\n",
      "Epoch 265/1000\n",
      "1988/1988 [==============================] - 1s 450us/step - loss: 64.9598 - mae: 5.7318 - val_loss: 748.1057 - val_mae: 21.2301\n",
      "Epoch 266/1000\n",
      "1988/1988 [==============================] - 1s 427us/step - loss: 64.0545 - mae: 5.5869 - val_loss: 695.3561 - val_mae: 19.8974\n",
      "Epoch 267/1000\n",
      "1988/1988 [==============================] - 1s 461us/step - loss: 60.6732 - mae: 5.4834 - val_loss: 786.6306 - val_mae: 21.5282\n",
      "Epoch 268/1000\n",
      "1988/1988 [==============================] - 1s 437us/step - loss: 67.5498 - mae: 5.6924 - val_loss: 704.9641 - val_mae: 20.1535\n",
      "Epoch 269/1000\n",
      "1988/1988 [==============================] - 1s 436us/step - loss: 64.9197 - mae: 5.5620 - val_loss: 1166.2659 - val_mae: 28.1441\n",
      "Epoch 270/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 67.9860 - mae: 5.8322 - val_loss: 708.4554 - val_mae: 20.2439\n",
      "Epoch 271/1000\n",
      "1988/1988 [==============================] - 1s 476us/step - loss: 59.6827 - mae: 5.5749 - val_loss: 753.0566 - val_mae: 20.9114\n",
      "Epoch 272/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 62.4777 - mae: 5.6092 - val_loss: 759.5100 - val_mae: 21.1590\n",
      "Epoch 273/1000\n",
      "1988/1988 [==============================] - 1s 459us/step - loss: 66.2720 - mae: 5.6203 - val_loss: 547.8181 - val_mae: 17.3389\n",
      "Epoch 274/1000\n",
      "1988/1988 [==============================] - 1s 508us/step - loss: 62.2377 - mae: 5.5586 - val_loss: 725.3029 - val_mae: 20.7215\n",
      "Epoch 275/1000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 59.9702 - mae: 5.5305 - val_loss: 744.7094 - val_mae: 21.0157\n",
      "Epoch 276/1000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 62.7890 - mae: 5.6310 - val_loss: 662.2960 - val_mae: 19.3573\n",
      "Epoch 277/1000\n",
      "1988/1988 [==============================] - 1s 479us/step - loss: 67.3662 - mae: 5.8880 - val_loss: 771.4583 - val_mae: 21.4434\n",
      "Epoch 278/1000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 53.7134 - mae: 5.2476 - val_loss: 626.8188 - val_mae: 18.6184\n",
      "Epoch 279/1000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 65.0855 - mae: 5.6386 - val_loss: 817.5356 - val_mae: 22.0833\n",
      "Epoch 280/1000\n",
      "1988/1988 [==============================] - 1s 478us/step - loss: 64.2808 - mae: 5.6595 - val_loss: 633.4736 - val_mae: 18.9162\n",
      "Epoch 281/1000\n",
      "1988/1988 [==============================] - 1s 476us/step - loss: 62.4849 - mae: 5.6922 - val_loss: 1073.3558 - val_mae: 27.2439\n",
      "Epoch 282/1000\n",
      "1988/1988 [==============================] - 1s 459us/step - loss: 56.6008 - mae: 5.4933 - val_loss: 726.6202 - val_mae: 20.8612\n",
      "Epoch 283/1000\n",
      "1988/1988 [==============================] - 1s 444us/step - loss: 64.7882 - mae: 5.7617 - val_loss: 620.5389 - val_mae: 18.7486\n",
      "Epoch 284/1000\n",
      "1988/1988 [==============================] - 1s 461us/step - loss: 72.9404 - mae: 6.1551 - val_loss: 783.1940 - val_mae: 21.6938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 61.2910 - mae: 5.6055 - val_loss: 650.5419 - val_mae: 19.0922\n",
      "Epoch 286/1000\n",
      "1988/1988 [==============================] - 1s 426us/step - loss: 64.0144 - mae: 5.6761 - val_loss: 693.3329 - val_mae: 19.8735\n",
      "Epoch 287/1000\n",
      "1988/1988 [==============================] - 1s 436us/step - loss: 61.5846 - mae: 5.6288 - val_loss: 898.0794 - val_mae: 23.7181\n",
      "Epoch 288/1000\n",
      "1988/1988 [==============================] - 1s 443us/step - loss: 64.1782 - mae: 5.6243 - val_loss: 749.3154 - val_mae: 21.3416\n",
      "Epoch 289/1000\n",
      "1988/1988 [==============================] - 1s 440us/step - loss: 61.5765 - mae: 5.5027 - val_loss: 602.9959 - val_mae: 18.2462\n",
      "Epoch 290/1000\n",
      "1988/1988 [==============================] - 1s 447us/step - loss: 71.6582 - mae: 5.9500 - val_loss: 1050.2386 - val_mae: 26.8739\n",
      "Epoch 291/1000\n",
      "1988/1988 [==============================] - 1s 446us/step - loss: 71.5787 - mae: 6.0411 - val_loss: 548.1394 - val_mae: 17.5525\n",
      "Epoch 292/1000\n",
      "1988/1988 [==============================] - 1s 445us/step - loss: 66.9622 - mae: 5.9870 - val_loss: 762.4995 - val_mae: 21.1649\n",
      "Epoch 293/1000\n",
      "1988/1988 [==============================] - 1s 435us/step - loss: 62.9598 - mae: 5.7103 - val_loss: 1233.2272 - val_mae: 29.8243\n",
      "Epoch 294/1000\n",
      "1988/1988 [==============================] - 1s 450us/step - loss: 64.5368 - mae: 5.7939 - val_loss: 833.5802 - val_mae: 22.4378\n",
      "Epoch 295/1000\n",
      "1988/1988 [==============================] - 1s 426us/step - loss: 69.6783 - mae: 6.1282 - val_loss: 653.9585 - val_mae: 19.1657\n",
      "Epoch 296/1000\n",
      "1988/1988 [==============================] - 1s 418us/step - loss: 62.7069 - mae: 5.6113 - val_loss: 610.2578 - val_mae: 18.3407\n",
      "Epoch 297/1000\n",
      "1988/1988 [==============================] - 1s 421us/step - loss: 62.8171 - mae: 5.5643 - val_loss: 774.6349 - val_mae: 21.4671\n",
      "Epoch 298/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 63.6647 - mae: 5.5680 - val_loss: 632.0583 - val_mae: 18.7238\n",
      "Epoch 299/1000\n",
      "1988/1988 [==============================] - 1s 449us/step - loss: 66.7355 - mae: 5.8393 - val_loss: 711.5464 - val_mae: 20.1370\n",
      "Epoch 300/1000\n",
      "1988/1988 [==============================] - 1s 442us/step - loss: 65.5819 - mae: 5.6730 - val_loss: 770.0969 - val_mae: 21.2235\n",
      "Epoch 301/1000\n",
      "1988/1988 [==============================] - 1s 435us/step - loss: 60.2996 - mae: 5.5203 - val_loss: 790.2787 - val_mae: 21.5729\n",
      "Epoch 302/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 69.4438 - mae: 5.7146 - val_loss: 950.0812 - val_mae: 24.3681\n",
      "Epoch 303/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 62.7946 - mae: 5.6896 - val_loss: 863.6838 - val_mae: 22.9323\n",
      "Epoch 304/1000\n",
      "1988/1988 [==============================] - 1s 447us/step - loss: 68.3792 - mae: 5.7935 - val_loss: 1479.7285 - val_mae: 33.9240\n",
      "Epoch 305/1000\n",
      "1988/1988 [==============================] - 1s 467us/step - loss: 68.6254 - mae: 5.7856 - val_loss: 1009.4885 - val_mae: 25.7383\n",
      "Epoch 306/1000\n",
      "1988/1988 [==============================] - 1s 471us/step - loss: 61.5034 - mae: 5.7032 - val_loss: 891.4339 - val_mae: 23.7342\n",
      "Epoch 307/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 62.7220 - mae: 5.6825 - val_loss: 672.6803 - val_mae: 19.4883\n",
      "Epoch 308/1000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 62.6852 - mae: 5.6642 - val_loss: 614.1897 - val_mae: 18.4775\n",
      "Epoch 309/1000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 59.7578 - mae: 5.5365 - val_loss: 749.4301 - val_mae: 21.0671\n",
      "Epoch 310/1000\n",
      "1988/1988 [==============================] - 1s 496us/step - loss: 58.5850 - mae: 5.5184 - val_loss: 826.0016 - val_mae: 22.2017\n",
      "Epoch 311/1000\n",
      "1988/1988 [==============================] - 1s 550us/step - loss: 60.1102 - mae: 5.5504 - val_loss: 845.2391 - val_mae: 22.7373\n",
      "Epoch 312/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 58.5448 - mae: 5.5281 - val_loss: 794.2971 - val_mae: 21.8912\n",
      "Epoch 313/1000\n",
      "1988/1988 [==============================] - 1s 492us/step - loss: 60.5865 - mae: 5.4575 - val_loss: 689.2921 - val_mae: 19.6740\n",
      "Epoch 314/1000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 69.6734 - mae: 5.8751 - val_loss: 763.7364 - val_mae: 20.9915\n",
      "Epoch 315/1000\n",
      "1988/1988 [==============================] - 1s 570us/step - loss: 72.6604 - mae: 6.0307 - val_loss: 932.5849 - val_mae: 24.9172\n",
      "Epoch 316/1000\n",
      "1988/1988 [==============================] - 1s 453us/step - loss: 60.9564 - mae: 5.5435 - val_loss: 944.5322 - val_mae: 25.1270\n",
      "Epoch 317/1000\n",
      "1988/1988 [==============================] - 1s 427us/step - loss: 59.4124 - mae: 5.4761 - val_loss: 729.2160 - val_mae: 20.6659\n",
      "Epoch 318/1000\n",
      "1988/1988 [==============================] - 1s 458us/step - loss: 62.6790 - mae: 5.5741 - val_loss: 945.7248 - val_mae: 24.4943\n",
      "Epoch 319/1000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 57.4764 - mae: 5.4867 - val_loss: 889.8439 - val_mae: 23.7253\n",
      "Epoch 320/1000\n",
      "1988/1988 [==============================] - 1s 451us/step - loss: 60.2655 - mae: 5.4892 - val_loss: 655.1514 - val_mae: 19.1568\n",
      "Epoch 321/1000\n",
      "1988/1988 [==============================] - 1s 440us/step - loss: 62.6938 - mae: 5.7001 - val_loss: 898.1683 - val_mae: 24.2835\n",
      "Epoch 322/1000\n",
      "1988/1988 [==============================] - 1s 438us/step - loss: 66.0398 - mae: 5.7331 - val_loss: 772.5834 - val_mae: 21.5986\n",
      "Epoch 323/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 60.6132 - mae: 5.5850 - val_loss: 658.3663 - val_mae: 19.3253\n",
      "Epoch 324/1000\n",
      "1988/1988 [==============================] - 1s 471us/step - loss: 67.5294 - mae: 5.6457 - val_loss: 915.8414 - val_mae: 25.5622\n",
      "Epoch 325/1000\n",
      "1988/1988 [==============================] - 1s 438us/step - loss: 57.4113 - mae: 5.3441 - val_loss: 599.0099 - val_mae: 18.1591\n",
      "Epoch 326/1000\n",
      "1988/1988 [==============================] - 1s 479us/step - loss: 60.9898 - mae: 5.5903 - val_loss: 915.5535 - val_mae: 24.4321\n",
      "Epoch 327/1000\n",
      "1988/1988 [==============================] - 1s 459us/step - loss: 68.7408 - mae: 5.8664 - val_loss: 611.4469 - val_mae: 18.5024\n",
      "Epoch 328/1000\n",
      "1988/1988 [==============================] - 1s 455us/step - loss: 61.4326 - mae: 5.7667 - val_loss: 835.0346 - val_mae: 22.6669\n",
      "Epoch 329/1000\n",
      "1988/1988 [==============================] - 1s 478us/step - loss: 64.5212 - mae: 5.6047 - val_loss: 749.4887 - val_mae: 20.9530\n",
      "Epoch 330/1000\n",
      "1988/1988 [==============================] - 1s 450us/step - loss: 57.5751 - mae: 5.4497 - val_loss: 901.4641 - val_mae: 23.6261\n",
      "Epoch 331/1000\n",
      "1988/1988 [==============================] - 1s 457us/step - loss: 60.4296 - mae: 5.6038 - val_loss: 884.4049 - val_mae: 23.2818\n",
      "Epoch 332/1000\n",
      "1988/1988 [==============================] - 1s 444us/step - loss: 63.9982 - mae: 5.7613 - val_loss: 727.5363 - val_mae: 20.5372\n",
      "Epoch 333/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 61.4588 - mae: 5.5462 - val_loss: 823.8806 - val_mae: 23.1463\n",
      "Epoch 334/1000\n",
      "1988/1988 [==============================] - 1s 494us/step - loss: 61.9174 - mae: 5.6872 - val_loss: 581.4931 - val_mae: 18.2253\n",
      "Epoch 335/1000\n",
      "1988/1988 [==============================] - 1s 488us/step - loss: 66.6080 - mae: 6.0951 - val_loss: 655.9590 - val_mae: 19.7564\n",
      "Epoch 336/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 64.7283 - mae: 5.7192 - val_loss: 695.6405 - val_mae: 20.3136\n",
      "Epoch 337/1000\n",
      "1988/1988 [==============================] - 1s 486us/step - loss: 67.8331 - mae: 5.8652 - val_loss: 631.4084 - val_mae: 18.8673\n",
      "Epoch 338/1000\n",
      "1988/1988 [==============================] - 1s 457us/step - loss: 63.3058 - mae: 5.6488 - val_loss: 823.4947 - val_mae: 23.0752\n",
      "Epoch 339/1000\n",
      "1988/1988 [==============================] - 1s 448us/step - loss: 60.5475 - mae: 5.5487 - val_loss: 645.6667 - val_mae: 19.3455\n",
      "Epoch 340/1000\n",
      "1988/1988 [==============================] - 1s 451us/step - loss: 66.3538 - mae: 5.6585 - val_loss: 557.0672 - val_mae: 17.4336\n",
      "Epoch 341/1000\n",
      "1988/1988 [==============================] - 1s 457us/step - loss: 61.8782 - mae: 5.5218 - val_loss: 843.9342 - val_mae: 22.9876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/1000\n",
      "1988/1988 [==============================] - 1s 453us/step - loss: 62.6362 - mae: 5.6268 - val_loss: 1090.3842 - val_mae: 27.7502\n",
      "Epoch 343/1000\n",
      "1988/1988 [==============================] - 1s 452us/step - loss: 59.7319 - mae: 5.3844 - val_loss: 753.9211 - val_mae: 21.2097\n",
      "Epoch 344/1000\n",
      "1988/1988 [==============================] - 1s 456us/step - loss: 61.5100 - mae: 5.5444 - val_loss: 777.4979 - val_mae: 21.7880\n",
      "Epoch 345/1000\n",
      "1988/1988 [==============================] - 1s 448us/step - loss: 61.3704 - mae: 5.4483 - val_loss: 717.1228 - val_mae: 20.8257\n",
      "Epoch 346/1000\n",
      "1988/1988 [==============================] - 1s 463us/step - loss: 60.5346 - mae: 5.5120 - val_loss: 588.2844 - val_mae: 18.0184\n",
      "Epoch 347/1000\n",
      "1988/1988 [==============================] - 1s 491us/step - loss: 61.4893 - mae: 5.5232 - val_loss: 575.1635 - val_mae: 18.0103\n",
      "Epoch 348/1000\n",
      "1988/1988 [==============================] - 1s 432us/step - loss: 60.5280 - mae: 5.5013 - val_loss: 946.7011 - val_mae: 24.4244\n",
      "Epoch 349/1000\n",
      "1988/1988 [==============================] - 1s 444us/step - loss: 71.5842 - mae: 5.8411 - val_loss: 1284.7619 - val_mae: 30.8101\n",
      "Epoch 350/1000\n",
      "1988/1988 [==============================] - 1s 433us/step - loss: 58.3687 - mae: 5.3751 - val_loss: 806.4875 - val_mae: 22.0172\n",
      "Epoch 351/1000\n",
      "1988/1988 [==============================] - 1s 423us/step - loss: 61.0965 - mae: 5.5800 - val_loss: 877.3867 - val_mae: 23.5098\n",
      "Epoch 352/1000\n",
      "1988/1988 [==============================] - 1s 447us/step - loss: 57.1798 - mae: 5.3943 - val_loss: 617.8968 - val_mae: 18.6055\n",
      "Epoch 353/1000\n",
      "1988/1988 [==============================] - 1s 446us/step - loss: 71.8208 - mae: 6.0994 - val_loss: 806.5924 - val_mae: 22.1350\n",
      "Epoch 354/1000\n",
      "1988/1988 [==============================] - 1s 435us/step - loss: 65.4750 - mae: 5.7235 - val_loss: 766.8483 - val_mae: 21.2771\n",
      "Epoch 355/1000\n",
      "1988/1988 [==============================] - 1s 429us/step - loss: 59.8830 - mae: 5.4413 - val_loss: 932.6441 - val_mae: 24.0997\n",
      "Epoch 356/1000\n",
      "1988/1988 [==============================] - 1s 430us/step - loss: 75.4030 - mae: 6.0413 - val_loss: 858.1996 - val_mae: 23.0590\n",
      "Epoch 357/1000\n",
      "1988/1988 [==============================] - 1s 439us/step - loss: 66.4780 - mae: 5.8268 - val_loss: 554.1325 - val_mae: 17.4774\n",
      "Epoch 358/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 62.8456 - mae: 5.6919 - val_loss: 855.2900 - val_mae: 23.1963\n",
      "Epoch 359/1000\n",
      "1988/1988 [==============================] - 1s 447us/step - loss: 59.3911 - mae: 5.5122 - val_loss: 768.7152 - val_mae: 21.7362\n",
      "Epoch 360/1000\n",
      "1988/1988 [==============================] - 1s 444us/step - loss: 57.5238 - mae: 5.4615 - val_loss: 765.4079 - val_mae: 21.4687\n",
      "Epoch 361/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 58.2298 - mae: 5.4893 - val_loss: 653.5645 - val_mae: 19.3073\n",
      "Epoch 362/1000\n",
      "1988/1988 [==============================] - 1s 444us/step - loss: 67.1263 - mae: 5.9611 - val_loss: 698.8021 - val_mae: 20.0182\n",
      "Epoch 363/1000\n",
      "1988/1988 [==============================] - 1s 431us/step - loss: 60.8501 - mae: 5.6339 - val_loss: 1094.2785 - val_mae: 27.3180\n",
      "Epoch 364/1000\n",
      "1988/1988 [==============================] - 1s 433us/step - loss: 63.4215 - mae: 5.6121 - val_loss: 859.7416 - val_mae: 23.1538\n",
      "Epoch 365/1000\n",
      "1988/1988 [==============================] - 1s 439us/step - loss: 65.3097 - mae: 5.6170 - val_loss: 748.6462 - val_mae: 21.0498\n",
      "Epoch 366/1000\n",
      "1988/1988 [==============================] - 1s 469us/step - loss: 65.1225 - mae: 5.6519 - val_loss: 685.8538 - val_mae: 19.9870\n",
      "Epoch 367/1000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 61.1733 - mae: 5.4915 - val_loss: 614.8559 - val_mae: 18.4383\n",
      "Epoch 368/1000\n",
      "1988/1988 [==============================] - 1s 432us/step - loss: 57.3440 - mae: 5.3878 - val_loss: 609.2653 - val_mae: 18.4105\n",
      "Epoch 369/1000\n",
      "1988/1988 [==============================] - 1s 455us/step - loss: 60.4957 - mae: 5.4962 - val_loss: 740.3481 - val_mae: 20.7812\n",
      "Epoch 370/1000\n",
      "1988/1988 [==============================] - 1s 445us/step - loss: 57.8327 - mae: 5.3613 - val_loss: 965.2756 - val_mae: 25.1158\n",
      "Epoch 371/1000\n",
      "1988/1988 [==============================] - 1s 450us/step - loss: 61.5792 - mae: 5.6415 - val_loss: 667.9552 - val_mae: 19.4781\n",
      "Epoch 372/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 57.2664 - mae: 5.4554 - val_loss: 628.6979 - val_mae: 18.7791\n",
      "Epoch 373/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 59.1280 - mae: 5.4006 - val_loss: 617.7874 - val_mae: 18.5768\n",
      "Epoch 374/1000\n",
      "1988/1988 [==============================] - 1s 445us/step - loss: 66.1267 - mae: 5.7024 - val_loss: 654.5458 - val_mae: 19.2461\n",
      "Epoch 375/1000\n",
      "1988/1988 [==============================] - 1s 447us/step - loss: 64.0529 - mae: 5.7809 - val_loss: 1050.0051 - val_mae: 26.4920\n",
      "Epoch 376/1000\n",
      "1988/1988 [==============================] - 1s 442us/step - loss: 64.2383 - mae: 5.6470 - val_loss: 1010.6744 - val_mae: 26.0212\n",
      "Epoch 377/1000\n",
      "1988/1988 [==============================] - 1s 437us/step - loss: 60.0007 - mae: 5.4575 - val_loss: 589.1562 - val_mae: 18.1238\n",
      "Epoch 378/1000\n",
      "1988/1988 [==============================] - 1s 424us/step - loss: 61.3134 - mae: 5.5799 - val_loss: 669.9234 - val_mae: 19.4874\n",
      "Epoch 379/1000\n",
      "1988/1988 [==============================] - 1s 417us/step - loss: 71.4187 - mae: 6.1339 - val_loss: 689.4350 - val_mae: 19.9053\n",
      "Epoch 380/1000\n",
      "1988/1988 [==============================] - 1s 411us/step - loss: 57.4929 - mae: 5.3799 - val_loss: 719.4342 - val_mae: 20.4767\n",
      "Epoch 381/1000\n",
      "1988/1988 [==============================] - 1s 412us/step - loss: 60.2090 - mae: 5.5452 - val_loss: 628.8563 - val_mae: 18.7654\n",
      "Epoch 382/1000\n",
      "1988/1988 [==============================] - 1s 431us/step - loss: 56.3333 - mae: 5.3587 - val_loss: 596.5284 - val_mae: 18.1794\n",
      "Epoch 383/1000\n",
      "1988/1988 [==============================] - 1s 437us/step - loss: 64.5399 - mae: 5.6085 - val_loss: 741.3392 - val_mae: 20.9066\n",
      "Epoch 384/1000\n",
      "1988/1988 [==============================] - 1s 422us/step - loss: 62.5256 - mae: 5.6036 - val_loss: 654.7569 - val_mae: 19.1882\n",
      "Epoch 385/1000\n",
      "1988/1988 [==============================] - 1s 431us/step - loss: 59.3280 - mae: 5.4370 - val_loss: 897.8092 - val_mae: 23.8087\n",
      "Epoch 386/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 59.8931 - mae: 5.4459 - val_loss: 838.0343 - val_mae: 22.6768\n",
      "Epoch 387/1000\n",
      "1988/1988 [==============================] - 1s 421us/step - loss: 55.2395 - mae: 5.2200 - val_loss: 599.2446 - val_mae: 18.2026\n",
      "Epoch 388/1000\n",
      "1988/1988 [==============================] - 1s 434us/step - loss: 56.6902 - mae: 5.3118 - val_loss: 906.1500 - val_mae: 24.1707\n",
      "Epoch 389/1000\n",
      "1988/1988 [==============================] - 1s 432us/step - loss: 60.0116 - mae: 5.4099 - val_loss: 597.5795 - val_mae: 18.2276\n",
      "Epoch 390/1000\n",
      "1988/1988 [==============================] - 1s 433us/step - loss: 61.3217 - mae: 5.6044 - val_loss: 998.4723 - val_mae: 25.8479\n",
      "Epoch 391/1000\n",
      "1988/1988 [==============================] - 1s 452us/step - loss: 62.9615 - mae: 5.7169 - val_loss: 790.7558 - val_mae: 21.7027\n",
      "Epoch 392/1000\n",
      "1988/1988 [==============================] - 1s 451us/step - loss: 59.7819 - mae: 5.4756 - val_loss: 777.0052 - val_mae: 21.3473\n",
      "Epoch 393/1000\n",
      "1988/1988 [==============================] - 1s 446us/step - loss: 63.5007 - mae: 5.6598 - val_loss: 876.5149 - val_mae: 23.1369\n",
      "Epoch 394/1000\n",
      "1988/1988 [==============================] - 1s 435us/step - loss: 60.9341 - mae: 5.4618 - val_loss: 739.2541 - val_mae: 20.6642\n",
      "Epoch 395/1000\n",
      "1988/1988 [==============================] - 1s 435us/step - loss: 65.0580 - mae: 5.6529 - val_loss: 630.7049 - val_mae: 18.8036\n",
      "Epoch 396/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 67.7307 - mae: 5.7149 - val_loss: 907.4932 - val_mae: 24.1728\n",
      "Epoch 397/1000\n",
      "1988/1988 [==============================] - 1s 448us/step - loss: 54.5439 - mae: 5.3990 - val_loss: 739.9490 - val_mae: 21.0315\n",
      "Epoch 398/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 66.3806 - mae: 5.7808 - val_loss: 600.0583 - val_mae: 18.3059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399/1000\n",
      "1988/1988 [==============================] - 1s 489us/step - loss: 64.0954 - mae: 5.8008 - val_loss: 630.0799 - val_mae: 18.8054\n",
      "Epoch 400/1000\n",
      "1988/1988 [==============================] - 1s 470us/step - loss: 64.1126 - mae: 5.7104 - val_loss: 744.8130 - val_mae: 21.0206\n",
      "Epoch 401/1000\n",
      "1988/1988 [==============================] - 1s 475us/step - loss: 62.8550 - mae: 5.6037 - val_loss: 620.3933 - val_mae: 18.6363\n",
      "Epoch 402/1000\n",
      "1988/1988 [==============================] - 1s 470us/step - loss: 64.3080 - mae: 5.7509 - val_loss: 670.2825 - val_mae: 19.5020\n",
      "Epoch 403/1000\n",
      "1988/1988 [==============================] - 1s 457us/step - loss: 59.5235 - mae: 5.4061 - val_loss: 766.0043 - val_mae: 21.0954\n",
      "Epoch 404/1000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 60.0080 - mae: 5.5402 - val_loss: 704.7805 - val_mae: 20.0641\n",
      "Epoch 405/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 72.5560 - mae: 5.9834 - val_loss: 965.7239 - val_mae: 24.9191\n",
      "Epoch 406/1000\n",
      "1988/1988 [==============================] - 1s 459us/step - loss: 62.2046 - mae: 5.5825 - val_loss: 638.4724 - val_mae: 18.8423\n",
      "Epoch 407/1000\n",
      "1988/1988 [==============================] - 1s 486us/step - loss: 63.2354 - mae: 5.5700 - val_loss: 828.2534 - val_mae: 22.2827\n",
      "Epoch 408/1000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 61.6375 - mae: 5.5968 - val_loss: 774.2037 - val_mae: 21.3040\n",
      "Epoch 409/1000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 59.7379 - mae: 5.4633 - val_loss: 914.4333 - val_mae: 23.7404\n",
      "Epoch 410/1000\n",
      "1988/1988 [==============================] - 1s 488us/step - loss: 56.0323 - mae: 5.3716 - val_loss: 891.0419 - val_mae: 23.4621\n",
      "Epoch 411/1000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 63.6505 - mae: 5.7339 - val_loss: 551.1307 - val_mae: 17.7084\n",
      "Epoch 412/1000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 64.1752 - mae: 5.9017 - val_loss: 740.3145 - val_mae: 20.7965\n",
      "Epoch 413/1000\n",
      "1988/1988 [==============================] - 1s 460us/step - loss: 61.6769 - mae: 5.6404 - val_loss: 698.3186 - val_mae: 20.0356\n",
      "Epoch 414/1000\n",
      "1988/1988 [==============================] - 1s 471us/step - loss: 60.6685 - mae: 5.6856 - val_loss: 698.3522 - val_mae: 19.9493\n",
      "Epoch 415/1000\n",
      "1988/1988 [==============================] - 1s 516us/step - loss: 59.0883 - mae: 5.4283 - val_loss: 790.6694 - val_mae: 21.5104\n",
      "Epoch 416/1000\n",
      "1988/1988 [==============================] - 1s 511us/step - loss: 64.8350 - mae: 5.6280 - val_loss: 805.1712 - val_mae: 21.9306\n",
      "Epoch 417/1000\n",
      "1988/1988 [==============================] - 1s 508us/step - loss: 62.5132 - mae: 5.5374 - val_loss: 784.3887 - val_mae: 21.4611\n",
      "Epoch 418/1000\n",
      "1988/1988 [==============================] - 1s 586us/step - loss: 60.7262 - mae: 5.5617 - val_loss: 1009.1749 - val_mae: 25.2979\n",
      "Epoch 419/1000\n",
      "1988/1988 [==============================] - 1s 582us/step - loss: 62.1644 - mae: 5.6493 - val_loss: 836.8502 - val_mae: 22.3520\n",
      "Epoch 420/1000\n",
      "1988/1988 [==============================] - 1s 510us/step - loss: 62.7738 - mae: 5.5450 - val_loss: 818.5737 - val_mae: 22.7563\n",
      "Epoch 421/1000\n",
      "1988/1988 [==============================] - 1s 458us/step - loss: 57.4448 - mae: 5.4002 - val_loss: 671.2818 - val_mae: 19.4637\n",
      "Epoch 422/1000\n",
      "1988/1988 [==============================] - 1s 489us/step - loss: 62.9809 - mae: 5.5245 - val_loss: 518.2078 - val_mae: 17.3322\n",
      "Epoch 423/1000\n",
      "1988/1988 [==============================] - 1s 475us/step - loss: 65.9946 - mae: 5.8602 - val_loss: 761.3660 - val_mae: 21.0056\n",
      "Epoch 424/1000\n",
      "1988/1988 [==============================] - 1s 435us/step - loss: 69.4213 - mae: 5.7202 - val_loss: 864.9077 - val_mae: 22.9342\n",
      "Epoch 425/1000\n",
      "1988/1988 [==============================] - 1s 437us/step - loss: 61.3698 - mae: 5.5445 - val_loss: 610.8795 - val_mae: 18.3357\n",
      "Epoch 426/1000\n",
      "1988/1988 [==============================] - 1s 419us/step - loss: 57.5623 - mae: 5.3012 - val_loss: 795.1417 - val_mae: 21.8734\n",
      "Epoch 427/1000\n",
      "1988/1988 [==============================] - 1s 417us/step - loss: 60.3891 - mae: 5.4634 - val_loss: 700.1643 - val_mae: 20.1171\n",
      "Epoch 428/1000\n",
      "1988/1988 [==============================] - 1s 425us/step - loss: 63.9551 - mae: 5.6735 - val_loss: 622.7143 - val_mae: 18.5947\n",
      "Epoch 429/1000\n",
      "1988/1988 [==============================] - 1s 432us/step - loss: 60.6790 - mae: 5.5586 - val_loss: 689.2058 - val_mae: 19.7545\n",
      "Epoch 430/1000\n",
      "1988/1988 [==============================] - 1s 412us/step - loss: 57.8027 - mae: 5.4737 - val_loss: 780.1970 - val_mae: 21.4042\n",
      "Epoch 431/1000\n",
      "1988/1988 [==============================] - 1s 398us/step - loss: 57.9982 - mae: 5.3776 - val_loss: 929.5528 - val_mae: 24.2952\n",
      "Epoch 432/1000\n",
      "1988/1988 [==============================] - 1s 407us/step - loss: 55.6288 - mae: 5.3331 - val_loss: 647.6848 - val_mae: 19.0314\n",
      "Epoch 433/1000\n",
      "1988/1988 [==============================] - 1s 399us/step - loss: 56.4765 - mae: 5.2770 - val_loss: 670.5362 - val_mae: 19.4235\n",
      "Epoch 434/1000\n",
      "1988/1988 [==============================] - 1s 414us/step - loss: 65.6919 - mae: 5.6348 - val_loss: 884.9274 - val_mae: 23.2895\n",
      "Epoch 435/1000\n",
      "1988/1988 [==============================] - 1s 401us/step - loss: 59.9133 - mae: 5.4295 - val_loss: 629.9968 - val_mae: 18.7716\n",
      "Epoch 436/1000\n",
      "1988/1988 [==============================] - 1s 410us/step - loss: 65.4774 - mae: 5.6039 - val_loss: 829.1506 - val_mae: 22.3190\n",
      "Epoch 437/1000\n",
      "1988/1988 [==============================] - 1s 428us/step - loss: 61.7489 - mae: 5.4265 - val_loss: 616.6492 - val_mae: 18.5082\n",
      "Epoch 438/1000\n",
      "1988/1988 [==============================] - 1s 444us/step - loss: 59.9517 - mae: 5.4701 - val_loss: 551.8494 - val_mae: 17.5159\n",
      "Epoch 439/1000\n",
      "1988/1988 [==============================] - 1s 415us/step - loss: 65.1020 - mae: 5.7442 - val_loss: 809.3838 - val_mae: 22.2283\n",
      "Epoch 440/1000\n",
      "1988/1988 [==============================] - 1s 430us/step - loss: 57.2927 - mae: 5.4013 - val_loss: 651.9036 - val_mae: 19.1130\n",
      "Epoch 441/1000\n",
      "1988/1988 [==============================] - 1s 438us/step - loss: 59.1531 - mae: 5.3299 - val_loss: 699.8973 - val_mae: 19.9766\n",
      "Epoch 442/1000\n",
      "1988/1988 [==============================] - 1s 543us/step - loss: 59.3637 - mae: 5.5422 - val_loss: 670.3941 - val_mae: 19.4412\n",
      "Epoch 443/1000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 59.3193 - mae: 5.4884 - val_loss: 707.5352 - val_mae: 20.0939\n",
      "Epoch 444/1000\n",
      "1988/1988 [==============================] - 1s 577us/step - loss: 58.5497 - mae: 5.3822 - val_loss: 994.8310 - val_mae: 25.2586\n",
      "Epoch 445/1000\n",
      "1988/1988 [==============================] - 1s 494us/step - loss: 61.8811 - mae: 5.5361 - val_loss: 659.7403 - val_mae: 19.2900\n",
      "Epoch 446/1000\n",
      "1988/1988 [==============================] - 1s 493us/step - loss: 64.1547 - mae: 5.6045 - val_loss: 610.7801 - val_mae: 18.5227\n",
      "Epoch 447/1000\n",
      "1988/1988 [==============================] - 1s 502us/step - loss: 61.5143 - mae: 5.5869 - val_loss: 741.0927 - val_mae: 20.6983\n",
      "Epoch 448/1000\n",
      "1988/1988 [==============================] - 1s 480us/step - loss: 58.3440 - mae: 5.4249 - val_loss: 717.3286 - val_mae: 20.2710\n",
      "Epoch 449/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 62.8592 - mae: 5.5581 - val_loss: 691.0488 - val_mae: 19.7449\n",
      "Epoch 450/1000\n",
      "1988/1988 [==============================] - 1s 458us/step - loss: 60.8187 - mae: 5.5467 - val_loss: 574.0890 - val_mae: 17.9111\n",
      "Epoch 451/1000\n",
      "1988/1988 [==============================] - 1s 448us/step - loss: 61.0928 - mae: 5.4479 - val_loss: 902.2729 - val_mae: 23.5032\n",
      "Epoch 452/1000\n",
      "1988/1988 [==============================] - 1s 453us/step - loss: 60.3949 - mae: 5.5107 - val_loss: 810.0387 - val_mae: 21.9691\n",
      "Epoch 453/1000\n",
      "1988/1988 [==============================] - 1s 444us/step - loss: 68.8726 - mae: 5.7183 - val_loss: 765.4475 - val_mae: 21.1522\n",
      "Epoch 454/1000\n",
      "1988/1988 [==============================] - 1s 437us/step - loss: 62.5908 - mae: 5.5396 - val_loss: 1097.3397 - val_mae: 27.0799\n",
      "Epoch 455/1000\n",
      "1988/1988 [==============================] - 1s 430us/step - loss: 58.7559 - mae: 5.3650 - val_loss: 922.1751 - val_mae: 24.4100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/1000\n",
      "1988/1988 [==============================] - 1s 429us/step - loss: 56.1004 - mae: 5.3401 - val_loss: 684.5932 - val_mae: 19.8816\n",
      "Epoch 457/1000\n",
      "1988/1988 [==============================] - 1s 603us/step - loss: 60.3999 - mae: 5.4264 - val_loss: 685.3251 - val_mae: 19.7515\n",
      "Epoch 458/1000\n",
      "1988/1988 [==============================] - 1s 486us/step - loss: 59.1422 - mae: 5.4570 - val_loss: 1007.7784 - val_mae: 25.4439\n",
      "Epoch 459/1000\n",
      "1988/1988 [==============================] - 1s 455us/step - loss: 60.2695 - mae: 5.4344 - val_loss: 1031.4836 - val_mae: 26.4644\n",
      "Epoch 460/1000\n",
      "1988/1988 [==============================] - 1s 433us/step - loss: 57.2144 - mae: 5.3388 - val_loss: 691.6606 - val_mae: 19.9221\n",
      "Epoch 461/1000\n",
      "1988/1988 [==============================] - 1s 447us/step - loss: 65.2160 - mae: 5.6434 - val_loss: 798.0946 - val_mae: 22.0979\n",
      "Epoch 462/1000\n",
      "1988/1988 [==============================] - 1s 461us/step - loss: 56.9522 - mae: 5.2232 - val_loss: 770.5744 - val_mae: 21.5045\n",
      "Epoch 463/1000\n",
      "1988/1988 [==============================] - 1s 461us/step - loss: 65.5905 - mae: 5.6435 - val_loss: 594.5484 - val_mae: 17.9886\n",
      "Epoch 464/1000\n",
      "1988/1988 [==============================] - 1s 451us/step - loss: 60.6151 - mae: 5.5728 - val_loss: 896.6481 - val_mae: 24.2983\n",
      "Epoch 465/1000\n",
      "1988/1988 [==============================] - 1s 516us/step - loss: 57.1377 - mae: 5.4948 - val_loss: 656.3118 - val_mae: 19.2556\n",
      "Epoch 466/1000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 57.3902 - mae: 5.3547 - val_loss: 713.6426 - val_mae: 20.2864\n",
      "Epoch 467/1000\n",
      "1988/1988 [==============================] - 1s 454us/step - loss: 62.2038 - mae: 5.5411 - val_loss: 707.6896 - val_mae: 20.2056\n",
      "Epoch 468/1000\n",
      "1988/1988 [==============================] - 1s 462us/step - loss: 56.0441 - mae: 5.3470 - val_loss: 733.8447 - val_mae: 20.6941\n",
      "Epoch 469/1000\n",
      "1988/1988 [==============================] - 1s 465us/step - loss: 60.7049 - mae: 5.5105 - val_loss: 761.6223 - val_mae: 21.2548\n",
      "Epoch 470/1000\n",
      "1988/1988 [==============================] - 1s 447us/step - loss: 57.5825 - mae: 5.3968 - val_loss: 685.7333 - val_mae: 19.7836\n",
      "Epoch 471/1000\n",
      "1988/1988 [==============================] - 1s 435us/step - loss: 61.2011 - mae: 5.4612 - val_loss: 813.2433 - val_mae: 22.0707\n",
      "Epoch 472/1000\n",
      "1988/1988 [==============================] - 1s 499us/step - loss: 60.6560 - mae: 5.4633 - val_loss: 644.2560 - val_mae: 19.0071\n",
      "Epoch 473/1000\n",
      "1988/1988 [==============================] - 1s 499us/step - loss: 63.1985 - mae: 5.5796 - val_loss: 725.2480 - val_mae: 20.3802\n",
      "Epoch 474/1000\n",
      "1988/1988 [==============================] - 1s 492us/step - loss: 62.0300 - mae: 5.5168 - val_loss: 962.9931 - val_mae: 24.5822\n",
      "Epoch 475/1000\n",
      "1988/1988 [==============================] - 1s 519us/step - loss: 55.0586 - mae: 5.3454 - val_loss: 691.2974 - val_mae: 19.8133\n",
      "Epoch 476/1000\n",
      "1988/1988 [==============================] - 1s 495us/step - loss: 69.3864 - mae: 5.7126 - val_loss: 774.4025 - val_mae: 21.2300\n",
      "Epoch 477/1000\n",
      "1988/1988 [==============================] - 1s 518us/step - loss: 70.9082 - mae: 5.8210 - val_loss: 785.4034 - val_mae: 21.4149\n",
      "Epoch 478/1000\n",
      "1988/1988 [==============================] - 1s 498us/step - loss: 68.2934 - mae: 5.8501 - val_loss: 747.3969 - val_mae: 20.7053\n",
      "Epoch 00478: early stopping\n",
      "\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 183us/step\n",
      "test loss, test acc: [1187.430645084381, 26.964181900024414]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 34.49509275078336\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>true_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171.868866</td>\n",
       "      <td>185.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171.507797</td>\n",
       "      <td>176.979996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171.483765</td>\n",
       "      <td>176.779999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170.888107</td>\n",
       "      <td>172.289993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168.450516</td>\n",
       "      <td>174.240005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>174.027771</td>\n",
       "      <td>259.429993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>173.920959</td>\n",
       "      <td>260.140015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>174.384613</td>\n",
       "      <td>262.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>174.312668</td>\n",
       "      <td>261.959991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>174.242523</td>\n",
       "      <td>264.470001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  true_value\n",
       "0    171.868866  185.860001\n",
       "1    171.507797  176.979996\n",
       "2    171.483765  176.779999\n",
       "3    170.888107  172.289993\n",
       "4    168.450516  174.240005\n",
       "..          ...         ...\n",
       "240  174.027771  259.429993\n",
       "241  173.920959  260.140015\n",
       "242  174.384613  262.200012\n",
       "243  174.312668  261.959991\n",
       "244  174.242523  264.470001\n",
       "\n",
       "[245 rows x 2 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_2stacks_true_value, 1, \n",
    "                                                    stock_with_abs_norm, label_value_1d, 64, batch_size, \"loss\")\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=7)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test, batch_size=7)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)\n",
    "predictFrame = pd.DataFrame({'prediction': predictions.reshape(X_test.shape[0]), 'true_value': y_test.reshape(X_test.shape[0])})\n",
    "predictFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCwAAAIdCAYAAAD25OyiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd5hU1eH/8c+dun2XsizSRRAVsSP2ABIVG9j9mqJGY0nRGPWHmK8mMcYSjcYYW6IxomiMoH5BUawYC2BXFESQXpddtpdp9/7+QBfuzJ3Z2d3Zndmd9+t5fLL3nHPPPTvMk+e5nz3FqK6utgQAAAAAAJBBXOkeAAAAAAAAQDQCCwAAAAAAkHEILAAAAAAAQMYhsAAAAAAAABmHwAIAAAAAAGQcAgsAAAAAAJBxCCwAAECPs3btWpWUlOikk07qcF+p6gcAALQNgQUAAOiwkpKSlv9WrlwZt93UqVNb2v3zn//swhECAIDuhsACAACkhMfjkSTNmDHDsX7NmjV66623WtoBAAAkQmABAABSonfv3ho7dqyeeuophUKhmPrHH39clmXphBNOSMPoAABAd0NgAQAAUubHP/6xtm3bpnnz5tnKw+GwZs6cqYMPPlijR4+Oe/+qVav0s5/9TPvss49KS0s1cuRIXXDBBVqyZIlj+7q6Ol1//fXaZ599VFZWprFjx+ree++VZVlxn2GapmbMmKHjjz9eQ4YMUVlZmQ4//HDdddddCgaD7fvFAQBAyhFYAACAlDn99NNVWFgYsyxk/vz52rJli84///y4937yyScaP368nnzySY0ZM0a//OUvddRRR+mFF17QpEmT9Oqrr9raBwIBTZkyRffff79KSkp02WWX6aijjtKf//xnXXfddY7PCIfDOu+883TFFVeosrJSZ5xxhi688EJ5PB7ddNNNOuussxQOhzv+QQAAgA5jESkAAEiZ/Px8nXnmmXrssce0bt06DRkyRNKOfS0KCgp0+umn69577425z7IsXXbZZaqtrdX999+v8847r6VuwYIFOu2003TZZZdpyZIlysvLkyT97W9/08cff6wTTzxRTzzxhFyuHX+HueqqqzR+/HjH8d199916+eWX9dOf/lS33Xab3G63pB2zLq666io99thjevjhh3XZZZel8mMBAADtwAwLAACQUueff75M09Tjjz8uSdq4caNee+01nXHGGSooKHC8Z/HixVq+fLkOOuggW1ghSePHj9fJJ5+syspKvfjiiy3lM2fOlGEY+v3vf98SVkjSkCFDdOmll8Y8wzRNPfjggyotLdWtt97aElZIksvl0k033STDMPT000936PcHAACpwQwLAACQUgcccID2228/zZw5U9ddd50ef/xxRSKRhMtBPvvsM0nSMccc41g/fvx4zZ07V5999pnOOuss1dXVadWqVerfv79GjhwZ0/7II4+MKVu5cqUqKyu1++6764477nB8Tm5urlasWJHMrwkAADoZgQUAAEi5888/X1dffbXmz5+vJ554Qvvuu68OOuiguO1ra2slSf369XOsLysrs7X77n9LS0sd2zv1s337dknS6tWrdfvttyf5mwAAgHRhSQgAAEi5s846S3l5ebr22mu1YcMGXXDBBQnbFxUVSZLKy8sd67du3Wpr993/btu2zbG9Uz/f3XPCCSeouro64X8AACD9CCwAAEDKFRUV6bTTTtPGjRuVm5urs846K2H7/fffX5L09ttvO9a/9dZbknYsN5GkwsJCDR8+XFu3btXKlStj2r/77rsxZXvuuaeKi4v10UcfcXwpAADdAIEFAADoFNdff72eeOIJzZ49W8XFxQnbjhs3TqNGjdJHH30Us+nlW2+9pblz56pPnz468cQTW8p/8IMfyLIs3XjjjTJNs6V83bp1euihh2Ke4fF4dNlll2nbtm265ppr1NjYGNOmsrJSn3/+eVt/VQAA0AnYwwIAAHSKgQMHauDAgUm1NQxDDzzwgKZOnarLLrtMzz33nEaPHq3Vq1drzpw58vl8evDBB1uONJWkX/ziF3rxxRc1b948HX300Zo0aZJqa2v13HPP6fDDD9dLL70U85xrr71WS5cu1YwZM/TKK6/omGOO0cCBA1VRUaHVq1dr0aJFuvjii7Xffvul7HMAAADtQ2ABAAAywkEHHaQFCxbojjvu0IIFC/T666+ruLhYJ510kq6++uqYEMHv9+v555/Xbbfdpueee04PPvighgwZoquvvlqnnHKKY2Dh8Xg0Y8YMzZ49WzNnztSrr76q+vp69e7dW4MHD9ZVV12lc889t6t+ZQAAkIBRXV1tpXsQAAAAAAAAu2IPCwAAAAAAkHEILAAAAAAAQMYhsAAAAAAAABmHwAIAAAAAAGQcAgsAAAAAAJBxCCwAAAAAAEDGIbAAAAAAAAAZh8CiG1ixYkW6hwB0Or7nyAZ8z5EN+J4jG/A9RzbIhO85gQUAAAAAAMg4BBYAAAAAACDjEFgAAAAAAICMQ2ABAAAAAAAyDoEFAAAAAADIOAQWAAAAAAAg4xBYAAAAAACAjENgAQAAAAAAMg6BBQAAAAAAyDgEFgAAAAAAIOMQWAAAAAAAgIxDYAEAAAAAADIOgQUAAAAAAMg4BBYAAAAAACDjEFgAAAAAAICMQ2ABAAAAAAAyDoEFAAAAAADIOAQWAAAAAAB0V6GgXBtWyaitksxIukeTUp50DwAAAAAAALSPa+sG5f3mJ5IkyzBkDt9bTTfen+ZRpQYzLAAAAAAA6KaMupqdP1uW5HancTSpRWABAAAAAEA35f7yI9u1VdQrTSNJPQILAAAAAAC6Kd/cJ2zXVmFxmkaSegQWAAAAAAD0EJYvJ91DSBkCCwAAAAAAuqNQMLZo/MlpGEjnILAAAAAAAKAbMqorY8qsAUPTMJLOQWABAAAAAEA35Nq60XYdGbZnmkbSOQgsAAAAAADohqJPCDGHjEjTSDoHgQUAAAAAAN2QsW2z7Toyav80jaRzEFgAAAAAANDdWJa8HyywF/UuTc9YOgmBBQAAAAAA3YznrRdjyszi3mkYSechsAAAAAAAoJvxLHwtpswq6ZOGkXQeAgsAAAAAALoZz1efxhbmFXT9QDoRgQUAAAAAAN1JJBxT1HTd3ZJhpGEwnYfAAgAAAACAbsSoq7FdWwVFiux9YJpG03kILAAAAAAA6E5CQdullZOXpoF0LgILAAAAAAC6k2DAfu31pWccnYzAAgAAAACAbsSInmHh86dpJJ2LwAIAAAAAgO6EGRYAAAAAACDTMMMCAAAAAABknhAzLDrVXXfdpQkTJmjw4MHaY489dM4552jp0qW2NpdffrlKSkps/02aNMnWJhAI6Nprr9Xw4cM1YMAAnXvuudq4cWNX/ioAAAAAAHSdqBkWBBYp9s477+iiiy7S/PnzNWfOHHk8Hk2dOlVVVVW2duPHj9fy5ctb/nvmmWds9dOnT9fcuXP1yCOPaN68eaqrq9M555yjSCTSlb8OAAAAAABdwghGLQnpoYGFJ10PfvbZZ23XDz30kIYMGaJFixZp8uTJLeV+v19lZWWOfdTU1Ojxxx/XfffdpwkTJrT0M2bMGC1YsEDHHnts5/0CAAAAAACkQ/QMC/aw6Fz19fUyTVMlJSW28oULF2rEiBE6+OCDdcUVV2jbtm0tdZ9++qlCoZAmTpzYUjZo0CCNGjVKixcv7rKxAwAAAADQVdxrvrZdM8Oik1133XUaM2aMDj300JaySZMm6ZRTTtHQoUO1bt063XzzzTr11FO1YMEC+f1+lZeXy+12q0+fPra+SktLVV5eHvdZK1as6LTfo7N0xzEDbcX3HNmA7zmyAd9zZAO+50iXvE2rNerNObayqvoGbeqE72Rnf89HjhyZsD4jAovrr79eixYt0ssvvyy3291SfsYZZ7T8PHr0aB1wwAEaM2aM5s+fr1NPPTVuf5ZlyTCMuPWtfSiZZsWKFd1uzEBb8T1HNuB7jmzA9xzZgO850in3qbtiynqV9Vd+ir+TmfA9T/uSkOnTp2v27NmaM2eOhg0blrDtbrvtpgEDBmjVqlWSpH79+ikSiaiystLWrqKiQqWlpZ01ZAAAAAAA0sL9zbKYsp66JCStgcW0adM0a9YszZkzR3vuuWer7SsrK7V58+aWTTgPOOAAeb1evfnmmy1tNm7cqOXLl2vcuHGdNm4AAAAAADKGt2duupm2JSHXXHONnn76aT3xxBMqKSnR1q1bJUn5+fkqKChQfX29brvtNp166qkqKyvTunXrdNNNN6m0tFQnn3yyJKm4uFg/+tGPdOONN6q0tFS9evXSb37zG40ePVrjx49P168GAAAAAECX6akzLNIWWDz88MOSpClTptjKp02bpunTp8vtdmvp0qX697//rZqaGpWVlenoo4/Wo48+qsLCwpb2t9xyi9xuty688EI1NzfrmGOO0YMPPmjbCwMAAAAAgB7LR2CRUtXV1Qnrc3Nz9eyzz7baT05Oju644w7dcccdqRoaAAAAAACZx7Kcy3voDIu0b7oJAAAAAACSEGx2LLZ8PXMPCwILAAAAAAC6AaOuxrmCGRYAAAAAACBdjPpax/KeuukmgQUAAAAAAN2AUbHVuaKHHmtKYAEAAAAAQDfgXvqRcwUzLAAAAAAAQLq4Nq+LKbP8OTL7D0rDaDofgQUAAAAAAN2A0VgfU9Z8+Y1SDz0lxJPuAQAAAAAAgNYZDfbAouH2x2X1H5ym0XQ+ZlgAAAAAAJDpQkG5tm2yFVl5hWkaTNcgsAAAAAAAIMN5Pn4ntjCvoOsH0oUILAAAAAAAyHDuzxfHFnp69i4PBBYAAAAAAGQ4K78o3UPocgQWAAAAAABkunDIdhk88dw0DaTrEFgAAAAAAJDhjKZG27U5YGiaRtJ1CCwAAAAAAMhwRrM9sLBy8tI0kq5DYAEAAAAAQAZzfbM09pQQAgsAAAAAAJAu7k8XKvcPP48pt3IJLAAAAAAAQJp4Fr0uw7Jiyq2C4jSMpmsRWAAAAAAAkKHca76OKbO8Xln9BqRhNF2LwAIAAAAAgAxlFTrMpHC5JVfPf53v+b8hAAAAAADdVSQSW7Tv2DQMpOsRWAAAAAAAkKlCgZii4InnpmEgXY/AAgAAAACADGUEg7br4InnyhwxOk2j6VoEFgAAAACA5FiWfE/cq/wLJir3hotkbNuc7hH1fEH7DIvQxClpGkjXI7AAAAAAACTFtW6lfK/OlmGZcq/7Rt75s9I9pJ4vZJ9hIZ8/PeNIAwILAAAAAEBSvC//x3bte3V2mkaSPYyoPSwsAgsAAAAAAOyM6L/2o/NFLQmR15eecaQBgQUAAAAAIDnhcLpHkF0iYRmm2XJpGS7J7UnjgLoWgQUAAAAAIDkRAosuFWi2X/t8kmGkZyxpQGABAAAAANgpGJD704VyrfxSsix7XTiUnjFlKaOhznZt5RemaSTpkT1zSQAAAAAAiVmWch64SZ6P35UkBc77uULHn9VSbUT/xf/be7Lpr/5dyaivsV1bBUVpGkl6MMMCAAAAACBJcq1Z3hJWSJL3FfspIEZddexNQYcQAylh1Nfarq2C4jSNJD0ILAAAAAAAkiT3Fx/Zrl0VW3aeUmFZMqoqYu9Z9mlXDC37BJqUe+f/sxVZ+cywAAAAAABkIaN2e0xZwU+Pl++Je2XUVjkea5p793ROD+kEnsULYgsL2MMCAAAAAJCFjJoqx3Lfq7Mlf07c+9zLPlFkzNjOGlZWci95P6aMJSEAAAAAgKxk1DoHFpLke2Fm/BsjnB6Sch5vTFG2nRJCYAEAAAAAkBR/hkWr/LmpHUi2M01533slttwhxOjJCCwAAAAAAJIkl8MeFsmwDF4tU8nz5px0DyEjsIcFAAAAAEAKh2OO0UyWEWHTzfYytm2W/4l7ZTTWKTj1AkVGH6ycGX9xbGv2Lu3i0aUXMRgAAAAAQEZddftvDofl+upTudauSN2AsoTvmX/I8+l7cn+9RLl/ulqeN/4vbtvI/od14cjSjxkWAAAAAICEG262JveuaS0/B084W+aQEYqM3FdWvwGpGFqP5l38hu0657G7Hds1XzQt6/awILAAAAAAAMior0lJP76X/yNJsnLy1Pj7h2T1H5ySfrOdOXBouofQ5VgSAgAAAACQGutT2p3R3CjfC0+mtM+s5vWnewRdjsACAAAAACCjIbWBhSR5Fr+Z8j6zleUjsAAAAAAAZCEjxTMsJMnKyU15nz2KZSXf1uvrvHFkKAILAAAAAECnBBYisEisDcfBWnn5nTiQzERgAQAAAACQ0VCX8j4tP4FFQqFg8m1zCSwAAAAAAFnIqNmedNvQ0ZOTa0hgkVgolO4RZDQCCwAAAADIdk2NMrZuSLq5VVCUXDuWhCRkJDnDwizq1ckjyUyedA8AAAAAAJA+vqful+/l/7TtJneSr5JZuFFkmyQZWAQuvq6TB5KZCCwAAAAAIEsZFVvaHlZIyQcWHl45E0lmhkXDbTNk7TakC0aTeVgSAgAAAABZyv3ZYsdys2+ZGm+4T+H9D4upC5xzmSy3O6n+rWSDjWwVThxYNN5wX9aGFRIzLAAAAAAge/n9jsVW7zKZI0ar+de3yajcKvdni+ReuVTmoN0VOv5MeZOdlWEYKRxsDxRotl2avfvJKiyRq3yjmi+ZLnPE6DQNLDMQWAAAAABAtnI5z5Qw+5a1/Gz1KVN44hSFJ07Z2SDZ0z9MsyOj6/GMumrbtTlspJqv/GOaRpN5WBICAAAAANkq0ORYHBm1f8LbrMKSpLo3zEibh5RNjNoa23Wyn2u2ILAAAAAAgCxlNMcGFpbXp/BBRyW8zypK8sWaGRYJRc+wILCwI7AAAAAAgCxlNDfGlAWnni+1EkiYZYOSe0CEGRaJuNettF0TWNgRWAAAAABAtoqaYREaO16hk85r9Tard6nChxzTch085YfODbN0SYhRs105t/9a+ZefLN9T90uWtaPCsuRau0LGprVSQ508H71tu2/XvUPAppsAAAAAkLWMJvsMi8jeByZ9skfzz26U++N3JH+uImMOlW/uE7GNsnRJiPf15+VZ+rEkyffyfxQeN1Hm8L3ke/Jv8r0yW5JzOJHtp4JEY4YFAAAAAGQpo7HOXpBfkPzNbo8iY8crst+4+CFHps+wqK2Wb+a98s34i4yqipR16/u/GfbruU9IzY3yvv58S5mrYmvMfVZJn5SNoScgsAAAAACAbNVgDyys/MJ2dxX63kkxZUaG72GR8/c/yvfKbPlef145997YeQ+KhGVUVST8PELfO7nznt9NEVgAAAAAQJYyGupt11ZeBwKLY06MLczkGRamKc+SD1ou3d8slZpiNyFNFaOmKvFwepd22rO7KwILAAAAAMhS0UtCOjLDwhwxWs2XXG8rc6/4QoqE291nZ/K+9mxMmdFU79AyBQxD3nfnJ26Tm9c5z+7GCCwAAAAAIBuFgjK2b7MVWW3Zw8KBuduQmDL/v+7qUJ+dxT/zbzFlRmMKAgvHjUYNuZe8n/A2Kze/48/uYQgsAAAAACCbBAPy//NOFVx8nIxdZj9Y+YVSflHH+na7Y4q8/50nhUMd67erNHQwsDBN+R+7O6bY/fXncrWyqafFDIsYBBYAAAAAkEU8b78k71svxJSHDx0vuTr4iuiKDSwkyajY0rF+u0hHZ1j4Zv1D3gVz29dvDjMsohFYAAAAAEAWyZnxF8fyyN4HdbhvK07g4dqyocN9p5RlyfJ4Y4o7Glh43n213fcywyIWgQUAAAAAZAvLilsVGbJHx/t3WBIiSUbN9o73nSLuJR8o74rTZTgsUzGaGtrfcTAgV3XiZR+JsIdFLAILAAAAAMgSrjVfx62zSgek4AFxAovmzjsutK18/75frto4R4wGA+3ut8PLXphhEYPAAgAAAACyhFGx2bHc7N1P8ng6/oB4fTQ3dbzvVDBNuTesjl8fCra7a1d1ZbvvlZhh4YTAAgAAAACyhBF0fiFvuv6elPRv+fzOz82UGRatLPkwAs3t7jrR7JWk+HM7dn8PlIIIDQAAAADQLUQtebB8fjXcN0eKEzS0mTdOYNGUGYGF0VCXuEGofUtCPIvflP/pB9t1b4uOntDSAxFYAAAAAECWMKJeyEPfOyl1YYUkeX3O5Rkyw8JoqE1cH2hbYGFsXifvf1+Sb95THRmWIsP27ND9PRWBBQAAAABki+hNJePMiGi3OLMEMmaGRX3iwKJNMyzqa5T320s6tIzkO8GzLulwHz0Rc04AAAAAIEvE7GHhizMjItWCHX+pTwXXyqUJ6402nBLim/d0SsKKyIh9Fdn3kA730xMRWAAAAABAtgjF7mHRJUyza57TCs9nCxPWuz9+J+m+3F9+2NHhSJICZzO7Ih4CCwAAAADIFp29JCQOIxMCi0Cz3KuXJ2ximGbsZxSvbWvLS5Jk7jkmJf30RAQWAAAAAJAlopc8ZNMMi9Y23PyOa/O65PqrqezIcHbpyEhNPz1Q2gKLu+66SxMmTNDgwYO1xx576JxzztHSpfb1RJZl6dZbb9Vee+2l/v3766STTtKyZctsbaqrq3XJJZdoyJAhGjJkiC655BJVV1d35a8CAAAAAN1DKGoPi3ineqSaFema5ySS5MafRvX21htZloxQKH614ZLlz2m1m+ZLf5PUmLJV2gKLd955RxdddJHmz5+vOXPmyOPxaOrUqaqqqmppc8899+i+++7T7bffrjfeeEOlpaU67bTTVFe38+zciy++WJ9//rmeeeYZzZo1S59//rkuvfTSdPxKAAAAAJDRumKGhZVXEFuY5hkWRuVW5d6TXDhg1LQeWBh18f9IHh77PTVd/xeZpQPitjF3G6ymK29W+IjvJzWmbJW2Y02fffZZ2/VDDz2kIUOGaNGiRZo8ebIsy9IDDzygX/3qV5oyZYok6YEHHtDIkSM1a9YsXXjhhVq+fLlee+01vfzyyxo3bpwk6e6779bkyZO1YsUKjRw5sst/LwAAAADIWJGw/drjTfkjAj+8Qjl/v8VemM7AorlR+b8+J+nmSS31iLN/hVm6m5p/8fsdF2533Nsbf/uQlJuX9JiyVcbsYVFfXy/TNFVSUiJJWrt2rbZu3aqJEye2tMnNzdURRxyhxYsXS5Lef/99FRQUtIQVknTYYYcpPz+/pQ0AAAAA4FvRgUWCl+r2Ch95nAI/vMJemMbAwvvK7Da1T7iZZjgk7yuzlD/9fMfq5l/etPPC7Tw/oOmqWwgrkpS2GRbRrrvuOo0ZM0aHHnqoJGnr1q2SpNLSUlu70tJSbd68WZJUXl6uPn36yNhlkxLDMNS3b1+Vl5fHfdaKFStSPfxO1x3HDLQV33NkA77nyAZ8z5ENuuv3fERdnQp3ud64eYvqclP/u+Tk9tLeu1wHm5vT9pkd8OyjcetMt0euqBCndssmrY8z1r4fvqnBLz8Zt7+vG4Kyvr13ZDAkh8UxWp7Tq6VNpuvsf7PWVkVkRGBx/fXXa9GiRXr55Zfljkr4jKgdUy3LigkookW3idbdloqwvAXZgO85sgHfc2QDvufIBt35e57rs2+yOWDoUJmd8Lu4cu2vmn6vJ22fmWE5z+4Ifv90Bf/nZ/K8v0A5D97cUl7idSvHaayWpYKbfxr3OYEzLtKIfUa3XOcUxMYVTb++TSP22jumPBNlwvc87UtCpk+frtmzZ2vOnDkaNmxYS3lZWZkkxcyUqKioaJl10a9fP1VUVMiyrJZ6y7JUWVkZMzMDAAAAALKeGXVaR5xlCx1luaJeNdO4JMQcMNSxPLLPwZLbIyuv0F7R2ODY3rViScLnhE79kb3A4bO1evdL2Afs0hpYTJs2TbNmzdKcOXO055572uqGDh2qsrIyvfnmmy1lzc3NWrhwYcueFYceeqjq6+v1/vvvt7R5//331dDQYNvXAgAAAAAghz0sOmnSvStqb4zooKQrNdbHFIXGTVDkoCMlSVZevq3OaHIOLDwfvxv3EY6nrXhiP1uzV99EI0WUtC0Jueaaa/T000/riSeeUElJScueFfn5+SooKJBhGLr88sv15z//WSNHjtSIESN05513Kj8/X2eeeaYkadSoUZo0aZKuuuoq3XPPPbIsS1dddZWOP/74tE9dAQAAAICM0wWbbkqSMmiGhdFQZ7tu/N1DMncftbMgNzqwiA04JElRR8LuKnDez2MLQ8HYsvzC2DLElbbA4uGHH5akliNLvzNt2jRNnz5dknTllVeqqalJ1157raqrq3XwwQfr2WefVWHhzn/kf/zjH5o2bZpOP/10SdLkyZP1pz/9qYt+CwAAAADoRsLRS0J6eGDR3Chjl+DAcntkDrPP7reiAgs1NTn35fU5l0sKHzYppsyor4ltmGCvRcRKW2BRXV3dahvDMDR9+vSWAMNJr1699Pe//z2VQwMAAACAHsmImmFhddaSECMzAgvX5vW2a6tvWWxoEB3axFm+YjQ7BxmRwXs4HlNqBJqTHygcZcQpIQAAAACAzmVUV8q1dYO9sNP2sIgKLOKc1NEZ3Es+kG/2I7IKCmX1LrPVmbsNiWlvRe01ER3qSJJCQXkWveb4PMf2khSIv4QEySGwAAAAAIAezijfpLzrL4it6GlLQsIh5TzwBxkNtY7VkWGjYgujNwiNDiDMiHJ/e0ncGRbBE852LDeCzLDoKAILAAAAAOhmjO3l8v/rLhlVFQpO+bEihxyTsL33rRdtezm06KJjTY0uCixcq5bFDSskKTxuQmxh9GcQsS8JcS/7RO6Naxz7iwzbU+HDJjo/LMQMi45K67GmAAAAAIC288+4R57PFsm9bqVyHvyDjMryhO09/53nWG512gyLqH6tLjrW1LTiVlk5ebIcloTEBhb2GRaudd849tf80+lquuF+yZ/rWG+EQonHilYRWAAAAABAd9JQJ88n77ZcGqGQPO+9mvieeKdTdNUeFl21JMQT//cxS3dz/hyiQhvDNCVrl+Aj7Bw8RIbvlfB50awEp4zAGYEFAAAAAHQjrvWrYsr8s/4hNTXEv6dmu3NFT9vDIs4JH5JkFRY7VxhGzBKWXZeFGDVVzv31G5BwKMEp59uvp54fpyXiIbAAAAAAgG7EtW2TY7l3/qzYQtNUzl3Xxe+shx1r6rhPx7es/I2I7g4AACAASURBVKL4NyZYFmLUxoY9kaF7Sh5vwrGEJp4qc7fBO9oPGaHQxCkJ2yMWm24CAAAAQDfiKncOLPzPParQLn/FN6orlX/lGYk762nHmiYILFSQKLBwS7uu/AiHJH+OJOcZFsFTftDqUKySPmq86WEZ1ZWyepe2GnAgFjMsAAAAAKAbMeIEFpJaXtiNTWtbDyuk+HtbdFS6loQkmmGRMLCwBzc5j/yp5WejttpW13TlHxUZ+73kxuPz71g6QljRLgQWAAAAAJCJImH5Zj2s3N9dJu9z/2p56Y+3JESSPB+9LUnKeeAPXTHC+KKWhBiWZd/IsrMem+BkDrNX3wQ32oMbz0dvt+wJ4opaEmIO36v9A0SbsCQEAAAAADKQ++N35Jv7xI6fV38lc9ieihx4hIxtW+Le49q8Tgo0yb1uZVcN05lhyDJcMnZdCmJGOm8JyncSzLAwBw6Lf19zU0yRe/VyRUbtJ6O+tqXMMoz4m3ci5ZhhAQAAAAAZyPf8Y7Zr/+P3SOGwXLXOp1ZIkmvdShl1NUn1H953bIfG16o0LAtJtOmmOWBY/Pscji51fbMsNsjIzev80AUtCCwAAAAAIAO5tmywX1duVcFFkxLe4/n43Zg9F77TdN3dMkt2LIuw8goUPPuS1Aw0nnTsYxEnsLAKi6WikjZ15V61TEbAHlhYvtx2Dw1tRzQEAAAAABnIyi9wPKGiNe6VXziWR/Y+UI23z5D76yUyB+8hK9GeDqkQfbRpZ58UYlny//sBx6qEy0HiMGqrpUCzvdDvb8fA0F7MsAAAAACADGQV9261jdm7X0yZ58O349+Qk6fIfuM6P6yQunyGhSvBvh3hfQ5ue4ehgIyowML69qhTdA0CCwAAAADIQEZTY6ttXNvLY/aicC//LKZdaMKpKRtX0ro4sHB/8l7cusiBR7S9v7UrYmdYsCSkSxFYAAAAAECmCYdkVG5ttZk5YGhSp1YET/lBKkbVNl29h4XXG7fK7Nu/XV16lrxvu2aGRdcisAAAAACADGNs3SgjiRf80GHHyrV9W8I2gdN/IqtPWaqGljTL5bZdG2akU59n1Nc5lkcGDpPyCtrV53fHyrYgsOhSbLoJAAAAABnGvfqruHWRwXvIKu4tc9DuCk0+R671q+SO21qSJ/7Mg04VNcPC8+58WXkFCh95vORL/eaVRnVlTFlk6J4K/PjKlD2DGRZdi8ACAAAAADKMe+WXMWWW16fQ8WcpeOoPJf/OvRTCR3xf3g8WxO+ss0/niMdjf930/+fvkqTwh2+r+do7Uv44o8YeWDRd+UdFDjoyqXuDJ54r37x/t97QR2DRlQgsAAAAACDDuDautV03XXWLIgc4bxwZ2X+cLLdHRiTs3Fmkc5dixOV2ntnh+eIDGdWVskr6pPRxRpU9sLB6lyZ9b/DkH8ioq5H37ZcStrPy27e0BO3DHhYAAAAAkGGMzets1+ZuQ+M3dnvUfNWtnTyitrM8Cf4+3lif8ue5omZYtCkQyS9U4OJprTazCoraOix0AIEFAAAAAGSSSFiuuuqWS8swZJUmPuUi0Ukh4SO+n7KhtUmivTOiTxDpqGBARsPOTTctwyWrqCS1z5Bk5RNYdCUCCwAAAADIJKGg/drrl1wJt9WM+5f/4Ck/lFW6W6pG1jbuBDMsUnzEqVGz3XZtFfdq9TNrj2SOkEXqsIcFAAAAAHQSz6LX5XvmH7IKixW4eJrMQcNbvykcsl97Wz/lwymwaLhndsr3iWiTBEtCjFBQVgofFX1CiFXcvt87dNTx8r4zP269VUBg0ZWYYQEAAAAAnaGpUf5H75SrYovcq5fL9+8HkrrNCNpnWFheX+s3+XNl5eXvvKeTlkS0hZVoSUg4zgah7WRUVdif3at9gUXggqsT1rOHRdcisAAAAACATuBe+YWM5qaWa8+SD5K7MXpJSKIX/+8YhgJnXybL7Zblcin4g190ypKINkm0JCR6FklHHxV1DKzZ3mUwXp8snz9+PYFFl2JJCAAAAAB0AsPpJIyGOim/MPGNMUtCkphhISk84RSFDz5ahmXKKu6d5Cg7UYKgxUh1YLH2a9t1ZNR+7e7LyiuQEQw417HpZpdihgUAAAAAJMH11WfyPvcv5W5ak7ihaUqWJaOyPLaPii2tPscItWNJyHeKSjIjrJAS7mGR6hkWxvZttmsr0TGwrcnNdyy2cvMT/05IOT5tAAAAAGiF95XZ8s+8V5K0p9ujppF7yhoQ+1LsWvWVcv7+R6mhXkYkElu/ZYPMoSMTPyzmlJAkloRkoMR7WKQwsLCsmE03zV59299dXoFzOctBuhwzLAAAAACgFd75z7T87IqE5X1zrmM73+xH5Nq8Xq7aKhkNtTH1Off/Pvamhjr5nrpf/n/cKtf6VbHLJdoywyKTJNjDIqVLQhrrbUs4LJ9fihM6JMN0CKIkycrJa3efaB8CCwAAAABIxLJilnJ4vnDeQDNe+a6Mmu22a/+sh+V7+T/yvjNfubf9KnZ5Q3cNLBLNsAilLrCI3ivEKiiWDKPd/UVGH+Jc0V3/HboxAgsAAAAASGSXkz6+49q0VrKsdnXn+vpz27X788UtPxv1tfK+9LT9Bk/3fFG2umoPi+gNMv0JTvlIQnj0wc4Vvu7579CdEVgAAAAAyB511bF7REQxNq2V5/XnZWxcI0nyzXvKsZ3n3fn2gjgnS0Rz1VTZr6Nmb7g2rLZd98QZFqlcEhJ9oofl7VhgoaISx+Ju++/QjRFYAAAAAMgKvlkPK/+Xpyn/56fK/eVHjm2MzeuUd+PFypnxF+XdcLFcG1bLN+dxx7bur5fY73U6xtRJ/S57WwRiZ28Ylhk18A6+gKeLPyd+XaoCCzMiz3uv2stSECwEjz8rtpDAossRWAAAAADo8Yzt2+R94UkZliUj0Cz/Qzc7zrTwzfu3jG/3VzAiYfmeezR+p40N9uuGuuTGUle98+fa6gQtdzAH75FUv5kmMiT+aShGfeyGpO3hn3GPfK/MspVZKQh4gqf+MKaswzM30GYEFgAAAAB6PNf6VbaZC66aKuXc+f+Ud+15yv3jFXJtWCVJ8rz7iu0+z4f/jdtn9Ckgyc6w8L323M57KstbbR/ZY++k+s00kVFj4tYZVRUd7t+1fpW8b86JrUjFjJS8wtiyBKeeoHMQWAAAAADo8VxbN8SUeb76VK7yTXJ//bn8/7xzR2Ebpv0bUTMqoq8T+rate/03rTb9+fuNundJG/rOFP7cuFWN5a0HNbtyrfhCvtmPyLP4jR2bnQYDyvvfnzg3TsXSDZfDq3Ire5+ki2VZ+u/mgJ5d1ahtTZF0DyeliIgAAAAA9HjuVo4bdX+zVDIjktcrNSfXZ3tnWEiSUV8jK79Q7i8+bLXtB1XS4x/WalCBW6ftnpf0MxKxLEtf14T1cUVIdUFTzRFLpiUNzHdrW7OpR76ql2lJx+zm1+8OKVYv/84X+A31YS2rDqshZGl5TUg5bkPfH5SjfXp5VRUwZUgq9hkyEgQH6zeWq6oiqAP7th4uuD9dqJy/XC/j21NZXlyxXcc2r4rbvsnl0wNf1mttXViX7VOg3Ys8ipiWHlzWoHc2B7RXiUdX71+oTytDqmgydfzgHOV6Wj8GNfrfO10sy9I/ljXor1/UqypgqiG887Saxyb01pRh8YOi7obAAgAAAECP517xRattjNpqWTn5MupqkurTaNgloLAs2/Gku2oauZ9yV9iPMjWam2QFmuT59L1WnxNw7Tht48IFVXpqZaM+2BZUVWDnS+qZw3N1wuAcHVHm14B8tyQpbFp6dHmDFmwKKGxailhSxJJqg6Y+qkh+w8vVdY167OtGDSt0K8dtqCFsaX197F/xf/uh/WU+z2Po+EE5eiZOvzmRkP75VYPuPar1wMIz798tYYUkFbz/uurNSsXb0nP2xoiuf3/Hv+FDyxr058OLNXdtsxZs2nGayEvrpbuX2MOlk4bk6Jr9CxMGKPWV23Xm/Ar1z3Xp94cUqyzP7dhuaVVI89c3q0+OSyOKPAqZlo7o75fXFT8UqQ+Z+r81TWoMW5o6LFelufa+19eHddNHtXpmVewmrd/J8xj6/qCetc8GgQUAAACAnq2pIanZD95XZsm1bVPS3RpNDVI4LHk88v/rLnkXvmarv33IKfrN8HMlSQvKb9JRNctb6vJu/GnSzwm4dr62vbIh9ujUWauaNOvbF9njB/l1UKlPt36S2iUka+rattSgMWzpuTXxX65zzKDmrG3SvUf1arUvY+WXtuvv1SxL2L5W9tDh6oWtB1AvrmvWi+uadexAv+48rES7F8W+Km+uC7aEHvPWNWtsP59e35jcUbZn7J6rh7/XS4ZhaNHWgOavb9bo3l6dNixXpqQzXqnU4vIdS06uXVSjQ0q9+s2BRfrfD2r0ZVU4qWccvZtfb24M6KShzLAAAAAAgG7BVbk1qXa+F59KWH/kgb/TnCV3qk94Z/ixpaJG/1wZ0B/+Oy+mfbUnv+XnOneCIz5bETC8SbedvyGg+Q6hRqbJNUOqCVoKm5Y8CWYeWJYly7Ti1jvxWO3fx+H1jQGd8nKF3jq1VNZ5v1Thk/e21N089LSWn2tDVtJhhSTNXt2k3Qs9uvNze5D05IpGNUWslrDiOx9uC+m0VyrbNPb565t1Wg9aDiKx6SYAAACAHu7FT2M33GyPD4r20HZvvq3sjGdW6JnPt8ptxr4kV+0SWNS62/8iuesMi1TL9xga18+nUcUe9c1xabc8lwbluzW21KtEuzqMKvZoTO/kg5RoOeaOZSkbGhKHCytrw3LtcrpLMj4qHN7ucUk7xrTHU1s0ZN1+er1ktEwZmtvnIM3pe3CH+o0OKyTpjU0BLdyams08nzuuj84dkZo9TjIFMywAAAAA9FgR09KLyyt1Vgf7earf4TINl8qC9uUF09bN0W1DpzjeE+jVr+XnOk/7Z1iMH1Kg0f3y9ODSetUEY2cbnLF7rj7fHtKKGuelAxftla/B+W69vrFZI4o92i3Pre0BUwf29ens4bkyjPjRxJq6sD7cFlQvv0tluW65DKnY59KAPJcMw5BlWfqiKqxVtTv+G1ns0dhSn0b/Z4vClvTQbhN16eY3YvrNNwP61fp5WlV9noYVxn8tfb88qAOV/AyLDwqH68myIzWun09uQ3ovKgz446HFynMb2tAQ1olDcvWfbxr10LIGx77qPHk6fv/pOy4SfEbpMHlwjm4dV6wX1zWr0GvonD3y5Hdn1hhTgcACAAAAQI9VGTDlCsTfSyGR9wv30N6NG7U0b6Bu2P1sSZLlcku7TAo4pfJj/XXQCY73/+2s/XR6sJee/qZRe1QVS5vbNQztW5qr6w4s0vQDi+K2CZmW/vBRrf76xc7lKr39Lr12cqmGf7sfw6/2K2zzs4cVehIGCoZhaExvb8xsi0/PLNO+z2zV73c/U33C9RraXKGxdfaTPe78ZqYWLMiXfnReTL+BiKU/f16nuz+u0qWtjHHriIO0YuxJ+tfHW/Rc37Ea1TdXL0zuK5ekuz6v0y2f1MmSdHiZT5fsnW/b/PLgUp9uG1esjQ0R7fuMw9Ihh6Di1/sVaOHWoL6sCqnWIUAaVezR09/vozV1YU2d37ZlHb85sFB/jLP/yOFlPt1zRIn2LNn5Wf98dEGb+u9uCCwAAAAAZLTtzRHdv7RBLmPHC1qxL/mV7VubTBVE2r6nw3N9D9FZ+14VU547bA9plxM/CiPNyjVjp/SbJX1l9emnCS63JgzMkbemVPqszcNQs+HVPr1bP0nD6zJ009hiXbJ3vj6uCOl7A/xt+pxSbVCBR9vOH6DSx6RzR18pSdr+9kUqitjPjB3/2t9V+8P/kSsqGPjX8gb96dM69Y60HjYVnTBFB4z9nn56aEgTasP6/qCdJ3Jce0CRThmWq00NER1R5nxSh2EYGlTg0cQBfr2xKfF3ZeIAv248uLjVMUk7wp4ZE3rrp//drkDUypcXJvfV376o18vrd3we+/fx6h/H9NKeJV5de0CRtjRG9MbGZq2qi+i4QX4dUuqL+YyyAYEFAAAAgIx2wYIq/XfzjhfJzyqCevr7fZO+t7wpoj7htp+YMa/PgTFldxxWrEjkx9Id17SUfZk3UHmR2MAi8KMrJdfOoymtwpI2j0GSml1ejShO/rVtUIFHgwoy4zXP6zJ03CB/y8km8V63X1jbrFN32SwyGLE0bfGOpTf5SYRNkQOPkCSN7u3VaId9NfYq8Wqvktb32/jbUb107aJqvbiuOW6bq/dv2yyVU4fl6qC+ZfrTZ3V6Y2NAv9i3QD8ZlS+f29BR/f2yLEumtWMix66BRP88t84bmZ+g5+yQGd9kAAAAAHBQHTBbwgppxykYr29s1rEDk9sTorK2Ub9d82ybn/uTg8qU17tA939ZryKfoWv3L9TFe+XLrBxka1ccblRO1AyLDSPHquSQo21lVlH7AouAy6MBee7WG2aostydYy+MOAcBr220BxavbtjZrrXAov6fr0nu1LzWDsh3a+axfWxl39SE9eM3K7WxIaKr9y/Ukf39be53UIFHfz3S+fhWwzDUA7eeSBkCCwAAAAAZ49OKoB5YWq+hhR79ekyhvqmN3UjyjFcq9c6Ufto3wSkV89Y16amVjRr80avtGsd+Y0ZozMBi/fbgIrkNyf3tUgIr1/5X7+JIU8ySkD69ChR99oVV5PzC2pqAy6tevu77RluWRNjy5faQ7Xpp1c7rRIFF/b/e7PTNMPco9uidKTs2T020OSk6B4EFAAAAgIxQFzI1ZX5Fy0kYf/o0/lKOf69s1M2HOu8lsKo2rB+9sV0RS1q0Zn7CZ0YGDpN74xpbWfigo2QNHCZJ8kX/+TvXfmxkYaRZ/6x/xVbm8efEBhYFye17EC3sT3yKR6ZrbXZIvcuvjypC2tIYUf88t/6+tN626WSBw6yMyJA9FLj0f7vs5I7u/Pl3dwQWAAAAADLCrG+aHI/tdPK3L+v1h7FFji+Tr2xoVuTbbiyH+uDU86VAs8yBwxQ+6gTJMOR5+yV5PnpHkb32V+j4BIegutyK+PxyB3f+5d+3fqWtieVzWDaQkxtbloTeJW0/2SOT7N9n5yyYPw8+SVevf9FWv92745SLvZ7e4rjpZb5pvw7vN07NV9/eSaNFpiGwAAAAAJARllWHWm+0i9mrm1Tic+mY3fy2mRBLdlliYDls9Rg+6CiZQ0fay46erPDRk5N6rum1BxYxHAKL6KUku4oMGSEjFJBr8/qYuoKiAsXfAjLz7dfHq/65Lm1pMvW/u5+t/p6wfrB656yXvF2WfDid0FEUbrQX+JPbuwQ9Q/rOuQEAAACAXayoid2vIpGL36rSma9W6sxXK2VZO6ZUfFIR1MwVO19yTYcZFuZuQzo0zuq9DkpYb+UVxBY6zbr4VvMvficrJ06g0c6ZGZnC6zL0t6N6aUSRRyN656j/T35mq+8brtfwpq1x75+2bo7t2iKwyCoEFgAAAADSzrIsvenwF/Zk/HdzQCtrw3pnS0AT5m6z1flMewgSLhucMDxIxraxExPWW/kOyzgS7INg9e0veZ03EE00M6O7mDQoRx+c3k/vTe2ncYNiP5vfr34m7r1e074biFXUO+XjQ+YisAAAAACQdvd9Wd9qm78cEf9o0JU1Yf3l89hNOoujlhQEr/h92wcXJdB3N0UGDY/fwCmwiKPpmjskt0eWz3nmgBW1yWd3ZRjGjv1GXLGvoP9TvtDxHp8Z0oio2Rfhwyd1yviQmQgsAAAAAKTdzR/XJqwfU79OFzZ/ofyw844OQVN6baPDHgiRJtt1e48XjWb12y1+XZKBRfMFv1ZkzFhJkjlsT+dGPWCGRXsNba6QR2bLtZVfJHPIHmkcEboagQUAAACAtGuOPgd0F/+75ll98uF05d81Tdu+vlHD/bGNa4Jm7I2WFTPDwspJzYwFy+OLX5df5FgeGjdhZxuXS5GDj265jozaL05f3fuUkI7wm/ZNWM0SloNkGwILAAAAAGn13YaZTnqF6vW7NbNbrn3lG7V4+CoNKXDb2lUHTLmitonwmyH5rJ3hhuXxdnj/ihYe5z0nJMkqLHYsD045X5GBw2Tl5it4zmW22R5WSR/nvuKEH9nAa0UFU24Oucw2BBYAAAAA0qo6GBtY/OWIEq04t79e6/1ZTF3e9i364Uj7TIkPtgVlRnXz/NFR+0Kkcj8Ir/MMC8vlktW71Llu4DA13fIvNTz4okInnG2vi7NUxSroeTMsmi/6fzFlhhU7Q+aiEVHhEoFF1iGwAAAAAJASVQFTs1c1amlVqPXGu6gL2V9WB+W7dcGofJXmujV6xXsx7V2b16l/nn2Gxdy19r0tinyGDiuKOmEi3tGh7WDFO9Wjd792vVjHm5Uhf/c+1tRJ+IjjYspyzWBM2dg+Ua+rBBZZh39xAAAAAB3WEDJ19P+Va0NDRG5Deva4vvregOSWXzSFd06NcFmmbl46Q3m/+kiR/Q+Xa/O6mPbe/87TsftPlBR/48sJA/xyh6ptZZY/RctBpLhLQqzidu6z4PYoNG6ivIvfsPfXAwMLeTyyCopk1O/caDU/ElCj2z4jJldRsy489pAKPR8zLAAAAAB02LOrm7ShYceMhoglTZlfkdR9pmXpyRU7NsacUPWlgm/9SD9cPV+uqgp5F8yVq2a7432j7r1GpcGauP32y3VLoahTQ+IcHdou8ZaEdGCPjMCF19iuzd79ZA4b2e7+Mll0EJMfiT3hJVdRM2SYYZF1+BcHAAAA0CaWZemZVU16Z0tAJw7J0QmDc7VgU+wL52eVQe3fJ/5pGpJ03eIa/X1Zg2RZuu/rf7ZpHLdWzNfFA86OKT+54mP96dlZytu61l7hSzyWtrDiBBYd2tQzN0/1D7wg35zHZdTXKnjS/0iuHjqrwG8Pj5wCixyx6Wa2418cAAAAQJu8siGgS/5bJUma8XWjFpxSqu2B2E0Tb/+0Tk8e63z6xXf+vqxBkrRbsFp7Nm1p0zgu+Pr/dG3piaryFrSU9QrV6/Gv7ldhuCmmvZXKGRbxTgmJF2QkK69AwXMv71gf3YAVHViYsYGF3yCwyHYsCQEAAADQJn9ZUme7Pv2VSr3pMMNi3rpmmQmOLN11s80D6tfGbZfIv5vma1w/n/bp5dGxA/16smSZY1ghKXVHmiboK+7MC9glMcPCH3OsaQ+dbYK4iKgAAAAAtMnCrfYTHZxmV3xnXX1EwwqdXztu+2Rn8LFfOwOLYz9+VuOu+KVkGJIk3zPb4rbtyP4SMX3FOyKVwCIpre1h4TEkt8keFtmOGRYAAAAAkmYlmDHhpD4Uv/19X9a3/LxffexpIMlyf/Vpy89GQ238ht4UBhZxjkhlhkVyopfnnF2+UPvXrWm59rsNKRK230RgkXUILAAAAAAkbVVtpPVGu9j1yNJdLasK2a4HBZxPA/mOuduQuHWuFV+0/LzrUZnRUjnDQrlxjhtN5TN6sqglIeeVv6cPPvpfnb11oSTJ55b8j/3Ffo+HwCLbEFgAAAAASNqbm5rb1L7RIbAwLUs/fKOy5Xqvho06svbrhP00XX27QmPHO9bZQoqGOsc2klIaJlg5LAnpiOhNNyXJJUsXblkgSTp387syglHfNfawyDoEFgAAAACS9t3xpQXhJimJ5SHNkdg2n1WG9M0uMzVuXv2fVvuxikoU+NmNarryjzF1Rl3Njh9CQXmWfhy/D4eX5HaLs7wkpbM4ejK/8wyV/evX6Zyt7+nez++LqWMPi+xDYAEAAAAgKaZl6e0tAd2//BFVv3OxVi76lUY0Jj6K1GlJyNKo5SBTKz5M2EdkyIgdL7gulyIHHammX91iqzfqdwQWOX+9IfH4EywraSsr3vIEZlgkJV541C9Uq5nLYsMKSZKLGRbZhsACAAAAQFKqAqaGV67SJZvfkCQNC1Ro9vK/yWOG9YvAZwov+IHCC36g1Qt/qVENmyRJjeHYE0S+qg7HlMUTHn2wAhdPs5VZhcW2a6OuRp53Xpbn88UJ+zL32Cfp57bG6tvfuTzOzAFEyXH+nLZ5C+Pekmh/EvRM7Q4s1qxZoxkzZujOO+/U2rU7jiAKBoNav369gsFgK3cDAAAA6G4qm02NiTrNY3TNalWOXKq7v3iopWxwYLvmf3aL8iLNanbYo/OrqBkW8Zj9Bqj5//1Z5tCRtnKrICqwqNmunH/clrCv8N4HyurTL6nnJsXnV+Dcy2OKrf6DUveMHizeDIvSUPw9SFzbt3bWcJCh2hVY/Pa3v9UhhxyiK6+8UrfccovWrFkjSWpubtZhhx2mhx9+OJVjBAAAAJABKgOmhjVXxJTnPvfozn0kvjUoWKXjti+JmWERMS19tj25wCJ4wjmO5dEzLFzbyxP3c+xUNV93d1LPbIvQ5NjxpXLZSY/ma/t+Ikbltk4YCDJZmwOLRx99VH/961918cUX67nnnrOdw1xUVKTJkyfr5ZdfTukgAQAAAKRfTU2Dblz7bEx5vMBgYGC73t4csJV9XBFSedPOEOPg5o2O90aGjlT46BOcB5KbL8uV+FWm/pHXVP/YAtU/tkDBH/8qYduOCB15fMvP4TFjZZX06bRn9STtWToTOvK4ThgJMlmbt1l9+OGHdfLJJ+u2227T9u2xZyWPHj1a7733XkoGBwAAACAzhExLi2fP0ZltuCfXDGr+hoCmLarW7YeVSJI2NuxcI3J85Wd6ccmfYu5r/MPDMgcMk+JtbOlyySoollFbFf/h8e5NscBPrlFkxD4ywiGFvndSlzyzR2jHiS3hYyZ3wkCQydo8w+Kbb77RhAkT4tb36dNHlZWVcesBAAAAdD+zVzXpR+tfb9M9ueaOve0eWtagk17aprqQqS1NOwOLR796MOYey+2WOWREq4FD9D4WuwoddXzcupTzeBWeOEWh486Me1QnYrX1iNnQoRNk9SnrpNEgU7U5dvT7/WpoaIhbv379ehUXx/8/DwAAsUaHbwAAIABJREFUAADdz+LygA4y2nasZG5k514V724JavATm1uue4fq1C/kcOpDkseCWn1KpU1rHOtCE05t0ziRBm0MLKxefTtpIMhkbZ5hcfDBB+vFF190rGtubtbTTz+tcePGdXhgAAAAALre55VBPfpVg9bV248erWg2lRcJxLnLWY4Z//TA729f4lzh8SbVt9l/cGxZcS813nCfzBGjk+oD6dPWPSzCR3y/k0aCTNbmwOKKK67Q+++/r0suuURffPGFJKm8vFyvv/66Tj75ZG3atEm//OUvk+rr3Xff1bnnnqu9995bJSUlmjlzpq3+8ssvV0lJie2/SZMm2doEAgFde+21Gj58uAYMGKBzzz1XGzc6b9wDAAAAIL6FWwM6Zs42XbWwWkc9X67yXZZvbG00VRhpjntveMyhMWW5cQILfySomcvuc6yzkp1hURZ7fGjTDfcTVnQXSc6wMMsGKnD6T2KOtkV2aHNgMX78eN11112aM2eOpk6dKkm69NJLddZZZ+mLL77QPffco0MPjf0/KycNDQ3aZ599dNtttyk31zlhGz9+vJYvX97y3zPPPGOrnz59uubOnatHHnlE8+bNU11dnc455xxFIg4HPgMAAACI67cf7FyiURuy9NTKxpbrDQ1hFUaa4t7b/Otb1fzT62xlOabz8aU/2/Rq/EF4kgsszLKBMWXRx50icyW7h0Xjn2YqNOXHkmF08oiQidq1de4FF1ygyZMn6/nnn9eKFStkWZaGDx+u0047TQMGDEi6n+OOO07HHbfjaJqf/exnjm38fr/Kypw3V6mpqdHjjz+u++67r2Uj0IceekhjxozRggULdOyxx7bxNwMAAACyU8i09P42+4yI335Yqz2LPYpY0uaGsAoSLQlxuSWv31YUb4bFNeteiNtNsjMszL79YwvZ9LL7aMcpIcg+7T7rp6ysTJdeemkqx+Jo4cKFGjFihIqLi3XkkUfqhhtuUGlpqSTp008/VSgU0sSJE1vaDxo0SKNGjdLixYsJLAAAAIAkBCKWymZscqy79O0q1QYtFUQCcslybGN9u++E5YsKLCKxgUVxqEFlTpttfseb3B4WVv/BMncbLNfm9ZK+XZLCX+G7D3fXHDuL7q3N35I1a9Zo2bJlmjzZ+Qzcl156Sfvss4+GDh3a4cFNmjRJp5xyioYOHap169bp5ptv1qmnnqoFCxbI7/ervLxcbrdbffr0sd1XWlqq8vLyuP2uWLGiw2Prat1xzEBb8T1HNuB7jmzA97z7eXGrW5Lfsa42uCOkiLd/hWW4tOaUC1S9YoUKy7dpxC51uQ5LQn6w9d2EYwk2NSX9Hco55SL1f3uu/j979x0eRbX3Afw727KbXkghQOhdmoAUBRERRREU9V4VxXbFXrBxLS8qoliuvV4s2Bv2gnjBCorSewsklJDes313Zt4/FpJMZralsSHfz/O8z5M558yZE93Xm/nt7/yObDAi/7Tz4WnFzx4/5003LEh/4dgpKOA/52OqpT/nvXsHrk0SdsBiwYIFOHz4sN+AxUsvvYROnTph0aJF4U6tcsEFF9T+PHDgQAwdOhSDBg3Cjz/+iGnT/B9VJMsyhADR1WD/UCJNdnZ2m1szUbj4Oaf2gJ9zag/4OY983x9w4Kv9DoxKM+HKvjH4INuOh7Irg94X71XWr5BSM+G8/n7IsQlIzeiMVAA6WRnUiJbUW0jOqPBzOsgRxtSM0D9DvXsDJ08AAHQL7Y5mwc95y/NMOBexl96I3tzmc8xEwuc87KKbf/31l2ILRkMTJ07E6tWrm7Qofzp27IjMzEzk5OQAANLS0iCKIsrKyhTjSktLa7eNEBERERGRz4o8J2b+XI4lOQ7c9VcVzvuxFLf9GTxYAQAXlfyluJbjEyD1Ggg5o95pHeZoxZi4ekGOcRm+2hT9bcoT/bzDxymuPRP9fzFJ7YfryjtYk4TCD1iUlJT4LYIJ+LZjlJSUNGlR/pSVlaGgoKD2+UOHDoXRaMQvv/xSO+bw4cPYvXs3Ro0a1SJrICIiIiJqa5xeGfPXV+HC5cov+lYVahfFbCjdVYmH9n+uaJPjklTj5OgYxXWK5DtlZHzHKHxxZgdsmpaEHs66rduyIMA163Z4B5wI2WCEZ8wkiCeeHNKa6DjHeiSERmwJSUhIQG5urt/+nJwcxMbGhjSX1WqtzZaQJAl5eXnYsmULkpKSkJSUhMcffxzTpk1Deno6Dh48iPnz5yM1NRVTp06tXcvll1+OefPmITU1FUlJSbj//vsxcOBATJgwIdxfjYiIiIjouJNT7cWJnxc1aY4p5ZvUjQb1q4RsUQYsMpwVsO57CILhBIg5k9A9IVlRuFNOSIacmALn3GeatD4iOj6FHbAYM2YM3nnnHVx//fWqTIuioiK8++67GDt2bEhzbdy4Eeeee27t9cKFC7Fw4UJccskleOaZZ7Bjxw58/PHHqKqqQnp6OsaNG4fFixcjLi6u9p7HHnsMer0eV111FZxOJ8aPH4/XXnsNer0+3F+NiIiIiOi4IkoyJn3X9Ozn0dXqwntyXKJ6oCVa1WQ+lA0cygZWfAkpIVk5R1KHJq+N2i7ZFAXBra5z4rrwX8dgNRSJwg5Y3HnnnVi2bBnGjx+Pm2++GYMGDYIgCNiyZQteeukl2Gw23HnnnSHNNW7cOFRW+t8z98UXXwSdw2w246mnnsJTTz0V8u9ARERERNQebC7zoNwlhXXPBd0t+DxXWWAzza0+htQ7aKT6Zl3gLw11VeWKazmRAYv2zHn9AzC/OA+C3OC4XB55SkeE/UkYPHgw3nnnHdx0002YN29e7WkcsiwjJSUFb7/9NoYNC3ZADRERERERtbTDdjGs8ZcX/o5Fh1bhifReeHnI5Xh2pwsmyYNpZRsU48RO3SAOCy2rOhApNaPJc1DbJQ4fB8eDryL6oeuVHRrbjah9atQn4ayzzsK2bdvw008/IScnB7Iso1evXpg4cSIsFlZyJSIiIiKKBAetoQUsTJIHi3e+hn8eOQmk277tWJCRgqTh52HQ2w+pxrtumOc3m0Ls3AP6vJyQnuuZcG7wQXRck7r3U7XJeuMxWAlFokaHriwWS23xSyIiIiIiiiw51V68vtMa0tj3d7yMGaVrFW1RX7yF2xecgugG2RUAIMfEqdqOct44DzH3XRn0md4hoyF36hbS+qidYT1COiLsY02JiIiIiCiyLc9zYsQXRdhfEzzDorOzTBWsOMr8zFzNdjk23u98cqdukLQKcjYg9h0cdAy1UwZmWJBP0AyLc889F4Ig4IsvvoDBYFCc6uGPIAj45ptvmmWBREREREQUnqt+KYckq9tvOyEWb+22ocbj69QJQC9Hod95dOXqE0akxA6AKSrwAqJjgBr/xfUBQOrYNfAc1H6x6CYdEfSTsH//fuh0OshHKrfu37+/ttAmERERERFFli1lbli96miFRS9g7rA4PDwyAb/lO7Gl3IPp3SxI25INbA59fvc/ZgcdI0cFr2snZXQO/aHUrsgGbgkhn6ABi61btwa8JiIiIiKiyPF5jkOz/aKeFkQbfDvCT80049SOUYh69zkYf/465Lnt81+H1LV30HFivyHQH9xbey0lp0Hq1huGDX/4rjtmQWbAgvyJ9l8jhdqXsHJtXC4X1q1bh4yMDPTs2bOl1kRERERERI209JBTs/2cLGXWg+HP5WEFKwBAyuoV0jjPOZfC+Ms3EDweAIDrkhsh9R0MKTkNgrUannMu9XvKCLU/nvFnw/j7UgC+LUdi/6HHeEUUKcIKWOj1ekyfPh0LFixgwIKIiIiIKAKd2MGI7Cqvou3sLDMmd65Xd8JaBfOixzTv9w44EYYd6pNBvMNOBkLcGi4npsDx4H9h+OsniN37QRwxDgDgvvy2EH8Lak9cl94M2RIDoboCnqkzGcyiWmEFLAwGA9LT02vrWRARERERUWT5Jd+luB6ZasT7E5NhWv45oj54Kej9njNmQJeXC111haLdeyToECqpSw+4u/QI6x5qpyzRcF9607FeBUWgsI81nT59Or766itIktQS6yEiIiIiokaq8Ugodij/Tr9zSBz0ZUUhBSvETt0gDhkN+9MfQ+xTd+yoFJcI7/Dxzb5eIqJAwj4vZtasWVi5ciXOO+883HDDDejZsycsFnUV4C5dujTLAomIiIhaSqlTxNu77ehg1uHSXtEw6XkSGrVtqwpcqrZ4ow76HRtDut/x0H99R0rqDXDc/4Iv0+JANrxDxwCW6OZeLhFRQGEHLMaMGVP786pVq/yOKy8vb9yKiIiIiFqBLMuY+kMpdlX69vrvq/bikZEJx3hVRE3j1kiCNuoEQPSqOxqwP/QaYIpStEmdu0Pq3L25lkdEFJawAxb33HMPhBCL7RARERFFqt1V3tpgBQC8uM2K+SPim/XvHJtHQp5NRJ8EA/9+olbhFNW15vokGqCrKPF7j/eEkXDe/VRLLouIqFHCCliUlpbijDPOQEpKCrp3Z6SViIiI2q6cavU3znurveidYGyW+beUuXH20lJYvTLO7ByFjyelMGhBLa7KpU6x6LD0PZi+fldzvGfMJLiuuaell0VE1CghBSwkScKdd96Jd999t/aEkJNOOgnvv/8+OnTo0KILJCIiImpuHknGpT+pt6/m28RmC1jcuKoSVq/v76Yf81z46bALkzqbm2VuIn+qPcoMi6fSixD1yWLVOOcVc+A9bVrIx5QSER0LIZ0SsmjRIrz99ttIT0/HueeeiwEDBuDvv//G7bff3tLrIyIiImp23x9warZXuJrn6PYKl4Rt5R5F24XLy5plbqJA8qzKzKFTdq3QHCd17s5gBRFFvJAyLD7++GP07dsXy5cvR1xcHADg1ltvxYcffojKykokJia26CKJiIiImtPKQvVJCgBQqVWxMIBKl4TVRS70TTSiR3zdn1X7a7QLHObbRGTG6IPOW+2W8NV+BzrF6HF6J2ZlkDZZlvFjnhP5NgkX9LBgc5kHb++x1/b3sx3GyM1LVfd5h4+D1HtQay6ViKhRQgpY7N27F/fcc09tsAIAZs+ejffeew/79u3D8OHDW2yBRERERM3N4dXOpChzhh6wOGT1YtCSotrrqVlmvH96CgDg2wMOzXs2lrqRGaM+Dr4+SZZx1tIS7KjwBT2eGJWA6wbEhrwuaj9e3WHDfWuqAAB3rK5EnFGZMfF4zkeqe9yTL4D70puZXUFEbUJIW0JsNhsyMjIUbR07dqztIyIiImpLih2iZvuHe0P/u+axjTWK6+8OOrGvyotyp4hXtls179lR4dFsr29Nsbs2WAEAc/+uCnlN1L48sr5acV1Tr36FXhJxRvlWRb+Y1QvumbcwWEFEbUbIp4Q0rGp99PpoEU4iIiKitsAjyVjlZ0vIvmoRsiwHPc1jV6UHH+21q9q3V3hw2CbCqR0PwZ4q7a0i9e3VOL2EIl+5U8QH2XZ0jNHjgu6WVjkRxqFxhOlRme4KRMnKz5JzzmMtvSQiomYVcsBi+fLlKCqqS3t0OBwQBAFff/01tm5VRm8FQcBNN93UfKskIiIiaiZ7q7xwikC6qxILcz5GoteGR7uej/XxPQAAj2yoxrzhCQHneHOXLxPDIHmR4rGixBQPSdBhb7UXm0rdfu/bXVn3AilKMl7ZbsUv+S6c2cWM2f1jIAgCTDr1i26NR0KcMaTEWDoGJFnG5O9La4NNa4rdeHJ0y9Z4W7JPHTCrr6uzVHEtdusDOTmtJZdERNTsQg5YLFmyBEuWLFG1L16sPiaJAQsiIiKKVNuPbMt4OXsxzitdBwAYZt2PvqOegVtnxDNbrLhtUBxW5DlRYBcxs3cMkqJ0+LPQhU/32TG0gwmv77QhxV2DZVsWYpj1ANbE9cTkIfdi/nrfM3SyhOez38F5pevwS+IAXN/3Gtj1Zmwp90CUZOh1Ar476MT/rfOl9P+c70K/RCNOzYxClVuCSfLAIIuw630FN/NtIvomMmARqbaWexSZMYt22nD7oDhFgVVJlnHYJiLNokeUvnHZFz8ddmJDiRsX9ojG7X9WBhw7/chn+yg5Jb1RzyQiOpZCClh8++23Lb0OIiIiolax/chxo+fVe6Hr4irH5PIt+K6Dr5D4OT+U1h5L+uYuG0anR9VtATlyCsNlRaswzHoAAHBSzT5cXLwab2ROBABMK12PG/J9x0leWvwnzi9di4Rxb0ESdEh7Nx+lV2Tiil/KFet6dEM1Ts1MRca2VShe9SyMsojbe8/C65mno9rNLbiRrNihLta6tsSN6UcKrC7ZZ8e1v1fU9m2+MB1d40L+3hAA8Ok+O2YfmeO5rVbY/BSOBYAYrxNz8n5QtEkZXcJ6HhFRJAjpv5SnnHJKS6+DiIiIqFXsqPAAGjW4hloP1AYsjgYrACC3RkRujTr9/ul97yuuX9qzuDZg8cqetxR9FsmDWYW/4+2OEyDKQNLb+ar5dlb6njn8p7cRK/lqbDyx70O8mz4ONZ7wjlul1lWqcbrMFb+UY1afaCSYdHhxm7II6/1rqvDCyYlINgc/4hYA9lZ5aoMVAAIGKwBgeczfqjapS8+QnkVEFEmYW0hERETtyo95Lpgl9WkdMaJ2Ic5QGVD30prmqVb1NwxiNKQXgPWFDvRxFNa2xYtODLAfVpz+QJGn1E+V1Xf32FXBCsB3okzfTwqx9KD28bf1SbKMWQ2ycbTcOywOL5yciLzLOmL4zp9V/eLAE4POQUQUaRiwICIionZj45GCmAledcbE3Ye+a5ZndHaWababZBEnVe/1e1+lW8bTqw6p2ns5CmFlhkVEK9PIsAjGIwHPbqkJOm5zmUdxzK2WLrF6zB0aj1l9YhArSNAd3q/od153P+T4pLDXSER0rDFgQURERO3G0ToUiRoBCwDoZzvc9GfseNFv358bHoRR8v/yeahAHezIdFUywyLCFdr9nGMbxNoSdaZPQx9mBz4NBAASTXV/0gtFeRDEus+YlNgB3rFnNGp9RETHGgMWRERE1C5UuiQs2uk7jjRB1H4J3Lb2HhgCBBSO6m0v0GyP8Toxpjo74L2P5nwCQdb+Rj5VYyuJRXLDyoBFRDtsa1zAIhRLckIJWNSdOqLPy1H0SZ26NfeSiIhaTXjliYmIiIjaoGq3hPHfFNde//vA137HzihZC4vkxvjKnfgi9SR830G9939M1R7Ne6tWXRN0LXfkLUWmuwKX9b8JAmTIQt33Rz0dRarxFsmNfEfLvRA7vDKe21qDzWUeXNY7GlO7WlrsWceDzWVuVLllnJJhgk7wBQqaErDwSjIMOt88n+yz47ojxTWv7huDWX2iURnCCTHp0UeKd4pemF+Zr+iTOndv9NqIiI41BiyIiIiozVhf4oYkAyNSjRAEIfgNRzy+qRoHrb6XSqPkxbSyDX7HPr/3HaR6fLUFrihaieHDH8XmuG6KMS93LgF2h7/+oy4uXo2Li1cDANbFdsf5g+5EQVQSBlsPqsZGiy78WeRu/MOCeHePDU9s8v2+Px924rdpaeifZGyx57Vlr2634t41VQCAi3pY8PqpyQCAIo1jTUNl9chIjBKwushVG6wAgLd22/DWbltIc3Q/ckSqLnubqk/q3KPRayMiOta4JYSIiIginiTL6PFhAU7/rgRnfF+CB9ept07445VkvLK97sVvmHV/wPFHgxVHvbF7EbKcJXh+z9t4Ovs9rOmTB8tPX4a27syusD/yZsAxI6y5eHrvewCAvhpbTSySG1vLPShpoSyLd/fU/bNxS8CYr4ohaxz72t4VO8TaYAUALMlxoMguwiPJQY8ZPap/ovq7wiq3L9gxf31on+lr+8eo2rrH+TIsdGXFqj7v0DEhzUtEFIkYsCAiIqKI9/J2K8pddd9iv7DNClEK7SWxoEFBxDPKt4b17GHWA8j563bclL8ctx1ehhMXzQ35Xufs+yBl9Qw67h8lfyPDVYEsZ6mqL1r0ZVdsLA1eoLExtmucQPHSdvVRnO1ZpUtCn48LVe17q734PCf40aQA8PaEZKw+Px0DGgQtqj0yZFnG6hCyaP7Rw4JYgzqzaHCKyfeDV+MzEp8Y0vqIiCIRAxZEREQU8f5vrfrbZ6cYWsDC3uDb76kNtoO4p89q/MKCkBNTAACuf14fdGze6pvRy6ldwwIAClsgw2JflXaB0YfCyGBpD74/qB2U+OGgE9evrNDsA4D/jE7AQ8PjsefiDJzX3VcbJNms/PN7U6kbeSHWwLjvxHjNehkDknxBEMGjDHp4TpsW0rxERJGKAQsiIiJqk1whBiwc9QIWyZ4ajKxRnqLgmTi9WddVnxwT53vGpPPh7Te0UXNEiy4AjT86M5Cjx7w2FOI/2nbhgTVVuGlVpWafv0yUOYNiseb8NPyrfyxuHxyHNIu+tu/kjCjF2Ke31CCnOvjJNFO6mNEtzoCsOPW2kqPFPxtmWMhGU9B5iYgiGQMWRERE1Ca5QqxzWD/DIstZpuiTOmZBTkyBmNWrOZcGAJD1esB05OXUFAXnv5+F88Z5Yc8zsXI7AKDQ3vjCjv5sLPO/DaF+HYsSh4hH1lfhxa01cIZYr+F4kFvtDXt7zPiOUXhwRAL6JGoXLj2xgzKIsL9GxKrC4NtBMmN8QY8Z3ZWnuLw2LqnuwtNgS4iRxVOJqG3jKSFEREQU0fxlUoSaYVE/YJHhVn5TLiV18M111V2Ifjj4to1wiANHKBsEAd5RE2EdNRG6XZthfvkh6Kr9byc4yiJ5EO+1Y2dl839b3jXWAMCl2VftkZFgEiDLMqb/WIodR2pd5NR48ezYJM17jjd7Q8h8aCjRFPj0mq5xelXbU5trNEYqJUX5vmcckGTEexOT8WWuAyelmfCPnnUBDMHbIPBhYIYFEbVtzLAgIiKiiLbskFOz/avc0Iod1g9YpLmrFH1ygu9YSqlHP0hpmY1coZqs18N17b/99kv9hsA1+96Q5zunbCNWF7mxvyb8F+hA9AHercudvoyOvdXe2mAFACzerb2N5HhUEWoaTz0JpsB/XvuCROFLiaqb99yuFrw1IRnXD4it2w4CqDIsZGZYEFEbx4AFERERRbTXdmin5D+0vhp/F2lnB9TnEOtnWGgHLABAbuS30VJiB8W12L0vbIt+hBwfOAtB6pgV8jMuK1wFALghQIHHxqjx+H8hLzvysl7jVmeyeEM8oaW5ybLcqkeuljciYJEYFfjPa4tBwLSu5rDnTTGH8Gd7g6KbYA0LImrjuCWEiIiIIlpVgJfGz3IcGJUe5bcfUBbdTA8QsBCk0Ipa2ha+g6j3X4SuOB/us/8J7/hzIBTlQb9vJwDAO3oiYAj+J5bcIQPe4eNgWL8y6NgzK7Yg01WOCldaSGsM1ZYy3zfyMV4nHHoTJKHupfjM70sgyoBOIwtjY6kHI9Na72VYkmXM/asKH+y1IzlKh+fGJmJS5/Bf+sPVmAyLxCAZFgDw7sQUJC4+HNa8yUECIQAgNDzW1MAMCyJq25hhQURERBFtWKr/F+MfjmwXOWT14ppfy/HP5aXY3KCQpK1+wMLjP2CBEAIWjlsfgZzZFc57/gP7fz6Ed+J0wGCA3KkbvOOnwDt+Sl2hzRA4b34Ijn8/G9LYOw99j12VXryy3Yovcuy4b00lvsp1NDrj4O3dNuyo9OLKgl+Rt/omlK66FlNL6458PZqYopVM8eSm1j32dFWhG6/vssHulZFnEzFntfapHc2tcVtCAtewOCpQlsUtJ8Sq2kLKsGh4SggDFkTUxjFgQURERBEt0Ovf0a0Jd62uxOe5DvyY58Ksn8sh1XuJdwQouqkIWAR58Rd79oc4fFzoCw+FTg+x/7CQhl5Z8Bsgy7hvTRWu/q0Cr2y34cpfy/HNAe0aH8HcsboSRsmLJ/Z9hDjRiXjRiTd3/RcmyRP03vgQsgia07Zy5ZoOWUW4W+Hs1RJH828JOapXgv8snDO7qIMZSUHm1a9fCePvS5WN3BJCRG0cAxZEREQU0d7P9l/k0eqRUeGS8GNeXS2LA1YR+2t82RIOr4wFG+qyAVRbQurVmfCOPl3R5x08Cvb7XoDYpSfcZ14Exx1PNOn3CIcUnwTvwOGKtgTRga7OUtXYxzaEn+3g8MqQZKCbswQp3roaISleK84q2xz0fnsrH21aaFdnv7TGGgo0nvv82EQ8PzbR7z2Z0epTQLRc3DMasQbtcJxWe8AMC1sNzIseUzXLDFgQURvHgAURERFFrHxb4G0aVq+Me/9Wbw84euRpw4KdqoBFYl2GheeMGZCjfan4ssEI94yrIfUdDMeCN+G+9CYgNr5Rv0NjOecsVLUNte5Xte2uCv/kkINW3z2DrAdVfV9sfzZotomjFbIb6jtWAYv8Bs99b2Iyrugbgyv6xuDpMQma2z8GJIW2DaNPohGrz9euSaJVN8RfcAMA9Af3QnBqnJrDLSFE1MYxYEFEREQR60c/R5rW9/E+9Yua1eN7mX14fV32QZToRrLXVnst63SQ6wUh5IRk2B9dDOfs+2B/5A1I3fs2ZelNZzTBM3G6oinLVdYsU+fZRECW8emOFzT7R9bkBLzf0coZFkUaWzNaeg2yLKsCJadl1tUnuaZfLHIv7YjM6Lo/pwclG0PeEgIAXWINuKpvtKo91aLO0hAE7YCFbt9OWB6fo9knZYZ+Eg0RUSRiwIKIiIgilr6Rf6lUudUvuEu2P6+4luMSAZ3yxVBOToX35MmQM7s27sHNTI5Tbj14du97WLFpAdIaZIrUjpdlPLSuCl0/yMeUpSXIrdbOvih3ShhiPeD3ueMqd6G/Lc/vc1p7S4hV4/hVewtneZS5JNT/GMUbBcQalR9InSDgpVOS0DvBgBOSjXg2wFYRf24eGKe4vmtIHDpG6xV1LP49NK7hbYDLgeg5FyF6/g2a87qnz4Kc0SXs9RARRRIea0pEREQRSzzywmgW3bj/wFfo6SjENx2G4+O0sYCfb5wB4MLlZRjWwZcOL8gS/rP3A5xdvknJ6VyKAAAgAElEQVQ5yNTyx2KGynPqVBh/+67uetL5AAA5Vv2iOqFyJ67LX4FHul0AwFeUVJZlCIKAbRVePLfVtw1mdZEbV/5ajt+mqbcdlFfW4K8N8/yu58mcD/FkzocAgGc7T0GS14YOnho8kTUNqxP6+A2EtARZlrGtQl0I1K4RxGhODbcjZfipTTGxkxlrZzT+s9QzwYBnxiTijZ1WDEg24uaBvm1J709MxncHHIg36TAxU33yTNTip6ErL9Gc0zXzFngmX9DoNRERRQoGLIiIiChivZttg0V0oWbl1bVt/yj5GzIEfJI+NuC9G0t9L7nzcz/DbYeXqQc4/RfzbG3ucy6GfsMq6GoqIaWkwzNxGgBAjtGum/Hg/i9qAxYyAKcIWAzArgYv9pvLPKhyS0hocKpHyXdfwigHP8YVAObk/VD789SyjUg45U1YYUaFSwp6ckVTrSxw4dxl6kKjQMvX0SiwKwMiHUMsphmQLEOXuxuCtQriCSMBne+f39X9YnB1vxjFUKNOwPnd1dtFAEAoL4Fx9QrtRxhN8J40oelrJSKKAAxYEBERUcTaWOrBzJI1qvYrCn8PGrA46t6DX2u2e867oklra05yemfYH3sbuvz9kLJ6AUeLf8YmhHT/wE8LcXaWWbPgY3aVFyNS606LcIsyHs/5uNFr/WTH85g6eC6+P+jAZb1jgt/QSHlWr99gBdDy21Le2m1TXKdbmhCccTmgO5ANw9a1MH3zHgDAM2oiXDf6z3IJRL9zo2a7lNEFjrufgpyY0uilEhFFEgYsiIiIKCLVHEn5v/nw/1R9/morhEoWdPCMO6tJczS7+ERI8UMVTXJMrN/h3RzF2G/xbfcod0l4P9uOVI2jLyd9V4LyKzOhO7KFpsotIVk1KnQjqn0FOX/Pd7VowOLbA4ELrn6e48DZWZYWebZLlFUFXxtmqYTK8PcvML/ysLp9zS9wXXYrEB9i3QuPG4Y1v0KfvQ2GNb+qut2Tzof78tsatUYiokjFoptEREQUkX4+7AIAxHvVp4CkJmsUIQyD7b/fA1Et87LbnI4es6plw7p70cNRpGgrcfqCPGbRrWj/cG/d9pdqd9MyExK9dkCW0S2+Zb/3yglSJ+PzXAfWFrsDjmmsnRo1M+I0jjANylqtGawAAEGWoav0n0FSny4vBzE3ToN50WMw/vINBFu1ot916U0MVhDRcYkBCyIiIopIN66sAACkedTZFHEaQQwtJkn94ukZOrZNBCsAAGbtGgYAEC868cD+LxVtEyq2Y9dfd6Bq5dV4Yc/i2vabV1ViTbEvALTf2rSCmQZIiBWdmid3NCezIXiA4IzvS+BugVoW9Y/DrV2PPvyAhX7X5oD9Qk0ImUKSCPPLD0Nw+884EQcMD3dpRERtAgMWREREFHFqPBJsXhkdXRVI8qqLY8YX5CBaDLxlAAASNO6NpNoVwciWwFsuZhWtrP1ZL4n4YMdL6OUsgh4ybsxfgWE1ubX9z2+1QpZlXPC/siav67r8n/DKdhtkueXqSBTaQysK+kVuaMGrcPyS71K1BYzPuF0wLv0Yxq/egVBdUdusK/B/dCwACNbgAQvDml+hy/c/j6zXQ+rI40uJ6PjEgAURERFFlFWFLnR5vwAA8M3Wp/yOuyXvx6BzJWhkYkjd+zZ+ca0tKvhxmQbJlzEx0J6HdI8yM2Dt+gcwqXwrAOD7g07sqWqe40ivzf8ZAJptvoYkWcZvBeqggZYvclvntBev5D84E/XWU4j65DVEfbkY5ucfAI4EcnSFeQHnDJph4bDD/OojAYeIfQYDBnWxVSKi4wEDFkRERBQxShwipv7g29cvyBKGWf1/s/xo7qfQyYG3JTTMsBCzejV9ka1JF/xPtSyXL2PixHrZFPUt2/I4Liz+CwBg9ciwiNqBAO/A0LcV9HL6amcUOVpmW0hOtRfFIc7tCi0Ro8ksAbaoGP76ufZn/d7tiLnmDOjyciBUBK5RIZQWBuw3Lvs06Lo8Uy4OOoaIqK1iwIKIiIgixq/1UvFPr9gedPyFxX8DsozLClfi6ez3VC/t0VKDl3NzG6ldEYZuzhIAwHA/AQsAtbUu9ALQxaW9JUTsM1jVJpstkAX1i/r62G4AgBp3ywQsdlWGnrnhCZD50Bj+trnM9HciiiRCaBA4E0Qvou+/Gobt6xTt7nMuUVzrDmQHXItp+RfKtRmMcNz8UO2188o7IA4ZFXAOIqK2jMeaEhERUcTYVVlXJPOZve8GHf/Wrv/CIrnx5u5FAHy1FXqOfg5FUb6jIi0Nim7KxqhmXG3r8A4coXrxre+h3M+Q4qnBiJocv2NOsOdBDxlOUcZdB79T9csGI+SMzqp2wemA66JrEbXkdUX70a02q4vcOKdr8weB9gU5IaS+qmYOmmhN98LJiegUo9e+wRl6DQ3vkDEwff9R7bVhxwagphKIUx9tKpSXqE8DueouiCMnwPrOryE/k4ioLWOGBREREUWMp7dYa392C8G/VzHLntpgxdHrmw7/z3chy3hhz9vKG4ym5lhmq3Jf9C9IHdL99o+tzsZHO17CyAABCwCI89ixJMehmYnhOecSSMmpqnY5LgGeqTNhe1F5GkmS1wYAIdeZCFe+LfR9HlVNPKa1IVuD6poJJgGz+vgvfio4QquhIUfHQuo9EFJCsqLd+NPXmuMN61eq2sS+6iwYIqLjGQMWREREFBH219T7Vl2W0ctRpOh33PM0ZFPwIpRnVGxFH3s+TqvcUVtroZah7SWXSt37wf70J5BS/ActQtHHUQDbb//DENtBRbv79PPgnnE1YIlV3eM5+UwAgBwdp2hP8tqQ4LGFfJJHuHY3LOYpyzilchemlayDWXQrupo7w8LmVQZAYg1B/lx2hhaw8I4YD+j0EPsNVbT7OwFEd1gdWJITU0J6FhHR8aLt/a82ERERHZd+r/dtfZqnGrEN6k+IA06EbLFAcAc+znRkTQ52rLlbu9Pj1m5vA1yX3ADzoseD/v7+/LnhQc1290XXAvDVq1D1nXel7weDAVJm19qXax1kTKjcga+NI/HC1hqkR+txUQ8LdBr1LsJh80h4c5dNUcsEAF7e8xauK/AVttwY2xUnDV8AWfAFEqweGZIsN/nZALAiz4mnt9Qo2qKNgecVHLaQ5nZPvRQA4B07Cca/6xXpPLgXAKDL3Q3z03Ohq6mEbDJD6tJdcb8s6NpkhhARUVMww4KIiIgiQv1tAL0cytMTxKyegCDAfdHspj1EbKUjJVqAOHICbP9dCuc1c4OOdZqiIcbEhzaxORqA79t7+cjPAHwZHZa664aZAfNzlwAA5q2rxnW/V+DxTcoX/XD9XuBCp/cLMG+dr26DXhJx3eEVyP7r9tpgBQAMsx7A6Oq9tdcyAIe36dtCvtnvwIXLy7C6SBnUig5wOggQ2pYQ7/BxkNN9NUKk1ExFn67gIAy/fgfLgpuhq6n0zel2Qr9vp2Kc8/YFQZ9DRHS8YcCCiIiIIkKZsy61v4ejWNEnp3UCAHhPOq1pDxFDL+YYkXQ6zUyIhrwmC6RhY0Kb82hmgtEE9wXXQNbpIEeZ4brkBsUwKTlNcT3QfhjRYl22x5NNDFg8uUlZYPLWw8vwcvZidD9yCkp9XZzKk04abuNojHnrqjTbgwUs4FRmWNQP+gCAlJwK5y3z6/o1tnWYF/8HgtejalfM06l7wH4iouMRt4QQERFRRCitF7BoWL9COhKwQJQZjtseheX5+xv1DKGtByyAkLYFGHUyZI2aFA0tzjoDF9W79ky+AJ5xZwEQFNkVADTrf/R0FGNrbFbQ54RiVaEys+GpfR/6HZvuUQYXbB4ZaMJhJbIsY3+NdvZNlD68DAvviHFwXX4bDOtXQUpJh9RviPKGGGU9kJDWFx0DuUNG2PcREbV1DFgQERFRRChxiojz2nF54So8cEB5KoWU3qnu58yujX9IG94SUstgDD4kygwx2v/JFke92uUsRcACAGDRvk/q1kfVdvS0kKManrDRWFkaWRX1dRUbBCyamGFRYPe/7ii9AKHwEEzLlkAoyoPUrS88k86HnOLLOGlYw0K2xADmaHhPnux3Tvc5lyiONw1G7N6vLhOGiKgdYcCCiIiIIkKZU8I7O1/FtLINqj65XsBC1jh+M2THQYaFHEKGheeia6Hbuz3gmN8T+mGnuWPIzxV7D1K1nVeyFmbJg+VJJ0AWdChxNk/AYsm25wL295SV20+aGijZW+3/czG0dCdi5j5Q17BjA0xLP4Jt4TuQM7sCDWtYNNgSokXs0T+s9UlhjiciOl6whgURERFFBLmqQjNYATTIqjBFQWrk8Y5ydPBtEhEvhICF98STAdl/1kGV3oIr+t+AfklhfHdlMPiO5qzn1sM/YumWJ/DezlcAAMWOxmWweCXlWodb9wccn+pu3gyLfQ2PUT3irLJNePSHBzT7Yu69ArpDOYj6crGiXfaToVJfuFlCYve+YY0nIjpeMGBBREREx9y+Ki9SKvI1+6T0zpATkhVtclKHRj3HfcE1jbovogQJWNguuRkwmuAdc7pm/7cpJyLllNdxyNwBC0YmhPXo+ltz6ru4eDUyXeWodDUucGD11LsvQKDlqA4uZcBCcX8j5NZoBywW5HwS8L7oB65WtckNa39okDO6+O3zDlUXS214QgsRUXvBgAUREREdc9f8Vo45eUs1+9xTZ6raQvkWuyHXP66D1HNA2PdFGtkYuIaFPHkGAN82AvfkC+AwWrA2rgcGjXwCA0c+hfWXz8M/e0Xj9fFJODkjKrxnx/oPcAywHUalu3FbM2rqbelI9lqDju9Vtk9x3dQMi1KNrSzprkoMtR0Mf7JQPps6HdzTr9Dskrr1ge2xtyElpkDq2AWOWx5pVKFOIqLjAWtYEBER0TGXX1CK80rXqdodtz8GUeMbZ4S5tePm3lfi8XMuaezyIkuADAux7xBAd+T7KEGAe+YtKL/gRryxsQZ9nCLmDIrD0A7Bt5T4I8f5D1h0dZagwiUBwWuCqlS7fQEHk+TBa7vfVPVL8UnQVVco2m7JW4YXO58FoOk1LLQCLaOq9zZqrlC3HbnPvxJCST6Mfy5XdlirIXfqBvvznzfq+URExxNmWBAREdEx5fTKOL9krardMecxiMPGap6OIMfEh/WMThnJwQe1FX4CFrLRBNclN6jak816PD0mEe+cltKkYAUQOGCR5SprdIbFzkoPLKILu/6+EzNKlZ8Fse8Q2F/8ElKDbUGzCn+v/blhhkWlS8LaYjesIQYyVha4FNc6WcKjuertIK6LroUcZQ44lxyfFNIzIQhwXXarqllivQoioloMWBAREVGLOWT1otQposguYsGGatz6RwW2lnsUY8pdEoZYD6juFfsO8TuvZ+I0xbWUlhlwHTP6N65IZySS/Rxral/wJqTu/Vr22QFOwOjkKkepo3EBi/XFLqzY9CiyXGWqvqOBCnHoWEX7MOsBmCTfZ6l+DYt9VV4M/7wIZ3xfgjO+K8FfRS7M/asSr263qop7AkCFS0JNgxoY83OXoL9dWVPF9c/r4Zk6E+LAEQF/l5ADFgAQEwfnNXPr7o1LUBU2JSJqz7glhIiIiFrE4xur8fimGlX7u3vsMOmA3Es7IsaoQ5lTxOyCnxVj3FNnBqwFIHXrA8/4s2H8fSmkhGS4rrgD5qfvgSBpvzB3yeqI5jlwMwJoZFhIHTICFnJsLnJymt++Kwt/xzNFN2F2GPVQZVnGc1utWL1mB16s2ac96EgRS9cVt8P423eKrplFf2Bxxwmweev+7X6w14Yyl+96Z6UXZy0tre1betCBb6coj8VdkedUPfLfB79RtUmpviNg3TOuhmHDKv+/U4AsFC3e8VPgSEmDLi8H3pGnhnQsKhFRe8EMCyIiImp2pU5RM1hxlFsCnjzSb1r/u6rfO+q0oM9wXXMPrC99Bft/PoJ4wgi4L5oN2WSu/b+jxE7dIHXu3ojfIkKFcKxpS5HTMuEdcKLffuHwfnjDiAytLnLj4fXV6Gc/7P+ZR7eC6A3wjD9b0TejZA0AwF4vQ+LPQrffuVYWurGuRNm/u8GRpoKs/QsczfiRuvQInAVhCq+QKQCIA4fDc+ZFAQNCRETtETMsiIiIqFkV2kW8sM1/sOKo57dZUWAXMfO3XxTtXkHv9/hMlbjE2h89Z18Mz5kXAjIAnQ769b9DV1kOz9gzNOtgtFlav4tLnSXQUpx3PA7D+pWQoyywPHefoq+rowQFrhT0D3Guj/fZAQB97IV+x4j1AiSeyRfC+HvdaTITK7YjzmuHzWupbTME+Truha01eHdi3RahfJuo6D+nbJP2jfF1nzXndffDMHQMzG88EfhhRETUJAxYEBERUbPJ/nMN1n33P5Qk9APSNE73aODTHAduqS5WtG3pOgJ9oix+7ghCX/enjThyAsQAQ48nQisGLGA0wTv6dAC+rSi60rpgQ5q7CnnO0IJDNR4J7+6xI8brRC+HOmAhdciAd8wkiP2G1rV17g4pLRO6Yl99iSjZi4G2w7B6fFkYH2bbsCpAhgUA/K/BFpDcGmWGxcvSatU9rhlXKxtMUfCOmwLp2w+gK8oL+DwiImo8BiyIiIioWejycjFo0b8xTJZw7eEVcOhM+K7DcECWMaV8M9LcVViSNgp2vfKUhc4NCi2uGH8l+rTmwo8HHlfwMS3AO/JUmH6oO00j3V2FCk9oAYvPcxyYl/s5HjjwJXRQFr103LYA4omnqG8SBF+QpLiuIGaM6ILVI8EtyrhvTVXQ5zpF38k0ZoNvnQcaBCwSnNWqe7wTpmpP5mf7CBERNQ/WsCAiIqJmUfnU/8FQ7wVuVuFKAMDdh77Dt1ufwpu7F2H5pscUNQISPTZkuioU8+hTuI8/GO+wkxXXmi/3raDhiRhp7mrUeEMLWLywcj/u1whWAICcGKByZ4MaHmbJDZtXRr5dRKVbPZeWS38qg0eSYfNIyLfXfR51AhBdqqynYX/wtbo6Gg2IvQcprqXUwKfVEBFReBiwICIioibZVu7BtU98hsxKZWr8jNK1yP/jBizM+bi2bVTNPpxctaf2+szyzdDXe2HNtqQjIbaR20HaEe+YSbU/y+ZouC6+4Ziso2HAIt1ThWovIBTmQTi8P+C9Qyr3Kv7dK+ZN1A4QAFAFLKIkDzaUemD3hhasAICf811IfScfKw4rM1O6x+kh2JQZFlKXHn7ncU+7DHK9miKuy28NeQ1ERBTcMQ1Y/PHHH7j44ovRv39/JCYm4oMPPlD0y7KMhQsXol+/fsjIyMA555yDnTt3KsZUVlZi9uzZyMrKQlZWFmbPno3KysrW/DWIiIjaLUmWcf93O/HRjpc0+9M86vT6SRXb0N+Wh3m5n2N2vvI40/8lDcaAJO5YDcY78lQ4b5wH9+QL4bj7Kchpx+abfTmhQcDCXYUx21cgZu5liLnvSpg+e8PvvT1F7e0bUnonyEmpmn0AIKsyLDwAgLd22UJddq0rfilXXI9M1kMQ6yqfyIIAGIz+15LRBY57n/f9e7j5YYhDRoe9BiIi8u+YBixsNhsGDBiAxx9/HBaL+tuU559/Hi+//DKeeOIJ/Pzzz0hNTcX555+Pmpq6yuP/+te/sGXLFixZsgSfffYZtmzZguuuu641fw0ioqbxuKHbsxVCRemxXglR2HYX2/DTT+F9q/zAgS+xde1czDvwBU6tUn4RccicgkHJ/l8Q6QidDt5RE+GeeTOkXgOP2TLUW0KqcMmGuowa07fvAy6H5r3jarI1270jJwQ+1cVPwOKNegGLoTX7MfvwCgyvzgm0fJURiQ0ajKagJ8xIfQfDPfNmiCNPDetZREQU3DH9CmPy5MmYPHkyAODGG29U9MmyjFdffRW33347pk+fDgB49dVX0bt3b3z22We46qqrsHv3bqxYsQLLli3DqFGjAADPPvsspkyZguzsbPTu3bt1fyEiojAJ5cWwPHprbZV9qWMX2BcsBgz8hpnaBvnbj5p1vk4dUyAcT0eQHuca1nYYaD+sGqPLPwipe19Fm1BZhrPy/tCcUxwwLPAz/QQsjhpTtQe/bpxfu93kmr6zsSRtFBw6E2TB/3d1/W15uGjVCmWjMSrgWoiIqGVFbA2LAwcOoKioCBMnTqxts1gsGDt2LP7++28AwJo1axAbG1sbrACA0aNHIyYmpnYMEVEkM326SHEkoK7gEIy/fHMMV0Tkn0eS8cNBB77e74BX8r0MZm1f2azPOHcwixa2JXJsfNAxUR+qtwvpDmRDr3HChnfEeIgDhgeeMEjAYlbh74raGG/uXoSKlf9C9l9z0N+mfQTpiTW5WL/uPnRat1zR3jA4QkRErStiv8IrKioCAKSmKvcwpqamoqCgAABQXFyMlBTlNzGCIKBDhw4oLlae6V5fdrZ2CmIka4trJgpXu/ucyzIGbfxT1ez8+zfkdBukcQMdD9rq59xQUwnpk3cwpvwwnutyNj4bPhnzetpxQoX6G/WmcLrsbfafUXs1RKeHThL99uv3bFX9O03K2YtuDcb9cPH/IaNnF2Dv3oDP61hjRUa9a4vkVvQP1AhK6CGjm6sUT+z7CNMG363qf2rvBzDJ6t/BIwj8PJJf/GxQe9DSn/NguyIiNmBxVMO0UFmWVQGKhhqOaaitbRXh9hZqD9rj51yoKIXBaVe1G8pL2t0/i/aiLX/O3W8+g+TCbQCAJ/d9iLGJ/aDvmQmjxkue6t70LjAVHQrpOV1OGAKZx5q2LZYYwKYurlrf0c+9UJwPXV4udCa9ov+NjhMwbMx49E4MXr/EuCNDcd0ww6KPvcDvvWeXb0KWScRBd93zEz02VS2VowzRMW32/2epZbXl/54ThSoSPucRuyUkPT0dAFSZEqWlpbVZF2lpaSgtLYUs16X9ybKMsrIyVWYGEVGk0eXlarZHlRcCAb6tJDoWkn+v26qkg4wX9yzG/G+2Ksb8ntBP8159QsNKhv7JcQmNWyAdO+bAx9B64311LnT7diD6vithef5+RH36X8WYGr0FFkOItUtUW0LqMixS3DXo4LUGvP2f7j2K69Mqt/sfbGINCyKiYyliAxZdu3ZFeno6fvnll9o2p9OJ1atX19asOOmkk2C1WrFmzZraMWvWrIHNZlPUtSAiijSlOfth+Y86LRkADLIE24/ftvKKiAJwqI+LHFmTg0kVyoDF4Q7dIHXIUI2VkjqE9BjZHM0XxLYoSJFgQ3U5lv26EaYv34bgcWuOqTFY0MEc4p+lDQIWUfUyLPrZ84PevvDXBRhas7/2en7uEr9jBaf2CSdERNQ6jmnAwmq1YsuWLdiyZQskSUJeXh62bNmCQ4cOQRAE3HDDDXjuuefwzTffYMeOHbjxxhsRExODCy+8EADQt29fTJo0CXPmzMHatWuxZs0azJkzB2eeeeYxT10hIlJx2mFY/gX2fv4Zuj18ZcCh6R8/B9nlbJ11EfkhVFdAv/ZXRM+9XLP/+vyfFNepAwfCfcE1qnHe06ZBjo6pvRazesH5r7nqCSV1EUaKfEJNVdAxFy6eA8PWNX77kxLjEG0I7c9SOUqZ0REv1gUV+oYQsACAfxX8DADo7CxD/wD3CMXNW6OFiIjCc0xrWGzcuBHnnntu7fXChQuxcOFCXHLJJXj11Vdx2223weFw4O6770ZlZSWGDx+OL774AnFxcbX3vP7665g7dy5mzJgBAJgyZQqefPLJVv9diIiCMT//AAw7NmBoiONXLPkOZ1x2YYuuicifksISdHz4GpjtgWsTHFVpiMaoqWfAa4qCw2iE5aWHIMfGw33elRD7DYVr5q2I+uBFyOZouC6/FVKfwcAbTygn8fPtO0U2wR54C0YoTu2RFPJYOTZOcZ3kqcsA6q9xrKqW6/N/gj29K6bsXhZwnCByex4R0bF0TAMW48aNQ2Vlpd9+QRBw77334t577/U7JikpCYsWLWqJ5RERNRv9upUw7NgQ1j3nL38JzyR2w7XnDA9YSJiouWVXulHzxEPoHmKwAgAODZ6A7lFmAIA4cgKs7/yq6Peecia8p5ypaJMSkqCrqqi77tSt0WumY8czcgKMa39t0hw9e3WpdxBpYHKM8ijVfnJdhkfDgIX73MvgOeVMxGhkCd2x8a2gz/JMnB7iqoiIqCVEbA0LIqLjhS4vB5YX/69R947+8b9YdohbQ6h1lbz/BsaXByhEqEHfU7vgZiDuf1ynvJ4+K+w56NgTR45v0v1lpnjI/UPNPQMQE6u47FmRi+WG3wAAg60HFX3e0RMhZ3SB7ZlPQpraedVd8A46CYAvoOY5nQELIqJjKeKPNSUiauui77+60feOrc5Gx1UVyL40cBV+osbQHdwLXe5uiINGQk4+cpSoLGPkxu/Cnivl5PBfWr2jT4dn7w7ot6+Dd8hoiMNPCXsOOva8J50GR5QZugN78R/TUNSs+ROP53wc8v3PX/EK7tHpgw88omGGBQBM+O1tVN/QBdG/1mXuygYjpIws38/xwbecuCdfCO+EqfCeeg6E0kLICcksAktEdIwxYEFE1JLkUJOc/XNbm74/nKgh/Y4NMD95JwRZhpSYAseDr0JOToNQUYokrz2sufal9ER6UuhHl9YyGOG68o7w76PIIggQh46FOHQsZjpFzHQmAmEELK4e0TGsx8kxcao2weNG9AvKTDY5PqnuBBOjCd4TT4Fhwyq/83pOP+/IZALk1PDWRERELYNbQoiIWpIztBe/d9LHYW1cD82+THeFZjtRU5i+egfCkYCarrIMxqW+F8yoNwMXrv5f0iAcikpWtGWcx+Kw5NPBrMdLJ7hCHu85+Ux0MIeeXQEg5KwHqWd/xbXr8tv8jnWdfxXkjM7hrYOIiFocAxZERC1IqNYuLHxbr1nYHt0JAFCjN+Olzmfi5BMfhjMuWTV269q5MD1yM3QHslt0rdSOuF3Q796saDJs/BOwVsGwbW3AWz/vPB6ThtyPXHMqREEH9+QLIZ8yuSVXS8ep8uEOUrYAACAASURBVNNmwHXpTY2619sveM2L2oyJI+TkVNjnv64aJ3brA895VzRqHURE1LK4JYSIqAUJNeqARbfRLyDPnIK3Ok7AUOsB7LOko9iUgLuGxKH87FeROfefqntMe7dB//rjcDzyBsATQ6iJypZ+jdgGbbrSQlgevjHovU/fcQG2VorYOeN9pCQDuiju8Sc179AxMGxardl32JSEk4YvwLqZ/WEyNu67M8/Zl8Cwa1PAMWL/Yao2OUEdFBaPFNkkIqLIwwwLIqIWJFSWKa63R3dCnjkFAPDDeZ2xJ60fik0JmJplxtyhcYjPSMeX/c/VnEt/aB+EorwWXzMd5yQRXb98RbNLX3xY1WbX1wUknP+aC+h0GJRsxPiOUQxWkF/uC6+F2Kkb5LgEOK+6C2Vv/oxpg+7Cbb1mYdTwR+CIS0KsofHBV3HIKNjvf8Fvv3PWHM12rYCFzMKaREQRixkWRHT8cjmg3/QX3LGJ0PUfAkHX+jFaXWmR4nplYt3Rj0M7mLDvUnVht0mDOgE7tecr27EDyRldmnWN1L7ocveEPHbaoLvw7j8GwPDXCkhZPeEdNbEFV0bHE6lLDzgee7v2OgrASZNPxUPrqwEADw6Kg9DEbDGpz2A45jwGy7P3qfs6d9e+SRAgmy0QnI66sd3DP5KXiIhaBwMWRHR88nphfupuGLK3wQJg/oBZGH3FTIzNaPlv0ipdEq5bWYEfDznxS+EejKvXdzCqAwCgX6L///wak1P99iW//yycp0yC2RRmkToiAEJJAaLn3xDy+K6DB8KQ1R3urNktuCpqL24fHIdzu1ogCECP+Ob5E1QcOhae8WfD+PtSZYclxu89rotvhPntpwEAUscuEAee2CxrISKi5seABREdl0yfvAZD9rba65k5yzD1z3Ow5vy0Jn+rp+W1HVa8n23HSakmLDvkQL5dQmdnGcbtWq4Yt9/sC0Y8elKC37mkTt389sWKTqxetRZjJo5ulnVT+2L86auQxx6KSsbUwZktuBpqj3omNP+fnmK3PqqAhRzTsEpLHe9p58KR2hFCSQG8J00AdAwAExFFKgYsiOj447DB9L/PFE09ncWoLi5Fni0FXWKb9z99q4tc+PffVQCAbeWe2vZ5+z9XjV0f50tTHhcg00Pqon286VEDlr0BMGBBjWD64RNV2+aYLAyxHVS1r04firNbISOJqKnkJHVWmhyXGPAe8YQRLbUcIiJqRiy6SUTHHcPmvzTbr8v/CWuL3c3+vClLSxXXSR4rPtv2LK4u/E3RvseSgX2WdHw+OQUmfYAsD0HAV1f+x293bE2Z3z4if4SSAs329ybfAcOED7Amrmdtm0swYNxVM1traURNIg4YBjmuLmvNffYlAAtpEhEdF5hhQUTHF0lE1FtPaXZdVrQSTxfPwowe0c32uCK7qGq749BSnFe6TtV+Vb/rsfPijugYHTz92NO9H/aZ09DTWazqMzmtUD+VKDD99vWqtm0J3XH/uYORssOG97s/AjFnGVIkB9LPOAtxXXtqzEIUgczRsD/0XxjW/gapUzceU0pEdBxhwIKIjiumTxdBcDk1+3o4S7CrzIlnttRgU6kbM7pH47zulkY9Ryg4iLxPP8I3JXpMi++NWw7/iApDDO7uORMTK7apxm+KycIN00eFFKwAgGiTAROHPoBHcz/BZUV/KPoskgfZVg86xhobtXZqn3QaGRb3TF6AT/U63DIoDkAcMOHq1l8YUTOQO2TAM+Wfx3oZRETUzBiwIKLjh8OuuUe/vlt/egpWvRm/Zp6Gqw70R1ZsKk5MNYX3HEmE7vl56FuwH3c36JpRulbzlrzTLw4rOHJSWhRq4jvg5t5XqQIWALDkf+tw64wxoa+Z2r2a/Hyk1Lu+t8fFOLW7/+KvRERERMcaa1gQ0XFBKC+B5bFbgo6bXrYeM4v/wI+bF2J49T58sNce9rN0ebmwFOwPebx93iuYcMHZYT3DYhDw3mkpiIvT3r5SvH1HWPNR+/bLIRtSNvyiaFsT1xODk5mlQ0RERJGLAQsiOi6YX1sA/cF9IY83yiLe2/kyVhe6wn6WZ9/ukMc6B4yA1HNA2M8AgFMzo7DzYu1jJU+s2Y9yJytZkB+yDN2eLdBvXQN4PUh8dq6iW4KAjXHd0DWOxzkSERFR5OKWECJq84TCQ9Dv3hz2fb0dRdhV4YZblAOf2tFA6c6dCDWRXhjS9OJvzllzYH73WUXb1LINWF3uwamZfOEkNcPX78L85WIAQH5SF4yrOKTo/zOhD2zGaGSGWFOFiIiI6FhghgURtXn6vY3fHpHqrsbCjdWKNrtXwlu7bHhntw07Kzx4aVuN4jhU84E9Ic/vmXR+o9d2lPf06ah58L+KtmSvDW989pufO6jd8npg/P7D2mAFAGQ2CFYAwG29ZiHVrINBF3qgjoiIiKi1McOCiNq8xmRXHJXiteLL/Q48OKIuZ2L2bxX47qD6pJGusXp0iwa+L84Nae6NJ52P3obmqREg9OgLa0IaYqvqjjm9Kedb/FU0AaPTo/D9AQdeWrYFfcr2YXnyINxzWg9c0ScagsAX0vbE9NU7MH37fsAxr2Wejs1x3TAugX8CEBERUWTjXytE1KYJJQUw/r5Us29dbHdESy4MsOf7vX9YTS4+iu0MryTDoBNQ4hBVwYp4rx0v7VmMUyt3Yk90R5glj6LfHp2AaHuVau6uWWmN+I38i7JEAfUeY5Y8mLC0FMWzMvHV939g5d8LavtGuhegW9wITMg0N+saKLLpt6wJOuaH5KEAgMEpYZ6OQ0RERNTKuCWEiNo0w98/a7aPOXE+Ro9YgDXxvQLe/86u12D0ulFgF1HpktD740JFf6LHhuez38GlxX+ik7sCp1Uqt5+sSDoBX9/1AaSOWaq5LZ3UbU3hvexWxfUpVbuR4arAp7sq8VG9YAUArF3/AL7f72jW51Pk04ewXWlzbFcAwA0DYlp6OURERERNwgwLImrT9FvXqtrOGHIf1sb3BOA7DSGYKeWbMX99ArrGGtDBXY1n976L7s4SpHhq0NtRFPDeDbHdMTjGCNmifvmTMjqH+FuERmu+uw99h/LV2qeQ1Bw8AIxNatY1UGSTLTEQHDa//c4+Q7HuXycgyiBwuxARERFFPAYsiKhNk5PV2y7+jO9d+3OFMTboHCdX7cbdOSMBAC/nLsElxatDfv6u6I6YEKUDDOr/nMqpHUOeJxRybLyq7ba8ZVhgVrcDwGBvsWY7HackURWssOqiECv5ju7d3W04Ot90H8xGJlcSERFR28CABRG1aYLdqri+ufeVcOnr9ua/1nky7shbCkGW/c4xJ+8HfN1hBCoN0biuQHuLiT8FUUnonWAAXOoinTA2c40Ac7Rmcwd7uWZ7YkVB8z6fQuN2wbBqGWAwwnvyZEDfSv9Ta1cGK6r0FowZPh9Xe3YiecgwXHDqCZB5KggRERG1IQxYEFGbJthrFNc7ozsprhM7dYTrmntgfuOJgPP8uumRRj3/gmGdYdQJEPsNhf5Adm27d8joRs0XkCBA7N4P+txdtU3VejMuKfpDc7jRUaPZTs1HKDqMmHtm1l7b578O05dvw7DR9+/Eu/kvOG9+GGiF7RdCdYXiusIQA096F1x/4cgWfzYRERFRS2BeKBG1bTblS3mFUVlL4unRifCOmwLv0LEt8viLR3QBAHgmToOU4KsXIZstcJ9zaYs8z3n7o4rreNGJBNFPcU2Xq0XWQEe4XYpgBQBEz7u2NlgBAIZ1v0O3b0fDO1uEYds6xfV+SyrSLfpWeTYRERFRS2CGBRG1aUJ1peK61BinuB6cYgQAeMecDsOmP5v12WJWLyAuAfh/9u4zOqpy7cP4NT2dBEhCKAEhoRp6RwFBRZQiKqBib0cUrCggFqw0RSmWYwMP4lFAzysWxAIqShUEkRqkSE0IkITUyczs90M0OMwkJCGQwv+3FmtlP23fAztD5s5TAKNWPbJe+gjz/t14ImMgOPQUvUvHCK9xyo0V/5ablU1qrodwh3LTZ4Jl24ZitQt69l4y3v4a0/E0HCkHoVFDMJd9IsF0xHuD2O/DmxMZqH97ERERqbyUsBCRysvtwpSR5lWUbDuxAeX1cUFY/1qz7+/Y0eI6VjuOzHEzsTjshO/YgO2reRgRNXH2v9F7qr/NjqdB41Lfp7iM4JBiJSwC3bn8fiyPC2o5znhM5yLzoX3Fbhsw/Qksm9fR3JUHb4Cndn1ybxiJu0X7MovH5PSeUXPEFkItzbAQERGRSkwJCxGptEzH07w20zxiDcFltnJNw0BiQyzcd/6JWQ6eOufhCa+BOfVI/nX1SFzte2D/esEp7xNww3BsIQEAuJu1wd2sTRm/kpIxgkKBoo9bBQjyODmQ6T5R4MzN/+PntBEpOVPS/mK3tf62yuvafGAPgZNHkTFrCZjLaBbESQmLbLOdblFlvPGriIiIyFmkuaIiUjkZBua9O72Kkuz5H8Tf7B7Bk+2qeS+FsFrJvf1RPLXr427QmJz7n8c5bAR5nXv7HT45sDqemFiynnwNd4t2Z+xllIYRdOqjWgG6pG3nYFZ+wsK8dQPBD1xD8IiB2Of9+0yGd84wJxV/hkVh7B+9UfC1KWkflnU/Q24he5L8LTsL/J16k+f0uswx22hR3XbaMYqIiIiUF82wEJFKx7xvJwHTHsecfMCrfJ+jBiFWE+ZCTmRwt+xEVstO3mUt2mFb+Z1P2/kPzuHmJsE+5RVCMRMWdZ3HOHw0AwjFvvA/mP7aoNT25Yfk9eiHEV2n6AGkaCVYElIY+1fzcF47nIDJD2PdvA4AT2QMWc/PAkeAd2PDwPHWRKzLv8EICyfpwitZbo4mq1VXBjYKwZTnO8MixKZjTEVERKTy0gwLEalcPG6/yQqAX0IbEmov2Qc0z3lNfcq2h9ZlcKPAUod4pp28b0dRXnn3OnDmYt209kR/w+DX//v8TIR27nC7MKcc8ipq0Hl6qYaybF5XkKwAMB8+iPWXH33bbVyN7efFmAwP5rSjxHz+LlcvnMDB2W8yakUaOE+eYWEn1Kb/5kVERKTy0k8yIlKpmP/Y4jdZAfBDeHMahZVs4pgnKsan7LzIUIKsFfft0ZR2rETtv3jsKZ+y7TsPsC01r6xCqpJScz3M2prJ+4mZpDs9XnWmo4cxe07sD3LIVo2I2rXYEl+y43NzTVYsn87xKT++bQsA5h2bCHzqLoLuG0TgS6P9jnHfvq/4fHMynDTDIsds0wwLERERqdS0JEREKhXz3j/8lmeYHSyJaM6ImiXcZNDhO5PC7Hb7aVhxuBvEl2j/hKGHV/qUBbtzWZ3spEm49jjwx3MkmeUz3qLtkT08ed5gRkQ0p2u0nfhqViZ2Cifk8EGv9rsDI+lTN4DYriPJfj2d3EMHCc88csr7pFqDiN623qf8yPp1RL8+gYiVi085RoCRxzUHf8a64zevcpfNjs2shIWIiIhUXkpYiEilYt63y2/5jsBoDJOZmKDTP8bR5DzFpoflLO/Sa7CtWupTbgQEYcrJKtYYtZxpLE91lXVoVUbO21MZsis/0fPdhudZWKMtk9IGkJ2Twu/zltAx+0+v9jsDoji/ug0jMgb3kzOxGgaZxw4T/OCQIu8TnZfut7xp2h5YuafY8U7f8Z5PWTo6IUREREQqt4o751lExA9LIQmLPwKjAajuKPnbmicy5qTr2iUP7CzyxLUg5x7fZR6mnCyyH3ihWGP0SNvCse++Itvl57SJc11uNlGbvWelDDiyjp9/Hc8HW2ZyUepmgnMzvOr/DIykZ23HiQKTCaN6FBmzviM3tPrZiNqH1aGEhYiIiFRuSliISOWRnYl59za/Ve/XugCAGgElf1vL63GF9/VFA0oe21nm6nQRrvbdvco8NaNxt+pEVsx5xRrjP1tfZ+nyjWcivEot972ZJe5jia7tfYzu38wW3A9PYH90HL+H1OP2tg+XQYTwv5rtT9mmRWRQmdxLREREpLxoSYiIVBq2ZV9hys3xKnu19iV8GtmeJRHnAxBRihkWeVdcBzYH5v27cHXogfuko08rKtf5HbxOk8i9djiYLRiPTIKHil6K8Lea/3sLU/NxGBE1waL/EnC7qPHzFyXuFtsottA6z3lNqDb5bUINg+HbE2FdoU2LZXtgLcx3j8U14Vqsbv8bp6ZZArmjS73Tu5GIiIhIOdNPpyJSKZjSjuKYO8Or7KV6VzC60fVeZfWCS7GHhdlC3mWDTye8cuG6oA/OI0mYd27B1bk37g49ATBqROGpFoG5GKeJ9Dq6ER6+FneTVmSPmgx2xyn7VGWmYyml6teyU6tTtjGbTFjMJtxWOxaX85Tt/9a35Wje3fpvAjxORsbfQmyfy3gsPozMS4dQbdFcv33stz1A61ohxb6HiIiISEWkhIWIVHjmresJmvCAT/nS8OY+ZdFlsOlmpWGz47zmDr9VpvTUEg1l2baB5NWriLqg+6kbV3CmnVs58tMPZMclUKfriWNGTUeTMQKCIKjwD/KmI8klvt/HcX3oE1H85Rcem/+ExSFbNfL+NZZ6M8cUlL3R6U5ev+1StqT2JD7CwUtmCLblzyKyDL2DrdGNaDr7Ga9xsh+cgLt1lxK/DhEREZGKRgkLEanwHHOm+y3/7q9lIH/7+NIaZyOcSsGIicV0oPinTABsX/gZNbtdiNlUeY/CPLDzT2o/ez+xnlz47r/88nlbmj72FI4F72BbuhAjMJic+5/D3ayN3/6mg3/6LS/M4cDqxI0o4b4UNjv4OYgmsUYj2nToTO5Vt2Fd/g0ZcS254ZbBYLNQs5bv8buYTNS9qBeZrRKwTx4FwaG4+l2vZIWIiIhUGUpYiEjFlpWBZd9On+JpdS4jz+z9FhYXpre0vzkH3EjAG8+VqM8OawQ101w0DbedoajOvN3vz6axJ7fguv3+dXDvwIJrU3YmtoVzChIW61OcpDo9rE52UtNh5uov/o86/xhvUmx/Lr3/blp89Q72bz7BCA4jZ+TTkJGOKf0YgZ160SikZKdxmApZdrPo/P60AfIG3kTewJso7lwho3okuRN9jzUVERERqez0072IVGjmvb7JCoD/i/Q+JaFeiIV6IefQcpBTcHXoiXPvTiyb1+Fu1QlT6lEs29bjatcd++f+9z04YI/A5TnLgZal9FQu/WPJKZtZN+fvejnip2O8n5hVUN7j2GbuS97h1fY/0d25LTwI5w33kTfgRgy7AwJO8/SNmLqQctCr6JeQ8whpfeqTP0RERETOJUpYiEiFZvvR/4kN60Pqe12/ekFEpV7KUOasVpxD7vJbVVjCwoxBrts4k1GdUZYlC4vddsOeI6xZvxN7YCROc/6Mkof3ej9rC2u0xVQnlkBr/nNlhEWUSZzu5u2wblzjVbakegL96vtZ9iEiIiJyDlPCQkQqLsPA9tNin+JZtXpw3Or9W+7uMef26RYlYZjMmAzfqRR2j4vsSpywcO5KpLgf+Ts+NYRNhhuAY9YgIlxZPm1eqtePN7uXTZLin/K698X245eY/9ovY1tUE1rceAMNtaRJRERExIt+OhKRiut4mk/Rz2GNubPJneUQTNVhWCyY/Kz9sBnuyj3DogSbjNr+SlYAfpMVq0IbMfTyjrSsUbL9KYolpBpZz7yF5bfVEBxCnSatqGM2l/19RERERCo5/YQkIhWWOf2oT1mPtk/BSUs/ukafgQ+VVVm16n6L7YaL7DwP5g2rOPDpJ6zeeRiPUXkSGDY/z0tpvRjbj+61A8psPB92B+72F+Zv/qlkhYiIiIhf+ilJRCosU9oxr+tl1Zr4tLGY4IWO1c5WSFVC7vUj/JbbPC7qr/qcoKmjafzJdOpOHsmAL5JweypB0sLjwZaT6VX0YVTpj/cM7HQhDUI1CVFERESkPClhISIVlik91es6ye6dmHi6fRhL+kfSuqZmWJSEu3UXnL0G+pTbDRddvnyt4Lpx9iFI3MSSA7k+bUtr3WEnH+7IIs1ZtseRpKUex8yJxMpxSwA3NLuXy1qO4fXaF5dorD879GVm9xplGp+IiIiIlJx+fSQiFZbp8AGv60P28IKv29S0cX9C6NkOqWqwWnHe/CCe+vEEzHqxoLhezhGfpgmZf7I9rROX1D392366O5ubl+Yv26gfYmH1VdE4LKd/sot5xybsr0/yKjtmDaZphI1vTQl8Wz0BA7jnwLenHMuw2qhx/a0YOnFGREREpNxphoWIVFjmP//wut4cVKfg64E6AvL0WW1el72PbfJpMiPxPRb/me23e1KWm4teW0W3GSt5Yk0aRhH7XexKd3Hf13vpl7KO6nnH2ZPhZuHu/HF3prt4aHkqz65N43heyWZeeHJzME9/isiUP73Kj1mDeSAhlOqO/P/mmmUd8Nf9xDgx9ch6+k0y/70Io3pUiWIQERERkTNDMyxEpGJyu7BuWedVtCGkPgAOC9zdPKQ8oqparN7/BQQYeX6bOTb/Ql6fy7CZT8w6eG9bJs7/zGTNvkUATEwewMLI4Qxs4JtI+t+uLKZ8sZEja0YDkGW2k9BhMl/+Gcig8wK5+usUdh3PP7Vj4Z4cfhoYVayZF9kug/Fzl/NqWopPXZo1iIZhFtZdHc0PB3M5P7gDfLHZ7zi7Bt9P5CV9wXEGN9kUERERkRLTDAsRqZBsX36EKSO94PqoNZi1oecBsKhvJAFWTdk/XcZJMywKs+i3Sfx+9EQyI83pYfnCxTz4V7ICYMyfC/lmue8MDYDn1hzjt7+SFQBBHif37f+KLLfBL4edBckKgMQ0Fx0+SSI199QzLUatTKXGHv9JiIP2cOqFWAl3mBnYIJDgzhcWOk7wpf2UrBARERGpgJSwEJEKx7JhFY4Fb3mVfV6jLS5z/oyACIfeusqEpfiT7DbsOlzw9Y4vFzH39+k+baL+WI/rpBNFslweIg8k+rS9f99X5CYnMeCrFGrlHiMu6xD8taTkzww3DT44iNNd+BKT5Gw3c7dn8szuBX7ra1YLIibIUnDtiY0jq0Ztn3YPxN1IkL14iRsRERERObv0U7+IVDj2k5IVAP+LbF/wdXSQ3rrKhLX4CYsRM4dBbg4AjZb5TxLEZ+5nR7rLq2xfhptG2Yf8tr964ycMOLSSfStGsHX1w7h+uIGe/9hHY/a2TL/9fjiQQ+MPD3HToR8LjfeC2DCfMmvnHj5l/6vZodAxRERERKR86ad+EalQbF9+iOXPHV5lqZYgvo1IACAmyEyQVW9dZaKYS0L+ljfpUTAMah7b77e+adYBEtO8ExavbMwgLjvJb/suadv5cPMMr7L5m16hhvM4AI+uSsN90oyNOdszGbg4/zSTockrC43V3aWXb6HJ97kxVY8sdAwRERERKV/6qV9EKo7cHBwfveFT3K/lo2RbHADM6BZxtqOqsoygkm1cGvHHb9z29k8EuJ1+65tkHWTDkTy+2JPNzN+PcyDTzbodh3hiz//8tm+d+adPWYQri/e3zCy47vBJEtmu/KTF70fzGPlzKgA2j4s+x37z6e8KDMF56TV44hN86xK8Z1O8W6sHEzqH+7QTERERkYpBp4SISIVhSvFdOvDEeYNZWS0+vx5oH2k/y1FVXZ7ImELrjOAwTJnpPuXzfnqi0D5Reel8vPIP/mNxkGyvxpQNxxmf6D9ZUZRLjv3OVcmr+CSqEzuPu3ljcwYPtgzlkZWpBW0SVz7g0y9j1hIwF56H9zRuSUaHiwhZs5StgTGs7X49E2K12aaIiIhIRaUZFiJSYZiyffcseK32JQVf1woyE64NN8tOETMscu59ko2h9Us8ZOKqB9m3/F5eSXyPtFwPtx78oVShzds8nTsOLAHg8z3Z7DnuYkVS/syOmNxj1HUe82rvantBkckKIL9+xFNkvLmI2q/OYdIVTTCbdNqMiIiISEWln/xFpEIwufKw/bjIq2xDcCxptuCC66hAy8nd5DTldb3Ut6zjRbibteW7e2ayIiy+xGOaMRix/2sGpawhxJNb6tje2P4Oww4tY21KHnO2ZwFww6Fl7F0xwjfmrhcXf2BHIOYSnJAiIiIiIuVDP7GJSIVQ96v/Ylu/zKvsj8Bor2uj8FMupZRybxiJEVoNU0Y6eZcNwVOrLtjsYDJxU5MQGrQaQ/qy20s19vxN03BhxoqnyHYzGg7kzl5NCXh7kk/d69vf5YPobkzdkMZXv03i4mO/+x3D3d73BBARERERqdyUsBCRYrNsWEng1DEAZI95GXezNqc3oMeD49/PY1v5Hf4WJ1gNt9f1+dVLdqqFFENwKM7r7/VbZbeY2H97Q1jmtxoA1/kdsP6+ptD6fyYrMu3B2Nt1wbbiW682iZfdgqtTBNmh4QS+PNarLsjj5Lycw3y4aTptM3b7j6F5W9DSDhEREZEqR0tCRKRYzPt2FSQrAAInPogp/VgRPU7NuvQzbCu/K7Q+IXOv1/WldbVB4tlmKiIR4G7UjNwb7yv2WMfDInF16+NV9uHwf/Nsh/yTOtytu5DXo59Pv2XrxhearAAwJ/k/ZlVEREREKjclLETEW04W1qWfYVnzPXhOzHCwbFjp09S8dcNp3cr24xdF1v9z/4QpnavRv74SFuXBE+Z7lOyh3kPJGfEMRq16ZD37drHGCYuJxp3QgezRU8m96jaynn+Xfp2bYDGfSIrk3jaKvJOSGtF5vqeVeLFobxMRERGRqkgJCxE5ITeHwGdHEDD7JQJnjidg5viCjSNMR5N9mpuyMkp9K9OhvVh2by+yzTsxFwEwoH4AdzYL8fpgK2eP89rhXtcjLxpPyE3DMapHAuCJjWPaFePZERDtr3sBW838enfztuQNvAlP3YZ+27kTOpQsvr5DS9ReRERERCoH7WEhIgWsq5Zg2bfzxPXaZZj/3IGnfjzmYyk+7R3z3sSoWQsjOBTryu8wgkLIu2wwOAKLvI9511aCxt9dZJvbm9zFDxHNAbi6YVApXo2UFVfnXuSmHCJj43oSG3fh6au6vI/5WQAAIABJREFU+7S57IoLuZimJPy5ls82TvE7jqdWvWLdz920NYbZjMnjf7PO3Ktuw/LHZqwbVuJq0Q5X597FfzEiIiIiUmkoYSFSFeU5cbz7IpZdW8jr0Y+8Yv4G2rJzq0+Z49/Pk/3CbEwph3zqTJnpBE4Z5VVmTt5P7p1jfdoW9Ek/5jdZcWeTO5kV09Nvn85R9lNELmeUxUrewJtwDLyJ8wtpUifYwoZrojm653zY6L+Np079Yt3OiKiJp14jLHsS/dbn9b+BPLM5f8mSWctBRERERKoqLQkRqYLsn72PbfnXmA/uxfHh68Xea8KUctCnzLJ/N9ZVSwv98Hgy20+Lizx/1PqD774VTpOF/0Z1LbRPdJA+lFYGFrOJyBrVCq13n9e02GMZUbX9lv9y+wtg/uu/LiUrRERERKo0JSxEqiD7p//xug6acD+W9StO2a+w0xYCXnu6ZAFkntgk0ZR6BMumtVjW/IBlwyocC3w3aPw2IoEci/9ZFP+5qHrJ7i3lKygEw8/JIu768RASVuxhCls+0rRJ8WZpiIiIiEjlpyUhIlWM6bDvLAmAwJfHkjntY4zwGgVl5sTfsWzbgLtpazx1zsOcfKBMYjAf+BNP4wQsG1YROHX0KdsPa36v3/IHE0IY0KDo/TCkgrFaMcJrYDppz5OcByeUaJi87pdj/+x934oaRW/sKSIiIiJVhxIWIlWMbenCwuu+XoBzyL8AsKxdRuD0JwrqnJdeU2YxBD0/kqwnXsXxn5eLbOe0BxLc5U0Mk+9kr81DalE7WFP+K6WTVgQZJhNGRM2SDRFVm7weV2D7xxIi93lNwar/tkRERETOFfrJT6Qq8bixf/HfQqvtX/wX886teBo1x7LNe18L+9cLSnXLA/ZwajtTfcqDnvU/a+KfuiWM85usOL+6jZggrVirrFw9LvdaluRO6FiqcZz9hmFdtQRTTjaG2Yzz6tvLKkQRERERqQSUsBCpQsz7dp+yjXXLr7Dl11O2m12rO7cc+tFv3aMNr6dTeiLzorrwcVQn7t7/DTMTZ5cwWtgR6Du9/4rYAJ5sF4bJzz4IUjk4L70a65ofMB/Yg2GxktdncKnGMaJqk/XUG1g3rMQdfz6euBZlHKmIiIiIVGRKWIhUIaakfWUyjgsz0+te5jdh4cbE1NgrvMq+rt6yxPdIswRy3BrkVXbk5tpYzEpUVHoh1ch65i0sWzfgiYrBiK5b6qGM2vXJq62NNkVERETORZpzLVKFWLb95nU9N6pbqcaZH9WJTUH+P2RaTt6gANgZGM2vISX7UPlRVBev66tq5SlZUZXY7LgTOpxWskJEREREzm1KWIhUIdZff/K6/qZ6An8ERJVojAyzg6cbXIPbbOGwLbTY/QYmjCq0bn1wLKMbXoeb/ITE4oiWPBB/U0F9oMXEsDquEsUpIiIiIiJVmxIWIqXxyzISZ0xl7qc/sT/TXd7RAHAg+RjmlKSC6zyThY8jO/JTtSYlGqd368fZEVQLgOfqD/KpT7aF+ZQFWU0ccFQnoPt7fsfcFlSbl2L70aTTVLq1Gc+AhFE4zTYAetZ28EHv6sQG+s7cEBERERGRc5f2sBApgezUVLZNeJoLDv1KG+D8tV9wyc7HuCZlDe3Td7CuVkuCr72NuHA7CdVtmIvYONLtMXhzSyZrDjv5ZFe2V93ABgG83aM6NrOJVUm5PL02nRCbidcujKBmgO9Rnx8v28JFcx73KtsaFEO2xUHOX4mBooxpeC27AqL4JLIDhsnMmNahjGoVyoS1V/LOgj+5/dD3BW0n1B/IaxeEc3lsILlug6hAc8EGmTvTXXx+4BL67fjGa/wDjghGtQrlxQ2wO/DEjI97WgTzQsdwABITTxmmiIiIiIicQyp0wmLChAlMmjTJqywqKort27cDYBgGEydO5L333iM1NZV27drx4osv0qxZs/IIV6oYwzBIyfFQzW7GbjGRnZJC5MPXEPmPNjbDzffrny247py+gx+mbeGKhIeJCA9hb4b37ItGYRYOZXnIdBU9m+DT3Tl8uvuAT3ncfw8VfN0s3EqXaAef/J7MptWjic5L92r7W3D+nhLGKU7b+CjuMppcfxM79+Ywo46DYXFBBQmIR9tG0DTxLl6teym3HvyeLUF1+E+9XuxvFOR3v4mGYVYajnoA7vZOWFx5WWci24YRaDHx7LoTcY5u7TtbQ0REREREBCp4wgIgPj6ezz//vODaYjnx2+Vp06bx6quv8uqrrxIfH8/kyZMZNGgQa9asITS0+GvvRf6WlOXm/3ZnM3F9OsdyvZMKLye+x8hijNEjbQupP93B5S1HM+z4Lq448isfR3bklbp9+SM9P4HhcDt5NXEWgw6vwWJ4uL7FSH6o1gwTBlkWBx7TqVdrbUl1sSXVxf2HfvRJVgBsDq1H8k21CfxPMPjmPgr0bhpNQKMgBjcK8qlzWExM7RLOQysa8GyNW3moZQiHzj/F91ZgMJmT5hD43AjMx9NwN25JZLcLAHi4VSg3NwnCBNTwM1NERERERETkbxU+YWG1WomOjvYpNwyD119/nQceeICBAwcC8PrrrxMfH8+CBQu49dZbz3aoUgl9fyCHt7ZkcjDLzfojeXgMCHNl8fq2d4jPPsTUepfzYXQ3QlzZjNz/dYnG/vK3E7ODuqQn8vDeL2jYeRp5Ziuj//zM68jQhRtfLPh6W2AMw5qPYH1oA5pl7uPJ3Z9Qy5nK9c1HctAR4XOfaw6v8nv/G4dcjN1iwhJa9CyG4IZxFLULx6DzguhfPxBrCU7wMGrVI2vqPEwphzBiYuEfszz8LWkRERERERE5WYVPWOzevZtmzZphs9lo3749Tz75JA0aNGDPnj0kJSXRq1evgraBgYF07dqVVatWKWEhPjLzPLz8Wwa/HnFy3Gmw/ogTp8e7jdXjIuWnuzD/dXTn+1te44G9i2ifseu07x/jTGX0nwt5rsFV3PSPZMXJmmQf5JsNz9Oi44t89tsUGuSmALB3xQhatZ/IppB6BW2bZ+6jS7rv5g+uxgnUbRoHQN5FA7B9PheTkf+aXDVjsKQdwZTnJK/TRbhbdfHpf7KSJCsK2B0YtUt21KmIiIiIiMjfTKmpqRV2a/5vvvmGjIwM4uPjSUlJYcqUKSQmJrJy5UoSExPp06cPGzdupF69Ex/g7r33Xg4ePMgnn3xS6LiJ2t2vytqZZeLno/m/wY8L9tAx3MPUnTbmHTz1xpMYBq4fbjij8bkw06TTVP5Y9UCp+ifGNOPiVo+xPyd/ycgdB5bwxvZ3vNoc7N6fQ90uB8uJfGT1DT8TuWYJOTVj2NfnOgyzGZPbhTtIS6dERERERKR8xMfHF1lfoWdYXHLJJV7X7du3p3Xr1nzwwQd06NABoGBzwL8ZhuFTdrJT/aVUNImJiZUu5vKw9rCToT8dLnX/HqlbyjAa/6x4Sp2sAIg7tJVNj1eDkDCyXB5qjvnSqz73mjsI7X8DPmmI+Hg819yCHWhY6rufWXrO5Vyg51zOBXrO5Vyg51zOBRXhOT/1zn4VSEhICE2bNmXnzp0F+1okJyd7tUlJSSEyMtJfd6nC1qc46f156ZMVDreT7zY8X6y2roQO5F5zh0955svzS31/AKMYG22aDIOQewdg+/JDImZPwXz4oFe9p2HT04pBRERERESkoqhUCYucnBwSExOJjo6mfv36REdHs3TpUq/6FStW0KlTp3KMUs4GwzB4aHkq4bP2Ez5rPz0/K32yAmB+zuLi3ddmI+eBF3B1vAjDfOLbJ+f2RzGqR+Ku37jE986+71ky3vuezH9/Uew+jo/ewLZskXdsJjPuBk1KfH8REREREZGKqEIvCXn88ce57LLLqFu3bsEeFllZWVx33XWYTCaGDx/OSy+9RHx8PHFxcbz44osEBwdzzTXXlHfocgYZhkGHT5LZke4q9RjD4gLh2BH+tWQqXY8VbymIJzSc7MdngtWGEV2H7HEzsP7yI+5mbXC36pzfpu55WPZs9+qXc8tDeBq1IOiJ2/2PG39+/heOQFzN2mDd8mupXpOrS28I1p4UIiIiIiJSNVTohMWBAwe44447OHLkCDVr1qR9+/Z88803xMbGAnD//feTnZ3NI488QmpqKu3ateOTTz4hNFQf2qoqt8eg7vsHyXbn7xVr9+ThNP9jQ03DoGXmn+SYbewOiPSqezM+g1vfvjv/tIzvi76Pq003rL/+7FWW9dJ/wRFYcO2Ja4EzroV3v26XYvvZe7aGq+sl4AjE2W8Y9s/ner+e85pghJ04qtR5w0hM70zBsrPk+2nk3vJwifuIiIiIiIhUVBU6YfHuu+8WWW8ymRg7dixjx449SxFJefojzcWcxEyy3QY2j4v/bp5B/5S1LK/WmP4JjxDgyePQ8uFefUZ3fohX7K1ZuOct+ny/rFj3yet6Cbk33EfwI9djyjwO5G9m+c9kRWHczduSO2wEjrkzAch8YXZBP+egWzACg3DMf6ugvXPgzV79PXUbkv3U6/kXmccxpxzCU68hpuNpBN93VaH3zR75DDgCivX6REREREREKoMKnbAQAUjKcnPl4hS2pJ5YAjJy32KuTPkFgAvTtjH8wLfE5B7z6Ttp5VQmlfB+rg49IDiU7DEvY/3+c4zoOuRdPKh4nU0m8i69hrxL/SxLstrI6zcMV/fLsaxfgafOeXgaNSt8rOBQPH8t8TCqVSf7oYkETh3jt6m73YXFi09ERERERKSSUMKigtuamseiJAsRnkzOC7VwQS0HFnPRx7ZWFSk5blrNTyLTZfjUTd75gdf1hJ0fnta98rpfDjnZuJu3xd2mGwCe2DicN5X+CNLCGGERuLpfXuJ+7ladyXjtM0Lu6e9Vnjn5fTjFUb4iIiIiIhVNZmYmLlfp96WTMysgIIC0tLTTHsdqtRIcHFy6vqd9dzmjFu/N4elEBySmAnB382Amdgov56i8pTs9HMv1EBtiwVQGH5x/P5rHBZ8mn7rhaXAldMB1YV/c9Rph1K5/Ru9VpoJDyevRD9sPnwPgatkJI7puOQclIiIiIlIyubm5AFSrVq2cI5HCOBwOAgJOf9l5ZmYmubm5OByOEvdVwqKCaxpu87p+f3vWWUtYeAyDrakuagdZCHf4PwH3rS0ZjF2Vxt+TIMa0DmV4ixCq2Ut3Yu6KpFz6fplSZJtAd26pxv5b5tR5GDWiTmuM8pQ7bATuuOaYsjPJu+Cy8g5HRERERKTEcnJyCAsLK+8w5CwICgoiPT1dCYuqqHmE9z9RhsvA5TGwnuFlIS6PwbAlR1m8NweHBV69IIKL6wTw4objzEnMpHedAD7Zle3Tb+L640xcf5xWNWzYzXB1wyD+1Sy4YOZFmtPDf7ZnsvFIHhfGOKgZYOatLZlkuQwGNwpkwq/HTxlb3yPrS/26sp58vVInKwBwBJRqSYmIiIiISEVSFrOzpeI7nX9nJSwquLrBFp+yXPeZS1j8keZi5qbj/Hgwlz/S3X/dD+74wXtDS3/Jin/acCQPgDWH0xizKg2HBeoEWdh53F3QZt5O7zFWJjv9jhXsyuEt+6/0bxCIu3MvAqcsKfLeRmAwrpadsK3ybpcxe6n2ehAREREREakklLCo4EwmE2FWg3TXiQ/auW6DYFsRnUrIMAy+2ZfLDUuO4PSU3bj/lOvGK1lRXFefF8i7a6YT/Puq/IJ3p/i0yR7zMpbff8H++VwMk4ncmx7A1aozlp1bMR8+gGEykzNqspIVIiIiIiIilYgSFpWA7aTP2blllFTI8xiMW53Gm1syy2bAMtQg1MLS/lFErlxEwN/JCj88MfVwN2mFu1kb8nr2wwgIhND8PT6yJr6HZftGPNF1K/8yEBEREREROafMmDGDN998k40bNwIwYcIEFi5cyIoVK0o95ty5c3n00UfZv39/WYV5RpVuZ0Q5qxxm72M9c92+x3yWlGEYnD/vUJklK7YOrUWYvWxmMEQ4THx0cQ0ictMJ8DOj4p+clw0Fc/5jbETGFCQrALDacDdvq2SFiIiIiIhUeiNHjuSLL74odvvw8HA+/fRTr7KrrrqK9etLvyfg2aaERSVgO+lf6XQTFltT87hy8RGSsk9/qkbtIDPHbqlNrSALu66LoWX1kq9VCbWZCLbmJzuubBDI74Nr0STchv2LD07Z19W9b4nvJyIiIiIicjY4nf736SuNkJAQqlevflpjBAYGEhkZWUYRnXlKWFQCJ58QmlOMhEW608NbWzKoPecA4bP2Ez5rP/csO8Z13x6h8/+S+eGg79GgJsM3gWE1wStdwwn6K6EwvHkwSTfVZuYF4UzuVI0NNdYQestFhNzck4DF8/imXyTv96rO4strcvSW2lxW78S5vb3rOLi4joNrGwWycXA0Pw2MYt7FNUi8Nob9N9Ym9dY6zL6oOsE2M+Q5sX81r8jXmH3/c2D23ZRURERERETkTLjiiit48MEHGT16NPXr16d+/fo88cQTeDz5n6USEhKYMGEC9957L7Gxsdx5550AHDhwgNtuu62gz5AhQ/jjjz+8xp42bRqNGzemTp06/Otf/yIjI8OrfsKECXTp0sWr7IMPPqBr165ERUURHx/P8OHDC+IAuPnmmwkPDy+4njt3LnXq1PEaY9asWbRp04bIyEjatGnDe++951UfHh7O7Nmzufnmm6lduzatWrXio48+Op2/xmLTHhaVgN3knaBwnmLvyrWHnfT+/LBP+Qc7svy2b5x1gI82TSMhcx87AqIZ1/VhunRsTssaNjpE2jGZTAxpFIjFZMJhyU9c3BAfjGXDSgLnTC0Yx/Hh65gOH6TfTQ8UlM3uWZ3lSblEOMy0qWn3um894PxCZmRYfit83woAw2bHE9eiyDYiIiIiIlJ5hM86u/sqpN5a59SN/Jg/fz7XXXcd33zzDZs2beL+++8nOjqaESNGAPDaa68xatQovv/+ewzDICsri/79+9OxY0e++OIL7HY7M2bMYODAgaxevZqgoCD+97//8dxzzzF58mQuvPBC/u///o9p06YRHh5eaByzZs1izJgxPPHEE/Tp04fMzEx+/PFHAJYuXUpcXBzTp0+nT58+WCz+f9H72Wef8cgjj/DCCy/Qq1cvvvvuOx5++GGioqK46KKLCtpNnjyZp556iqeeeoo5c+YwYsQIunTpQmxsbKn+DotLCYtKoCQzLDYdzfObrCjKnB1vk5C5D4C4nCQ+WvIorqNdyXnwhYI2Qdb8IExpRzGlHsFTryH2z32XbNi/+z8IDsV59e0ABFhN9KoTABlpmLdtxdOgMdgdRZ/Y4fH4HTtz6jwc707BnHIIZ/8bMMIiSvQ6RURERERETld0dDSTJ0/GZDLRuHFjduzYwWuvvVaQsOjatSv3339/Qfs5c+ZgGAavvfYapr8+B73yyivExcWxePFiBg0axOuvv851113HrbfeCsCoUaNYtmwZO3fuLDSOKVOmMHz48IL7ArRu3RqAmjVrAlCtWjWio6MLHWPmzJkMHTqUu+66C4C4uDjWr1/PtGnTvBIWQ4cOZejQoQCMGzeON954gxUrVihhIeA4KWHh9PhPWKTmerj665QSjb23r5WY77f5lFvXLyfo4Wtxx7XAOegWjFr1sGxYRcArYzF5PHii62BO8p8Btf64COelV4M9ABwBmHdtI2j8v3zaOS+5Guf193gv63C5CHrgGszHU73b9r8Bo0YUOY8UvQmniIiIiIjImdS+ffuCxANAx44def7550lPTwegTZs2Xu03bNjAnj17qFu3rld5VlYWu3btAmDbtm3ceOONXvUdOnQoNGFx+PBhDhw4QI8ePU7rtWzbto1hw4Z5lXXp0oVFixZ5lbVocWJ2u9VqpUaNGhw+XLJflJeGEhaVgO2kU0KST9osM9dtMHtbJqNXpRV7zOvigpjYqRrhG34stI055VD+n307yX72bRxzp2P6a21WYckKAHNqCiEjrsQwm3FecweOeW/6bWf/5mMwgXPYyIKywOdH+iQrPDGxOK+8udivTUREREREpLwEBwd7XXs8HhISEnj33Xd92kZElG7WuGGc/smRfzP5mf1+cpnNZvOpL8sYCqOERSVw8gyL4cuO8X5iJh/0rkGYzcT58w5xOMf/iR+ze1anY5Qdp8fAY0DNADNh/1hjYtmdeMr7W/btIuCVcUUmKfwxeTyFJiv+Zv/6Y4zwmrg69wbDg2XnFp82OXeMBmvJTx8REREREZHKo7R7Spxta9euxTCMgg/1a9asISYmhrCwML/tW7VqxYIFC6hevXqhe1I0adKEX375xWuWxS+//FJoDFFRUdSuXZsffvjBa+nGP9lsNtzuojdAbNKkCStXrvS674oVK2jatGmR/c4WJSwqgVCrb+bq50NO6s89WGifAAtsGlKLGgFFnKLhysO25P+KFYN1w8pitSsNx7x/45j3b/Iu6u9TlzvoVm2uKSIiIiIiFcahQ4cYM2YMd9xxB5s3b2b69Ok88sgjhbYfPHgwM2bM4Prrr+exxx6jbt267N+/ny+//JLbbruNRo0acffdd3P33XfTtm1bLrjgAj799FPWrl1b5KabDz/8MI899hiRkZH06dOHrKwsfvjhB0aOzJ/BHhsbyw8//EC3bt1wOBx+xxo5ciS33HILrVu3plevXnz77bfMnz+fOXPmnP5fVBlQwqIS6B/l5vNkG3n+J1H4sJrgtQsiik5WGAaONydgysoskxhdLdph3bT2tMawLf3MpyxPS0FERERERKQCGTx4MB6Ph969e2Mymbjxxhu55557Cm0fFBTEl19+yfjx47nllltIT0+nVq1aXHjhhQVJhKuuuordu3fz7LPPkp2dTd++fbnnnnv44APfwwj+dvvtt2Oz2Xj11VcZP348ERERXHLJJQX1zz33HOPGjaNFixbExMSwceNGnzH69evH5MmTmTFjBmPHjqVevXq89NJL9O3bl5ycnNP4WyobptTU1DO/8EROS2JiIkfDYhm3Jo1fDucV2TYuzMqcXtVpFlHEEgqXi4BXxmLduManyl2/MZY920sUn6tpa5xX3UbQC/cV2S7vov64m7TGsnU9tu99kxMny3rmLTz140sUi1ReiYmJxMfr31uqNj3nci7Qcy7nAj3npy8tLY1q1aqVdxgldsUVV9C8eXOmTKn6hwHk5OQQEBBQJmOV9t/bfOomUhF0inbwbb8oPuhdvdA2My8I55ero4tMVph3biX4zkv9JisA3E1blTg25+A78TROwB13ftHtLhuCq0tvnANvOuWYntr1lawQERERERE5hylhUclcHhvIqkFRdKtlLyirH2Lhl6uiuCE+uIieYPt8LkFP311w0sfJ3HHn42lYss1VjNBqeGLjwGQi554ncTduiWE2k9flYnKH3FXQztW+O0atevl9qkeS8caX5HW/vNBxXee3L1EcIiIiIiIiUrVoD4tKqEm4jS/6Rpaoj/X7z3HMf6vQeucV15F32RBMB/4schxP9Uhy73oM+wczMWVnkXvdPWB3AGDUiCJ73PQTjQ0Dd/O2mI6n4W7RznugwCByb38U5+A7CR45yKvKXb8xeZdfV6LXJyIiIiIicqZ98cUX5R3COUUJi6oqOwvbt59g2fYbll1bMWWk+2824mncHXoUXBuBwXjCIjCnH/Np644/n+wxr4DVSvaz74BhgJ8zewuYTHjOK3rGhhEWQfZDEwmcOgYA55U34+x/I1j1aIqIiIiIiJzL9KmwKkpPJeiFkZgP7i20iREQROarn4L1pP0ubHZyhz+BfcHbGEEh5N54H3g8mJMP5O9v8c9EQlHJihJwt+pMxnvfl8lYIiIiIiIiUjUoYVEFBY8eVuRxpZ7oumRNmlNowsHdvC3ZT77mXRYTW6YxioiIiIiIiBRFm25WAuGbfyHg5bHYPpubvwyjCJYtvxaZrADI+de4MpsdISIiIiIiInImaIZFBWf74r+c98m/AbCuXwE2O3mXDS60vfWnxYXWGQFBZE2ZixEWUeZxioiIiIiIiJQlJSwqMMtvq3DM+7dXmeO/r2Jd8z05d4zGOGmZhvXHRdh++sqrLK/rpTj7XY855RDupq3BEXDG4xYRERERERE5XVoSUoF56jTwW27ZsQnHf/P3mLCs+Z7AZ+8laMxNBLwzyaet88qbMeo0wN2qs5IVIiIiIiIiUiAhIYEZM2aUdxiF0gyLCsz+39cLrbNuWEnAtMexrvup0DZ5HS/CiK5zJkITERERERE551xxxRU0b96cKVOmlHco5wTNsKjA8i65qsj6opIVALl3jinLcEREREREROQU8vLyyjuEKkMJiwrM06QlGbOXcrRFxxL1MxwBZE77GOyOMxSZiIiIiIjIuWX48OH8/PPPvPXWW4SHhxMeHs7cuXMJDw/n66+/plevXkRGRvLdd98xYcIEunTp4tV/7ty51KnjPQN+0aJF9OjRg+joaFq2bMmzzz6L0+k8ZSxPP/00PXr08Cm/9NJLGT16NADr1q1j0KBBNGzYkHr16nHZZZexevXqIscNDw/n008/9So7edlIWloa999/P3FxcdStW5fLL7+cX3/99ZQxl4aWhFR0JhN7r7iJ8H07MKcdLVaX3KF3Y4TXOMOBiYiIiIiIlJ2Qm3ue1ftlvPd9idpPnDiRP/74g/j4eJ588kkAtm7dCsD48eN57rnnaNiwISEhIcX6AP/dd99x1113MWHCBLp168bevXt56KGHyM3N5bnnniuy79ChQ3n55ZfZvn07jRs3BmD37t2sXr2aiRMnAnD8+HGGDh3KxIkTMZlMvPXWWwwePJh169ZRo0bpPi8ahsHQoUMJCwvjo48+IiIigg8++IABAwawZs0aatWqVapxC6MZFpWAx+4ge+wreGrVK7SN87IhuJq3JfeaO3H1GngWoxMREREREan6qlWrhs1mIygoiOjoaKKjozGb8z9Sjx49ml69etGgQQNq1qxZrPFefPFFRo4cyQ033MDdJEwDAAASt0lEQVR5551H9+7dGT9+PLNmzcIwjCL7Nm3alISEBObNm1dQNn/+fOLi4mjbti0APXr04Nprr6VJkyY0btyYyZMnExAQwLffflvKvwH48ccf2bhxI++99x7t2rWjYcOGPP7449SvX5+PPvqo1OMWRjMsKgkjJpasCe/h+Pfz2FZ+d6LcZidzxv8gMLgcoxMRERERETl3tWnTpsR9NmzYwLp165g2bVpBmcfjITs7m6SkpFPOVhgyZAjvvPMOjz/+OJCfsBgyZEhB/eHDh3n++edZtmwZhw8fxu12k52dzb59+0oc6z9jzsrKIi4uzqs8JyeHXbt2lXrcwihhUZmYzeQOf4LcO8cWXGPWJBkREREREZHyFBzs/Qtks9nsM0vC5XJ5XXs8HkaPHs2VV17pM15xZmkMHjyYp556itWrV2O329m+fbtXwmL48OEkJyfzwgsvEBsbi8PhYMCAAUXukWEymYqM2+PxEBUVxaJFi3z6hoaGnjLmklLCojKy6p9NRERERESqlpLuKVEe7HY7brf7lO1q1qxJcnIyhmFgMpkA2Lhxo1ebVq1asX37dho2bFiqWGrVqkX37t2ZP38+drudTp060aBBg4L6lStXMnHiRPr06QNAcnIySUlJp4z70KFDBdfJycle161atSI5ORmz2ex1rzNFn3xFREREREREiiE2Npa1a9eyZ88eQkJC8Hg8fttdcMEFHDt2jJdeeomrr76aZcuW+Zy+8eijjzJ06FDq1avHoEGDsFqtbNmyhbVr1/LMM88UK54hQ4bwxBNPYLfbGTVqlFddo0aNmDdvHu3btycrK4snn3wSu91e5Hjdu3fn7bffplOnTrhcLiZNmkRAQEBBfc+ePencuTPXX389Tz/9NPHx8SQnJ/Ptt9/Ss2dPunbtWqy4i0vrCURERERERESKYeTIkdjtdjp37kyjRo0K3Q+iSZMmTJ06ldmzZ9OtWze+//57HnroIa82vXv3Zt68efz000/07t2b3r178/LLL1O3bt1ixzNgwACys7NJSUlh0KBBXnUzZ84kMzOTnj17ctttt3HDDTcQGxtb5HjPPfccDRo0oF+/ftxxxx3ceOONXstTTCYT8+bN48ILL+T++++nQ4cO3HrrrezYsYOYmJhix11cptTU1KK3H5Vyl5iYSHx8fHmHIXJG6TmXc4GeczkX6DmXc4Ge89OXlpZGtWrVyjsMKUJOTo7X7IrTUdp/b82wEBEREREREZEKR3tYiIiIiIiIiFQgy5cvZ/DgwYXW79+//yxGU36UsBARERERERGpQNq0acOyZcvKO4xyp4SFiIiIiIiISAUSGBhY6uNOqxLtYSEiIiIiIiIiFY4SFiIiIiIiIiJS4ShhISIiIiIiImeV2WzG6XSWdxhyFjidTszm0qUetIeFiIiIiIiInFUhISFkZGSQnZ1d3qFIIdLT0wkLCzvtccxmMyEhIaXqq4SFiIiIiIiInFUmk4nQ0NDyDkOKkJycTL169co1Bi0JEREREREREZEKRwkLEREREREREalwlLAQERERERERkQpHCQsRERERERERqXBMqampRnkHISIiIiIiIiLyT5phISIiIiIiIiIVjhIWIiIiIiIiIlLhKGEhIiIiIiIiIhWOEhYiIiIiIiIiUuEoYSEiIiIiIiIiFY4SFhXY22+/TcuWLYmOjqZHjx4sX768vEMSKbYJEyYQHh7u9adx48YF9YZhMGHCBJo2bUqtWrW44oor2LJli9cYqamp3HXXXcTGxhIbG8tdd91Famrq2X4pIgV+/vlnrr32Wpo1a0Z4eDhz5871qi+r53rTpk1cfvnl1KpVi2bNmjFp0iQMQ4d6ydlxqud8+PDhPu/vF198sVeb3NxcHnnkERo2bEjt2rW59tpr2b9/v1ebvXv3MnToUGrXrk3Dhg159NFHcTqdZ/z1iUydOpWLLrqIevXq0ahRI4YOHcrmzZu92uj9XCq74jznleH9XAmLCuqTTz5hzJgxPPzww/z444907NiRwYMHs3fv3vIOTaTY4uPj2bZtW8Gffybdpk2bxquvvsqkSZNYsmQJkZGRDBo0iOPHjxe0ueOOO/jtt9+YP38+CxYs4LfffuNf//pXebwUEQAyMzNp3rw5EydOJDAw0Ke+LJ7r9PR0Bg0aRFRUFEuWLGHixInMmDGDmTNnnpXXKHKq5xygZ8+eXu/v8+fP96ofO3Ysn332Ge+88w5ffvklx48fZ+jQobjdbgDcbjdDhw4lIyODL7/8knfeeYeFCxcybty4M/76RH766Sduv/12Fi9ezMKFC7FarVx55ZUcO3asoI3ez6WyK85zDhX//dyUmpqqFF8F1Lt3b1q0aMH06dMLytq2bcvAgQN56qmnyjEykeKZMGECCxcuZMWKFT51hmHQtGlT7rzzTkaNGgVAdnY28fHxPPvss9x6661s27aNTp068dVXX9G5c2cAVqxYQd++fVmzZg3x8fFn9fWInKxOnTpMnjyZYcOGAWX3XL/zzjuMHz+e7du3F3xYnDJlCu+++y6bN2/GZDKVzwuWc9LJzznk/0bu6NGjfPTRR377pKWlERcXx6uvvsqQIUMA2LdvHwkJCSxYsIDevXvzzTffMGTIEDZu3EjdunUB+Oijj7jvvvtITEwkLCzszL84kb9kZGQQGxvL3Llz6du3r97PpUo6+TmHyvF+rhkWFZDT6WT9+vX06tXLq7xXr16sWrWqnKISKbndu3fTrFkzWrZsyW233cbu3bsB2LNnD0lJSV7PeGBgIF27di14xlevXk1ISAidOnUqaNO5c2eCg4P1fSAVUlk916tXr6ZLly5ev9nu3bs3Bw8eZM+ePWfp1YgUbcWKFcTFxdGuXTvuu+8+Dh8+XFC3fv168vLyvL4X6tatS5MmTbye8yZNmhT8cAv5z3lubi7r168/ey9EhPwPch6Ph/DwcEDv51I1nfyc/62iv58rYVEBHTlyBLfbTWRkpFd5ZGQkycnJ5RSVSMm0b9+e1157jfnz5zN9+nSSkpK49NJLOXr0KElJSQBFPuPJycnUqFHD67cPJpOJmjVr6vtAKqSyeq6Tk5P9jvF3nUh5u/jii3njjTf49NNPee6551i7di0DBgwgNzcXyH9OLRYLNWrU8Op38vfCyc95jRo1sFgses7lrBszZgwJCQl07NgR0Pu5VE0nP+dQOd7Prac9gpwxJ08TMwxDU8ek0rjkkku8rtu3b0/r1q354IMP6NChA3DqZ9zf867vA6noyuK59jdGYX1Fzrarr7664OsWLVrQunVrEhISWLx4MQMGDCi0X3G+F4oqFzkTHnvsMVauXMlXX32FxWLxqtP7uVQVhT3nleH9XDMsKqDCMlIpKSk+2SuRyiIkJISmTZuyc+dOoqOjAd/fLvzzGY+KiiIlJcVrJ23DMDhy5Ii+D6RCKqvnOioqyu8Y4PvbPpGKICYmhtq1a7Nz504g/xl2u90cOXLEq93J3wsnP+eFzTAVOVPGjh3Lxx9/zMKFC2nQoEFBud7PpSop7Dn3pyK+nythUQHZ7XZat27N0qVLvcqXLl3qtU5OpDLJyckhMTGR6Oho6tevT3R0tNcznpOTw4oVKwqe8Y4dO5KRkcHq1asL2qxevZrMzEx9H0iFVFbPdceOHVmxYgU5OTkFbZYuXUpMTAz169c/S69GpPiOHDnCwYMHCz7ktW7dGpvN5vW9sH///oJNCiH/Od+2bZvX0XhLly7F4XDQunXrs/sC5Jw0evRoFixYwMKFC72OXQe9n0vVUdRz7k9FfD+3jBkzZvxpjyJlLjQ0lAkTJlCrVi0CAgKYMmUKy5cvZ+bMmVSrVq28wxM5pccffxy73Y7H42HHjh088sgj7Ny5k5dffpnw8HDcbjcvv/wycXFxuN1uxo0bR1JSEq+88goOh4OaNWvyyy+/sGDBAlq2bMn+/ft58MEHadu2rY42lXKTkZHB1q1bSUpKYs6cOTRv3pywsDCcTifVqlUrk+e6UaNGzJo1i40bNxIfH8+KFSt48skneeCBB5Ssk7OiqOfcYrHwzDPPEBISgsvlYuPGjYwcORK3282UKVNwOBwEBARw6NAh3nrrLc4//3zS0tJ48MEHCQsL4+mnn8ZsNtOgQQM+++wzlixZQosWLdi6dSujRo1i8ODB9O/fv7z/CqSKGzVqFB9++CGzZ8+mbt26ZGZmkpmZCeT/4tBkMun9XCq9Uz3nGRkZleL9XMeaVmBvv/0206ZNIykpiWbNmvHCCy/QrVu38g5LpFhuu+02li9fzpEjR6hZsybt27dn3LhxNG3aFMifNjlx4kRmz55Namoq7dq148UXX6R58+YFYxw7dozRo0ezaNEiAPr27cvkyZN9djcWOVuWLVvm9z/f6667jtdff73MnutNmzYxatQo1q1bR3h4OLfeeiujR4/Wmmc5K4p6zqdOncqwYcP47bffSEtLIzo6mgsvvJBx48Z57RCfk5PDE0/8f3t3E1LV1sdx/Js9YWGGSKFUJBY56AaZFamVHLSkQSKUIFINoheshga92cuglAaSDSywKAsJDaNB2SCESAOjSVg4KEMKg9A4KGZZmXkHweHp+kQ+1dV9ud8P7MHZ57/XWWtzOGfzY6+1j9DQ0MCHDx/IysqioqLim5quri727dtHc3MzU6dOpaCggBMnThAdHT0u49S/1/euI/bv38/BgweB33ed4u+5JsqPvueDg4P/iN9zAwtJkiRJkhQ4rmEhSZIkSZICx8BCkiRJkiQFjoGFJEmSJEkKHAMLSZIkSZIUOAYWkiRJkiQpcAwsJEmSJElS4BhYSJIkSZKkwDGwkCRJ46KlpYW4uLjIFh8fT1JSEhkZGRQXF9PU1MTIyMhPt//48WPKy8t5+fLlb+y1JEmaKP+Z6A5IkqR/l4KCAtatW8fIyAgDAwN0dHTQ2NhIXV0doVCImpoa4uLi/u92nzx5wqlTp1i9ejVJSUl/Q88lSdJ4MrCQJEnjasmSJRQWFn6zr6ysjKNHj1JVVcWOHTtoaGiYoN5JkqSgcEqIJEmacJMnT+bkyZNkZGTQ1NREa2srAK9fv+bw4cORuyYSEhJYuXIllZWVDA8PR44vLy9n7969AOTl5UWmnezevTtS8/HjRyoqKkhPTychIYF58+ZRWFhIW1vb+A5WkiSNiXdYSJKkwNiyZQutra3cuXOHjIwM2tvbuXnzJhs2bCA5OZmhoSGampo4fvw4L168oLKyEvgaUnR3d1NTU0NJSQkpKSkAJCcnAzA0NMSmTZt4+PAhhYWF7Ny5k/7+fi5fvsz69eu5ffs2S5cunbBxS5Kk0QwsJElSYPzxxx8APH/+HIBVq1bR1tbGpEmTIjV79uxh165dXLlyhQMHDpCYmMjixYtZsWIFNTU1hEIh1qxZ80271dXV3L9/n+vXr5OTkxPZv337djIzMyktLaWxsXEcRihJksbKKSGSJCkwZsyYAcDbt28BmDZtWiSs+PTpE729vYTDYXJycvjy5QuPHj0aU7vXrl0jJSWF1NRUwuFwZBsaGiIUCvHgwQMGBwf/nkFJkqSf4h0WkiQpMPr7+wGIjY0F4PPnz5w+fZq6ujo6OztHPfa0r69vTO0+e/aMwcFBFixY8N2acDjM3Llzf7LnkiTpdzOwkCRJgdHe3g7AwoULATh06BDV1dVs3LiRkpISZs2axZQpU2hra+PYsWN8+fJlTO2OjIywaNEiysrKvlszc+bMXx+AJEn6bQwsJElSYNTW1gKQm5sLQH19PZmZmVy8ePGbus7OzlHH/vc6F381f/58wuEwWVlZREU5I1aSpH8C/7ElSdKEGx4eprS0lNbWVnJzc0lPTwe+Pu70r9NA3r17x9mzZ0e1ERMTA0Bvb++o94qKiuju7qaqqup/fn5PT8+vDkGSJP1m3mEhSZLGVVtbG/X19QAMDAzQ0dFBY2MjXV1dZGdnc/78+Uhtfn4+ly5dYtu2bYRCIXp6eqitrSU+Pn5Uu2lpaURFRVFRUUFfXx8xMTEkJSWxfPlyiouLuXv3LkeOHKG5uZmsrCxiY2N59eoV9+7dIzo6mlu3bo3bOZAkST82qa+vb+THZZIkSb+mpaWFvLy8yOuoqCimT5/O7NmzSU1NpaCggLVr135zzPv37ykvL+fGjRu8efOGOXPmsHXrVtLS0sjPz6eqqorNmzdH6q9evcqZM2fo7OxkaGiIoqIizp07B3xdwPPChQvU19fz9OlTABITE1m2bBlFRUVkZ2ePw1mQJEljZWAhSZIkSZICxzUsJEmSJElS4BhYSJIkSZKkwDGwkCRJkiRJgWNgIUmSJEmSAsfAQpIkSZIkBY6BhSRJkiRJChwDC0mSJEmSFDgGFpIkSZIkKXAMLCRJkiRJUuAYWEiSJEmSpMD5E7L9Gt9NX0w7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = predicted_train.reshape(X_train.shape[0]).tolist()\n",
    "b = predicted_valid.reshape(X_valid.shape[0]).tolist()\n",
    "c = predicted_test.reshape(X_test.shape[0]).tolist()\n",
    "d = y_train.reshape(X_train.shape[0]).tolist()\n",
    "e = y_valid.reshape(X_valid.shape[0]).tolist()\n",
    "f = y_test.reshape(X_test.shape[0]).tolist()\n",
    "a.extend(b)\n",
    "a.extend(c)\n",
    "d.extend(e)\n",
    "d.extend(f)\n",
    "predictFrame = pd.DataFrame({'prediction': a, 'true_value': d})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1988 samples, validate on 217 samples\n",
      "Epoch 1/1000\n",
      "1988/1988 [==============================] - 3s 1ms/step - loss: 575.2840 - mae: 20.8943 - val_loss: 2725.3807 - val_mae: 48.0904\n",
      "Epoch 2/1000\n",
      "1988/1988 [==============================] - 2s 808us/step - loss: 1527.2857 - mae: 28.7414 - val_loss: 4243.5641 - val_mae: 61.8941\n",
      "Epoch 3/1000\n",
      "1988/1988 [==============================] - 2s 844us/step - loss: 1412.2627 - mae: 28.6492 - val_loss: 4438.4577 - val_mae: 63.4489\n",
      "Epoch 4/1000\n",
      "1988/1988 [==============================] - 2s 857us/step - loss: 1405.7181 - mae: 28.6899 - val_loss: 4626.6545 - val_mae: 64.9150\n",
      "Epoch 5/1000\n",
      "1988/1988 [==============================] - 2s 905us/step - loss: 1426.5238 - mae: 28.8789 - val_loss: 4616.9133 - val_mae: 64.8400\n",
      "Epoch 6/1000\n",
      "1988/1988 [==============================] - 2s 942us/step - loss: 1448.8211 - mae: 29.1882 - val_loss: 4826.8235 - val_mae: 66.4390\n",
      "Epoch 7/1000\n",
      "1988/1988 [==============================] - 2s 908us/step - loss: 1416.6272 - mae: 28.9626 - val_loss: 4937.0253 - val_mae: 67.2632\n",
      "Epoch 8/1000\n",
      "1988/1988 [==============================] - 2s 916us/step - loss: 1456.0707 - mae: 29.5376 - val_loss: 5108.5675 - val_mae: 68.5265\n",
      "Epoch 9/1000\n",
      "1988/1988 [==============================] - 2s 935us/step - loss: 1392.1692 - mae: 28.9168 - val_loss: 5048.8502 - val_mae: 68.0893\n",
      "Epoch 10/1000\n",
      "1988/1988 [==============================] - 2s 899us/step - loss: 1404.2505 - mae: 28.9773 - val_loss: 5006.0822 - val_mae: 67.7746\n",
      "Epoch 11/1000\n",
      "1988/1988 [==============================] - 2s 885us/step - loss: 1252.0496 - mae: 26.1543 - val_loss: 4888.8460 - val_mae: 66.9042\n",
      "Epoch 12/1000\n",
      "1988/1988 [==============================] - 2s 845us/step - loss: 1206.0746 - mae: 25.9117 - val_loss: 4348.2083 - val_mae: 62.7338\n",
      "Epoch 13/1000\n",
      "1988/1988 [==============================] - 2s 846us/step - loss: 906.3366 - mae: 22.2817 - val_loss: 3759.3096 - val_mae: 57.8612\n",
      "Epoch 14/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 483.4764 - mae: 16.1154 - val_loss: 2462.4179 - val_mae: 45.2883\n",
      "Epoch 15/1000\n",
      "1988/1988 [==============================] - 2s 840us/step - loss: 328.0421 - mae: 13.9817 - val_loss: 2407.8225 - val_mae: 44.7024\n",
      "Epoch 16/1000\n",
      "1988/1988 [==============================] - 2s 856us/step - loss: 271.7364 - mae: 13.0114 - val_loss: 1844.9932 - val_mae: 37.9268\n",
      "Epoch 17/1000\n",
      "1988/1988 [==============================] - 2s 858us/step - loss: 240.6956 - mae: 12.2181 - val_loss: 1753.3611 - val_mae: 36.7808\n",
      "Epoch 18/1000\n",
      "1988/1988 [==============================] - 2s 873us/step - loss: 224.1750 - mae: 11.5692 - val_loss: 1680.7789 - val_mae: 35.8602\n",
      "Epoch 19/1000\n",
      "1988/1988 [==============================] - 2s 880us/step - loss: 213.6492 - mae: 11.2404 - val_loss: 1550.5553 - val_mae: 34.1924\n",
      "Epoch 20/1000\n",
      "1988/1988 [==============================] - 2s 914us/step - loss: 219.1970 - mae: 11.3527 - val_loss: 1584.6474 - val_mae: 34.9843\n",
      "Epoch 21/1000\n",
      "1988/1988 [==============================] - 2s 974us/step - loss: 192.5656 - mae: 10.6876 - val_loss: 1522.6361 - val_mae: 34.3446\n",
      "Epoch 22/1000\n",
      "1988/1988 [==============================] - 2s 848us/step - loss: 198.8268 - mae: 10.7760 - val_loss: 1114.6155 - val_mae: 28.0463\n",
      "Epoch 23/1000\n",
      "1988/1988 [==============================] - 2s 888us/step - loss: 193.0098 - mae: 10.8995 - val_loss: 915.3519 - val_mae: 24.1321\n",
      "Epoch 24/1000\n",
      "1988/1988 [==============================] - 2s 927us/step - loss: 183.5579 - mae: 10.4131 - val_loss: 983.4996 - val_mae: 25.8599\n",
      "Epoch 25/1000\n",
      "1988/1988 [==============================] - 2s 907us/step - loss: 178.4054 - mae: 10.3413 - val_loss: 1064.4428 - val_mae: 27.4371\n",
      "Epoch 26/1000\n",
      "1988/1988 [==============================] - 2s 978us/step - loss: 187.3702 - mae: 10.5156 - val_loss: 954.0019 - val_mae: 25.5455\n",
      "Epoch 27/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 189.4511 - mae: 10.6409 - val_loss: 948.9060 - val_mae: 25.5864\n",
      "Epoch 28/1000\n",
      "1988/1988 [==============================] - 2s 858us/step - loss: 174.3556 - mae: 10.0524 - val_loss: 915.1591 - val_mae: 24.9486\n",
      "Epoch 29/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 191.4246 - mae: 10.4598 - val_loss: 841.8129 - val_mae: 23.7129\n",
      "Epoch 30/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 184.1963 - mae: 10.2933 - val_loss: 778.5826 - val_mae: 22.2396\n",
      "Epoch 31/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 164.4689 - mae: 9.8340 - val_loss: 819.7864 - val_mae: 22.9462\n",
      "Epoch 32/1000\n",
      "1988/1988 [==============================] - 2s 822us/step - loss: 171.5407 - mae: 9.8877 - val_loss: 828.2539 - val_mae: 23.2102\n",
      "Epoch 33/1000\n",
      "1988/1988 [==============================] - 2s 814us/step - loss: 185.2897 - mae: 10.2297 - val_loss: 787.1535 - val_mae: 23.0923\n",
      "Epoch 34/1000\n",
      "1988/1988 [==============================] - 2s 806us/step - loss: 166.2902 - mae: 9.7807 - val_loss: 801.1722 - val_mae: 23.2071\n",
      "Epoch 35/1000\n",
      "1988/1988 [==============================] - 2s 811us/step - loss: 181.4709 - mae: 10.2146 - val_loss: 736.6776 - val_mae: 22.0111\n",
      "Epoch 36/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 164.8144 - mae: 9.8989 - val_loss: 724.5794 - val_mae: 21.8264\n",
      "Epoch 37/1000\n",
      "1988/1988 [==============================] - 2s 789us/step - loss: 152.4864 - mae: 9.4769 - val_loss: 689.6452 - val_mae: 21.3626\n",
      "Epoch 38/1000\n",
      "1988/1988 [==============================] - 2s 798us/step - loss: 163.9157 - mae: 9.6746 - val_loss: 681.3414 - val_mae: 20.9423\n",
      "Epoch 39/1000\n",
      "1988/1988 [==============================] - 2s 793us/step - loss: 150.3532 - mae: 9.2254 - val_loss: 589.4637 - val_mae: 18.8134\n",
      "Epoch 40/1000\n",
      "1988/1988 [==============================] - 2s 786us/step - loss: 153.1428 - mae: 9.4636 - val_loss: 625.4149 - val_mae: 19.7178\n",
      "Epoch 41/1000\n",
      "1988/1988 [==============================] - 2s 800us/step - loss: 148.8607 - mae: 9.2568 - val_loss: 649.8098 - val_mae: 20.3643\n",
      "Epoch 42/1000\n",
      "1988/1988 [==============================] - 2s 802us/step - loss: 145.9981 - mae: 9.1480 - val_loss: 848.1599 - val_mae: 24.6331\n",
      "Epoch 43/1000\n",
      "1988/1988 [==============================] - 2s 787us/step - loss: 152.5160 - mae: 9.3076 - val_loss: 555.4871 - val_mae: 18.1421\n",
      "Epoch 44/1000\n",
      "1988/1988 [==============================] - 2s 791us/step - loss: 150.4504 - mae: 9.2650 - val_loss: 955.0683 - val_mae: 26.5624\n",
      "Epoch 45/1000\n",
      "1988/1988 [==============================] - 2s 808us/step - loss: 156.3756 - mae: 9.5831 - val_loss: 556.2535 - val_mae: 18.1803\n",
      "Epoch 46/1000\n",
      "1988/1988 [==============================] - 2s 790us/step - loss: 135.6968 - mae: 8.8072 - val_loss: 772.2713 - val_mae: 22.9891\n",
      "Epoch 47/1000\n",
      "1988/1988 [==============================] - 2s 796us/step - loss: 144.9899 - mae: 8.9172 - val_loss: 607.0798 - val_mae: 19.4889\n",
      "Epoch 48/1000\n",
      "1988/1988 [==============================] - 2s 788us/step - loss: 132.0318 - mae: 8.7115 - val_loss: 579.3619 - val_mae: 18.6193\n",
      "Epoch 49/1000\n",
      "1988/1988 [==============================] - 2s 796us/step - loss: 140.3336 - mae: 8.9272 - val_loss: 686.7003 - val_mae: 21.0154\n",
      "Epoch 50/1000\n",
      "1988/1988 [==============================] - 2s 782us/step - loss: 119.2722 - mae: 8.2437 - val_loss: 919.6394 - val_mae: 26.1445\n",
      "Epoch 51/1000\n",
      "1988/1988 [==============================] - 2s 784us/step - loss: 133.4603 - mae: 8.6141 - val_loss: 723.7077 - val_mae: 22.1413\n",
      "Epoch 52/1000\n",
      "1988/1988 [==============================] - 2s 794us/step - loss: 119.7997 - mae: 8.0995 - val_loss: 556.0066 - val_mae: 18.2859\n",
      "Epoch 53/1000\n",
      "1988/1988 [==============================] - 2s 791us/step - loss: 118.9088 - mae: 8.1560 - val_loss: 811.7536 - val_mae: 24.2843\n",
      "Epoch 54/1000\n",
      "1988/1988 [==============================] - 2s 792us/step - loss: 124.7016 - mae: 8.2805 - val_loss: 628.0412 - val_mae: 20.3341\n",
      "Epoch 55/1000\n",
      "1988/1988 [==============================] - 2s 846us/step - loss: 119.8471 - mae: 8.2447 - val_loss: 515.2494 - val_mae: 17.6707\n",
      "Epoch 56/1000\n",
      "1988/1988 [==============================] - 2s 858us/step - loss: 109.7456 - mae: 7.7979 - val_loss: 798.4961 - val_mae: 24.5516\n",
      "Epoch 57/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 120.1879 - mae: 8.1105 - val_loss: 352.0997 - val_mae: 14.0634\n",
      "Epoch 58/1000\n",
      "1988/1988 [==============================] - 2s 861us/step - loss: 114.1028 - mae: 7.9553 - val_loss: 519.4372 - val_mae: 17.5445\n",
      "Epoch 59/1000\n",
      "1988/1988 [==============================] - 2s 816us/step - loss: 101.3887 - mae: 7.5868 - val_loss: 620.5554 - val_mae: 20.1273\n",
      "Epoch 60/1000\n",
      "1988/1988 [==============================] - 2s 820us/step - loss: 104.2099 - mae: 7.6236 - val_loss: 669.5735 - val_mae: 21.8209\n",
      "Epoch 61/1000\n",
      "1988/1988 [==============================] - 2s 891us/step - loss: 101.6176 - mae: 7.4761 - val_loss: 567.5328 - val_mae: 19.3773\n",
      "Epoch 62/1000\n",
      "1988/1988 [==============================] - 2s 844us/step - loss: 102.3481 - mae: 7.5579 - val_loss: 603.6966 - val_mae: 20.2167\n",
      "Epoch 63/1000\n",
      "1988/1988 [==============================] - 2s 870us/step - loss: 95.6727 - mae: 7.2464 - val_loss: 421.8200 - val_mae: 15.5943\n",
      "Epoch 64/1000\n",
      "1988/1988 [==============================] - 2s 852us/step - loss: 95.1068 - mae: 7.1789 - val_loss: 573.0319 - val_mae: 19.2629\n",
      "Epoch 65/1000\n",
      "1988/1988 [==============================] - 2s 843us/step - loss: 97.4625 - mae: 7.2993 - val_loss: 599.7263 - val_mae: 20.5429\n",
      "Epoch 66/1000\n",
      "1988/1988 [==============================] - 2s 831us/step - loss: 93.4833 - mae: 7.2247 - val_loss: 472.8783 - val_mae: 17.6103\n",
      "Epoch 67/1000\n",
      "1988/1988 [==============================] - 2s 861us/step - loss: 88.3903 - mae: 7.0513 - val_loss: 373.6631 - val_mae: 14.7567\n",
      "Epoch 68/1000\n",
      "1988/1988 [==============================] - 2s 835us/step - loss: 77.2830 - mae: 6.4721 - val_loss: 319.0586 - val_mae: 13.4559\n",
      "Epoch 69/1000\n",
      "1988/1988 [==============================] - 2s 824us/step - loss: 85.9826 - mae: 6.8434 - val_loss: 269.9797 - val_mae: 12.3250\n",
      "Epoch 70/1000\n",
      "1988/1988 [==============================] - 2s 807us/step - loss: 75.8954 - mae: 6.4653 - val_loss: 380.5098 - val_mae: 15.1913\n",
      "Epoch 71/1000\n",
      "1988/1988 [==============================] - 2s 797us/step - loss: 84.9631 - mae: 6.9262 - val_loss: 576.8711 - val_mae: 20.2901\n",
      "Epoch 72/1000\n",
      "1988/1988 [==============================] - 2s 820us/step - loss: 77.0293 - mae: 6.6291 - val_loss: 442.6232 - val_mae: 17.2297\n",
      "Epoch 73/1000\n",
      "1988/1988 [==============================] - 2s 881us/step - loss: 79.7763 - mae: 6.6824 - val_loss: 454.4173 - val_mae: 17.5470\n",
      "Epoch 74/1000\n",
      "1988/1988 [==============================] - 2s 903us/step - loss: 73.4634 - mae: 6.3597 - val_loss: 343.2141 - val_mae: 14.2530\n",
      "Epoch 75/1000\n",
      "1988/1988 [==============================] - 2s 856us/step - loss: 74.4271 - mae: 6.3154 - val_loss: 354.5060 - val_mae: 14.7411\n",
      "Epoch 76/1000\n",
      "1988/1988 [==============================] - 2s 890us/step - loss: 73.6025 - mae: 6.3731 - val_loss: 260.6167 - val_mae: 12.1847\n",
      "Epoch 77/1000\n",
      "1988/1988 [==============================] - 2s 904us/step - loss: 73.7140 - mae: 6.5171 - val_loss: 363.8413 - val_mae: 14.5341\n",
      "Epoch 78/1000\n",
      "1988/1988 [==============================] - 2s 933us/step - loss: 65.7095 - mae: 6.0940 - val_loss: 342.2900 - val_mae: 13.9750\n",
      "Epoch 79/1000\n",
      "1988/1988 [==============================] - 2s 943us/step - loss: 64.5478 - mae: 6.0120 - val_loss: 319.9902 - val_mae: 13.4122\n",
      "Epoch 80/1000\n",
      "1988/1988 [==============================] - 2s 979us/step - loss: 68.2767 - mae: 6.1474 - val_loss: 474.4081 - val_mae: 17.4487\n",
      "Epoch 81/1000\n",
      "1988/1988 [==============================] - 2s 998us/step - loss: 67.2669 - mae: 6.0845 - val_loss: 335.3500 - val_mae: 13.8755\n",
      "Epoch 82/1000\n",
      "1988/1988 [==============================] - 2s 992us/step - loss: 64.4050 - mae: 6.0165 - val_loss: 449.7083 - val_mae: 16.6585\n",
      "Epoch 83/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 61.8795 - mae: 5.9323 - val_loss: 413.3314 - val_mae: 15.9402\n",
      "Epoch 84/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 65.2534 - mae: 6.1200 - val_loss: 313.8750 - val_mae: 13.3294\n",
      "Epoch 85/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 62.6292 - mae: 5.9490 - val_loss: 377.6203 - val_mae: 14.8663\n",
      "Epoch 86/1000\n",
      "1988/1988 [==============================] - 2s 982us/step - loss: 60.7443 - mae: 5.8166 - val_loss: 439.1308 - val_mae: 16.7360\n",
      "Epoch 87/1000\n",
      "1988/1988 [==============================] - 2s 990us/step - loss: 60.1973 - mae: 5.9809 - val_loss: 365.2608 - val_mae: 14.8441\n",
      "Epoch 88/1000\n",
      "1988/1988 [==============================] - 2s 922us/step - loss: 62.6068 - mae: 6.0119 - val_loss: 376.0786 - val_mae: 15.2669\n",
      "Epoch 89/1000\n",
      "1988/1988 [==============================] - 2s 900us/step - loss: 58.2008 - mae: 5.7995 - val_loss: 352.9947 - val_mae: 14.5741\n",
      "Epoch 90/1000\n",
      "1988/1988 [==============================] - 2s 889us/step - loss: 55.2960 - mae: 5.6287 - val_loss: 319.3830 - val_mae: 13.8150\n",
      "Epoch 91/1000\n",
      "1988/1988 [==============================] - 2s 907us/step - loss: 60.0293 - mae: 5.8774 - val_loss: 277.6077 - val_mae: 12.6945\n",
      "Epoch 92/1000\n",
      "1988/1988 [==============================] - 2s 994us/step - loss: 55.2051 - mae: 5.5918 - val_loss: 231.0401 - val_mae: 11.5191\n",
      "Epoch 93/1000\n",
      "1988/1988 [==============================] - 2s 983us/step - loss: 54.6228 - mae: 5.5643 - val_loss: 555.0869 - val_mae: 20.0157\n",
      "Epoch 94/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 55.5989 - mae: 5.6953 - val_loss: 329.7889 - val_mae: 14.0623\n",
      "Epoch 95/1000\n",
      "1988/1988 [==============================] - 2s 987us/step - loss: 55.3256 - mae: 5.6960 - val_loss: 323.6237 - val_mae: 14.1179\n",
      "Epoch 96/1000\n",
      "1988/1988 [==============================] - 2s 931us/step - loss: 54.9955 - mae: 5.6033 - val_loss: 393.9079 - val_mae: 16.3569\n",
      "Epoch 97/1000\n",
      "1988/1988 [==============================] - 2s 934us/step - loss: 54.9828 - mae: 5.5424 - val_loss: 181.5935 - val_mae: 10.4039\n",
      "Epoch 98/1000\n",
      "1988/1988 [==============================] - 2s 923us/step - loss: 53.9835 - mae: 5.5227 - val_loss: 408.9043 - val_mae: 16.7289\n",
      "Epoch 99/1000\n",
      "1988/1988 [==============================] - 2s 853us/step - loss: 53.2296 - mae: 5.5509 - val_loss: 311.6808 - val_mae: 13.8768\n",
      "Epoch 100/1000\n",
      "1988/1988 [==============================] - 2s 846us/step - loss: 51.4714 - mae: 5.3050 - val_loss: 226.1838 - val_mae: 11.4454\n",
      "Epoch 101/1000\n",
      "1988/1988 [==============================] - 2s 856us/step - loss: 47.5848 - mae: 5.1841 - val_loss: 240.0250 - val_mae: 12.1618\n",
      "Epoch 102/1000\n",
      "1988/1988 [==============================] - 2s 811us/step - loss: 51.8252 - mae: 5.4638 - val_loss: 297.2303 - val_mae: 13.8758\n",
      "Epoch 103/1000\n",
      "1988/1988 [==============================] - 2s 826us/step - loss: 55.9677 - mae: 5.4975 - val_loss: 415.7007 - val_mae: 17.3975\n",
      "Epoch 104/1000\n",
      "1988/1988 [==============================] - 2s 842us/step - loss: 51.0658 - mae: 5.4767 - val_loss: 274.1623 - val_mae: 13.0137\n",
      "Epoch 105/1000\n",
      "1988/1988 [==============================] - 2s 861us/step - loss: 57.6189 - mae: 5.7231 - val_loss: 165.7184 - val_mae: 9.8019\n",
      "Epoch 106/1000\n",
      "1988/1988 [==============================] - 2s 844us/step - loss: 53.6113 - mae: 5.4538 - val_loss: 241.2722 - val_mae: 12.0567\n",
      "Epoch 107/1000\n",
      "1988/1988 [==============================] - 2s 881us/step - loss: 48.0435 - mae: 5.1470 - val_loss: 252.3378 - val_mae: 12.4098\n",
      "Epoch 108/1000\n",
      "1988/1988 [==============================] - 2s 862us/step - loss: 50.5397 - mae: 5.3418 - val_loss: 260.9226 - val_mae: 12.6327\n",
      "Epoch 109/1000\n",
      "1988/1988 [==============================] - 2s 891us/step - loss: 52.1057 - mae: 5.3996 - val_loss: 273.7823 - val_mae: 12.8985\n",
      "Epoch 110/1000\n",
      "1988/1988 [==============================] - 2s 901us/step - loss: 57.0769 - mae: 5.5546 - val_loss: 449.6491 - val_mae: 17.4285\n",
      "Epoch 111/1000\n",
      "1988/1988 [==============================] - 2s 891us/step - loss: 52.2727 - mae: 5.4149 - val_loss: 292.9083 - val_mae: 13.1084\n",
      "Epoch 112/1000\n",
      "1988/1988 [==============================] - 2s 826us/step - loss: 50.0359 - mae: 5.2861 - val_loss: 264.6891 - val_mae: 12.3060\n",
      "Epoch 113/1000\n",
      "1988/1988 [==============================] - 2s 837us/step - loss: 52.8148 - mae: 5.3462 - val_loss: 342.5574 - val_mae: 14.8683\n",
      "Epoch 114/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 814us/step - loss: 50.8470 - mae: 5.1985 - val_loss: 414.9620 - val_mae: 16.6511\n",
      "Epoch 115/1000\n",
      "1988/1988 [==============================] - 2s 805us/step - loss: 53.0840 - mae: 5.2893 - val_loss: 298.9240 - val_mae: 13.0717\n",
      "Epoch 116/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 46.6570 - mae: 5.0935 - val_loss: 327.2272 - val_mae: 13.7698\n",
      "Epoch 117/1000\n",
      "1988/1988 [==============================] - 2s 844us/step - loss: 47.3292 - mae: 5.0424 - val_loss: 278.1288 - val_mae: 12.5165\n",
      "Epoch 118/1000\n",
      "1988/1988 [==============================] - 2s 821us/step - loss: 49.1002 - mae: 5.0680 - val_loss: 420.0397 - val_mae: 15.8392\n",
      "Epoch 119/1000\n",
      "1988/1988 [==============================] - 2s 828us/step - loss: 51.4463 - mae: 5.2687 - val_loss: 468.9924 - val_mae: 16.9186\n",
      "Epoch 120/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 50.0910 - mae: 5.2064 - val_loss: 368.0618 - val_mae: 14.4380\n",
      "Epoch 121/1000\n",
      "1988/1988 [==============================] - 2s 842us/step - loss: 50.0797 - mae: 5.1861 - val_loss: 340.8943 - val_mae: 13.9535\n",
      "Epoch 122/1000\n",
      "1988/1988 [==============================] - 2s 845us/step - loss: 46.4065 - mae: 5.0106 - val_loss: 413.7265 - val_mae: 15.3606\n",
      "Epoch 123/1000\n",
      "1988/1988 [==============================] - 2s 870us/step - loss: 47.8151 - mae: 5.1476 - val_loss: 363.5672 - val_mae: 14.1887\n",
      "Epoch 124/1000\n",
      "1988/1988 [==============================] - 2s 894us/step - loss: 52.6860 - mae: 5.2937 - val_loss: 425.2334 - val_mae: 15.6183\n",
      "Epoch 125/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 47.3669 - mae: 4.9775 - val_loss: 264.2939 - val_mae: 12.2918\n",
      "Epoch 126/1000\n",
      "1988/1988 [==============================] - 2s 961us/step - loss: 46.3713 - mae: 4.9449 - val_loss: 463.7371 - val_mae: 17.8617\n",
      "Epoch 127/1000\n",
      "1988/1988 [==============================] - 2s 959us/step - loss: 47.7157 - mae: 4.9668 - val_loss: 391.4021 - val_mae: 16.2882\n",
      "Epoch 128/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 47.0228 - mae: 5.0306 - val_loss: 190.5500 - val_mae: 10.3790\n",
      "Epoch 129/1000\n",
      "1988/1988 [==============================] - 2s 973us/step - loss: 45.1086 - mae: 4.9078 - val_loss: 428.7646 - val_mae: 16.9610\n",
      "Epoch 130/1000\n",
      "1988/1988 [==============================] - 2s 909us/step - loss: 50.2178 - mae: 5.1164 - val_loss: 286.5809 - val_mae: 12.7545\n",
      "Epoch 131/1000\n",
      "1988/1988 [==============================] - 2s 937us/step - loss: 46.3172 - mae: 5.0028 - val_loss: 284.9162 - val_mae: 12.6983\n",
      "Epoch 132/1000\n",
      "1988/1988 [==============================] - 2s 940us/step - loss: 44.4900 - mae: 4.9325 - val_loss: 334.1396 - val_mae: 13.8963\n",
      "Epoch 133/1000\n",
      "1988/1988 [==============================] - 2s 844us/step - loss: 45.6080 - mae: 4.9381 - val_loss: 395.1481 - val_mae: 15.6953\n",
      "Epoch 134/1000\n",
      "1988/1988 [==============================] - 2s 852us/step - loss: 43.5006 - mae: 4.7401 - val_loss: 304.4053 - val_mae: 13.0886\n",
      "Epoch 135/1000\n",
      "1988/1988 [==============================] - 2s 833us/step - loss: 47.0509 - mae: 5.0124 - val_loss: 249.6764 - val_mae: 11.7325\n",
      "Epoch 136/1000\n",
      "1988/1988 [==============================] - 2s 883us/step - loss: 43.3239 - mae: 4.7696 - val_loss: 225.0753 - val_mae: 11.2567\n",
      "Epoch 137/1000\n",
      "1988/1988 [==============================] - 2s 900us/step - loss: 41.5847 - mae: 4.6412 - val_loss: 254.9928 - val_mae: 11.9697\n",
      "Epoch 138/1000\n",
      "1988/1988 [==============================] - 2s 892us/step - loss: 50.0066 - mae: 5.1516 - val_loss: 422.4479 - val_mae: 16.5049\n",
      "Epoch 139/1000\n",
      "1988/1988 [==============================] - 2s 986us/step - loss: 45.9103 - mae: 4.9436 - val_loss: 253.3115 - val_mae: 12.0114\n",
      "Epoch 140/1000\n",
      "1988/1988 [==============================] - 2s 910us/step - loss: 45.9043 - mae: 4.9979 - val_loss: 267.8371 - val_mae: 12.6232\n",
      "Epoch 141/1000\n",
      "1988/1988 [==============================] - 2s 888us/step - loss: 46.0101 - mae: 4.8707 - val_loss: 338.1821 - val_mae: 14.2403\n",
      "Epoch 142/1000\n",
      "1988/1988 [==============================] - 2s 855us/step - loss: 48.7184 - mae: 5.0037 - val_loss: 201.2419 - val_mae: 10.6306\n",
      "Epoch 143/1000\n",
      "1988/1988 [==============================] - 2s 834us/step - loss: 43.4489 - mae: 4.7380 - val_loss: 344.1471 - val_mae: 14.4588\n",
      "Epoch 144/1000\n",
      "1988/1988 [==============================] - 2s 805us/step - loss: 43.9128 - mae: 4.7765 - val_loss: 284.5528 - val_mae: 12.6794\n",
      "Epoch 145/1000\n",
      "1988/1988 [==============================] - 2s 801us/step - loss: 43.4922 - mae: 4.8583 - val_loss: 272.0104 - val_mae: 12.4445\n",
      "Epoch 146/1000\n",
      "1988/1988 [==============================] - 2s 807us/step - loss: 48.8183 - mae: 5.0615 - val_loss: 304.4377 - val_mae: 13.2704\n",
      "Epoch 147/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 46.5573 - mae: 4.8790 - val_loss: 425.9965 - val_mae: 17.0842\n",
      "Epoch 148/1000\n",
      "1988/1988 [==============================] - 2s 815us/step - loss: 43.3687 - mae: 4.7972 - val_loss: 305.0005 - val_mae: 13.4518\n",
      "Epoch 149/1000\n",
      "1988/1988 [==============================] - 2s 817us/step - loss: 46.1522 - mae: 4.9044 - val_loss: 304.8295 - val_mae: 13.3043\n",
      "Epoch 150/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 44.5087 - mae: 4.7528 - val_loss: 191.3199 - val_mae: 10.4680\n",
      "Epoch 151/1000\n",
      "1988/1988 [==============================] - 2s 800us/step - loss: 47.2475 - mae: 4.9564 - val_loss: 212.3302 - val_mae: 11.0172\n",
      "Epoch 152/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 45.3371 - mae: 4.7506 - val_loss: 405.4351 - val_mae: 16.4240\n",
      "Epoch 153/1000\n",
      "1988/1988 [==============================] - 2s 795us/step - loss: 42.6672 - mae: 4.6604 - val_loss: 368.2082 - val_mae: 15.0075\n",
      "Epoch 154/1000\n",
      "1988/1988 [==============================] - 2s 800us/step - loss: 51.3055 - mae: 5.1458 - val_loss: 223.1395 - val_mae: 11.3046\n",
      "Epoch 155/1000\n",
      "1988/1988 [==============================] - 2s 806us/step - loss: 48.8549 - mae: 5.0109 - val_loss: 275.9649 - val_mae: 12.8042\n",
      "Epoch 156/1000\n",
      "1988/1988 [==============================] - 2s 813us/step - loss: 43.3110 - mae: 4.7327 - val_loss: 354.7380 - val_mae: 15.3695\n",
      "Epoch 157/1000\n",
      "1988/1988 [==============================] - 2s 806us/step - loss: 41.3778 - mae: 4.6607 - val_loss: 264.8132 - val_mae: 12.5204\n",
      "Epoch 158/1000\n",
      "1988/1988 [==============================] - 2s 810us/step - loss: 43.6434 - mae: 4.7817 - val_loss: 335.1906 - val_mae: 14.6880\n",
      "Epoch 159/1000\n",
      "1988/1988 [==============================] - 2s 797us/step - loss: 45.1920 - mae: 4.8164 - val_loss: 184.3281 - val_mae: 10.3467\n",
      "Epoch 160/1000\n",
      "1988/1988 [==============================] - 2s 820us/step - loss: 46.6084 - mae: 4.8598 - val_loss: 215.4298 - val_mae: 11.4493\n",
      "Epoch 161/1000\n",
      "1988/1988 [==============================] - 2s 817us/step - loss: 44.6828 - mae: 4.8024 - val_loss: 212.7473 - val_mae: 11.2985\n",
      "Epoch 162/1000\n",
      "1988/1988 [==============================] - 2s 830us/step - loss: 47.2091 - mae: 4.9606 - val_loss: 187.6099 - val_mae: 10.5290\n",
      "Epoch 163/1000\n",
      "1988/1988 [==============================] - 2s 825us/step - loss: 49.4740 - mae: 5.0986 - val_loss: 223.9356 - val_mae: 11.3960\n",
      "Epoch 164/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 47.4820 - mae: 4.8343 - val_loss: 251.6397 - val_mae: 12.3984\n",
      "Epoch 165/1000\n",
      "1988/1988 [==============================] - 2s 831us/step - loss: 42.3397 - mae: 4.6580 - val_loss: 245.3780 - val_mae: 12.5208\n",
      "Epoch 166/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 44.0673 - mae: 4.8131 - val_loss: 191.3507 - val_mae: 10.6926\n",
      "Epoch 167/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 43.5992 - mae: 4.7240 - val_loss: 174.7587 - val_mae: 10.0014\n",
      "Epoch 168/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 42.6903 - mae: 4.7108 - val_loss: 264.3093 - val_mae: 13.3576\n",
      "Epoch 169/1000\n",
      "1988/1988 [==============================] - 2s 800us/step - loss: 44.1843 - mae: 4.6982 - val_loss: 216.0079 - val_mae: 11.7074\n",
      "Epoch 170/1000\n",
      "1988/1988 [==============================] - 2s 773us/step - loss: 46.9376 - mae: 4.8869 - val_loss: 331.8490 - val_mae: 15.2334\n",
      "Epoch 171/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 774us/step - loss: 43.2120 - mae: 4.7755 - val_loss: 267.9324 - val_mae: 13.0125\n",
      "Epoch 172/1000\n",
      "1988/1988 [==============================] - 2s 776us/step - loss: 46.5919 - mae: 4.9378 - val_loss: 351.2515 - val_mae: 15.9660\n",
      "Epoch 173/1000\n",
      "1988/1988 [==============================] - 2s 801us/step - loss: 45.6448 - mae: 4.8161 - val_loss: 104.1548 - val_mae: 8.0744\n",
      "Epoch 174/1000\n",
      "1988/1988 [==============================] - 2s 793us/step - loss: 45.2986 - mae: 4.8641 - val_loss: 150.6906 - val_mae: 9.4072\n",
      "Epoch 175/1000\n",
      "1988/1988 [==============================] - 2s 813us/step - loss: 46.5171 - mae: 4.8418 - val_loss: 300.5973 - val_mae: 14.6119\n",
      "Epoch 176/1000\n",
      "1988/1988 [==============================] - 2s 817us/step - loss: 46.8622 - mae: 4.9061 - val_loss: 185.4872 - val_mae: 10.4161\n",
      "Epoch 177/1000\n",
      "1988/1988 [==============================] - 2s 822us/step - loss: 48.1115 - mae: 4.9010 - val_loss: 319.2247 - val_mae: 14.2123\n",
      "Epoch 178/1000\n",
      "1988/1988 [==============================] - 2s 835us/step - loss: 45.5984 - mae: 4.8708 - val_loss: 186.0532 - val_mae: 10.5564\n",
      "Epoch 179/1000\n",
      "1988/1988 [==============================] - 2s 821us/step - loss: 42.8356 - mae: 4.7126 - val_loss: 322.4841 - val_mae: 14.8052\n",
      "Epoch 180/1000\n",
      "1988/1988 [==============================] - 2s 839us/step - loss: 43.2895 - mae: 4.7724 - val_loss: 203.8554 - val_mae: 11.1215\n",
      "Epoch 181/1000\n",
      "1988/1988 [==============================] - 2s 840us/step - loss: 45.6136 - mae: 4.7802 - val_loss: 157.1065 - val_mae: 9.6565\n",
      "Epoch 182/1000\n",
      "1988/1988 [==============================] - 2s 809us/step - loss: 40.2856 - mae: 4.5120 - val_loss: 464.8472 - val_mae: 18.1235\n",
      "Epoch 183/1000\n",
      "1988/1988 [==============================] - 2s 797us/step - loss: 45.0813 - mae: 4.7899 - val_loss: 444.8961 - val_mae: 17.3661\n",
      "Epoch 184/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 44.4470 - mae: 4.7774 - val_loss: 264.9180 - val_mae: 12.8745\n",
      "Epoch 185/1000\n",
      "1988/1988 [==============================] - 2s 800us/step - loss: 44.5648 - mae: 4.7488 - val_loss: 180.1744 - val_mae: 10.2721\n",
      "Epoch 186/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 44.9676 - mae: 4.8013 - val_loss: 217.2410 - val_mae: 11.3016\n",
      "Epoch 187/1000\n",
      "1988/1988 [==============================] - 2s 807us/step - loss: 49.7891 - mae: 5.0632 - val_loss: 184.0115 - val_mae: 10.5509\n",
      "Epoch 188/1000\n",
      "1988/1988 [==============================] - 2s 790us/step - loss: 46.5395 - mae: 4.8372 - val_loss: 165.3661 - val_mae: 9.8224\n",
      "Epoch 189/1000\n",
      "1988/1988 [==============================] - 2s 774us/step - loss: 48.3699 - mae: 5.0108 - val_loss: 183.9561 - val_mae: 10.9084\n",
      "Epoch 190/1000\n",
      "1988/1988 [==============================] - 2s 780us/step - loss: 42.6877 - mae: 4.7874 - val_loss: 196.3059 - val_mae: 10.7999\n",
      "Epoch 191/1000\n",
      "1988/1988 [==============================] - 2s 791us/step - loss: 46.8907 - mae: 5.0224 - val_loss: 283.8417 - val_mae: 13.8631\n",
      "Epoch 192/1000\n",
      "1988/1988 [==============================] - 2s 798us/step - loss: 45.5201 - mae: 4.9116 - val_loss: 144.8876 - val_mae: 9.4072\n",
      "Epoch 193/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 45.8871 - mae: 4.9104 - val_loss: 404.3190 - val_mae: 17.4596\n",
      "Epoch 194/1000\n",
      "1988/1988 [==============================] - 2s 840us/step - loss: 46.0648 - mae: 4.9049 - val_loss: 155.0018 - val_mae: 9.7208\n",
      "Epoch 195/1000\n",
      "1988/1988 [==============================] - 2s 829us/step - loss: 45.7916 - mae: 4.8029 - val_loss: 126.1413 - val_mae: 8.8058\n",
      "Epoch 196/1000\n",
      "1988/1988 [==============================] - 2s 828us/step - loss: 44.5938 - mae: 4.8642 - val_loss: 246.8502 - val_mae: 13.1929\n",
      "Epoch 197/1000\n",
      "1988/1988 [==============================] - 2s 829us/step - loss: 40.5822 - mae: 4.6477 - val_loss: 206.6218 - val_mae: 11.3947\n",
      "Epoch 198/1000\n",
      "1988/1988 [==============================] - 2s 832us/step - loss: 43.9408 - mae: 4.8779 - val_loss: 218.7365 - val_mae: 11.7901\n",
      "Epoch 199/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 43.3080 - mae: 4.6914 - val_loss: 195.8385 - val_mae: 11.2962\n",
      "Epoch 200/1000\n",
      "1988/1988 [==============================] - 2s 806us/step - loss: 41.8479 - mae: 4.6867 - val_loss: 219.6755 - val_mae: 11.9564\n",
      "Epoch 201/1000\n",
      "1988/1988 [==============================] - 2s 858us/step - loss: 46.7867 - mae: 4.9092 - val_loss: 425.9376 - val_mae: 17.2621\n",
      "Epoch 202/1000\n",
      "1988/1988 [==============================] - 2s 804us/step - loss: 47.2688 - mae: 5.0832 - val_loss: 183.5553 - val_mae: 10.9007\n",
      "Epoch 203/1000\n",
      "1988/1988 [==============================] - 2s 773us/step - loss: 45.6838 - mae: 4.8886 - val_loss: 172.1836 - val_mae: 10.4765\n",
      "Epoch 204/1000\n",
      "1988/1988 [==============================] - 2s 782us/step - loss: 41.1627 - mae: 4.6187 - val_loss: 179.0371 - val_mae: 10.5533\n",
      "Epoch 205/1000\n",
      "1988/1988 [==============================] - 2s 770us/step - loss: 42.3026 - mae: 4.6620 - val_loss: 278.5126 - val_mae: 14.4091\n",
      "Epoch 206/1000\n",
      "1988/1988 [==============================] - 2s 797us/step - loss: 39.3233 - mae: 4.6188 - val_loss: 197.1181 - val_mae: 11.1014\n",
      "Epoch 207/1000\n",
      "1988/1988 [==============================] - 2s 806us/step - loss: 44.6019 - mae: 4.8679 - val_loss: 161.8756 - val_mae: 10.0011\n",
      "Epoch 208/1000\n",
      "1988/1988 [==============================] - 2s 801us/step - loss: 41.0198 - mae: 4.5794 - val_loss: 201.9675 - val_mae: 11.7362\n",
      "Epoch 209/1000\n",
      "1988/1988 [==============================] - 2s 818us/step - loss: 44.3022 - mae: 4.8515 - val_loss: 126.2374 - val_mae: 8.8920\n",
      "Epoch 210/1000\n",
      "1988/1988 [==============================] - 2s 830us/step - loss: 45.6414 - mae: 4.7848 - val_loss: 89.8387 - val_mae: 7.4621\n",
      "Epoch 211/1000\n",
      "1988/1988 [==============================] - 2s 823us/step - loss: 42.8192 - mae: 4.6650 - val_loss: 139.0795 - val_mae: 9.2615\n",
      "Epoch 212/1000\n",
      "1988/1988 [==============================] - 2s 820us/step - loss: 45.2396 - mae: 4.9601 - val_loss: 475.2484 - val_mae: 17.2870\n",
      "Epoch 213/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 51.7920 - mae: 5.2466 - val_loss: 417.3902 - val_mae: 16.2002\n",
      "Epoch 214/1000\n",
      "1988/1988 [==============================] - 2s 817us/step - loss: 42.5240 - mae: 4.8203 - val_loss: 421.2344 - val_mae: 17.0929\n",
      "Epoch 215/1000\n",
      "1988/1988 [==============================] - 2s 795us/step - loss: 43.6957 - mae: 4.7605 - val_loss: 257.1481 - val_mae: 12.5416\n",
      "Epoch 216/1000\n",
      "1988/1988 [==============================] - 2s 802us/step - loss: 45.8642 - mae: 4.8112 - val_loss: 223.1139 - val_mae: 11.4487\n",
      "Epoch 217/1000\n",
      "1988/1988 [==============================] - 2s 801us/step - loss: 42.1289 - mae: 4.8065 - val_loss: 343.7897 - val_mae: 15.8559\n",
      "Epoch 218/1000\n",
      "1988/1988 [==============================] - 2s 792us/step - loss: 43.8161 - mae: 4.7959 - val_loss: 515.4278 - val_mae: 18.8374\n",
      "Epoch 219/1000\n",
      "1988/1988 [==============================] - 2s 771us/step - loss: 44.6661 - mae: 4.8723 - val_loss: 430.6046 - val_mae: 16.6998\n",
      "Epoch 220/1000\n",
      "1988/1988 [==============================] - 2s 780us/step - loss: 46.3257 - mae: 4.8918 - val_loss: 505.1743 - val_mae: 19.1389\n",
      "Epoch 221/1000\n",
      "1988/1988 [==============================] - 2s 771us/step - loss: 42.7723 - mae: 4.6781 - val_loss: 220.3993 - val_mae: 11.2551\n",
      "Epoch 222/1000\n",
      "1988/1988 [==============================] - 2s 796us/step - loss: 44.1164 - mae: 4.7757 - val_loss: 323.1398 - val_mae: 14.5337\n",
      "Epoch 223/1000\n",
      "1988/1988 [==============================] - 2s 800us/step - loss: 41.4797 - mae: 4.5663 - val_loss: 266.0256 - val_mae: 12.4104\n",
      "Epoch 224/1000\n",
      "1988/1988 [==============================] - 2s 804us/step - loss: 43.0036 - mae: 4.6868 - val_loss: 340.6611 - val_mae: 14.6213\n",
      "Epoch 225/1000\n",
      "1988/1988 [==============================] - 2s 819us/step - loss: 42.6241 - mae: 4.6494 - val_loss: 529.1172 - val_mae: 18.9770\n",
      "Epoch 226/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 40.8044 - mae: 4.6340 - val_loss: 372.1483 - val_mae: 14.9206\n",
      "Epoch 227/1000\n",
      "1988/1988 [==============================] - 2s 837us/step - loss: 44.4268 - mae: 4.7468 - val_loss: 551.3401 - val_mae: 19.0313\n",
      "Epoch 228/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 824us/step - loss: 43.6503 - mae: 4.6421 - val_loss: 361.3229 - val_mae: 14.4325\n",
      "Epoch 229/1000\n",
      "1988/1988 [==============================] - 2s 837us/step - loss: 47.3164 - mae: 4.9496 - val_loss: 370.4716 - val_mae: 15.4245\n",
      "Epoch 230/1000\n",
      "1988/1988 [==============================] - 2s 821us/step - loss: 44.6315 - mae: 4.9429 - val_loss: 342.5498 - val_mae: 14.0717\n",
      "Epoch 231/1000\n",
      "1988/1988 [==============================] - 2s 796us/step - loss: 44.7167 - mae: 4.7547 - val_loss: 398.4043 - val_mae: 16.1382\n",
      "Epoch 232/1000\n",
      "1988/1988 [==============================] - 2s 802us/step - loss: 40.0103 - mae: 4.4974 - val_loss: 363.2795 - val_mae: 14.6893\n",
      "Epoch 233/1000\n",
      "1988/1988 [==============================] - 2s 809us/step - loss: 46.2409 - mae: 4.7577 - val_loss: 353.7340 - val_mae: 14.4088\n",
      "Epoch 234/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 43.0084 - mae: 4.7195 - val_loss: 376.7133 - val_mae: 15.7552\n",
      "Epoch 235/1000\n",
      "1988/1988 [==============================] - 2s 794us/step - loss: 41.0154 - mae: 4.5672 - val_loss: 228.7231 - val_mae: 11.7599\n",
      "Epoch 236/1000\n",
      "1988/1988 [==============================] - 2s 775us/step - loss: 40.6470 - mae: 4.5676 - val_loss: 298.2914 - val_mae: 13.7828\n",
      "Epoch 237/1000\n",
      "1988/1988 [==============================] - 2s 775us/step - loss: 39.2015 - mae: 4.5067 - val_loss: 354.6076 - val_mae: 15.3097\n",
      "Epoch 238/1000\n",
      "1988/1988 [==============================] - 2s 777us/step - loss: 40.4366 - mae: 4.6315 - val_loss: 380.0501 - val_mae: 16.4696\n",
      "Epoch 239/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 36.8586 - mae: 4.4049 - val_loss: 357.4895 - val_mae: 15.7326\n",
      "Epoch 240/1000\n",
      "1988/1988 [==============================] - 2s 919us/step - loss: 43.2382 - mae: 4.7183 - val_loss: 241.5763 - val_mae: 12.0695\n",
      "Epoch 241/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 44.5460 - mae: 4.6931 - val_loss: 311.9868 - val_mae: 13.6800\n",
      "Epoch 242/1000\n",
      "1988/1988 [==============================] - 2s 881us/step - loss: 42.0427 - mae: 4.6038 - val_loss: 482.0253 - val_mae: 16.9620\n",
      "Epoch 243/1000\n",
      "1988/1988 [==============================] - 2s 885us/step - loss: 42.3867 - mae: 4.7753 - val_loss: 313.6413 - val_mae: 13.4728\n",
      "Epoch 244/1000\n",
      "1988/1988 [==============================] - 2s 873us/step - loss: 41.2074 - mae: 4.6044 - val_loss: 497.3863 - val_mae: 18.6382\n",
      "Epoch 245/1000\n",
      "1988/1988 [==============================] - 2s 860us/step - loss: 43.1358 - mae: 4.7246 - val_loss: 375.1034 - val_mae: 14.7298\n",
      "Epoch 246/1000\n",
      "1988/1988 [==============================] - 2s 860us/step - loss: 47.3153 - mae: 4.9704 - val_loss: 352.4354 - val_mae: 15.2048\n",
      "Epoch 247/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 43.4967 - mae: 4.7388 - val_loss: 275.3096 - val_mae: 12.9678\n",
      "Epoch 248/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 46.8841 - mae: 4.9596 - val_loss: 334.3615 - val_mae: 14.3331\n",
      "Epoch 249/1000\n",
      "1988/1988 [==============================] - 2s 840us/step - loss: 42.2266 - mae: 4.6709 - val_loss: 413.0798 - val_mae: 16.7428\n",
      "Epoch 250/1000\n",
      "1988/1988 [==============================] - 2s 855us/step - loss: 44.0224 - mae: 4.7474 - val_loss: 373.3129 - val_mae: 15.1789\n",
      "Epoch 251/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 40.3840 - mae: 4.5543 - val_loss: 331.4614 - val_mae: 14.1788\n",
      "Epoch 252/1000\n",
      "1988/1988 [==============================] - 2s 848us/step - loss: 42.7495 - mae: 4.6351 - val_loss: 238.3329 - val_mae: 11.8565\n",
      "Epoch 253/1000\n",
      "1988/1988 [==============================] - 2s 893us/step - loss: 42.7442 - mae: 4.6502 - val_loss: 414.0892 - val_mae: 16.3203\n",
      "Epoch 254/1000\n",
      "1988/1988 [==============================] - 2s 845us/step - loss: 40.2026 - mae: 4.5805 - val_loss: 277.1875 - val_mae: 13.0059\n",
      "Epoch 255/1000\n",
      "1988/1988 [==============================] - 2s 847us/step - loss: 43.2999 - mae: 4.5877 - val_loss: 261.7094 - val_mae: 12.9148\n",
      "Epoch 256/1000\n",
      "1988/1988 [==============================] - 2s 955us/step - loss: 39.7956 - mae: 4.5427 - val_loss: 341.3193 - val_mae: 14.7348\n",
      "Epoch 257/1000\n",
      "1988/1988 [==============================] - 2s 957us/step - loss: 39.5488 - mae: 4.5219 - val_loss: 116.9972 - val_mae: 8.3316\n",
      "Epoch 258/1000\n",
      "1988/1988 [==============================] - 2s 903us/step - loss: 42.0604 - mae: 4.6421 - val_loss: 285.7837 - val_mae: 13.8549\n",
      "Epoch 259/1000\n",
      "1988/1988 [==============================] - 2s 903us/step - loss: 41.7838 - mae: 4.6521 - val_loss: 110.3296 - val_mae: 8.1128\n",
      "Epoch 260/1000\n",
      "1988/1988 [==============================] - 2s 895us/step - loss: 41.2315 - mae: 4.6149 - val_loss: 505.5925 - val_mae: 19.9344\n",
      "Epoch 261/1000\n",
      "1988/1988 [==============================] - 2s 890us/step - loss: 38.6931 - mae: 4.5392 - val_loss: 270.0560 - val_mae: 12.9662\n",
      "Epoch 262/1000\n",
      "1988/1988 [==============================] - 2s 918us/step - loss: 47.8733 - mae: 5.1458 - val_loss: 210.2880 - val_mae: 11.4543\n",
      "Epoch 263/1000\n",
      "1988/1988 [==============================] - 2s 969us/step - loss: 39.7980 - mae: 4.5898 - val_loss: 218.8501 - val_mae: 11.4347\n",
      "Epoch 264/1000\n",
      "1988/1988 [==============================] - 2s 903us/step - loss: 42.6878 - mae: 4.6933 - val_loss: 354.8134 - val_mae: 15.4941\n",
      "Epoch 265/1000\n",
      "1988/1988 [==============================] - 2s 925us/step - loss: 43.9528 - mae: 4.6992 - val_loss: 472.4256 - val_mae: 17.3113\n",
      "Epoch 266/1000\n",
      "1988/1988 [==============================] - 2s 921us/step - loss: 39.9517 - mae: 4.6884 - val_loss: 296.5402 - val_mae: 13.4862\n",
      "Epoch 267/1000\n",
      "1988/1988 [==============================] - 2s 913us/step - loss: 42.1118 - mae: 4.6573 - val_loss: 345.1295 - val_mae: 14.6742\n",
      "Epoch 268/1000\n",
      "1988/1988 [==============================] - 2s 890us/step - loss: 42.4709 - mae: 4.5965 - val_loss: 386.3373 - val_mae: 15.8131\n",
      "Epoch 269/1000\n",
      "1988/1988 [==============================] - 2s 901us/step - loss: 44.8868 - mae: 4.7228 - val_loss: 315.4107 - val_mae: 14.0886\n",
      "Epoch 270/1000\n",
      "1988/1988 [==============================] - 2s 901us/step - loss: 39.8028 - mae: 4.5096 - val_loss: 234.8914 - val_mae: 11.8270\n",
      "Epoch 271/1000\n",
      "1988/1988 [==============================] - 2s 884us/step - loss: 39.6168 - mae: 4.5998 - val_loss: 334.4601 - val_mae: 15.2836\n",
      "Epoch 272/1000\n",
      "1988/1988 [==============================] - 2s 927us/step - loss: 39.1690 - mae: 4.5721 - val_loss: 187.5935 - val_mae: 10.3886\n",
      "Epoch 273/1000\n",
      "1988/1988 [==============================] - 2s 915us/step - loss: 43.8421 - mae: 4.7866 - val_loss: 285.3349 - val_mae: 14.4467\n",
      "Epoch 274/1000\n",
      "1988/1988 [==============================] - 2s 869us/step - loss: 38.6694 - mae: 4.5075 - val_loss: 322.9399 - val_mae: 15.3358\n",
      "Epoch 275/1000\n",
      "1988/1988 [==============================] - 2s 867us/step - loss: 42.2682 - mae: 4.5851 - val_loss: 295.1020 - val_mae: 13.9453\n",
      "Epoch 276/1000\n",
      "1988/1988 [==============================] - 2s 868us/step - loss: 37.7677 - mae: 4.5076 - val_loss: 347.2149 - val_mae: 15.7017\n",
      "Epoch 277/1000\n",
      "1988/1988 [==============================] - 2s 873us/step - loss: 41.6732 - mae: 4.6397 - val_loss: 260.3777 - val_mae: 12.3634\n",
      "Epoch 278/1000\n",
      "1988/1988 [==============================] - 2s 875us/step - loss: 38.6305 - mae: 4.5270 - val_loss: 279.1641 - val_mae: 12.8240\n",
      "Epoch 279/1000\n",
      "1988/1988 [==============================] - 2s 876us/step - loss: 41.0597 - mae: 4.5907 - val_loss: 188.7882 - val_mae: 10.4451\n",
      "Epoch 280/1000\n",
      "1988/1988 [==============================] - 2s 880us/step - loss: 40.7948 - mae: 4.5110 - val_loss: 229.0618 - val_mae: 12.0071\n",
      "Epoch 281/1000\n",
      "1988/1988 [==============================] - 2s 885us/step - loss: 41.3237 - mae: 4.6018 - val_loss: 160.6020 - val_mae: 9.6342\n",
      "Epoch 282/1000\n",
      "1988/1988 [==============================] - 2s 873us/step - loss: 41.4938 - mae: 4.7589 - val_loss: 516.8727 - val_mae: 18.8655\n",
      "Epoch 283/1000\n",
      "1988/1988 [==============================] - 2s 859us/step - loss: 39.2712 - mae: 4.4753 - val_loss: 271.5666 - val_mae: 12.5816\n",
      "Epoch 284/1000\n",
      "1988/1988 [==============================] - 2s 847us/step - loss: 42.7970 - mae: 4.8040 - val_loss: 316.4070 - val_mae: 14.1898\n",
      "Epoch 285/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 837us/step - loss: 37.3925 - mae: 4.4470 - val_loss: 249.2074 - val_mae: 12.1196\n",
      "Epoch 286/1000\n",
      "1988/1988 [==============================] - 2s 829us/step - loss: 43.0611 - mae: 4.7437 - val_loss: 359.2518 - val_mae: 15.3632\n",
      "Epoch 287/1000\n",
      "1988/1988 [==============================] - 2s 848us/step - loss: 37.2316 - mae: 4.3752 - val_loss: 174.6030 - val_mae: 10.0653\n",
      "Epoch 288/1000\n",
      "1988/1988 [==============================] - 2s 842us/step - loss: 39.8810 - mae: 4.5015 - val_loss: 291.7684 - val_mae: 14.0216\n",
      "Epoch 289/1000\n",
      "1988/1988 [==============================] - 2s 838us/step - loss: 41.1528 - mae: 4.6260 - val_loss: 380.9433 - val_mae: 15.9251\n",
      "Epoch 290/1000\n",
      "1988/1988 [==============================] - 2s 850us/step - loss: 40.9598 - mae: 4.5178 - val_loss: 310.2836 - val_mae: 13.5361\n",
      "Epoch 291/1000\n",
      "1988/1988 [==============================] - 2s 840us/step - loss: 40.0565 - mae: 4.6025 - val_loss: 366.4955 - val_mae: 14.2785\n",
      "Epoch 292/1000\n",
      "1988/1988 [==============================] - 2s 838us/step - loss: 41.8033 - mae: 4.7272 - val_loss: 377.9679 - val_mae: 14.6652\n",
      "Epoch 293/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 41.9740 - mae: 4.7068 - val_loss: 339.2732 - val_mae: 14.6301\n",
      "Epoch 294/1000\n",
      "1988/1988 [==============================] - 2s 836us/step - loss: 42.4548 - mae: 4.6570 - val_loss: 264.4566 - val_mae: 12.0892\n",
      "Epoch 295/1000\n",
      "1988/1988 [==============================] - 2s 863us/step - loss: 42.1113 - mae: 4.6698 - val_loss: 365.9426 - val_mae: 15.1232\n",
      "Epoch 296/1000\n",
      "1988/1988 [==============================] - 2s 867us/step - loss: 40.3264 - mae: 4.6347 - val_loss: 299.3551 - val_mae: 13.3829\n",
      "Epoch 297/1000\n",
      "1988/1988 [==============================] - 2s 869us/step - loss: 39.8235 - mae: 4.5403 - val_loss: 252.8150 - val_mae: 11.9132\n",
      "Epoch 298/1000\n",
      "1988/1988 [==============================] - 2s 872us/step - loss: 41.2714 - mae: 4.6270 - val_loss: 180.6141 - val_mae: 10.0210\n",
      "Epoch 299/1000\n",
      "1988/1988 [==============================] - 2s 880us/step - loss: 39.2019 - mae: 4.4684 - val_loss: 292.6490 - val_mae: 13.4549\n",
      "Epoch 300/1000\n",
      "1988/1988 [==============================] - 2s 898us/step - loss: 37.4177 - mae: 4.3752 - val_loss: 295.1053 - val_mae: 12.8200\n",
      "Epoch 301/1000\n",
      "1988/1988 [==============================] - 2s 882us/step - loss: 40.6407 - mae: 4.7030 - val_loss: 288.8539 - val_mae: 12.9071\n",
      "Epoch 302/1000\n",
      "1988/1988 [==============================] - 2s 873us/step - loss: 38.0798 - mae: 4.4486 - val_loss: 395.0945 - val_mae: 15.7107\n",
      "Epoch 303/1000\n",
      "1988/1988 [==============================] - 2s 852us/step - loss: 39.1041 - mae: 4.4341 - val_loss: 290.0134 - val_mae: 12.9637\n",
      "Epoch 304/1000\n",
      "1988/1988 [==============================] - 2s 856us/step - loss: 35.9170 - mae: 4.3124 - val_loss: 358.5214 - val_mae: 15.2214\n",
      "Epoch 305/1000\n",
      "1988/1988 [==============================] - 2s 880us/step - loss: 40.6661 - mae: 4.5186 - val_loss: 421.6653 - val_mae: 16.4889\n",
      "Epoch 306/1000\n",
      "1988/1988 [==============================] - 2s 860us/step - loss: 42.3114 - mae: 4.6519 - val_loss: 241.5031 - val_mae: 11.6767\n",
      "Epoch 307/1000\n",
      "1988/1988 [==============================] - 2s 864us/step - loss: 40.5597 - mae: 4.5688 - val_loss: 439.0689 - val_mae: 16.5925\n",
      "Epoch 308/1000\n",
      "1988/1988 [==============================] - 2s 860us/step - loss: 40.4346 - mae: 4.5623 - val_loss: 348.7811 - val_mae: 14.0740\n",
      "Epoch 309/1000\n",
      "1988/1988 [==============================] - 2s 905us/step - loss: 37.5827 - mae: 4.4838 - val_loss: 304.0800 - val_mae: 13.4648\n",
      "Epoch 310/1000\n",
      "1988/1988 [==============================] - 2s 895us/step - loss: 37.6914 - mae: 4.3292 - val_loss: 451.2688 - val_mae: 17.9735\n",
      "Epoch 311/1000\n",
      "1988/1988 [==============================] - 2s 895us/step - loss: 40.0057 - mae: 4.5301 - val_loss: 212.5444 - val_mae: 10.9642\n",
      "Epoch 312/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 41.7384 - mae: 4.6123 - val_loss: 284.6243 - val_mae: 12.9388\n",
      "Epoch 313/1000\n",
      "1988/1988 [==============================] - 2s 882us/step - loss: 38.4101 - mae: 4.4383 - val_loss: 185.0899 - val_mae: 10.2496\n",
      "Epoch 314/1000\n",
      "1988/1988 [==============================] - 2s 923us/step - loss: 40.1854 - mae: 4.4889 - val_loss: 298.9076 - val_mae: 13.3228\n",
      "Epoch 315/1000\n",
      "1988/1988 [==============================] - 2s 890us/step - loss: 41.8441 - mae: 4.7420 - val_loss: 232.1257 - val_mae: 11.8720\n",
      "Epoch 316/1000\n",
      "1988/1988 [==============================] - 2s 901us/step - loss: 37.7440 - mae: 4.4505 - val_loss: 328.7471 - val_mae: 14.3933\n",
      "Epoch 317/1000\n",
      "1988/1988 [==============================] - 2s 922us/step - loss: 37.2678 - mae: 4.3374 - val_loss: 278.0196 - val_mae: 13.3147\n",
      "Epoch 318/1000\n",
      "1988/1988 [==============================] - 2s 872us/step - loss: 38.4712 - mae: 4.5095 - val_loss: 376.3051 - val_mae: 16.0468\n",
      "Epoch 319/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 42.5012 - mae: 4.7038 - val_loss: 212.8931 - val_mae: 10.9769\n",
      "Epoch 320/1000\n",
      "1988/1988 [==============================] - 2s 871us/step - loss: 39.0832 - mae: 4.4337 - val_loss: 150.8010 - val_mae: 9.4480\n",
      "Epoch 321/1000\n",
      "1988/1988 [==============================] - 2s 873us/step - loss: 41.0188 - mae: 4.6003 - val_loss: 215.6940 - val_mae: 11.1037\n",
      "Epoch 322/1000\n",
      "1988/1988 [==============================] - 2s 876us/step - loss: 40.4889 - mae: 4.5570 - val_loss: 199.5642 - val_mae: 10.7597\n",
      "Epoch 323/1000\n",
      "1988/1988 [==============================] - 2s 870us/step - loss: 39.0594 - mae: 4.3975 - val_loss: 201.5397 - val_mae: 10.9172\n",
      "Epoch 324/1000\n",
      "1988/1988 [==============================] - 2s 880us/step - loss: 39.1166 - mae: 4.4446 - val_loss: 159.4571 - val_mae: 9.4703\n",
      "Epoch 325/1000\n",
      "1988/1988 [==============================] - 2s 876us/step - loss: 38.5723 - mae: 4.4329 - val_loss: 231.1497 - val_mae: 12.1272\n",
      "Epoch 326/1000\n",
      "1988/1988 [==============================] - 2s 898us/step - loss: 44.5468 - mae: 4.7403 - val_loss: 165.3664 - val_mae: 9.6683\n",
      "Epoch 327/1000\n",
      "1988/1988 [==============================] - 2s 895us/step - loss: 39.1382 - mae: 4.4386 - val_loss: 327.8430 - val_mae: 15.4922\n",
      "Epoch 328/1000\n",
      "1988/1988 [==============================] - 2s 895us/step - loss: 38.3835 - mae: 4.4664 - val_loss: 248.4140 - val_mae: 12.6708\n",
      "Epoch 329/1000\n",
      "1988/1988 [==============================] - 2s 932us/step - loss: 40.4255 - mae: 4.6351 - val_loss: 173.1706 - val_mae: 9.9723\n",
      "Epoch 330/1000\n",
      "1988/1988 [==============================] - 2s 884us/step - loss: 41.1698 - mae: 4.6093 - val_loss: 252.0196 - val_mae: 12.4353\n",
      "Epoch 331/1000\n",
      "1988/1988 [==============================] - 2s 863us/step - loss: 35.9751 - mae: 4.3277 - val_loss: 241.8591 - val_mae: 11.9641\n",
      "Epoch 332/1000\n",
      "1988/1988 [==============================] - 2s 853us/step - loss: 39.7615 - mae: 4.4543 - val_loss: 133.9865 - val_mae: 8.7648\n",
      "Epoch 333/1000\n",
      "1988/1988 [==============================] - 2s 862us/step - loss: 40.8709 - mae: 4.5869 - val_loss: 127.4503 - val_mae: 8.4713\n",
      "Epoch 334/1000\n",
      "1988/1988 [==============================] - 2s 874us/step - loss: 40.5736 - mae: 4.5735 - val_loss: 237.6328 - val_mae: 11.4405\n",
      "Epoch 335/1000\n",
      "1988/1988 [==============================] - 2s 872us/step - loss: 39.3801 - mae: 4.6356 - val_loss: 275.7885 - val_mae: 12.5927\n",
      "Epoch 336/1000\n",
      "1988/1988 [==============================] - 2s 862us/step - loss: 40.4872 - mae: 4.6224 - val_loss: 294.6195 - val_mae: 13.4959\n",
      "Epoch 337/1000\n",
      "1988/1988 [==============================] - 2s 887us/step - loss: 39.8999 - mae: 4.5385 - val_loss: 209.4551 - val_mae: 10.9795\n",
      "Epoch 338/1000\n",
      "1988/1988 [==============================] - 2s 865us/step - loss: 42.8384 - mae: 4.7877 - val_loss: 213.1640 - val_mae: 11.1712\n",
      "Epoch 339/1000\n",
      "1988/1988 [==============================] - 2s 886us/step - loss: 38.3396 - mae: 4.4491 - val_loss: 148.4522 - val_mae: 9.3249\n",
      "Epoch 340/1000\n",
      "1988/1988 [==============================] - 2s 901us/step - loss: 38.9801 - mae: 4.4662 - val_loss: 213.1786 - val_mae: 11.6673\n",
      "Epoch 341/1000\n",
      "1988/1988 [==============================] - 2s 968us/step - loss: 40.2075 - mae: 4.5223 - val_loss: 100.7699 - val_mae: 7.7073\n",
      "Epoch 342/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 909us/step - loss: 38.6966 - mae: 4.5110 - val_loss: 191.3699 - val_mae: 11.1779\n",
      "Epoch 343/1000\n",
      "1988/1988 [==============================] - 2s 915us/step - loss: 38.7269 - mae: 4.4670 - val_loss: 137.9793 - val_mae: 8.9695\n",
      "Epoch 344/1000\n",
      "1988/1988 [==============================] - 2s 895us/step - loss: 40.1360 - mae: 4.5318 - val_loss: 164.2219 - val_mae: 10.3906\n",
      "Epoch 345/1000\n",
      "1988/1988 [==============================] - 2s 909us/step - loss: 39.2767 - mae: 4.4295 - val_loss: 182.1732 - val_mae: 11.1095\n",
      "Epoch 346/1000\n",
      "1988/1988 [==============================] - 2s 906us/step - loss: 38.2850 - mae: 4.3914 - val_loss: 102.8410 - val_mae: 7.8937\n",
      "Epoch 347/1000\n",
      "1988/1988 [==============================] - 2s 893us/step - loss: 36.1342 - mae: 4.3593 - val_loss: 159.4366 - val_mae: 9.9949\n",
      "Epoch 348/1000\n",
      "1988/1988 [==============================] - 2s 891us/step - loss: 39.6666 - mae: 4.5046 - val_loss: 216.1531 - val_mae: 12.0987\n",
      "Epoch 349/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 39.9116 - mae: 4.5083 - val_loss: 158.8639 - val_mae: 10.0289\n",
      "Epoch 350/1000\n",
      "1988/1988 [==============================] - 2s 898us/step - loss: 38.0535 - mae: 4.3609 - val_loss: 110.2788 - val_mae: 8.0548\n",
      "Epoch 351/1000\n",
      "1988/1988 [==============================] - 2s 921us/step - loss: 42.2693 - mae: 4.5925 - val_loss: 313.1625 - val_mae: 15.2340\n",
      "Epoch 352/1000\n",
      "1988/1988 [==============================] - 2s 902us/step - loss: 43.1075 - mae: 4.8387 - val_loss: 436.7101 - val_mae: 16.8939\n",
      "Epoch 353/1000\n",
      "1988/1988 [==============================] - 2s 811us/step - loss: 40.4096 - mae: 4.5720 - val_loss: 295.6921 - val_mae: 13.5548\n",
      "Epoch 354/1000\n",
      "1988/1988 [==============================] - 2s 819us/step - loss: 42.0384 - mae: 4.5076 - val_loss: 112.9389 - val_mae: 8.2971\n",
      "Epoch 355/1000\n",
      "1988/1988 [==============================] - 2s 806us/step - loss: 39.4417 - mae: 4.5685 - val_loss: 368.0380 - val_mae: 15.8097\n",
      "Epoch 356/1000\n",
      "1988/1988 [==============================] - 2s 820us/step - loss: 39.1533 - mae: 4.4790 - val_loss: 244.8445 - val_mae: 11.9637\n",
      "Epoch 357/1000\n",
      "1988/1988 [==============================] - 2s 864us/step - loss: 40.7801 - mae: 4.5183 - val_loss: 163.8299 - val_mae: 9.7283\n",
      "Epoch 358/1000\n",
      "1988/1988 [==============================] - 2s 842us/step - loss: 41.8169 - mae: 4.6531 - val_loss: 245.0095 - val_mae: 12.4143\n",
      "Epoch 359/1000\n",
      "1988/1988 [==============================] - 2s 917us/step - loss: 41.2411 - mae: 4.6729 - val_loss: 307.1759 - val_mae: 14.0510\n",
      "Epoch 360/1000\n",
      "1988/1988 [==============================] - 2s 966us/step - loss: 45.9001 - mae: 4.8985 - val_loss: 174.8885 - val_mae: 10.1029\n",
      "Epoch 361/1000\n",
      "1988/1988 [==============================] - 2s 959us/step - loss: 40.6741 - mae: 4.5968 - val_loss: 135.6278 - val_mae: 8.9202\n",
      "Epoch 362/1000\n",
      "1988/1988 [==============================] - 2s 953us/step - loss: 42.5284 - mae: 4.6294 - val_loss: 107.6021 - val_mae: 8.0968\n",
      "Epoch 363/1000\n",
      "1988/1988 [==============================] - 2s 875us/step - loss: 37.8061 - mae: 4.4477 - val_loss: 191.2532 - val_mae: 10.9237\n",
      "Epoch 364/1000\n",
      "1988/1988 [==============================] - 2s 907us/step - loss: 39.6436 - mae: 4.5896 - val_loss: 115.5854 - val_mae: 8.3902\n",
      "Epoch 365/1000\n",
      "1988/1988 [==============================] - 2s 880us/step - loss: 39.5929 - mae: 4.4881 - val_loss: 181.7215 - val_mae: 10.3821\n",
      "Epoch 366/1000\n",
      "1988/1988 [==============================] - 2s 922us/step - loss: 42.0153 - mae: 4.6562 - val_loss: 212.1804 - val_mae: 11.3600\n",
      "Epoch 367/1000\n",
      "1988/1988 [==============================] - 2s 939us/step - loss: 37.5929 - mae: 4.3711 - val_loss: 181.5561 - val_mae: 10.4330\n",
      "Epoch 368/1000\n",
      "1988/1988 [==============================] - 2s 907us/step - loss: 37.0746 - mae: 4.3415 - val_loss: 176.6263 - val_mae: 10.2768\n",
      "Epoch 369/1000\n",
      "1988/1988 [==============================] - 2s 903us/step - loss: 37.1365 - mae: 4.4213 - val_loss: 357.8102 - val_mae: 15.8376\n",
      "Epoch 370/1000\n",
      "1988/1988 [==============================] - 2s 870us/step - loss: 38.4490 - mae: 4.4053 - val_loss: 275.0966 - val_mae: 13.0052\n",
      "Epoch 371/1000\n",
      "1988/1988 [==============================] - 2s 848us/step - loss: 38.4962 - mae: 4.4411 - val_loss: 225.0638 - val_mae: 11.3828\n",
      "Epoch 372/1000\n",
      "1988/1988 [==============================] - 2s 864us/step - loss: 38.3242 - mae: 4.4371 - val_loss: 147.9624 - val_mae: 9.2641\n",
      "Epoch 373/1000\n",
      "1988/1988 [==============================] - 2s 855us/step - loss: 39.0769 - mae: 4.4496 - val_loss: 219.9245 - val_mae: 11.6757\n",
      "Epoch 374/1000\n",
      "1988/1988 [==============================] - 2s 860us/step - loss: 39.7529 - mae: 4.6392 - val_loss: 232.1019 - val_mae: 11.6176\n",
      "Epoch 375/1000\n",
      "1988/1988 [==============================] - 2s 868us/step - loss: 39.6820 - mae: 4.4887 - val_loss: 166.1413 - val_mae: 9.9056\n",
      "Epoch 376/1000\n",
      "1988/1988 [==============================] - 2s 863us/step - loss: 41.0283 - mae: 4.5343 - val_loss: 136.1229 - val_mae: 9.0407\n",
      "Epoch 377/1000\n",
      "1988/1988 [==============================] - 2s 859us/step - loss: 44.6875 - mae: 4.7666 - val_loss: 261.6727 - val_mae: 13.0737\n",
      "Epoch 378/1000\n",
      "1988/1988 [==============================] - 2s 873us/step - loss: 41.7939 - mae: 4.7632 - val_loss: 282.9387 - val_mae: 12.8818\n",
      "Epoch 379/1000\n",
      "1988/1988 [==============================] - 2s 863us/step - loss: 40.8836 - mae: 4.6225 - val_loss: 165.7559 - val_mae: 9.8036\n",
      "Epoch 380/1000\n",
      "1988/1988 [==============================] - 2s 865us/step - loss: 39.2157 - mae: 4.4279 - val_loss: 367.5568 - val_mae: 15.7329\n",
      "Epoch 381/1000\n",
      "1988/1988 [==============================] - 2s 853us/step - loss: 37.3673 - mae: 4.3654 - val_loss: 195.7935 - val_mae: 10.6973\n",
      "Epoch 382/1000\n",
      "1988/1988 [==============================] - 2s 910us/step - loss: 38.1779 - mae: 4.5089 - val_loss: 260.6056 - val_mae: 12.8916\n",
      "Epoch 383/1000\n",
      "1988/1988 [==============================] - 2s 884us/step - loss: 37.8911 - mae: 4.3831 - val_loss: 310.3381 - val_mae: 13.9780\n",
      "Epoch 384/1000\n",
      "1988/1988 [==============================] - 2s 857us/step - loss: 37.3508 - mae: 4.3837 - val_loss: 388.9583 - val_mae: 16.0769\n",
      "Epoch 385/1000\n",
      "1988/1988 [==============================] - 2s 841us/step - loss: 41.3344 - mae: 4.5944 - val_loss: 198.6356 - val_mae: 10.7363\n",
      "Epoch 386/1000\n",
      "1988/1988 [==============================] - 2s 825us/step - loss: 39.6596 - mae: 4.5195 - val_loss: 302.6191 - val_mae: 14.2782\n",
      "Epoch 387/1000\n",
      "1988/1988 [==============================] - 2s 818us/step - loss: 40.2455 - mae: 4.6618 - val_loss: 453.7837 - val_mae: 17.8259\n",
      "Epoch 388/1000\n",
      "1988/1988 [==============================] - 2s 842us/step - loss: 40.6124 - mae: 4.5554 - val_loss: 171.7391 - val_mae: 9.9455\n",
      "Epoch 389/1000\n",
      "1988/1988 [==============================] - 2s 829us/step - loss: 39.5869 - mae: 4.5074 - val_loss: 227.4676 - val_mae: 11.8151\n",
      "Epoch 390/1000\n",
      "1988/1988 [==============================] - 2s 820us/step - loss: 41.3459 - mae: 4.5326 - val_loss: 102.9779 - val_mae: 8.4109\n",
      "Epoch 391/1000\n",
      "1988/1988 [==============================] - 2s 821us/step - loss: 40.1832 - mae: 4.4740 - val_loss: 144.4875 - val_mae: 9.2529\n",
      "Epoch 392/1000\n",
      "1988/1988 [==============================] - 2s 832us/step - loss: 39.4661 - mae: 4.5164 - val_loss: 265.8130 - val_mae: 13.5337\n",
      "Epoch 393/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 40.1402 - mae: 4.5182 - val_loss: 243.5897 - val_mae: 12.2080\n",
      "Epoch 394/1000\n",
      "1988/1988 [==============================] - 2s 874us/step - loss: 42.5657 - mae: 4.6075 - val_loss: 375.7114 - val_mae: 16.2734\n",
      "Epoch 395/1000\n",
      "1988/1988 [==============================] - 2s 853us/step - loss: 41.5779 - mae: 4.4933 - val_loss: 178.1389 - val_mae: 10.2063\n",
      "Epoch 396/1000\n",
      "1988/1988 [==============================] - 2s 850us/step - loss: 43.1303 - mae: 4.6246 - val_loss: 121.4565 - val_mae: 8.5190\n",
      "Epoch 397/1000\n",
      "1988/1988 [==============================] - 2s 839us/step - loss: 38.6175 - mae: 4.4696 - val_loss: 199.2422 - val_mae: 11.7765\n",
      "Epoch 398/1000\n",
      "1988/1988 [==============================] - 2s 846us/step - loss: 39.1898 - mae: 4.6095 - val_loss: 222.3546 - val_mae: 12.5735\n",
      "Epoch 399/1000\n",
      "1988/1988 [==============================] - 2s 847us/step - loss: 40.5239 - mae: 4.6140 - val_loss: 135.5267 - val_mae: 8.9380\n",
      "Epoch 400/1000\n",
      "1988/1988 [==============================] - 2s 855us/step - loss: 38.1751 - mae: 4.5233 - val_loss: 127.7604 - val_mae: 8.6437\n",
      "Epoch 401/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 37.4039 - mae: 4.4045 - val_loss: 168.6612 - val_mae: 10.2679\n",
      "Epoch 402/1000\n",
      "1988/1988 [==============================] - 2s 838us/step - loss: 37.9961 - mae: 4.4177 - val_loss: 269.2572 - val_mae: 13.7223\n",
      "Epoch 403/1000\n",
      "1988/1988 [==============================] - 2s 817us/step - loss: 40.3330 - mae: 4.5794 - val_loss: 120.0721 - val_mae: 8.5110\n",
      "Epoch 404/1000\n",
      "1988/1988 [==============================] - 2s 819us/step - loss: 37.9085 - mae: 4.4743 - val_loss: 223.1093 - val_mae: 12.4691\n",
      "Epoch 405/1000\n",
      "1988/1988 [==============================] - 2s 816us/step - loss: 42.1518 - mae: 4.7477 - val_loss: 195.8383 - val_mae: 11.1039\n",
      "Epoch 406/1000\n",
      "1988/1988 [==============================] - 2s 813us/step - loss: 39.7654 - mae: 4.5307 - val_loss: 285.8816 - val_mae: 14.4288\n",
      "Epoch 407/1000\n",
      "1988/1988 [==============================] - 2s 833us/step - loss: 39.4006 - mae: 4.4655 - val_loss: 256.2637 - val_mae: 13.3592\n",
      "Epoch 408/1000\n",
      "1988/1988 [==============================] - 2s 817us/step - loss: 41.9069 - mae: 4.6625 - val_loss: 133.8283 - val_mae: 9.0339\n",
      "Epoch 409/1000\n",
      "1988/1988 [==============================] - 2s 815us/step - loss: 41.8492 - mae: 4.5993 - val_loss: 142.0713 - val_mae: 9.7487\n",
      "Epoch 410/1000\n",
      "1988/1988 [==============================] - 2s 850us/step - loss: 43.6366 - mae: 4.7244 - val_loss: 158.2956 - val_mae: 10.6334\n",
      "Epoch 411/1000\n",
      "1988/1988 [==============================] - 2s 920us/step - loss: 38.9489 - mae: 4.4678 - val_loss: 83.3725 - val_mae: 7.1749\n",
      "Epoch 412/1000\n",
      "1988/1988 [==============================] - 2s 844us/step - loss: 40.5826 - mae: 4.5634 - val_loss: 129.0659 - val_mae: 9.2026\n",
      "Epoch 413/1000\n",
      "1988/1988 [==============================] - 2s 876us/step - loss: 37.3759 - mae: 4.4203 - val_loss: 118.5217 - val_mae: 8.5603\n",
      "Epoch 414/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 38.0904 - mae: 4.4821 - val_loss: 125.9282 - val_mae: 8.8293\n",
      "Epoch 415/1000\n",
      "1988/1988 [==============================] - 2s 839us/step - loss: 36.7729 - mae: 4.4954 - val_loss: 125.0645 - val_mae: 8.6591\n",
      "Epoch 416/1000\n",
      "1988/1988 [==============================] - 2s 845us/step - loss: 42.0243 - mae: 4.5170 - val_loss: 182.0443 - val_mae: 10.7769\n",
      "Epoch 417/1000\n",
      "1988/1988 [==============================] - 2s 837us/step - loss: 39.4402 - mae: 4.4527 - val_loss: 135.4528 - val_mae: 9.0712\n",
      "Epoch 418/1000\n",
      "1988/1988 [==============================] - 2s 852us/step - loss: 34.0252 - mae: 4.2131 - val_loss: 275.9327 - val_mae: 13.8009\n",
      "Epoch 419/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 36.6742 - mae: 4.3043 - val_loss: 304.9024 - val_mae: 14.7327\n",
      "Epoch 420/1000\n",
      "1988/1988 [==============================] - 2s 884us/step - loss: 40.3513 - mae: 4.4393 - val_loss: 149.4955 - val_mae: 9.5419\n",
      "Epoch 421/1000\n",
      "1988/1988 [==============================] - 2s 831us/step - loss: 36.0662 - mae: 4.3189 - val_loss: 172.6444 - val_mae: 10.1428\n",
      "Epoch 422/1000\n",
      "1988/1988 [==============================] - 2s 816us/step - loss: 37.0658 - mae: 4.3863 - val_loss: 89.7034 - val_mae: 7.3880\n",
      "Epoch 423/1000\n",
      "1988/1988 [==============================] - 2s 820us/step - loss: 41.9228 - mae: 4.5619 - val_loss: 138.3449 - val_mae: 9.3018\n",
      "Epoch 424/1000\n",
      "1988/1988 [==============================] - 2s 823us/step - loss: 37.8549 - mae: 4.5050 - val_loss: 155.1245 - val_mae: 9.7841\n",
      "Epoch 425/1000\n",
      "1988/1988 [==============================] - 2s 881us/step - loss: 39.8248 - mae: 4.6316 - val_loss: 99.1538 - val_mae: 7.7025\n",
      "Epoch 426/1000\n",
      "1988/1988 [==============================] - 2s 826us/step - loss: 41.3039 - mae: 4.5231 - val_loss: 101.6772 - val_mae: 7.7328\n",
      "Epoch 427/1000\n",
      "1988/1988 [==============================] - 2s 832us/step - loss: 37.9330 - mae: 4.3185 - val_loss: 186.8166 - val_mae: 10.6084\n",
      "Epoch 428/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 39.3191 - mae: 4.4819 - val_loss: 282.1447 - val_mae: 14.0178\n",
      "Epoch 429/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 41.8089 - mae: 4.7486 - val_loss: 393.8069 - val_mae: 15.9120\n",
      "Epoch 430/1000\n",
      "1988/1988 [==============================] - 2s 819us/step - loss: 41.1583 - mae: 4.6647 - val_loss: 210.8182 - val_mae: 10.9810\n",
      "Epoch 431/1000\n",
      "1988/1988 [==============================] - 2s 833us/step - loss: 39.9711 - mae: 4.4816 - val_loss: 292.3759 - val_mae: 13.5798\n",
      "Epoch 432/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 39.9980 - mae: 4.4846 - val_loss: 97.5653 - val_mae: 7.6029\n",
      "Epoch 433/1000\n",
      "1988/1988 [==============================] - 2s 813us/step - loss: 37.7251 - mae: 4.3683 - val_loss: 152.4247 - val_mae: 9.7710\n",
      "Epoch 434/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 39.9671 - mae: 4.5190 - val_loss: 152.7388 - val_mae: 9.4158\n",
      "Epoch 435/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 43.6133 - mae: 4.6244 - val_loss: 319.8579 - val_mae: 14.4594\n",
      "Epoch 436/1000\n",
      "1988/1988 [==============================] - 2s 874us/step - loss: 38.4861 - mae: 4.3997 - val_loss: 194.3847 - val_mae: 10.7293\n",
      "Epoch 437/1000\n",
      "1988/1988 [==============================] - 2s 853us/step - loss: 37.5765 - mae: 4.3546 - val_loss: 290.6948 - val_mae: 14.1266\n",
      "Epoch 438/1000\n",
      "1988/1988 [==============================] - 2s 879us/step - loss: 36.8870 - mae: 4.3135 - val_loss: 192.2977 - val_mae: 10.9151\n",
      "Epoch 439/1000\n",
      "1988/1988 [==============================] - 2s 839us/step - loss: 38.8218 - mae: 4.3607 - val_loss: 164.0142 - val_mae: 9.8977\n",
      "Epoch 440/1000\n",
      "1988/1988 [==============================] - 2s 826us/step - loss: 39.5533 - mae: 4.5351 - val_loss: 410.4895 - val_mae: 16.0834\n",
      "Epoch 441/1000\n",
      "1988/1988 [==============================] - 2s 809us/step - loss: 38.9503 - mae: 4.5687 - val_loss: 289.6193 - val_mae: 13.4262\n",
      "Epoch 442/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 38.4796 - mae: 4.6780 - val_loss: 260.2768 - val_mae: 12.6728\n",
      "Epoch 443/1000\n",
      "1988/1988 [==============================] - 2s 807us/step - loss: 39.6763 - mae: 4.5485 - val_loss: 238.5031 - val_mae: 12.1261\n",
      "Epoch 444/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 35.8522 - mae: 4.3287 - val_loss: 192.1874 - val_mae: 10.7537\n",
      "Epoch 445/1000\n",
      "1988/1988 [==============================] - 2s 788us/step - loss: 39.0606 - mae: 4.3902 - val_loss: 140.9491 - val_mae: 9.2486\n",
      "Epoch 446/1000\n",
      "1988/1988 [==============================] - 2s 783us/step - loss: 37.6917 - mae: 4.3687 - val_loss: 323.2522 - val_mae: 14.7713\n",
      "Epoch 447/1000\n",
      "1988/1988 [==============================] - 2s 822us/step - loss: 38.0944 - mae: 4.4450 - val_loss: 240.5860 - val_mae: 11.6985\n",
      "Epoch 448/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 38.8870 - mae: 4.5360 - val_loss: 332.0968 - val_mae: 14.9595\n",
      "Epoch 449/1000\n",
      "1988/1988 [==============================] - 2s 859us/step - loss: 38.2455 - mae: 4.4530 - val_loss: 186.4251 - val_mae: 10.8238\n",
      "Epoch 450/1000\n",
      "1988/1988 [==============================] - 2s 914us/step - loss: 34.1748 - mae: 4.2886 - val_loss: 178.3186 - val_mae: 10.3242\n",
      "Epoch 451/1000\n",
      "1988/1988 [==============================] - 2s 874us/step - loss: 41.7973 - mae: 4.4977 - val_loss: 252.8088 - val_mae: 12.4250\n",
      "Epoch 452/1000\n",
      "1988/1988 [==============================] - 2s 895us/step - loss: 36.8879 - mae: 4.3868 - val_loss: 232.2708 - val_mae: 12.0487\n",
      "Epoch 453/1000\n",
      "1988/1988 [==============================] - 2s 865us/step - loss: 35.5609 - mae: 4.2832 - val_loss: 247.5302 - val_mae: 12.1558\n",
      "Epoch 454/1000\n",
      "1988/1988 [==============================] - 2s 898us/step - loss: 33.3471 - mae: 4.1473 - val_loss: 285.7729 - val_mae: 13.0626\n",
      "Epoch 455/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 36.9014 - mae: 4.3806 - val_loss: 238.6537 - val_mae: 11.8253\n",
      "Epoch 456/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 839us/step - loss: 35.3865 - mae: 4.2471 - val_loss: 152.8831 - val_mae: 9.3432\n",
      "Epoch 457/1000\n",
      "1988/1988 [==============================] - 2s 813us/step - loss: 38.2352 - mae: 4.4097 - val_loss: 350.8902 - val_mae: 15.0736\n",
      "Epoch 458/1000\n",
      "1988/1988 [==============================] - 2s 808us/step - loss: 41.3057 - mae: 4.6311 - val_loss: 410.8417 - val_mae: 15.6089\n",
      "Epoch 459/1000\n",
      "1988/1988 [==============================] - 2s 810us/step - loss: 39.2074 - mae: 4.6267 - val_loss: 384.3496 - val_mae: 15.6119\n",
      "Epoch 460/1000\n",
      "1988/1988 [==============================] - 2s 807us/step - loss: 40.0508 - mae: 4.4796 - val_loss: 209.2720 - val_mae: 10.9598\n",
      "Epoch 461/1000\n",
      "1988/1988 [==============================] - 2s 780us/step - loss: 36.7701 - mae: 4.4041 - val_loss: 228.7876 - val_mae: 11.4259\n",
      "Epoch 462/1000\n",
      "1988/1988 [==============================] - 2s 837us/step - loss: 36.0725 - mae: 4.3463 - val_loss: 302.6792 - val_mae: 13.0857\n",
      "Epoch 463/1000\n",
      "1988/1988 [==============================] - 2s 840us/step - loss: 42.8550 - mae: 4.5244 - val_loss: 427.4135 - val_mae: 17.2090\n",
      "Epoch 464/1000\n",
      "1988/1988 [==============================] - 2s 860us/step - loss: 39.1103 - mae: 4.4814 - val_loss: 301.2855 - val_mae: 13.6197\n",
      "Epoch 465/1000\n",
      "1988/1988 [==============================] - 2s 855us/step - loss: 38.1984 - mae: 4.3908 - val_loss: 298.4497 - val_mae: 13.1077\n",
      "Epoch 466/1000\n",
      "1988/1988 [==============================] - 2s 837us/step - loss: 38.3148 - mae: 4.5039 - val_loss: 340.9587 - val_mae: 14.3939 loss: 27.4114 - mae\n",
      "Epoch 467/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 39.1165 - mae: 4.4788 - val_loss: 368.2000 - val_mae: 14.5700\n",
      "Epoch 468/1000\n",
      "1988/1988 [==============================] - 2s 833us/step - loss: 39.6874 - mae: 4.4855 - val_loss: 461.7075 - val_mae: 16.1518\n",
      "Epoch 469/1000\n",
      "1988/1988 [==============================] - 2s 839us/step - loss: 43.3568 - mae: 4.8030 - val_loss: 439.4409 - val_mae: 16.2626\n",
      "Epoch 470/1000\n",
      "1988/1988 [==============================] - 2s 845us/step - loss: 37.2839 - mae: 4.5179 - val_loss: 510.0885 - val_mae: 17.3402\n",
      "Epoch 471/1000\n",
      "1988/1988 [==============================] - 2s 838us/step - loss: 36.4707 - mae: 4.3351 - val_loss: 618.7166 - val_mae: 20.4821\n",
      "Epoch 472/1000\n",
      "1988/1988 [==============================] - 2s 848us/step - loss: 35.9782 - mae: 4.3621 - val_loss: 330.2119 - val_mae: 13.4641\n",
      "Epoch 473/1000\n",
      "1988/1988 [==============================] - 2s 841us/step - loss: 39.6100 - mae: 4.4063 - val_loss: 293.7151 - val_mae: 12.8069\n",
      "Epoch 474/1000\n",
      "1988/1988 [==============================] - 2s 843us/step - loss: 38.9286 - mae: 4.4434 - val_loss: 279.5793 - val_mae: 13.5249\n",
      "Epoch 475/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 35.8893 - mae: 4.3958 - val_loss: 407.3383 - val_mae: 15.5876\n",
      "Epoch 476/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 37.8863 - mae: 4.5114 - val_loss: 298.2373 - val_mae: 12.7387\n",
      "Epoch 477/1000\n",
      "1988/1988 [==============================] - 2s 810us/step - loss: 36.2457 - mae: 4.3209 - val_loss: 355.2005 - val_mae: 13.9745\n",
      "Epoch 478/1000\n",
      "1988/1988 [==============================] - 2s 823us/step - loss: 36.5657 - mae: 4.3225 - val_loss: 419.6212 - val_mae: 16.5408\n",
      "Epoch 479/1000\n",
      "1988/1988 [==============================] - 2s 813us/step - loss: 41.1699 - mae: 4.6811 - val_loss: 229.7145 - val_mae: 11.2006\n",
      "Epoch 480/1000\n",
      "1988/1988 [==============================] - 2s 828us/step - loss: 41.9997 - mae: 4.7342 - val_loss: 365.9519 - val_mae: 14.8185\n",
      "Epoch 481/1000\n",
      "1988/1988 [==============================] - 2s 836us/step - loss: 38.0063 - mae: 4.3807 - val_loss: 282.6248 - val_mae: 12.5146\n",
      "Epoch 482/1000\n",
      "1988/1988 [==============================] - 2s 819us/step - loss: 38.4058 - mae: 4.4524 - val_loss: 303.4588 - val_mae: 13.7547\n",
      "Epoch 483/1000\n",
      "1988/1988 [==============================] - 2s 880us/step - loss: 38.3344 - mae: 4.4390 - val_loss: 257.6686 - val_mae: 11.7601\n",
      "Epoch 484/1000\n",
      "1988/1988 [==============================] - 2s 882us/step - loss: 41.0425 - mae: 4.5186 - val_loss: 301.1227 - val_mae: 13.2528\n",
      "Epoch 485/1000\n",
      "1988/1988 [==============================] - 2s 847us/step - loss: 40.0000 - mae: 4.5112 - val_loss: 195.4877 - val_mae: 10.4526\n",
      "Epoch 486/1000\n",
      "1988/1988 [==============================] - 2s 833us/step - loss: 37.7794 - mae: 4.4255 - val_loss: 221.5314 - val_mae: 11.3883\n",
      "Epoch 487/1000\n",
      "1988/1988 [==============================] - 2s 835us/step - loss: 35.8212 - mae: 4.5327 - val_loss: 246.1661 - val_mae: 11.9613\n",
      "Epoch 488/1000\n",
      "1988/1988 [==============================] - 2s 837us/step - loss: 38.0905 - mae: 4.4539 - val_loss: 234.5694 - val_mae: 11.6694\n",
      "Epoch 489/1000\n",
      "1988/1988 [==============================] - 2s 806us/step - loss: 37.9557 - mae: 4.2990 - val_loss: 334.2994 - val_mae: 15.0071\n",
      "Epoch 490/1000\n",
      "1988/1988 [==============================] - 2s 830us/step - loss: 34.1265 - mae: 4.2068 - val_loss: 383.5416 - val_mae: 16.5994\n",
      "Epoch 491/1000\n",
      "1988/1988 [==============================] - 2s 812us/step - loss: 40.2916 - mae: 4.5565 - val_loss: 381.3306 - val_mae: 16.0126\n",
      "Epoch 492/1000\n",
      "1988/1988 [==============================] - 2s 805us/step - loss: 41.7379 - mae: 4.6710 - val_loss: 341.2037 - val_mae: 14.8183\n",
      "Epoch 493/1000\n",
      "1988/1988 [==============================] - 2s 814us/step - loss: 39.4160 - mae: 4.4712 - val_loss: 362.1463 - val_mae: 15.0536\n",
      "Epoch 494/1000\n",
      "1988/1988 [==============================] - 2s 830us/step - loss: 37.8212 - mae: 4.3943 - val_loss: 241.8099 - val_mae: 11.4696\n",
      "Epoch 495/1000\n",
      "1988/1988 [==============================] - 2s 805us/step - loss: 40.6643 - mae: 4.4330 - val_loss: 309.6728 - val_mae: 13.3083\n",
      "Epoch 496/1000\n",
      "1988/1988 [==============================] - 2s 793us/step - loss: 39.6719 - mae: 4.6747 - val_loss: 280.9528 - val_mae: 13.6823\n",
      "Epoch 497/1000\n",
      "1988/1988 [==============================] - 2s 780us/step - loss: 42.7454 - mae: 4.8289 - val_loss: 304.2875 - val_mae: 14.6424\n",
      "Epoch 498/1000\n",
      "1988/1988 [==============================] - 2s 786us/step - loss: 39.7128 - mae: 4.4803 - val_loss: 228.2387 - val_mae: 12.0007\n",
      "Epoch 499/1000\n",
      "1988/1988 [==============================] - 2s 810us/step - loss: 39.5377 - mae: 4.5855 - val_loss: 200.8218 - val_mae: 10.6538\n",
      "Epoch 500/1000\n",
      "1988/1988 [==============================] - 2s 831us/step - loss: 34.6109 - mae: 4.1996 - val_loss: 225.0547 - val_mae: 11.0961\n",
      "Epoch 501/1000\n",
      "1988/1988 [==============================] - 2s 841us/step - loss: 38.9467 - mae: 4.4425 - val_loss: 313.5038 - val_mae: 13.5855\n",
      "Epoch 502/1000\n",
      "1988/1988 [==============================] - 2s 847us/step - loss: 35.4208 - mae: 4.2780 - val_loss: 234.5150 - val_mae: 11.4721\n",
      "Epoch 503/1000\n",
      "1988/1988 [==============================] - 2s 833us/step - loss: 40.7330 - mae: 4.5743 - val_loss: 348.5366 - val_mae: 15.2581\n",
      "Epoch 504/1000\n",
      "1988/1988 [==============================] - 2s 850us/step - loss: 40.0704 - mae: 4.5475 - val_loss: 161.3224 - val_mae: 9.4628\n",
      "Epoch 505/1000\n",
      "1988/1988 [==============================] - 2s 826us/step - loss: 38.0517 - mae: 4.4149 - val_loss: 268.5023 - val_mae: 12.2461\n",
      "Epoch 506/1000\n",
      "1988/1988 [==============================] - 2s 823us/step - loss: 41.0165 - mae: 4.6590 - val_loss: 461.2539 - val_mae: 17.3383\n",
      "Epoch 507/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 38.5918 - mae: 4.4530 - val_loss: 396.7913 - val_mae: 15.0510\n",
      "Epoch 508/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 41.1535 - mae: 4.5765 - val_loss: 276.6305 - val_mae: 12.3017\n",
      "Epoch 509/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 37.3404 - mae: 4.3911 - val_loss: 266.7120 - val_mae: 12.7358\n",
      "Epoch 510/1000\n",
      "1988/1988 [==============================] - 2s 779us/step - loss: 37.2842 - mae: 4.3765 - val_loss: 294.6332 - val_mae: 13.1244\n",
      "Epoch 511/1000\n",
      "1988/1988 [==============================] - 2s 778us/step - loss: 40.5981 - mae: 4.6920 - val_loss: 335.0459 - val_mae: 15.2404\n",
      "Epoch 512/1000\n",
      "1988/1988 [==============================] - 2s 785us/step - loss: 37.3367 - mae: 4.3928 - val_loss: 217.0310 - val_mae: 10.8330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 513/1000\n",
      "1988/1988 [==============================] - 2s 786us/step - loss: 35.4367 - mae: 4.2835 - val_loss: 233.8021 - val_mae: 11.2593\n",
      "Epoch 514/1000\n",
      "1988/1988 [==============================] - 2s 802us/step - loss: 38.1045 - mae: 4.4274 - val_loss: 233.4532 - val_mae: 11.3022\n",
      "Epoch 515/1000\n",
      "1988/1988 [==============================] - 2s 807us/step - loss: 38.7610 - mae: 4.6305 - val_loss: 433.1732 - val_mae: 16.1689\n",
      "Epoch 516/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 42.7541 - mae: 4.6468 - val_loss: 282.8985 - val_mae: 12.3887\n",
      "Epoch 517/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 37.1131 - mae: 4.3576 - val_loss: 178.1433 - val_mae: 10.0375\n",
      "Epoch 518/1000\n",
      "1988/1988 [==============================] - 2s 875us/step - loss: 35.9199 - mae: 4.4052 - val_loss: 168.3742 - val_mae: 9.5153\n",
      "Epoch 519/1000\n",
      "1988/1988 [==============================] - 2s 862us/step - loss: 40.8962 - mae: 4.4956 - val_loss: 278.7670 - val_mae: 12.3768\n",
      "Epoch 520/1000\n",
      "1988/1988 [==============================] - 2s 832us/step - loss: 38.7815 - mae: 4.6529 - val_loss: 165.7396 - val_mae: 9.4526\n",
      "Epoch 521/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 36.1350 - mae: 4.3112 - val_loss: 235.7910 - val_mae: 11.6141\n",
      "Epoch 522/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 38.6967 - mae: 4.3738 - val_loss: 222.5350 - val_mae: 11.2409\n",
      "Epoch 523/1000\n",
      "1988/1988 [==============================] - 2s 806us/step - loss: 36.6369 - mae: 4.3406 - val_loss: 164.9868 - val_mae: 9.4669\n",
      "Epoch 524/1000\n",
      "1988/1988 [==============================] - 2s 801us/step - loss: 38.5912 - mae: 4.4850 - val_loss: 164.2590 - val_mae: 9.4852\n",
      "Epoch 525/1000\n",
      "1988/1988 [==============================] - 2s 787us/step - loss: 39.6004 - mae: 4.5639 - val_loss: 165.7911 - val_mae: 9.4447\n",
      "Epoch 526/1000\n",
      "1988/1988 [==============================] - 2s 778us/step - loss: 40.5049 - mae: 4.6190 - val_loss: 384.1928 - val_mae: 16.2825\n",
      "Epoch 527/1000\n",
      "1988/1988 [==============================] - 2s 784us/step - loss: 37.3586 - mae: 4.3884 - val_loss: 228.9315 - val_mae: 11.4549\n",
      "Epoch 528/1000\n",
      "1988/1988 [==============================] - 2s 774us/step - loss: 38.7125 - mae: 4.5372 - val_loss: 165.3053 - val_mae: 9.6088\n",
      "Epoch 529/1000\n",
      "1988/1988 [==============================] - 2s 809us/step - loss: 36.6887 - mae: 4.3557 - val_loss: 153.2546 - val_mae: 9.1540\n",
      "Epoch 530/1000\n",
      "1988/1988 [==============================] - 2s 801us/step - loss: 41.8704 - mae: 4.5931 - val_loss: 209.3117 - val_mae: 11.4760\n",
      "Epoch 531/1000\n",
      "1988/1988 [==============================] - 2s 810us/step - loss: 36.4574 - mae: 4.3266 - val_loss: 465.1328 - val_mae: 17.7963\n",
      "Epoch 532/1000\n",
      "1988/1988 [==============================] - 2s 816us/step - loss: 37.9499 - mae: 4.3829 - val_loss: 287.9069 - val_mae: 13.1102\n",
      "Epoch 533/1000\n",
      "1988/1988 [==============================] - 2s 831us/step - loss: 36.5612 - mae: 4.3908 - val_loss: 173.6045 - val_mae: 9.8526\n",
      "Epoch 534/1000\n",
      "1988/1988 [==============================] - 2s 826us/step - loss: 35.5409 - mae: 4.2540 - val_loss: 145.8595 - val_mae: 8.9519\n",
      "Epoch 535/1000\n",
      "1988/1988 [==============================] - 2s 839us/step - loss: 36.9633 - mae: 4.2761 - val_loss: 168.7117 - val_mae: 9.4872\n",
      "Epoch 536/1000\n",
      "1988/1988 [==============================] - 2s 822us/step - loss: 36.2735 - mae: 4.2974 - val_loss: 228.2904 - val_mae: 12.1645\n",
      "Epoch 537/1000\n",
      "1988/1988 [==============================] - 2s 800us/step - loss: 34.9831 - mae: 4.2425 - val_loss: 157.7554 - val_mae: 9.2776\n",
      "Epoch 538/1000\n",
      "1988/1988 [==============================] - 2s 808us/step - loss: 40.9677 - mae: 4.5293 - val_loss: 249.8772 - val_mae: 12.1081\n",
      "Epoch 539/1000\n",
      "1988/1988 [==============================] - 2s 790us/step - loss: 35.7954 - mae: 4.2618 - val_loss: 166.7934 - val_mae: 9.6977\n",
      "Epoch 540/1000\n",
      "1988/1988 [==============================] - 2s 775us/step - loss: 37.6767 - mae: 4.3463 - val_loss: 175.2191 - val_mae: 10.2295\n",
      "Epoch 541/1000\n",
      "1988/1988 [==============================] - 2s 776us/step - loss: 41.6903 - mae: 4.5240 - val_loss: 126.8177 - val_mae: 8.3955\n",
      "Epoch 542/1000\n",
      "1988/1988 [==============================] - 2s 774us/step - loss: 37.9465 - mae: 4.3529 - val_loss: 260.7685 - val_mae: 13.1909\n",
      "Epoch 543/1000\n",
      "1988/1988 [==============================] - 2s 792us/step - loss: 35.7114 - mae: 4.3012 - val_loss: 177.9595 - val_mae: 9.9496\n",
      "Epoch 544/1000\n",
      "1988/1988 [==============================] - 2s 798us/step - loss: 36.9860 - mae: 4.2343 - val_loss: 202.9845 - val_mae: 11.3467\n",
      "Epoch 545/1000\n",
      "1988/1988 [==============================] - 2s 806us/step - loss: 40.2240 - mae: 4.4493 - val_loss: 237.9876 - val_mae: 12.0916\n",
      "Epoch 546/1000\n",
      "1988/1988 [==============================] - 2s 804us/step - loss: 38.7177 - mae: 4.4206 - val_loss: 264.5783 - val_mae: 12.8399\n",
      "Epoch 547/1000\n",
      "1988/1988 [==============================] - 2s 803us/step - loss: 35.8782 - mae: 4.2495 - val_loss: 129.2523 - val_mae: 8.4876\n",
      "Epoch 548/1000\n",
      "1988/1988 [==============================] - 2s 824us/step - loss: 37.9395 - mae: 4.4431 - val_loss: 145.1737 - val_mae: 9.3664\n",
      "Epoch 549/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 34.1886 - mae: 4.2176 - val_loss: 171.3712 - val_mae: 9.7831\n",
      "Epoch 550/1000\n",
      "1988/1988 [==============================] - 2s 841us/step - loss: 38.8140 - mae: 4.3908 - val_loss: 138.3471 - val_mae: 8.6567\n",
      "Epoch 551/1000\n",
      "1988/1988 [==============================] - 2s 835us/step - loss: 32.6827 - mae: 4.0795 - val_loss: 123.1576 - val_mae: 8.2948\n",
      "Epoch 552/1000\n",
      "1988/1988 [==============================] - 2s 809us/step - loss: 37.6183 - mae: 4.3344 - val_loss: 175.4204 - val_mae: 10.2574\n",
      "Epoch 553/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 36.8997 - mae: 4.3606 - val_loss: 347.4862 - val_mae: 15.0402\n",
      "Epoch 554/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 41.0185 - mae: 4.6100 - val_loss: 299.0861 - val_mae: 13.1164\n",
      "Epoch 555/1000\n",
      "1988/1988 [==============================] - 2s 777us/step - loss: 40.6490 - mae: 4.6801 - val_loss: 213.4845 - val_mae: 10.6986\n",
      "Epoch 556/1000\n",
      "1988/1988 [==============================] - 2s 775us/step - loss: 41.6262 - mae: 4.6038 - val_loss: 236.1899 - val_mae: 11.1719\n",
      "Epoch 557/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 36.9019 - mae: 4.2888 - val_loss: 362.5511 - val_mae: 15.0325\n",
      "Epoch 558/1000\n",
      "1988/1988 [==============================] - 2s 876us/step - loss: 36.6685 - mae: 4.3067 - val_loss: 270.4707 - val_mae: 12.3615\n",
      "Epoch 559/1000\n",
      "1988/1988 [==============================] - 2s 902us/step - loss: 37.3343 - mae: 4.4062 - val_loss: 300.4668 - val_mae: 13.4725\n",
      "Epoch 560/1000\n",
      "1988/1988 [==============================] - 2s 932us/step - loss: 38.0849 - mae: 4.3948 - val_loss: 219.3520 - val_mae: 11.0384\n",
      "Epoch 561/1000\n",
      "1988/1988 [==============================] - 2s 891us/step - loss: 42.0699 - mae: 4.6178 - val_loss: 221.8998 - val_mae: 11.3759\n",
      "Epoch 562/1000\n",
      "1988/1988 [==============================] - 2s 856us/step - loss: 43.7504 - mae: 4.8386 - val_loss: 225.0160 - val_mae: 10.9654\n",
      "Epoch 563/1000\n",
      "1988/1988 [==============================] - 2s 846us/step - loss: 44.7949 - mae: 4.8058 - val_loss: 180.9922 - val_mae: 9.8244\n",
      "Epoch 564/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 39.1030 - mae: 4.4551 - val_loss: 282.4863 - val_mae: 13.2763\n",
      "Epoch 565/1000\n",
      "1988/1988 [==============================] - 2s 866us/step - loss: 38.8318 - mae: 4.4608 - val_loss: 188.1899 - val_mae: 10.5059\n",
      "Epoch 566/1000\n",
      "1988/1988 [==============================] - 2s 842us/step - loss: 36.7669 - mae: 4.3720 - val_loss: 277.5832 - val_mae: 13.5283\n",
      "Epoch 567/1000\n",
      "1988/1988 [==============================] - 2s 862us/step - loss: 36.7605 - mae: 4.3848 - val_loss: 178.1857 - val_mae: 10.5864\n",
      "Epoch 568/1000\n",
      "1988/1988 [==============================] - 2s 874us/step - loss: 38.7824 - mae: 4.5018 - val_loss: 335.4105 - val_mae: 15.7885\n",
      "Epoch 569/1000\n",
      "1988/1988 [==============================] - 2s 883us/step - loss: 39.3095 - mae: 4.5884 - val_loss: 365.0725 - val_mae: 14.3852\n",
      "Epoch 570/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 878us/step - loss: 42.6100 - mae: 4.8150 - val_loss: 351.0118 - val_mae: 14.6725\n",
      "Epoch 571/1000\n",
      "1988/1988 [==============================] - 2s 905us/step - loss: 38.8762 - mae: 4.5678 - val_loss: 163.5233 - val_mae: 9.3172\n",
      "Epoch 572/1000\n",
      "1988/1988 [==============================] - 2s 921us/step - loss: 40.7327 - mae: 4.5451 - val_loss: 251.4830 - val_mae: 12.7720\n",
      "Epoch 573/1000\n",
      "1988/1988 [==============================] - 2s 878us/step - loss: 37.4565 - mae: 4.4721 - val_loss: 235.8095 - val_mae: 11.9181\n",
      "Epoch 574/1000\n",
      "1988/1988 [==============================] - 2s 866us/step - loss: 38.7886 - mae: 4.3919 - val_loss: 222.1127 - val_mae: 11.6101\n",
      "Epoch 575/1000\n",
      "1988/1988 [==============================] - 2s 894us/step - loss: 37.5020 - mae: 4.4484 - val_loss: 191.1432 - val_mae: 10.5809\n",
      "Epoch 576/1000\n",
      "1988/1988 [==============================] - 2s 904us/step - loss: 38.1795 - mae: 4.3941 - val_loss: 223.9150 - val_mae: 11.2593\n",
      "Epoch 577/1000\n",
      "1988/1988 [==============================] - 2s 897us/step - loss: 38.4960 - mae: 4.4034 - val_loss: 384.0088 - val_mae: 14.9131\n",
      "Epoch 578/1000\n",
      "1988/1988 [==============================] - 2s 901us/step - loss: 36.6490 - mae: 4.3576 - val_loss: 264.1223 - val_mae: 11.9632\n",
      "Epoch 579/1000\n",
      "1988/1988 [==============================] - 2s 885us/step - loss: 36.3360 - mae: 4.3445 - val_loss: 272.0077 - val_mae: 12.9549\n",
      "Epoch 580/1000\n",
      "1988/1988 [==============================] - 2s 878us/step - loss: 36.0147 - mae: 4.4027 - val_loss: 330.4559 - val_mae: 14.6505\n",
      "Epoch 581/1000\n",
      "1988/1988 [==============================] - 2s 848us/step - loss: 38.8788 - mae: 4.3878 - val_loss: 231.2857 - val_mae: 11.1216\n",
      "Epoch 582/1000\n",
      "1988/1988 [==============================] - 2s 845us/step - loss: 39.1747 - mae: 4.6040 - val_loss: 422.3092 - val_mae: 16.0550\n",
      "Epoch 583/1000\n",
      "1988/1988 [==============================] - 2s 816us/step - loss: 37.3527 - mae: 4.4932 - val_loss: 351.7751 - val_mae: 14.0233\n",
      "Epoch 584/1000\n",
      "1988/1988 [==============================] - 2s 841us/step - loss: 35.7037 - mae: 4.2540 - val_loss: 235.4752 - val_mae: 11.1553\n",
      "Epoch 585/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 39.8740 - mae: 4.5863 - val_loss: 180.0324 - val_mae: 9.8521\n",
      "Epoch 586/1000\n",
      "1988/1988 [==============================] - 2s 867us/step - loss: 36.3720 - mae: 4.3489 - val_loss: 298.8185 - val_mae: 13.0338\n",
      "Epoch 587/1000\n",
      "1988/1988 [==============================] - 2s 893us/step - loss: 36.2519 - mae: 4.3842 - val_loss: 331.1909 - val_mae: 13.4970\n",
      "Epoch 588/1000\n",
      "1988/1988 [==============================] - 2s 860us/step - loss: 38.6481 - mae: 4.5207 - val_loss: 363.5398 - val_mae: 14.6174\n",
      "Epoch 589/1000\n",
      "1988/1988 [==============================] - 2s 868us/step - loss: 39.3489 - mae: 4.4875 - val_loss: 285.4043 - val_mae: 12.9140\n",
      "Epoch 590/1000\n",
      "1988/1988 [==============================] - 2s 882us/step - loss: 37.5038 - mae: 4.2923 - val_loss: 361.5728 - val_mae: 14.8122\n",
      "Epoch 591/1000\n",
      "1988/1988 [==============================] - 2s 898us/step - loss: 33.8905 - mae: 4.1932 - val_loss: 253.4081 - val_mae: 11.7474\n",
      "Epoch 592/1000\n",
      "1988/1988 [==============================] - 2s 895us/step - loss: 39.5595 - mae: 4.4657 - val_loss: 286.0931 - val_mae: 12.9856\n",
      "Epoch 593/1000\n",
      "1988/1988 [==============================] - 2s 995us/step - loss: 36.6891 - mae: 4.3901 - val_loss: 178.6000 - val_mae: 9.9407\n",
      "Epoch 594/1000\n",
      "1988/1988 [==============================] - 2s 931us/step - loss: 34.5790 - mae: 4.2446 - val_loss: 375.9221 - val_mae: 15.5876\n",
      "Epoch 595/1000\n",
      "1988/1988 [==============================] - 2s 952us/step - loss: 37.5617 - mae: 4.2782 - val_loss: 298.8657 - val_mae: 13.5714\n",
      "Epoch 596/1000\n",
      "1988/1988 [==============================] - 2s 853us/step - loss: 40.7265 - mae: 4.6007 - val_loss: 136.6680 - val_mae: 8.5867\n",
      "Epoch 597/1000\n",
      "1988/1988 [==============================] - 2s 955us/step - loss: 38.5143 - mae: 4.3062 - val_loss: 242.3061 - val_mae: 12.6825\n",
      "Epoch 598/1000\n",
      "1988/1988 [==============================] - 2s 964us/step - loss: 34.4789 - mae: 4.2911 - val_loss: 226.4651 - val_mae: 11.5949\n",
      "Epoch 599/1000\n",
      "1988/1988 [==============================] - 2s 948us/step - loss: 36.2974 - mae: 4.3445 - val_loss: 249.8345 - val_mae: 11.6719\n",
      "Epoch 600/1000\n",
      "1988/1988 [==============================] - 2s 924us/step - loss: 37.7013 - mae: 4.3421 - val_loss: 229.9274 - val_mae: 11.1996\n",
      "Epoch 601/1000\n",
      "1988/1988 [==============================] - 2s 952us/step - loss: 39.9836 - mae: 4.4490 - val_loss: 185.1454 - val_mae: 10.0392\n",
      "Epoch 602/1000\n",
      "1988/1988 [==============================] - 2s 890us/step - loss: 33.9730 - mae: 4.1157 - val_loss: 199.1841 - val_mae: 10.6710\n",
      "Epoch 603/1000\n",
      "1988/1988 [==============================] - 2s 909us/step - loss: 35.7949 - mae: 4.2903 - val_loss: 89.8788 - val_mae: 7.5251\n",
      "Epoch 604/1000\n",
      "1988/1988 [==============================] - 2s 948us/step - loss: 41.2606 - mae: 4.5389 - val_loss: 186.0452 - val_mae: 9.9539\n",
      "Epoch 605/1000\n",
      "1988/1988 [==============================] - 2s 950us/step - loss: 36.4627 - mae: 4.3539 - val_loss: 278.6637 - val_mae: 12.5597\n",
      "Epoch 606/1000\n",
      "1988/1988 [==============================] - 2s 930us/step - loss: 39.5113 - mae: 4.4765 - val_loss: 243.0725 - val_mae: 11.9872\n",
      "Epoch 607/1000\n",
      "1988/1988 [==============================] - 2s 997us/step - loss: 36.6301 - mae: 4.2546 - val_loss: 188.5655 - val_mae: 10.3499\n",
      "Epoch 608/1000\n",
      "1988/1988 [==============================] - 2s 948us/step - loss: 39.9710 - mae: 4.5990 - val_loss: 234.5507 - val_mae: 11.9224\n",
      "Epoch 609/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.3477 - mae: 4.2477 - val_loss: 218.7559 - val_mae: 10.8549\n",
      "Epoch 610/1000\n",
      "1988/1988 [==============================] - 2s 968us/step - loss: 36.4128 - mae: 4.3748 - val_loss: 200.5627 - val_mae: 10.4182\n",
      "Epoch 611/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 37.5432 - mae: 4.4686 - val_loss: 184.2272 - val_mae: 9.9089\n",
      "Epoch 612/1000\n",
      "1988/1988 [==============================] - 2s 858us/step - loss: 35.7110 - mae: 4.3680 - val_loss: 182.8735 - val_mae: 10.0806\n",
      "Epoch 613/1000\n",
      "1988/1988 [==============================] - 2s 874us/step - loss: 37.0299 - mae: 4.3508 - val_loss: 156.3491 - val_mae: 9.5437\n",
      "Epoch 614/1000\n",
      "1988/1988 [==============================] - 2s 905us/step - loss: 36.6169 - mae: 4.3476 - val_loss: 184.4763 - val_mae: 10.0186\n",
      "Epoch 615/1000\n",
      "1988/1988 [==============================] - 2s 899us/step - loss: 38.8685 - mae: 4.3962 - val_loss: 359.5201 - val_mae: 15.5312\n",
      "Epoch 616/1000\n",
      "1988/1988 [==============================] - 2s 876us/step - loss: 34.8346 - mae: 4.1946 - val_loss: 120.4957 - val_mae: 8.1186\n",
      "Epoch 617/1000\n",
      "1988/1988 [==============================] - 2s 888us/step - loss: 34.8991 - mae: 4.2348 - val_loss: 124.2518 - val_mae: 8.2110\n",
      "Epoch 618/1000\n",
      "1988/1988 [==============================] - 2s 920us/step - loss: 36.9748 - mae: 4.3574 - val_loss: 271.2784 - val_mae: 13.1195\n",
      "Epoch 619/1000\n",
      "1988/1988 [==============================] - 2s 930us/step - loss: 36.0640 - mae: 4.2822 - val_loss: 219.2088 - val_mae: 11.0257\n",
      "Epoch 620/1000\n",
      "1988/1988 [==============================] - 2s 922us/step - loss: 35.0299 - mae: 4.2481 - val_loss: 206.7832 - val_mae: 10.8370\n",
      "Epoch 621/1000\n",
      "1988/1988 [==============================] - 2s 922us/step - loss: 36.8489 - mae: 4.1871 - val_loss: 180.3093 - val_mae: 9.9711\n",
      "Epoch 622/1000\n",
      "1988/1988 [==============================] - 2s 977us/step - loss: 34.7048 - mae: 4.1937 - val_loss: 301.4379 - val_mae: 13.5724\n",
      "Epoch 623/1000\n",
      "1988/1988 [==============================] - 2s 968us/step - loss: 36.6020 - mae: 4.3302 - val_loss: 212.2970 - val_mae: 10.6812\n",
      "Epoch 624/1000\n",
      "1988/1988 [==============================] - 2s 976us/step - loss: 34.8416 - mae: 4.3214 - val_loss: 297.3969 - val_mae: 13.8864\n",
      "Epoch 625/1000\n",
      "1988/1988 [==============================] - 2s 868us/step - loss: 45.0202 - mae: 4.7474 - val_loss: 237.5693 - val_mae: 11.9498\n",
      "Epoch 626/1000\n",
      "1988/1988 [==============================] - 2s 902us/step - loss: 41.7854 - mae: 4.7608 - val_loss: 207.6842 - val_mae: 10.8566\n",
      "Epoch 627/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.3342 - mae: 4.3354 - val_loss: 262.7797 - val_mae: 12.7197\n",
      "Epoch 628/1000\n",
      "1988/1988 [==============================] - 2s 820us/step - loss: 36.2985 - mae: 4.3150 - val_loss: 223.3688 - val_mae: 11.1114\n",
      "Epoch 629/1000\n",
      "1988/1988 [==============================] - 2s 891us/step - loss: 39.7355 - mae: 4.6274 - val_loss: 157.8167 - val_mae: 9.2719\n",
      "Epoch 630/1000\n",
      "1988/1988 [==============================] - 2s 859us/step - loss: 36.2926 - mae: 4.4770 - val_loss: 203.6867 - val_mae: 11.1106\n",
      "Epoch 631/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 36.2312 - mae: 4.3105 - val_loss: 272.6253 - val_mae: 13.0630\n",
      "Epoch 632/1000\n",
      "1988/1988 [==============================] - 2s 863us/step - loss: 38.2182 - mae: 4.4163 - val_loss: 256.8573 - val_mae: 12.6782\n",
      "Epoch 633/1000\n",
      "1988/1988 [==============================] - 2s 868us/step - loss: 38.5211 - mae: 4.4790 - val_loss: 179.0091 - val_mae: 10.0285\n",
      "Epoch 634/1000\n",
      "1988/1988 [==============================] - 2s 868us/step - loss: 34.6284 - mae: 4.2775 - val_loss: 226.3185 - val_mae: 11.6771\n",
      "Epoch 635/1000\n",
      "1988/1988 [==============================] - 2s 900us/step - loss: 36.3301 - mae: 4.3170 - val_loss: 157.8645 - val_mae: 9.3922\n",
      "Epoch 636/1000\n",
      "1988/1988 [==============================] - 2s 929us/step - loss: 39.2069 - mae: 4.5074 - val_loss: 144.7698 - val_mae: 9.0960\n",
      "Epoch 637/1000\n",
      "1988/1988 [==============================] - 2s 908us/step - loss: 35.9951 - mae: 4.3509 - val_loss: 173.5662 - val_mae: 9.8774\n",
      "Epoch 638/1000\n",
      "1988/1988 [==============================] - 2s 896us/step - loss: 36.8365 - mae: 4.3763 - val_loss: 103.7625 - val_mae: 7.6750\n",
      "Epoch 639/1000\n",
      "1988/1988 [==============================] - 2s 888us/step - loss: 39.7926 - mae: 4.5116 - val_loss: 217.3978 - val_mae: 11.2558\n",
      "Epoch 640/1000\n",
      "1988/1988 [==============================] - 2s 886us/step - loss: 40.5170 - mae: 4.4807 - val_loss: 296.6715 - val_mae: 13.6111\n",
      "Epoch 641/1000\n",
      "1988/1988 [==============================] - 2s 891us/step - loss: 38.5671 - mae: 4.5055 - val_loss: 180.8953 - val_mae: 9.8273\n",
      "Epoch 642/1000\n",
      "1988/1988 [==============================] - 2s 863us/step - loss: 36.6623 - mae: 4.2861 - val_loss: 125.2425 - val_mae: 8.2293\n",
      "Epoch 643/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 34.9331 - mae: 4.2801 - val_loss: 249.8839 - val_mae: 11.8403\n",
      "Epoch 644/1000\n",
      "1988/1988 [==============================] - 2s 892us/step - loss: 37.3734 - mae: 4.3485 - val_loss: 320.0585 - val_mae: 14.0506\n",
      "Epoch 645/1000\n",
      "1988/1988 [==============================] - 2s 840us/step - loss: 37.5760 - mae: 4.3208 - val_loss: 325.1512 - val_mae: 14.3950\n",
      "Epoch 646/1000\n",
      "1988/1988 [==============================] - 2s 828us/step - loss: 36.5075 - mae: 4.2867 - val_loss: 135.4649 - val_mae: 8.6808\n",
      "Epoch 647/1000\n",
      "1988/1988 [==============================] - 2s 833us/step - loss: 39.1708 - mae: 4.4092 - val_loss: 208.3861 - val_mae: 10.8744\n",
      "Epoch 648/1000\n",
      "1988/1988 [==============================] - 2s 870us/step - loss: 38.5747 - mae: 4.4423 - val_loss: 205.3594 - val_mae: 10.7494\n",
      "Epoch 649/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 38.3205 - mae: 4.3471 - val_loss: 233.0710 - val_mae: 11.6945\n",
      "Epoch 650/1000\n",
      "1988/1988 [==============================] - 2s 850us/step - loss: 36.7811 - mae: 4.4318 - val_loss: 238.2032 - val_mae: 11.3480\n",
      "Epoch 651/1000\n",
      "1988/1988 [==============================] - 2s 867us/step - loss: 34.9611 - mae: 4.2353 - val_loss: 248.5989 - val_mae: 12.0103\n",
      "Epoch 652/1000\n",
      "1988/1988 [==============================] - 2s 855us/step - loss: 37.3032 - mae: 4.3177 - val_loss: 395.9719 - val_mae: 16.4800\n",
      "Epoch 653/1000\n",
      "1988/1988 [==============================] - 2s 862us/step - loss: 36.9211 - mae: 4.3340 - val_loss: 208.3395 - val_mae: 11.1638\n",
      "Epoch 654/1000\n",
      "1988/1988 [==============================] - 2s 913us/step - loss: 38.1916 - mae: 4.3692 - val_loss: 194.8446 - val_mae: 10.5401\n",
      "Epoch 655/1000\n",
      "1988/1988 [==============================] - 2s 869us/step - loss: 36.2398 - mae: 4.3052 - val_loss: 201.3310 - val_mae: 10.6643\n",
      "Epoch 656/1000\n",
      "1988/1988 [==============================] - 2s 865us/step - loss: 33.8833 - mae: 4.1598 - val_loss: 200.5839 - val_mae: 10.5031\n",
      "Epoch 657/1000\n",
      "1988/1988 [==============================] - 2s 833us/step - loss: 39.1399 - mae: 4.3597 - val_loss: 216.6981 - val_mae: 11.0944\n",
      "Epoch 658/1000\n",
      "1988/1988 [==============================] - 2s 871us/step - loss: 35.3534 - mae: 4.2653 - val_loss: 269.0499 - val_mae: 12.8808\n",
      "Epoch 659/1000\n",
      "1988/1988 [==============================] - 2s 834us/step - loss: 39.7895 - mae: 4.4134 - val_loss: 223.5524 - val_mae: 11.3629\n",
      "Epoch 660/1000\n",
      "1988/1988 [==============================] - 2s 858us/step - loss: 38.2014 - mae: 4.3897 - val_loss: 248.7542 - val_mae: 12.4467\n",
      "Epoch 661/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 35.2688 - mae: 4.2480 - val_loss: 188.9208 - val_mae: 10.7605\n",
      "Epoch 662/1000\n",
      "1988/1988 [==============================] - 2s 862us/step - loss: 36.5234 - mae: 4.3961 - val_loss: 278.9902 - val_mae: 13.1676\n",
      "Epoch 663/1000\n",
      "1988/1988 [==============================] - 2s 946us/step - loss: 36.3131 - mae: 4.3007 - val_loss: 205.8015 - val_mae: 11.0883\n",
      "Epoch 664/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 35.8970 - mae: 4.2166 - val_loss: 236.8220 - val_mae: 12.0621\n",
      "Epoch 665/1000\n",
      "1988/1988 [==============================] - 2s 916us/step - loss: 35.8395 - mae: 4.2510 - val_loss: 202.4507 - val_mae: 10.8731\n",
      "Epoch 666/1000\n",
      "1988/1988 [==============================] - 2s 866us/step - loss: 35.7834 - mae: 4.2270 - val_loss: 219.0338 - val_mae: 11.3370\n",
      "Epoch 667/1000\n",
      "1988/1988 [==============================] - 2s 868us/step - loss: 36.9218 - mae: 4.3075 - val_loss: 393.8864 - val_mae: 16.4553\n",
      "Epoch 668/1000\n",
      "1988/1988 [==============================] - 2s 934us/step - loss: 40.6586 - mae: 4.5292 - val_loss: 125.1235 - val_mae: 8.4302\n",
      "Epoch 669/1000\n",
      "1988/1988 [==============================] - 2s 932us/step - loss: 36.7791 - mae: 4.2746 - val_loss: 188.0740 - val_mae: 10.1636\n",
      "Epoch 670/1000\n",
      "1988/1988 [==============================] - 2s 903us/step - loss: 33.7679 - mae: 4.1909 - val_loss: 107.9421 - val_mae: 7.8379\n",
      "Epoch 671/1000\n",
      "1988/1988 [==============================] - 2s 894us/step - loss: 41.7987 - mae: 4.4486 - val_loss: 250.1154 - val_mae: 12.5680\n",
      "Epoch 672/1000\n",
      "1988/1988 [==============================] - 2s 893us/step - loss: 40.8975 - mae: 4.5173 - val_loss: 145.0689 - val_mae: 8.8812\n",
      "Epoch 673/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 41.4528 - mae: 4.7417 - val_loss: 225.7143 - val_mae: 11.2073\n",
      "Epoch 674/1000\n",
      "1988/1988 [==============================] - 2s 995us/step - loss: 35.5967 - mae: 4.2516 - val_loss: 256.3103 - val_mae: 12.2503\n",
      "Epoch 675/1000\n",
      "1988/1988 [==============================] - 2s 945us/step - loss: 32.6938 - mae: 4.1064 - val_loss: 350.0442 - val_mae: 15.0860\n",
      "Epoch 676/1000\n",
      "1988/1988 [==============================] - 2s 973us/step - loss: 39.1572 - mae: 4.5064 - val_loss: 264.3180 - val_mae: 12.5507\n",
      "Epoch 677/1000\n",
      "1988/1988 [==============================] - 2s 853us/step - loss: 35.5754 - mae: 4.2869 - val_loss: 191.3817 - val_mae: 10.2770\n",
      "Epoch 678/1000\n",
      "1988/1988 [==============================] - 2s 928us/step - loss: 36.8529 - mae: 4.3771 - val_loss: 368.9526 - val_mae: 15.6953\n",
      "Epoch 679/1000\n",
      "1988/1988 [==============================] - 2s 909us/step - loss: 38.3115 - mae: 4.3301 - val_loss: 366.5018 - val_mae: 15.8150\n",
      "Epoch 680/1000\n",
      "1988/1988 [==============================] - 2s 968us/step - loss: 37.4936 - mae: 4.3758 - val_loss: 212.9015 - val_mae: 11.1651\n",
      "Epoch 681/1000\n",
      "1988/1988 [==============================] - 2s 898us/step - loss: 37.5192 - mae: 4.4614 - val_loss: 292.9031 - val_mae: 13.8626\n",
      "Epoch 682/1000\n",
      "1988/1988 [==============================] - 2s 901us/step - loss: 39.6891 - mae: 4.4615 - val_loss: 219.2997 - val_mae: 11.6583\n",
      "Epoch 683/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 39.5432 - mae: 4.5516 - val_loss: 209.0906 - val_mae: 10.9774\n",
      "Epoch 684/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 883us/step - loss: 41.1667 - mae: 4.5443 - val_loss: 215.1979 - val_mae: 11.3420\n",
      "Epoch 685/1000\n",
      "1988/1988 [==============================] - 2s 890us/step - loss: 37.1814 - mae: 4.4742 - val_loss: 329.3382 - val_mae: 14.4956\n",
      "Epoch 686/1000\n",
      "1988/1988 [==============================] - 2s 927us/step - loss: 35.2846 - mae: 4.2386 - val_loss: 220.7591 - val_mae: 11.3194\n",
      "Epoch 687/1000\n",
      "1988/1988 [==============================] - 2s 890us/step - loss: 33.9406 - mae: 4.1377 - val_loss: 309.6110 - val_mae: 14.0566\n",
      "Epoch 688/1000\n",
      "1988/1988 [==============================] - 2s 888us/step - loss: 35.0580 - mae: 4.1896 - val_loss: 270.6105 - val_mae: 12.6462\n",
      "Epoch 689/1000\n",
      "1988/1988 [==============================] - 2s 896us/step - loss: 38.3750 - mae: 4.4699 - val_loss: 282.7696 - val_mae: 12.9780\n",
      "Epoch 690/1000\n",
      "1988/1988 [==============================] - 2s 889us/step - loss: 38.2471 - mae: 4.3588 - val_loss: 416.5554 - val_mae: 16.3847\n",
      "Epoch 691/1000\n",
      "1988/1988 [==============================] - 2s 883us/step - loss: 35.9965 - mae: 4.2573 - val_loss: 375.3610 - val_mae: 15.5900\n",
      "Epoch 692/1000\n",
      "1988/1988 [==============================] - 2s 867us/step - loss: 36.3024 - mae: 4.3364 - val_loss: 365.0398 - val_mae: 15.1609\n",
      "Epoch 693/1000\n",
      "1988/1988 [==============================] - 2s 859us/step - loss: 37.9754 - mae: 4.4070 - val_loss: 393.2023 - val_mae: 15.9625\n",
      "Epoch 694/1000\n",
      "1988/1988 [==============================] - 2s 869us/step - loss: 34.1783 - mae: 4.1257 - val_loss: 284.9128 - val_mae: 12.9804\n",
      "Epoch 695/1000\n",
      "1988/1988 [==============================] - 2s 884us/step - loss: 41.1251 - mae: 4.5999 - val_loss: 427.3063 - val_mae: 16.9947\n",
      "Epoch 696/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 42.1347 - mae: 4.5442 - val_loss: 291.3963 - val_mae: 12.8947\n",
      "Epoch 697/1000\n",
      "1988/1988 [==============================] - 2s 891us/step - loss: 36.9589 - mae: 4.3065 - val_loss: 253.8849 - val_mae: 12.0089\n",
      "Epoch 698/1000\n",
      "1988/1988 [==============================] - 2s 933us/step - loss: 35.7559 - mae: 4.2415 - val_loss: 206.0028 - val_mae: 10.9566\n",
      "Epoch 699/1000\n",
      "1988/1988 [==============================] - 2s 875us/step - loss: 36.3385 - mae: 4.2565 - val_loss: 251.6630 - val_mae: 12.3151\n",
      "Epoch 700/1000\n",
      "1988/1988 [==============================] - 2s 885us/step - loss: 36.7979 - mae: 4.2416 - val_loss: 329.0720 - val_mae: 14.2740\n",
      "Epoch 701/1000\n",
      "1988/1988 [==============================] - 2s 890us/step - loss: 39.7232 - mae: 4.4341 - val_loss: 233.0192 - val_mae: 11.2434\n",
      "Epoch 702/1000\n",
      "1988/1988 [==============================] - 2s 872us/step - loss: 35.7867 - mae: 4.3916 - val_loss: 372.5637 - val_mae: 15.7171\n",
      "Epoch 703/1000\n",
      "1988/1988 [==============================] - 2s 844us/step - loss: 36.9685 - mae: 4.4889 - val_loss: 196.4048 - val_mae: 10.2531\n",
      "Epoch 704/1000\n",
      "1988/1988 [==============================] - 2s 881us/step - loss: 42.9092 - mae: 4.6426 - val_loss: 264.9044 - val_mae: 12.7234\n",
      "Epoch 705/1000\n",
      "1988/1988 [==============================] - 2s 871us/step - loss: 38.8302 - mae: 4.4870 - val_loss: 207.2027 - val_mae: 10.5239\n",
      "Epoch 706/1000\n",
      "1988/1988 [==============================] - 2s 863us/step - loss: 35.6458 - mae: 4.3086 - val_loss: 307.7540 - val_mae: 13.1725\n",
      "Epoch 707/1000\n",
      "1988/1988 [==============================] - 2s 844us/step - loss: 38.8751 - mae: 4.5110 - val_loss: 406.0536 - val_mae: 16.3286\n",
      "Epoch 708/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 39.0289 - mae: 4.4140 - val_loss: 298.3226 - val_mae: 13.2160\n",
      "Epoch 709/1000\n",
      "1988/1988 [==============================] - 2s 796us/step - loss: 37.4313 - mae: 4.2961 - val_loss: 309.6876 - val_mae: 13.9871\n",
      "Epoch 710/1000\n",
      "1988/1988 [==============================] - 2s 802us/step - loss: 34.5009 - mae: 4.2141 - val_loss: 245.4984 - val_mae: 11.9343\n",
      "Epoch 711/1000\n",
      "1988/1988 [==============================] - 2s 797us/step - loss: 33.2745 - mae: 4.1276 - val_loss: 269.7439 - val_mae: 12.5897\n",
      "Epoch 712/1000\n",
      "1988/1988 [==============================] - 2s 821us/step - loss: 35.3304 - mae: 4.1585 - val_loss: 217.6274 - val_mae: 10.6747\n",
      "Epoch 713/1000\n",
      "1988/1988 [==============================] - 2s 812us/step - loss: 39.2399 - mae: 4.4417 - val_loss: 255.4367 - val_mae: 12.2532\n",
      "Epoch 714/1000\n",
      "1988/1988 [==============================] - 2s 889us/step - loss: 32.4974 - mae: 4.1429 - val_loss: 279.5117 - val_mae: 12.7788\n",
      "Epoch 715/1000\n",
      "1988/1988 [==============================] - 2s 827us/step - loss: 38.0639 - mae: 4.4019 - val_loss: 375.6963 - val_mae: 15.7369\n",
      "Epoch 716/1000\n",
      "1988/1988 [==============================] - 2s 914us/step - loss: 40.6056 - mae: 4.4959 - val_loss: 327.6147 - val_mae: 14.3138\n",
      "Epoch 717/1000\n",
      "1988/1988 [==============================] - 2s 886us/step - loss: 36.7987 - mae: 4.2601 - val_loss: 191.8544 - val_mae: 10.5094\n",
      "Epoch 718/1000\n",
      "1988/1988 [==============================] - 2s 888us/step - loss: 38.5141 - mae: 4.3186 - val_loss: 203.0246 - val_mae: 11.4443\n",
      "Epoch 719/1000\n",
      "1988/1988 [==============================] - 2s 898us/step - loss: 37.6074 - mae: 4.3597 - val_loss: 162.2527 - val_mae: 9.4658\n",
      "Epoch 720/1000\n",
      "1988/1988 [==============================] - 2s 888us/step - loss: 39.5499 - mae: 4.2653 - val_loss: 239.1495 - val_mae: 12.0876\n",
      "Epoch 721/1000\n",
      "1988/1988 [==============================] - 2s 857us/step - loss: 35.7775 - mae: 4.2927 - val_loss: 177.2516 - val_mae: 10.1446\n",
      "Epoch 722/1000\n",
      "1988/1988 [==============================] - 2s 926us/step - loss: 33.1974 - mae: 4.2130 - val_loss: 130.8308 - val_mae: 8.5341\n",
      "Epoch 723/1000\n",
      "1988/1988 [==============================] - 2s 916us/step - loss: 38.3491 - mae: 4.5170 - val_loss: 234.7297 - val_mae: 12.0409\n",
      "Epoch 724/1000\n",
      "1988/1988 [==============================] - 2s 895us/step - loss: 37.8290 - mae: 4.3876 - val_loss: 286.8015 - val_mae: 13.6508\n",
      "Epoch 725/1000\n",
      "1988/1988 [==============================] - 2s 855us/step - loss: 36.5261 - mae: 4.2197 - val_loss: 340.4262 - val_mae: 15.0027\n",
      "Epoch 726/1000\n",
      "1988/1988 [==============================] - 2s 861us/step - loss: 37.0985 - mae: 4.2878 - val_loss: 198.2635 - val_mae: 10.3959\n",
      "Epoch 727/1000\n",
      "1988/1988 [==============================] - 2s 856us/step - loss: 35.7916 - mae: 4.2719 - val_loss: 277.3900 - val_mae: 12.8283\n",
      "Epoch 728/1000\n",
      "1988/1988 [==============================] - 2s 891us/step - loss: 36.3308 - mae: 4.3763 - val_loss: 291.3465 - val_mae: 13.1055\n",
      "Epoch 729/1000\n",
      "1988/1988 [==============================] - 2s 903us/step - loss: 39.4467 - mae: 4.5319 - val_loss: 235.1295 - val_mae: 11.6578\n",
      "Epoch 730/1000\n",
      "1988/1988 [==============================] - 2s 921us/step - loss: 38.1367 - mae: 4.3346 - val_loss: 292.0192 - val_mae: 13.9681\n",
      "Epoch 731/1000\n",
      "1988/1988 [==============================] - 2s 949us/step - loss: 35.3918 - mae: 4.3575 - val_loss: 239.5551 - val_mae: 12.6117\n",
      "Epoch 732/1000\n",
      "1988/1988 [==============================] - 2s 929us/step - loss: 34.8586 - mae: 4.3751 - val_loss: 190.2080 - val_mae: 10.4705\n",
      "Epoch 733/1000\n",
      "1988/1988 [==============================] - 2s 919us/step - loss: 37.5749 - mae: 4.2924 - val_loss: 187.7433 - val_mae: 10.4829\n",
      "Epoch 734/1000\n",
      "1988/1988 [==============================] - 2s 927us/step - loss: 39.7524 - mae: 4.3775 - val_loss: 234.9326 - val_mae: 12.2338\n",
      "Epoch 735/1000\n",
      "1988/1988 [==============================] - 2s 906us/step - loss: 35.9303 - mae: 4.2784 - val_loss: 163.5009 - val_mae: 9.8424\n",
      "Epoch 736/1000\n",
      "1988/1988 [==============================] - 2s 920us/step - loss: 37.7080 - mae: 4.4656 - val_loss: 168.7190 - val_mae: 9.8674\n",
      "Epoch 737/1000\n",
      "1988/1988 [==============================] - 2s 852us/step - loss: 34.6831 - mae: 4.2487 - val_loss: 244.4140 - val_mae: 12.3860\n",
      "Epoch 738/1000\n",
      "1988/1988 [==============================] - 2s 862us/step - loss: 37.5688 - mae: 4.3426 - val_loss: 250.8296 - val_mae: 12.5505\n",
      "Epoch 739/1000\n",
      "1988/1988 [==============================] - 2s 889us/step - loss: 37.0747 - mae: 4.3645 - val_loss: 239.4581 - val_mae: 12.2429\n",
      "Epoch 740/1000\n",
      "1988/1988 [==============================] - 2s 866us/step - loss: 37.1071 - mae: 4.3683 - val_loss: 208.3245 - val_mae: 10.9151\n",
      "Epoch 741/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 874us/step - loss: 34.9114 - mae: 4.1879 - val_loss: 362.8076 - val_mae: 16.5366\n",
      "Epoch 742/1000\n",
      "1988/1988 [==============================] - 2s 882us/step - loss: 39.8402 - mae: 4.6756 - val_loss: 221.0736 - val_mae: 11.5584\n",
      "Epoch 743/1000\n",
      "1988/1988 [==============================] - 2s 919us/step - loss: 41.0735 - mae: 4.6172 - val_loss: 225.3732 - val_mae: 11.4404\n",
      "Epoch 744/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 33.7096 - mae: 4.1847 - val_loss: 288.6832 - val_mae: 13.5135\n",
      "Epoch 745/1000\n",
      "1988/1988 [==============================] - 2s 886us/step - loss: 38.4937 - mae: 4.4736 - val_loss: 248.8173 - val_mae: 11.8964\n",
      "Epoch 746/1000\n",
      "1988/1988 [==============================] - 2s 901us/step - loss: 38.3875 - mae: 4.5039 - val_loss: 195.9347 - val_mae: 10.1798\n",
      "Epoch 747/1000\n",
      "1988/1988 [==============================] - 2s 942us/step - loss: 34.7600 - mae: 4.1948 - val_loss: 304.0322 - val_mae: 14.0625\n",
      "Epoch 748/1000\n",
      "1988/1988 [==============================] - 2s 948us/step - loss: 34.2760 - mae: 4.0864 - val_loss: 283.3584 - val_mae: 13.2492\n",
      "Epoch 749/1000\n",
      "1988/1988 [==============================] - 2s 919us/step - loss: 34.9860 - mae: 4.1899 - val_loss: 265.7327 - val_mae: 12.5395\n",
      "Epoch 750/1000\n",
      "1988/1988 [==============================] - 2s 931us/step - loss: 39.0015 - mae: 4.3284 - val_loss: 298.8430 - val_mae: 13.7237\n",
      "Epoch 751/1000\n",
      "1988/1988 [==============================] - 2s 933us/step - loss: 35.7312 - mae: 4.2100 - val_loss: 319.5745 - val_mae: 14.0800\n",
      "Epoch 752/1000\n",
      "1988/1988 [==============================] - 2s 870us/step - loss: 35.0356 - mae: 4.2453 - val_loss: 192.6649 - val_mae: 10.1729\n",
      "Epoch 753/1000\n",
      "1988/1988 [==============================] - 2s 908us/step - loss: 36.0876 - mae: 4.1940 - val_loss: 252.6605 - val_mae: 12.5539\n",
      "Epoch 754/1000\n",
      "1988/1988 [==============================] - 2s 886us/step - loss: 37.5983 - mae: 4.3530 - val_loss: 290.6098 - val_mae: 13.3699\n",
      "Epoch 755/1000\n",
      "1988/1988 [==============================] - 2s 860us/step - loss: 36.9637 - mae: 4.3781 - val_loss: 258.2093 - val_mae: 12.4204\n",
      "Epoch 756/1000\n",
      "1988/1988 [==============================] - 2s 777us/step - loss: 35.4928 - mae: 4.3227 - val_loss: 281.9054 - val_mae: 12.9288\n",
      "Epoch 757/1000\n",
      "1988/1988 [==============================] - 2s 779us/step - loss: 36.5468 - mae: 4.3137 - val_loss: 245.6780 - val_mae: 11.5010\n",
      "Epoch 758/1000\n",
      "1988/1988 [==============================] - 2s 769us/step - loss: 36.5632 - mae: 4.4274 - val_loss: 212.3705 - val_mae: 10.6989\n",
      "Epoch 759/1000\n",
      "1988/1988 [==============================] - 2s 756us/step - loss: 39.5623 - mae: 4.6040 - val_loss: 288.2184 - val_mae: 13.5886\n",
      "Epoch 760/1000\n",
      "1988/1988 [==============================] - 2s 797us/step - loss: 39.0835 - mae: 4.4638 - val_loss: 147.7078 - val_mae: 8.8382\n",
      "Epoch 761/1000\n",
      "1988/1988 [==============================] - 2s 861us/step - loss: 38.9680 - mae: 4.3809 - val_loss: 299.8118 - val_mae: 13.9315\n",
      "Epoch 762/1000\n",
      "1988/1988 [==============================] - 2s 872us/step - loss: 35.7202 - mae: 4.2346 - val_loss: 187.2861 - val_mae: 10.1469\n",
      "Epoch 763/1000\n",
      "1988/1988 [==============================] - 2s 941us/step - loss: 39.1397 - mae: 4.4520 - val_loss: 185.9521 - val_mae: 10.1298\n",
      "Epoch 764/1000\n",
      "1988/1988 [==============================] - 2s 927us/step - loss: 37.8296 - mae: 4.3669 - val_loss: 226.0443 - val_mae: 11.6577\n",
      "Epoch 765/1000\n",
      "1988/1988 [==============================] - 2s 919us/step - loss: 35.0987 - mae: 4.1974 - val_loss: 145.6397 - val_mae: 8.8174\n",
      "Epoch 766/1000\n",
      "1988/1988 [==============================] - 2s 898us/step - loss: 34.5090 - mae: 4.2325 - val_loss: 255.5437 - val_mae: 12.4283\n",
      "Epoch 767/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 37.2996 - mae: 4.3539 - val_loss: 169.0202 - val_mae: 9.5205\n",
      "Epoch 768/1000\n",
      "1988/1988 [==============================] - 2s 912us/step - loss: 39.8140 - mae: 4.4453 - val_loss: 194.7839 - val_mae: 10.8411\n",
      "Epoch 769/1000\n",
      "1988/1988 [==============================] - 2s 904us/step - loss: 39.3641 - mae: 4.6554 - val_loss: 225.9649 - val_mae: 11.5140\n",
      "Epoch 770/1000\n",
      "1988/1988 [==============================] - 2s 897us/step - loss: 36.4394 - mae: 4.3427 - val_loss: 216.1044 - val_mae: 10.8174\n",
      "Epoch 771/1000\n",
      "1988/1988 [==============================] - 2s 923us/step - loss: 36.7828 - mae: 4.2967 - val_loss: 261.1522 - val_mae: 12.2799\n",
      "Epoch 772/1000\n",
      "1988/1988 [==============================] - 2s 862us/step - loss: 36.4059 - mae: 4.2465 - val_loss: 271.5726 - val_mae: 12.9272\n",
      "Epoch 773/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 35.4613 - mae: 4.2355 - val_loss: 172.8812 - val_mae: 9.9157\n",
      "Epoch 774/1000\n",
      "1988/1988 [==============================] - 2s 874us/step - loss: 35.4726 - mae: 4.3397 - val_loss: 254.6772 - val_mae: 12.1093\n",
      "Epoch 775/1000\n",
      "1988/1988 [==============================] - 2s 876us/step - loss: 38.9081 - mae: 4.3930 - val_loss: 353.4785 - val_mae: 15.0846\n",
      "Epoch 776/1000\n",
      "1988/1988 [==============================] - 2s 845us/step - loss: 37.5957 - mae: 4.3168 - val_loss: 170.4155 - val_mae: 9.5492\n",
      "Epoch 777/1000\n",
      "1988/1988 [==============================] - 2s 861us/step - loss: 35.7388 - mae: 4.1639 - val_loss: 185.0107 - val_mae: 9.9164\n",
      "Epoch 778/1000\n",
      "1988/1988 [==============================] - 2s 848us/step - loss: 33.8507 - mae: 4.1438 - val_loss: 234.8417 - val_mae: 11.2021\n",
      "Epoch 779/1000\n",
      "1988/1988 [==============================] - 2s 856us/step - loss: 33.2869 - mae: 4.1333 - val_loss: 169.3268 - val_mae: 9.5040\n",
      "Epoch 780/1000\n",
      "1988/1988 [==============================] - 2s 865us/step - loss: 35.9608 - mae: 4.1968 - val_loss: 179.0958 - val_mae: 10.3204\n",
      "Epoch 781/1000\n",
      "1988/1988 [==============================] - 2s 939us/step - loss: 38.0163 - mae: 4.3934 - val_loss: 124.0896 - val_mae: 8.1470\n",
      "Epoch 782/1000\n",
      "1988/1988 [==============================] - 2s 925us/step - loss: 38.0236 - mae: 4.5482 - val_loss: 310.5477 - val_mae: 14.6112\n",
      "Epoch 783/1000\n",
      "1988/1988 [==============================] - 2s 920us/step - loss: 40.5061 - mae: 4.5165 - val_loss: 248.3930 - val_mae: 12.2009\n",
      "Epoch 784/1000\n",
      "1988/1988 [==============================] - 2s 916us/step - loss: 36.5710 - mae: 4.2075 - val_loss: 364.6000 - val_mae: 15.5335\n",
      "Epoch 785/1000\n",
      "1988/1988 [==============================] - 2s 886us/step - loss: 36.4039 - mae: 4.1891 - val_loss: 241.2130 - val_mae: 11.7300\n",
      "Epoch 786/1000\n",
      "1988/1988 [==============================] - 2s 997us/step - loss: 34.8730 - mae: 4.1772 - val_loss: 305.2522 - val_mae: 14.0718\n",
      "Epoch 787/1000\n",
      "1988/1988 [==============================] - 2s 944us/step - loss: 37.6715 - mae: 4.3081 - val_loss: 241.5624 - val_mae: 11.8257\n",
      "Epoch 788/1000\n",
      "1988/1988 [==============================] - 2s 886us/step - loss: 36.3281 - mae: 4.2529 - val_loss: 193.5900 - val_mae: 10.1784\n",
      "Epoch 789/1000\n",
      "1988/1988 [==============================] - 2s 899us/step - loss: 36.4402 - mae: 4.2992 - val_loss: 256.3139 - val_mae: 12.5972\n",
      "Epoch 790/1000\n",
      "1988/1988 [==============================] - 2s 911us/step - loss: 38.7976 - mae: 4.3718 - val_loss: 208.3616 - val_mae: 11.2678\n",
      "Epoch 791/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 38.2852 - mae: 4.3586 - val_loss: 243.2588 - val_mae: 12.2110\n",
      "Epoch 792/1000\n",
      "1988/1988 [==============================] - 2s 878us/step - loss: 34.2970 - mae: 4.1129 - val_loss: 186.8287 - val_mae: 10.4852\n",
      "Epoch 793/1000\n",
      "1988/1988 [==============================] - 2s 897us/step - loss: 35.6758 - mae: 4.2379 - val_loss: 214.4866 - val_mae: 11.5267\n",
      "Epoch 794/1000\n",
      "1988/1988 [==============================] - 2s 880us/step - loss: 35.5004 - mae: 4.1931 - val_loss: 158.7070 - val_mae: 9.2502\n",
      "Epoch 795/1000\n",
      "1988/1988 [==============================] - 2s 903us/step - loss: 33.9374 - mae: 4.1807 - val_loss: 319.6458 - val_mae: 14.7843\n",
      "Epoch 796/1000\n",
      "1988/1988 [==============================] - 2s 872us/step - loss: 38.7128 - mae: 4.4348 - val_loss: 257.3232 - val_mae: 12.3409\n",
      "Epoch 797/1000\n",
      "1988/1988 [==============================] - 2s 872us/step - loss: 35.7756 - mae: 4.2273 - val_loss: 260.5176 - val_mae: 12.3845\n",
      "Epoch 798/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 914us/step - loss: 35.8607 - mae: 4.1237 - val_loss: 239.0457 - val_mae: 11.7392\n",
      "Epoch 799/1000\n",
      "1988/1988 [==============================] - 2s 961us/step - loss: 35.2491 - mae: 4.2594 - val_loss: 244.6045 - val_mae: 11.9679\n",
      "Epoch 800/1000\n",
      "1988/1988 [==============================] - 2s 888us/step - loss: 37.9213 - mae: 4.3414 - val_loss: 216.3306 - val_mae: 11.1033\n",
      "Epoch 801/1000\n",
      "1988/1988 [==============================] - 2s 955us/step - loss: 35.2568 - mae: 4.2535 - val_loss: 178.4977 - val_mae: 10.3355\n",
      "Epoch 802/1000\n",
      "1988/1988 [==============================] - 2s 897us/step - loss: 35.8541 - mae: 4.2304 - val_loss: 101.6338 - val_mae: 7.6460\n",
      "Epoch 803/1000\n",
      "1988/1988 [==============================] - 2s 875us/step - loss: 37.5095 - mae: 4.5916 - val_loss: 273.2977 - val_mae: 13.1088\n",
      "Epoch 804/1000\n",
      "1988/1988 [==============================] - 2s 951us/step - loss: 36.0155 - mae: 4.3102 - val_loss: 291.2355 - val_mae: 13.4176\n",
      "Epoch 805/1000\n",
      "1988/1988 [==============================] - 2s 909us/step - loss: 36.6907 - mae: 4.4601 - val_loss: 293.8527 - val_mae: 13.5329\n",
      "Epoch 806/1000\n",
      "1988/1988 [==============================] - 2s 923us/step - loss: 32.0700 - mae: 4.0831 - val_loss: 377.1943 - val_mae: 15.5370\n",
      "Epoch 807/1000\n",
      "1988/1988 [==============================] - 2s 867us/step - loss: 36.5972 - mae: 4.2469 - val_loss: 210.1175 - val_mae: 10.5268\n",
      "Epoch 808/1000\n",
      "1988/1988 [==============================] - 2s 905us/step - loss: 35.9928 - mae: 4.2451 - val_loss: 297.7328 - val_mae: 13.4904\n",
      "Epoch 809/1000\n",
      "1988/1988 [==============================] - 2s 958us/step - loss: 34.8463 - mae: 4.1665 - val_loss: 231.7408 - val_mae: 11.4902\n",
      "Epoch 810/1000\n",
      "1988/1988 [==============================] - 2s 940us/step - loss: 36.1919 - mae: 4.1971 - val_loss: 287.9155 - val_mae: 13.6583\n",
      "Epoch 811/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 34.7904 - mae: 4.2399 - val_loss: 319.9383 - val_mae: 14.5189\n",
      "Epoch 812/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 36.9307 - mae: 4.2805 - val_loss: 193.0458 - val_mae: 10.2207\n",
      "Epoch 813/1000\n",
      "1988/1988 [==============================] - 2s 837us/step - loss: 39.4308 - mae: 4.5094 - val_loss: 269.3999 - val_mae: 12.8255\n",
      "Epoch 814/1000\n",
      "1988/1988 [==============================] - 2s 837us/step - loss: 38.6139 - mae: 4.4552 - val_loss: 197.2821 - val_mae: 10.3784\n",
      "Epoch 815/1000\n",
      "1988/1988 [==============================] - 2s 825us/step - loss: 37.7095 - mae: 4.3790 - val_loss: 229.9841 - val_mae: 11.5563\n",
      "Epoch 816/1000\n",
      "1988/1988 [==============================] - 2s 869us/step - loss: 38.7336 - mae: 4.4015 - val_loss: 266.3299 - val_mae: 12.8847\n",
      "Epoch 817/1000\n",
      "1988/1988 [==============================] - 2s 865us/step - loss: 37.7236 - mae: 4.2774 - val_loss: 139.3731 - val_mae: 8.5151\n",
      "Epoch 818/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 35.3200 - mae: 4.2351 - val_loss: 180.3715 - val_mae: 10.0445\n",
      "Epoch 819/1000\n",
      "1988/1988 [==============================] - 2s 826us/step - loss: 35.1443 - mae: 4.1609 - val_loss: 271.0182 - val_mae: 13.2451\n",
      "Epoch 820/1000\n",
      "1988/1988 [==============================] - 2s 836us/step - loss: 36.6564 - mae: 4.3044 - val_loss: 365.8344 - val_mae: 15.1511\n",
      "Epoch 821/1000\n",
      "1988/1988 [==============================] - 2s 878us/step - loss: 36.4979 - mae: 4.3174 - val_loss: 285.0272 - val_mae: 12.6540\n",
      "Epoch 822/1000\n",
      "1988/1988 [==============================] - 2s 853us/step - loss: 38.1722 - mae: 4.4379 - val_loss: 393.9568 - val_mae: 15.9604\n",
      "Epoch 823/1000\n",
      "1988/1988 [==============================] - 2s 782us/step - loss: 35.5433 - mae: 4.3069 - val_loss: 422.8778 - val_mae: 16.6166\n",
      "Epoch 824/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.4788 - mae: 4.2210 - val_loss: 307.4145 - val_mae: 13.4019\n",
      "Epoch 825/1000\n",
      "1988/1988 [==============================] - 2s 929us/step - loss: 37.5133 - mae: 4.2590 - val_loss: 384.3865 - val_mae: 15.4822\n",
      "Epoch 826/1000\n",
      "1988/1988 [==============================] - 2s 882us/step - loss: 34.9016 - mae: 4.1708 - val_loss: 298.5227 - val_mae: 13.0982\n",
      "Epoch 827/1000\n",
      "1988/1988 [==============================] - 2s 888us/step - loss: 36.3086 - mae: 4.3657 - val_loss: 291.8891 - val_mae: 13.1967\n",
      "Epoch 828/1000\n",
      "1988/1988 [==============================] - 2s 967us/step - loss: 37.7533 - mae: 4.4145 - val_loss: 266.2318 - val_mae: 12.5629\n",
      "Epoch 829/1000\n",
      "1988/1988 [==============================] - 2s 938us/step - loss: 36.3347 - mae: 4.3275 - val_loss: 367.6075 - val_mae: 15.2890\n",
      "Epoch 830/1000\n",
      "1988/1988 [==============================] - 2s 858us/step - loss: 36.4898 - mae: 4.3180 - val_loss: 327.7794 - val_mae: 14.4242\n",
      "Epoch 831/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 34.3852 - mae: 4.2108 - val_loss: 301.4103 - val_mae: 13.5115\n",
      "Epoch 832/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 37.5889 - mae: 4.3258 - val_loss: 386.1962 - val_mae: 15.9683\n",
      "Epoch 833/1000\n",
      "1988/1988 [==============================] - 2s 824us/step - loss: 34.7128 - mae: 4.2372 - val_loss: 222.4403 - val_mae: 11.0050\n",
      "Epoch 834/1000\n",
      "1988/1988 [==============================] - 2s 832us/step - loss: 35.2204 - mae: 4.3055 - val_loss: 198.1399 - val_mae: 10.3546\n",
      "Epoch 835/1000\n",
      "1988/1988 [==============================] - 2s 812us/step - loss: 36.8572 - mae: 4.3229 - val_loss: 372.4434 - val_mae: 15.7628\n",
      "Epoch 836/1000\n",
      "1988/1988 [==============================] - 2s 786us/step - loss: 36.3116 - mae: 4.2472 - val_loss: 339.1208 - val_mae: 14.6252\n",
      "Epoch 837/1000\n",
      "1988/1988 [==============================] - 2s 783us/step - loss: 36.7342 - mae: 4.3859 - val_loss: 300.9155 - val_mae: 13.4240\n",
      "Epoch 838/1000\n",
      "1988/1988 [==============================] - 2s 808us/step - loss: 37.7025 - mae: 4.4128 - val_loss: 368.3042 - val_mae: 15.4981\n",
      "Epoch 839/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 35.0788 - mae: 4.2480 - val_loss: 257.9555 - val_mae: 11.9846\n",
      "Epoch 840/1000\n",
      "1988/1988 [==============================] - 2s 850us/step - loss: 38.0416 - mae: 4.3925 - val_loss: 212.4058 - val_mae: 11.0226\n",
      "Epoch 841/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 36.9279 - mae: 4.5195 - val_loss: 148.9754 - val_mae: 8.8450\n",
      "Epoch 842/1000\n",
      "1988/1988 [==============================] - 2s 817us/step - loss: 39.7883 - mae: 4.6157 - val_loss: 226.3446 - val_mae: 11.4998\n",
      "Epoch 843/1000\n",
      "1988/1988 [==============================] - 2s 838us/step - loss: 37.2422 - mae: 4.4243 - val_loss: 265.0678 - val_mae: 12.4582\n",
      "Epoch 844/1000\n",
      "1988/1988 [==============================] - 2s 897us/step - loss: 33.7913 - mae: 4.1808 - val_loss: 314.7733 - val_mae: 14.0102\n",
      "Epoch 845/1000\n",
      "1988/1988 [==============================] - 2s 984us/step - loss: 36.2525 - mae: 4.3895 - val_loss: 281.8660 - val_mae: 13.0118\n",
      "Epoch 846/1000\n",
      "1988/1988 [==============================] - 2s 866us/step - loss: 34.9697 - mae: 4.2022 - val_loss: 319.7932 - val_mae: 13.9457\n",
      "Epoch 847/1000\n",
      "1988/1988 [==============================] - 2s 853us/step - loss: 35.7849 - mae: 4.1979 - val_loss: 266.7318 - val_mae: 12.1336\n",
      "Epoch 848/1000\n",
      "1988/1988 [==============================] - 2s 845us/step - loss: 40.1144 - mae: 4.6131 - val_loss: 403.3677 - val_mae: 16.5267\n",
      "Epoch 849/1000\n",
      "1988/1988 [==============================] - 2s 847us/step - loss: 36.6680 - mae: 4.3735 - val_loss: 287.4138 - val_mae: 12.8756\n",
      "Epoch 850/1000\n",
      "1988/1988 [==============================] - 2s 924us/step - loss: 37.1147 - mae: 4.3109 - val_loss: 336.8267 - val_mae: 14.7378\n",
      "Epoch 851/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 36.0124 - mae: 4.2579 - val_loss: 277.5511 - val_mae: 12.4262\n",
      "Epoch 852/1000\n",
      "1988/1988 [==============================] - 2s 845us/step - loss: 37.0436 - mae: 4.3154 - val_loss: 381.7243 - val_mae: 15.3884\n",
      "Epoch 853/1000\n",
      "1988/1988 [==============================] - 2s 835us/step - loss: 36.0014 - mae: 4.4278 - val_loss: 230.3114 - val_mae: 10.9843\n",
      "Epoch 854/1000\n",
      "1988/1988 [==============================] - 2s 799us/step - loss: 38.1827 - mae: 4.4275 - val_loss: 281.2212 - val_mae: 12.7716\n",
      "Epoch 855/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 2s 857us/step - loss: 41.3404 - mae: 4.6519 - val_loss: 209.8283 - val_mae: 10.6339\n",
      "Epoch 856/1000\n",
      "1988/1988 [==============================] - 2s 911us/step - loss: 35.5052 - mae: 4.3358 - val_loss: 234.4752 - val_mae: 11.4479\n",
      "Epoch 857/1000\n",
      "1988/1988 [==============================] - 2s 920us/step - loss: 36.5899 - mae: 4.2274 - val_loss: 323.4910 - val_mae: 13.9644\n",
      "Epoch 858/1000\n",
      "1988/1988 [==============================] - 2s 949us/step - loss: 34.8832 - mae: 4.1726 - val_loss: 504.0987 - val_mae: 18.9955\n",
      "Epoch 859/1000\n",
      "1988/1988 [==============================] - 2s 916us/step - loss: 36.5756 - mae: 4.2524 - val_loss: 385.0012 - val_mae: 15.6271\n",
      "Epoch 860/1000\n",
      "1988/1988 [==============================] - 2s 890us/step - loss: 33.0323 - mae: 4.2228 - val_loss: 230.4436 - val_mae: 11.1751\n",
      "Epoch 861/1000\n",
      "1988/1988 [==============================] - 2s 908us/step - loss: 38.3682 - mae: 4.3882 - val_loss: 470.4264 - val_mae: 18.3180\n",
      "Epoch 862/1000\n",
      "1988/1988 [==============================] - 2s 929us/step - loss: 36.5479 - mae: 4.3110 - val_loss: 267.0213 - val_mae: 12.8661\n",
      "Epoch 863/1000\n",
      "1988/1988 [==============================] - 2s 886us/step - loss: 36.1507 - mae: 4.3010 - val_loss: 230.2779 - val_mae: 11.1688\n",
      "Epoch 864/1000\n",
      "1988/1988 [==============================] - 2s 860us/step - loss: 37.1968 - mae: 4.3737 - val_loss: 345.6772 - val_mae: 14.4957\n",
      "Epoch 865/1000\n",
      "1988/1988 [==============================] - 2s 877us/step - loss: 34.5399 - mae: 4.2623 - val_loss: 298.8395 - val_mae: 13.1308\n",
      "Epoch 866/1000\n",
      "1988/1988 [==============================] - 2s 892us/step - loss: 42.3676 - mae: 4.5647 - val_loss: 417.8835 - val_mae: 17.0800\n",
      "Epoch 867/1000\n",
      "1988/1988 [==============================] - 2s 830us/step - loss: 34.0782 - mae: 4.1249 - val_loss: 258.4157 - val_mae: 12.2993\n",
      "Epoch 868/1000\n",
      "1988/1988 [==============================] - 2s 849us/step - loss: 34.3212 - mae: 4.1926 - val_loss: 235.8000 - val_mae: 11.7022\n",
      "Epoch 869/1000\n",
      "1988/1988 [==============================] - 2s 852us/step - loss: 35.4423 - mae: 4.2259 - val_loss: 415.7451 - val_mae: 17.1783\n",
      "Epoch 870/1000\n",
      "1988/1988 [==============================] - 2s 873us/step - loss: 36.7097 - mae: 4.4551 - val_loss: 268.9049 - val_mae: 12.4458\n",
      "Epoch 871/1000\n",
      "1988/1988 [==============================] - 2s 851us/step - loss: 37.1915 - mae: 4.4241 - val_loss: 362.8785 - val_mae: 15.3433\n",
      "Epoch 872/1000\n",
      "1988/1988 [==============================] - 2s 871us/step - loss: 38.6256 - mae: 4.3984 - val_loss: 290.2845 - val_mae: 13.2148\n",
      "Epoch 873/1000\n",
      "1988/1988 [==============================] - 2s 892us/step - loss: 38.0504 - mae: 4.3399 - val_loss: 271.9256 - val_mae: 12.4837\n",
      "Epoch 874/1000\n",
      "1988/1988 [==============================] - 2s 879us/step - loss: 37.8946 - mae: 4.3584 - val_loss: 385.5095 - val_mae: 15.8027\n",
      "Epoch 875/1000\n",
      "1988/1988 [==============================] - 2s 872us/step - loss: 36.6582 - mae: 4.2971 - val_loss: 233.9533 - val_mae: 11.2440\n",
      "Epoch 876/1000\n",
      "1988/1988 [==============================] - 2s 863us/step - loss: 32.5974 - mae: 4.1366 - val_loss: 274.5868 - val_mae: 12.4134\n",
      "Epoch 877/1000\n",
      "1988/1988 [==============================] - 2s 843us/step - loss: 36.8346 - mae: 4.3742 - val_loss: 305.1506 - val_mae: 13.6801\n",
      "Epoch 878/1000\n",
      "1988/1988 [==============================] - 2s 823us/step - loss: 31.7045 - mae: 4.1178 - val_loss: 169.5595 - val_mae: 9.5507\n",
      "Epoch 879/1000\n",
      "1988/1988 [==============================] - 2s 855us/step - loss: 37.7769 - mae: 4.4776 - val_loss: 263.1321 - val_mae: 12.5219\n",
      "Epoch 880/1000\n",
      "1988/1988 [==============================] - 2s 854us/step - loss: 40.3091 - mae: 4.4280 - val_loss: 290.2394 - val_mae: 13.3916\n",
      "Epoch 881/1000\n",
      "1988/1988 [==============================] - 2s 920us/step - loss: 37.2096 - mae: 4.3352 - val_loss: 267.5802 - val_mae: 13.0485\n",
      "Epoch 882/1000\n",
      "1988/1988 [==============================] - 2s 928us/step - loss: 31.2731 - mae: 4.0918 - val_loss: 288.8179 - val_mae: 13.3421\n",
      "Epoch 883/1000\n",
      "1988/1988 [==============================] - 2s 950us/step - loss: 36.9882 - mae: 4.3534 - val_loss: 321.9341 - val_mae: 14.2279\n",
      "Epoch 884/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 40.0949 - mae: 4.5698 - val_loss: 198.0112 - val_mae: 10.2824\n",
      "Epoch 885/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 38.5623 - mae: 4.4290 - val_loss: 279.6611 - val_mae: 12.9505\n",
      "Epoch 886/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.7460 - mae: 4.5134 - val_loss: 230.4995 - val_mae: 11.5480\n",
      "Epoch 887/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.8773 - mae: 4.3812 - val_loss: 231.3508 - val_mae: 11.4532\n",
      "Epoch 888/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.2501 - mae: 4.3067 - val_loss: 205.1272 - val_mae: 10.5445\n",
      "Epoch 889/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 38.9917 - mae: 4.3307 - val_loss: 344.6775 - val_mae: 15.1595\n",
      "Epoch 890/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.2464 - mae: 4.2064 - val_loss: 199.1442 - val_mae: 10.6104\n",
      "Epoch 891/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.7407 - mae: 4.1664 - val_loss: 222.8707 - val_mae: 11.3843\n",
      "Epoch 892/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.0030 - mae: 4.1769 - val_loss: 258.4304 - val_mae: 12.5526\n",
      "Epoch 893/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.0597 - mae: 4.1841 - val_loss: 213.1524 - val_mae: 10.8969\n",
      "Epoch 894/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.1586 - mae: 4.2238 - val_loss: 249.2813 - val_mae: 12.4003\n",
      "Epoch 895/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.5367 - mae: 4.1553 - val_loss: 244.7709 - val_mae: 12.7352\n",
      "Epoch 896/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 38.5155 - mae: 4.3432 - val_loss: 109.4084 - val_mae: 7.8762\n",
      "Epoch 897/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.9979 - mae: 4.4612 - val_loss: 175.0794 - val_mae: 9.8596\n",
      "Epoch 898/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.3197 - mae: 4.3957 - val_loss: 185.6120 - val_mae: 10.3862\n",
      "Epoch 899/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 32.1971 - mae: 4.0728 - val_loss: 216.5034 - val_mae: 11.5007\n",
      "Epoch 900/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.3226 - mae: 4.2343 - val_loss: 186.2021 - val_mae: 10.2304\n",
      "Epoch 901/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.2257 - mae: 4.1632 - val_loss: 225.9201 - val_mae: 11.6823\n",
      "Epoch 902/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.2800 - mae: 4.2500 - val_loss: 259.9693 - val_mae: 12.8751\n",
      "Epoch 903/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.6321 - mae: 4.2668 - val_loss: 201.9059 - val_mae: 10.9390\n",
      "Epoch 904/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.6630 - mae: 4.1719 - val_loss: 173.9655 - val_mae: 10.1600\n",
      "Epoch 905/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.9046 - mae: 4.1720 - val_loss: 114.6045 - val_mae: 7.8482\n",
      "Epoch 906/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.6906 - mae: 4.2750 - val_loss: 152.2247 - val_mae: 9.2171\n",
      "Epoch 907/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 40.8340 - mae: 4.6180 - val_loss: 124.7979 - val_mae: 8.1866\n",
      "Epoch 908/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.5410 - mae: 4.3298 - val_loss: 149.3517 - val_mae: 8.9095\n",
      "Epoch 909/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.5227 - mae: 4.3380 - val_loss: 306.2142 - val_mae: 14.8154\n",
      "Epoch 910/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.2700 - mae: 4.2194 - val_loss: 150.4402 - val_mae: 9.3261\n",
      "Epoch 911/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.1476 - mae: 4.2917 - val_loss: 122.2273 - val_mae: 8.0476\n",
      "Epoch 912/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.3648 - mae: 4.2164 - val_loss: 271.0099 - val_mae: 13.4465\n",
      "Epoch 913/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.6009 - mae: 4.2521 - val_loss: 232.2168 - val_mae: 12.1738\n",
      "Epoch 914/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.5655 - mae: 4.1730 - val_loss: 232.5267 - val_mae: 12.1835\n",
      "Epoch 915/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.1801 - mae: 4.2246 - val_loss: 126.0848 - val_mae: 8.2631\n",
      "Epoch 916/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 39.2387 - mae: 4.4064 - val_loss: 177.9541 - val_mae: 10.2033\n",
      "Epoch 917/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.8761 - mae: 4.2418 - val_loss: 191.5145 - val_mae: 10.7702\n",
      "Epoch 918/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.2020 - mae: 4.2498 - val_loss: 228.6417 - val_mae: 11.6794\n",
      "Epoch 919/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.6731 - mae: 4.2034 - val_loss: 174.0838 - val_mae: 9.8020\n",
      "Epoch 920/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 38.0481 - mae: 4.3683 - val_loss: 199.4635 - val_mae: 10.5859\n",
      "Epoch 921/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.3458 - mae: 4.2980 - val_loss: 182.6785 - val_mae: 10.0415\n",
      "Epoch 922/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.2298 - mae: 4.1896 - val_loss: 207.1298 - val_mae: 10.8572\n",
      "Epoch 923/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.2915 - mae: 4.1725 - val_loss: 181.3127 - val_mae: 10.0102\n",
      "Epoch 924/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 40.6873 - mae: 4.5196 - val_loss: 282.6784 - val_mae: 13.3634\n",
      "Epoch 925/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.9644 - mae: 4.2515 - val_loss: 280.3773 - val_mae: 13.3312\n",
      "Epoch 926/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 33.9202 - mae: 4.1300 - val_loss: 185.2447 - val_mae: 10.2776\n",
      "Epoch 927/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.0918 - mae: 4.2421 - val_loss: 213.0990 - val_mae: 11.3773\n",
      "Epoch 928/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.7453 - mae: 4.2037 - val_loss: 197.5882 - val_mae: 11.0277\n",
      "Epoch 929/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.0421 - mae: 4.1848 - val_loss: 244.7226 - val_mae: 12.3788\n",
      "Epoch 930/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 38.4843 - mae: 4.3488 - val_loss: 386.5346 - val_mae: 16.8328\n",
      "Epoch 931/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 39.9363 - mae: 4.3364 - val_loss: 302.1927 - val_mae: 14.2913\n",
      "Epoch 932/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 38.8210 - mae: 4.3777 - val_loss: 320.2860 - val_mae: 14.7104\n",
      "Epoch 933/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.6751 - mae: 4.3438 - val_loss: 167.0800 - val_mae: 9.7056\n",
      "Epoch 934/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.7724 - mae: 4.3159 - val_loss: 125.1000 - val_mae: 8.0189\n",
      "Epoch 935/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.6998 - mae: 4.2009 - val_loss: 106.7026 - val_mae: 7.1993\n",
      "Epoch 936/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 33.8333 - mae: 4.0837 - val_loss: 138.9465 - val_mae: 8.7005\n",
      "Epoch 937/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.7746 - mae: 4.2768 - val_loss: 238.8434 - val_mae: 12.2959\n",
      "Epoch 938/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.0308 - mae: 4.3445 - val_loss: 317.1642 - val_mae: 14.4207\n",
      "Epoch 939/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.6653 - mae: 4.3562 - val_loss: 344.0349 - val_mae: 15.3377\n",
      "Epoch 940/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.4054 - mae: 4.1969 - val_loss: 274.4616 - val_mae: 13.3419\n",
      "Epoch 941/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 33.7079 - mae: 4.0862 - val_loss: 298.7619 - val_mae: 14.1908\n",
      "Epoch 942/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.4406 - mae: 4.2512 - val_loss: 209.9711 - val_mae: 11.4385\n",
      "Epoch 943/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.5014 - mae: 4.1990 - val_loss: 180.8187 - val_mae: 10.4084\n",
      "Epoch 944/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 32.6203 - mae: 4.0532 - val_loss: 148.9474 - val_mae: 9.2128\n",
      "Epoch 945/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.4235 - mae: 4.3551 - val_loss: 212.0078 - val_mae: 11.3652\n",
      "Epoch 946/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.0719 - mae: 4.1825 - val_loss: 162.9551 - val_mae: 9.5534\n",
      "Epoch 947/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 39.4190 - mae: 4.2870 - val_loss: 167.0014 - val_mae: 9.5922\n",
      "Epoch 948/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.5628 - mae: 4.3103 - val_loss: 136.9522 - val_mae: 8.4466\n",
      "Epoch 949/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.4718 - mae: 4.2766 - val_loss: 156.1650 - val_mae: 9.2561\n",
      "Epoch 950/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.1683 - mae: 4.2841 - val_loss: 200.0163 - val_mae: 10.9670\n",
      "Epoch 951/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.4407 - mae: 4.3672 - val_loss: 229.6872 - val_mae: 11.9156\n",
      "Epoch 952/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.6617 - mae: 4.3826 - val_loss: 221.1481 - val_mae: 11.7297\n",
      "Epoch 953/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.3377 - mae: 4.2010 - val_loss: 202.8796 - val_mae: 11.0397\n",
      "Epoch 954/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.1091 - mae: 4.1505 - val_loss: 385.6573 - val_mae: 16.6797\n",
      "Epoch 955/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.2307 - mae: 4.1631 - val_loss: 284.9660 - val_mae: 13.7966\n",
      "Epoch 956/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 32.4107 - mae: 4.0617 - val_loss: 192.0010 - val_mae: 10.7466\n",
      "Epoch 957/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 33.9493 - mae: 4.0941 - val_loss: 296.8814 - val_mae: 14.3089\n",
      "Epoch 958/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.0214 - mae: 4.1967 - val_loss: 200.1750 - val_mae: 11.0760\n",
      "Epoch 959/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.6690 - mae: 4.3331 - val_loss: 119.6098 - val_mae: 7.8708\n",
      "Epoch 960/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.4423 - mae: 4.2310 - val_loss: 123.6567 - val_mae: 7.9813\n",
      "Epoch 961/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.3819 - mae: 4.2299 - val_loss: 216.6754 - val_mae: 11.6491\n",
      "Epoch 962/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.8366 - mae: 4.2117 - val_loss: 150.4964 - val_mae: 8.9329\n",
      "Epoch 963/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.5446 - mae: 4.2722 - val_loss: 169.1669 - val_mae: 10.1986\n",
      "Epoch 964/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.2933 - mae: 4.3371 - val_loss: 234.3200 - val_mae: 12.6280\n",
      "Epoch 965/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 31.7827 - mae: 4.0144 - val_loss: 113.9592 - val_mae: 7.8302\n",
      "Epoch 966/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.4779 - mae: 4.2541 - val_loss: 139.1426 - val_mae: 8.7899\n",
      "Epoch 967/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.9525 - mae: 4.1409 - val_loss: 193.6485 - val_mae: 10.9082\n",
      "Epoch 968/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.5996 - mae: 4.1786 - val_loss: 125.0399 - val_mae: 8.0515\n",
      "Epoch 969/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.1448 - mae: 4.1365 - val_loss: 183.3776 - val_mae: 10.6320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.7474 - mae: 4.2431 - val_loss: 194.9082 - val_mae: 10.9031\n",
      "Epoch 971/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.0899 - mae: 4.1797 - val_loss: 199.6050 - val_mae: 10.8764\n",
      "Epoch 972/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 38.1972 - mae: 4.3703 - val_loss: 210.7917 - val_mae: 11.4317\n",
      "Epoch 973/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.3640 - mae: 4.2586 - val_loss: 231.5809 - val_mae: 12.0789\n",
      "Epoch 974/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.1832 - mae: 4.0934 - val_loss: 179.4843 - val_mae: 10.1877\n",
      "Epoch 975/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.0325 - mae: 4.1091 - val_loss: 134.3356 - val_mae: 8.6004\n",
      "Epoch 976/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.5241 - mae: 4.1632 - val_loss: 138.7999 - val_mae: 8.7464\n",
      "Epoch 977/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 38.2137 - mae: 4.2696 - val_loss: 190.7807 - val_mae: 10.6238\n",
      "Epoch 978/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.1948 - mae: 4.0730 - val_loss: 243.1633 - val_mae: 12.3912\n",
      "Epoch 979/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 33.8615 - mae: 4.1717 - val_loss: 190.5667 - val_mae: 10.5548\n",
      "Epoch 980/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.5918 - mae: 4.1887 - val_loss: 187.5830 - val_mae: 10.3959\n",
      "Epoch 981/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.2208 - mae: 4.1190 - val_loss: 143.2639 - val_mae: 8.8596\n",
      "Epoch 982/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.2899 - mae: 4.2192 - val_loss: 158.4627 - val_mae: 9.1207\n",
      "Epoch 983/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.2176 - mae: 4.0732 - val_loss: 299.8836 - val_mae: 14.0159\n",
      "Epoch 984/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.5338 - mae: 4.1283 - val_loss: 296.5320 - val_mae: 13.8284\n",
      "Epoch 985/1000\n",
      "1988/1988 [==============================] - 2s 963us/step - loss: 34.3613 - mae: 4.2031 - val_loss: 165.6481 - val_mae: 9.4025\n",
      "Epoch 986/1000\n",
      "1988/1988 [==============================] - 2s 975us/step - loss: 35.8828 - mae: 4.2181 - val_loss: 246.9782 - val_mae: 12.3322\n",
      "Epoch 987/1000\n",
      "1988/1988 [==============================] - 2s 944us/step - loss: 40.5255 - mae: 4.4718 - val_loss: 288.8098 - val_mae: 13.7276\n",
      "Epoch 988/1000\n",
      "1988/1988 [==============================] - 2s 949us/step - loss: 36.3941 - mae: 4.2579 - val_loss: 164.7680 - val_mae: 9.4779\n",
      "Epoch 989/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 36.6408 - mae: 4.2591 - val_loss: 259.4742 - val_mae: 12.7497\n",
      "Epoch 990/1000\n",
      "1988/1988 [==============================] - 2s 946us/step - loss: 37.0344 - mae: 4.2560 - val_loss: 263.9726 - val_mae: 12.8410\n",
      "Epoch 991/1000\n",
      "1988/1988 [==============================] - 2s 990us/step - loss: 35.6951 - mae: 4.1857 - val_loss: 169.5784 - val_mae: 9.7433\n",
      "Epoch 992/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.2643 - mae: 4.2383 - val_loss: 248.0385 - val_mae: 12.5319\n",
      "Epoch 993/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.9007 - mae: 4.2023 - val_loss: 170.8129 - val_mae: 9.8533\n",
      "Epoch 994/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 40.1162 - mae: 4.4534 - val_loss: 234.3280 - val_mae: 12.1466\n",
      "Epoch 995/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.4370 - mae: 4.3126 - val_loss: 195.9911 - val_mae: 10.7874\n",
      "Epoch 996/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.9735 - mae: 4.2414 - val_loss: 188.0411 - val_mae: 10.3976\n",
      "Epoch 997/1000\n",
      "1988/1988 [==============================] - 2s 999us/step - loss: 36.5206 - mae: 4.2208 - val_loss: 234.0704 - val_mae: 12.1906\n",
      "Epoch 998/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 35.6703 - mae: 4.2021 - val_loss: 240.6438 - val_mae: 12.1056\n",
      "Epoch 999/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 34.9795 - mae: 4.2338 - val_loss: 366.3120 - val_mae: 15.9817\n",
      "Epoch 1000/1000\n",
      "1988/1988 [==============================] - 2s 1ms/step - loss: 37.0849 - mae: 4.3428 - val_loss: 253.8908 - val_mae: 12.7483\n",
      "\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 373us/step\n",
      "test loss, test acc: [261.760806792123, 12.573570251464844]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 16.22102092762645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>true_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166.021179</td>\n",
       "      <td>185.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.285522</td>\n",
       "      <td>176.979996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150.645950</td>\n",
       "      <td>176.779999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154.483429</td>\n",
       "      <td>172.289993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157.120148</td>\n",
       "      <td>174.240005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>223.864151</td>\n",
       "      <td>259.429993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>222.262772</td>\n",
       "      <td>260.140015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>233.876389</td>\n",
       "      <td>262.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>212.728867</td>\n",
       "      <td>261.959991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>217.471359</td>\n",
       "      <td>264.470001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  true_value\n",
       "0    166.021179  185.860001\n",
       "1    172.285522  176.979996\n",
       "2    150.645950  176.779999\n",
       "3    154.483429  172.289993\n",
       "4    157.120148  174.240005\n",
       "..          ...         ...\n",
       "240  223.864151  259.429993\n",
       "241  222.262772  260.140015\n",
       "242  233.876389  262.200012\n",
       "243  212.728867  261.959991\n",
       "244  217.471359  264.470001\n",
       "\n",
       "[245 rows x 2 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_2stacks_true_value, 5, \n",
    "                                                    stock_with_abs_norm, label_value_1d, 64, batch_size, \"loss\")\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=7)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test, batch_size=7)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)\n",
    "predictFrame = pd.DataFrame({'prediction': predictions.reshape(X_test.shape[0]), 'true_value': y_test.reshape(X_test.shape[0])})\n",
    "predictFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = predicted_train.reshape(X_train.shape[0]).tolist()\n",
    "b = predicted_valid.reshape(X_valid.shape[0]).tolist()\n",
    "c = predicted_test.reshape(X_test.shape[0]).tolist()\n",
    "d = y_train.reshape(X_train.shape[0]).tolist()\n",
    "e = y_valid.reshape(X_valid.shape[0]).tolist()\n",
    "f = y_test.reshape(X_test.shape[0]).tolist()\n",
    "a.extend(b)\n",
    "a.extend(c)\n",
    "d.extend(e)\n",
    "d.extend(f)\n",
    "predictFrame = pd.DataFrame({'prediction': a, 'true_value': d})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1988 samples, validate on 217 samples\n",
      "Epoch 1/2000\n",
      "1988/1988 [==============================] - 2s 903us/step - loss: 606.2199 - mae: 21.3403 - val_loss: 2807.8717 - val_mae: 48.9406\n",
      "Epoch 2/2000\n",
      "1988/1988 [==============================] - 1s 542us/step - loss: 1491.1955 - mae: 28.6209 - val_loss: 4308.2564 - val_mae: 62.4145\n",
      "Epoch 3/2000\n",
      "1988/1988 [==============================] - 1s 548us/step - loss: 1397.2132 - mae: 28.4161 - val_loss: 4565.4683 - val_mae: 64.4420\n",
      "Epoch 4/2000\n",
      "1988/1988 [==============================] - 1s 584us/step - loss: 1408.1130 - mae: 28.4804 - val_loss: 4662.8683 - val_mae: 65.1934\n",
      "Epoch 5/2000\n",
      "1988/1988 [==============================] - 1s 557us/step - loss: 1383.8300 - mae: 28.5844 - val_loss: 4702.1698 - val_mae: 65.4941\n",
      "Epoch 6/2000\n",
      "1988/1988 [==============================] - 1s 500us/step - loss: 1388.4983 - mae: 28.5395 - val_loss: 4783.3525 - val_mae: 66.1110\n",
      "Epoch 7/2000\n",
      "1988/1988 [==============================] - 1s 497us/step - loss: 1457.6437 - mae: 29.5705 - val_loss: 4979.9539 - val_mae: 67.5816\n",
      "Epoch 8/2000\n",
      "1988/1988 [==============================] - 1s 491us/step - loss: 1359.7908 - mae: 28.5213 - val_loss: 4927.6002 - val_mae: 67.1931\n",
      "Epoch 9/2000\n",
      "1988/1988 [==============================] - 1s 501us/step - loss: 1436.8230 - mae: 29.2485 - val_loss: 4937.8978 - val_mae: 67.2697\n",
      "Epoch 10/2000\n",
      "1988/1988 [==============================] - 1s 495us/step - loss: 1349.2640 - mae: 27.7309 - val_loss: 4892.9182 - val_mae: 66.9345\n",
      "Epoch 11/2000\n",
      "1988/1988 [==============================] - 1s 522us/step - loss: 1196.8277 - mae: 25.6706 - val_loss: 4573.0309 - val_mae: 64.5013\n",
      "Epoch 12/2000\n",
      "1988/1988 [==============================] - 1s 513us/step - loss: 918.9121 - mae: 22.8007 - val_loss: 4317.8279 - val_mae: 62.4929\n",
      "Epoch 13/2000\n",
      "1988/1988 [==============================] - 1s 497us/step - loss: 632.8209 - mae: 18.8663 - val_loss: 3093.1400 - val_mae: 51.7790\n",
      "Epoch 14/2000\n",
      "1988/1988 [==============================] - 1s 491us/step - loss: 418.3217 - mae: 15.2547 - val_loss: 1897.2285 - val_mae: 38.5586\n",
      "Epoch 15/2000\n",
      "1988/1988 [==============================] - 1s 483us/step - loss: 308.0593 - mae: 13.6584 - val_loss: 1518.9641 - val_mae: 33.3953\n",
      "Epoch 16/2000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 260.3036 - mae: 12.8583 - val_loss: 1399.0040 - val_mae: 31.7374\n",
      "Epoch 17/2000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 232.0074 - mae: 12.1017 - val_loss: 1546.9897 - val_mae: 33.9414\n",
      "Epoch 18/2000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 208.0222 - mae: 11.2619 - val_loss: 1445.6880 - val_mae: 32.5149\n",
      "Epoch 19/2000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 197.0686 - mae: 10.8734 - val_loss: 1262.8786 - val_mae: 29.8081\n",
      "Epoch 20/2000\n",
      "1988/1988 [==============================] - 1s 501us/step - loss: 198.2578 - mae: 11.0137 - val_loss: 1194.3309 - val_mae: 28.8134\n",
      "Epoch 21/2000\n",
      "1988/1988 [==============================] - 1s 504us/step - loss: 195.0683 - mae: 10.8766 - val_loss: 1253.4147 - val_mae: 30.0668\n",
      "Epoch 22/2000\n",
      "1988/1988 [==============================] - 1s 483us/step - loss: 186.3099 - mae: 10.4731 - val_loss: 1259.9107 - val_mae: 30.1335\n",
      "Epoch 23/2000\n",
      "1988/1988 [==============================] - 1s 500us/step - loss: 190.9895 - mae: 10.6406 - val_loss: 1166.7531 - val_mae: 28.7208\n",
      "Epoch 24/2000\n",
      "1988/1988 [==============================] - 1s 473us/step - loss: 187.5342 - mae: 10.4907 - val_loss: 1084.6715 - val_mae: 27.4326\n",
      "Epoch 25/2000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 186.2124 - mae: 10.5241 - val_loss: 951.5345 - val_mae: 24.9579\n",
      "Epoch 26/2000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 182.9453 - mae: 10.2051 - val_loss: 1147.7888 - val_mae: 28.5567\n",
      "Epoch 27/2000\n",
      "1988/1988 [==============================] - 1s 493us/step - loss: 173.1796 - mae: 10.0050 - val_loss: 1120.3138 - val_mae: 28.1686\n",
      "Epoch 28/2000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 177.4686 - mae: 10.2025 - val_loss: 1141.7935 - val_mae: 28.5486\n",
      "Epoch 29/2000\n",
      "1988/1988 [==============================] - 1s 479us/step - loss: 180.4183 - mae: 10.2766 - val_loss: 1115.8835 - val_mae: 28.2133\n",
      "Epoch 30/2000\n",
      "1988/1988 [==============================] - 1s 469us/step - loss: 178.9317 - mae: 10.1956 - val_loss: 718.7467 - val_mae: 20.6261\n",
      "Epoch 31/2000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 170.8177 - mae: 9.9105 - val_loss: 814.9143 - val_mae: 22.5973\n",
      "Epoch 32/2000\n",
      "1988/1988 [==============================] - 1s 472us/step - loss: 174.5556 - mae: 10.0832 - val_loss: 1093.9008 - val_mae: 28.4530\n",
      "Epoch 33/2000\n",
      "1988/1988 [==============================] - 1s 486us/step - loss: 182.6355 - mae: 10.1753 - val_loss: 887.6582 - val_mae: 24.0826\n",
      "Epoch 34/2000\n",
      "1988/1988 [==============================] - 1s 486us/step - loss: 173.3285 - mae: 10.2681 - val_loss: 962.4617 - val_mae: 25.2801\n",
      "Epoch 35/2000\n",
      "1988/1988 [==============================] - 1s 488us/step - loss: 172.3637 - mae: 10.1002 - val_loss: 981.4131 - val_mae: 25.5616\n",
      "Epoch 36/2000\n",
      "1988/1988 [==============================] - 1s 478us/step - loss: 175.9699 - mae: 9.9776 - val_loss: 1075.8828 - val_mae: 27.3839\n",
      "Epoch 37/2000\n",
      "1988/1988 [==============================] - 1s 477us/step - loss: 178.4865 - mae: 10.2604 - val_loss: 984.1539 - val_mae: 25.8955\n",
      "Epoch 38/2000\n",
      "1988/1988 [==============================] - 1s 481us/step - loss: 166.8050 - mae: 9.8659 - val_loss: 901.6581 - val_mae: 24.2666\n",
      "Epoch 39/2000\n",
      "1988/1988 [==============================] - 1s 476us/step - loss: 161.3762 - mae: 9.4564 - val_loss: 763.3829 - val_mae: 21.7994\n",
      "Epoch 40/2000\n",
      "1988/1988 [==============================] - 1s 476us/step - loss: 159.8808 - mae: 9.5620 - val_loss: 541.5981 - val_mae: 17.3621\n",
      "Epoch 41/2000\n",
      "1988/1988 [==============================] - 1s 485us/step - loss: 146.1484 - mae: 9.0220 - val_loss: 951.3354 - val_mae: 25.8094\n",
      "Epoch 42/2000\n",
      "1988/1988 [==============================] - 1s 486us/step - loss: 160.7449 - mae: 9.4427 - val_loss: 919.3293 - val_mae: 25.1172\n",
      "Epoch 43/2000\n",
      "1988/1988 [==============================] - 1s 488us/step - loss: 156.6881 - mae: 9.5449 - val_loss: 911.0006 - val_mae: 24.9436\n",
      "Epoch 44/2000\n",
      "1988/1988 [==============================] - 1s 505us/step - loss: 138.5238 - mae: 8.8598 - val_loss: 1018.0682 - val_mae: 27.0319\n",
      "Epoch 45/2000\n",
      "1988/1988 [==============================] - 1s 498us/step - loss: 151.3323 - mae: 9.2775 - val_loss: 655.3381 - val_mae: 19.9485\n",
      "Epoch 46/2000\n",
      "1988/1988 [==============================] - 1s 487us/step - loss: 139.6348 - mae: 8.8840 - val_loss: 694.5682 - val_mae: 20.8300\n",
      "Epoch 47/2000\n",
      "1988/1988 [==============================] - 1s 474us/step - loss: 135.0060 - mae: 8.7939 - val_loss: 658.0991 - val_mae: 20.2530\n",
      "Epoch 48/2000\n",
      "1988/1988 [==============================] - 1s 488us/step - loss: 136.5158 - mae: 8.6951 - val_loss: 603.7929 - val_mae: 19.1091\n",
      "Epoch 49/2000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 136.3361 - mae: 8.7392 - val_loss: 916.2799 - val_mae: 25.5627\n",
      "Epoch 50/2000\n",
      "1988/1988 [==============================] - 1s 484us/step - loss: 128.5372 - mae: 8.5248 - val_loss: 612.5457 - val_mae: 19.5282\n",
      "Epoch 51/2000\n",
      "1988/1988 [==============================] - 1s 489us/step - loss: 128.6500 - mae: 8.4635 - val_loss: 715.1761 - val_mae: 22.2902\n",
      "Epoch 52/2000\n",
      "1988/1988 [==============================] - 1s 493us/step - loss: 134.4159 - mae: 8.5616 - val_loss: 406.7078 - val_mae: 14.9245\n",
      "Epoch 53/2000\n",
      "1988/1988 [==============================] - 1s 490us/step - loss: 117.2837 - mae: 7.9549 - val_loss: 711.9822 - val_mae: 21.8058\n",
      "Epoch 54/2000\n",
      "1988/1988 [==============================] - 1s 496us/step - loss: 127.0348 - mae: 8.3596 - val_loss: 449.4534 - val_mae: 15.7063\n",
      "Epoch 55/2000\n",
      "1988/1988 [==============================] - 1s 482us/step - loss: 111.6466 - mae: 7.9537 - val_loss: 549.4337 - val_mae: 18.0144\n",
      "Epoch 56/2000\n",
      "1988/1988 [==============================] - 1s 464us/step - loss: 111.1946 - mae: 7.8274 - val_loss: 549.6897 - val_mae: 18.5222\n",
      "Epoch 57/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 1s 499us/step - loss: 123.1733 - mae: 8.1805 - val_loss: 434.3390 - val_mae: 15.7436\n",
      "Epoch 58/2000\n",
      "1988/1988 [==============================] - 1s 491us/step - loss: 106.6823 - mae: 7.8268 - val_loss: 541.0187 - val_mae: 18.4290\n",
      "Epoch 59/2000\n",
      "1988/1988 [==============================] - 1s 492us/step - loss: 116.5833 - mae: 7.9466 - val_loss: 488.1802 - val_mae: 17.2461\n",
      "Epoch 60/2000\n",
      "1988/1988 [==============================] - 1s 491us/step - loss: 107.1450 - mae: 7.7910 - val_loss: 401.8670 - val_mae: 14.9405\n",
      "Epoch 61/2000\n",
      "1988/1988 [==============================] - 1s 488us/step - loss: 100.0049 - mae: 7.5087 - val_loss: 441.8152 - val_mae: 15.9747\n",
      "Epoch 62/2000\n",
      "1988/1988 [==============================] - 1s 495us/step - loss: 102.4426 - mae: 7.4454 - val_loss: 435.3143 - val_mae: 16.4582\n",
      "Epoch 63/2000\n",
      "1988/1988 [==============================] - 1s 496us/step - loss: 102.0847 - mae: 7.3963 - val_loss: 434.6633 - val_mae: 16.5231\n",
      "Epoch 64/2000\n",
      "1988/1988 [==============================] - 1s 487us/step - loss: 92.4410 - mae: 7.1320 - val_loss: 335.2388 - val_mae: 13.9451\n",
      "Epoch 65/2000\n",
      "1988/1988 [==============================] - 1s 501us/step - loss: 93.6849 - mae: 7.1537 - val_loss: 300.0876 - val_mae: 12.9329\n",
      "Epoch 66/2000\n",
      "1988/1988 [==============================] - 1s 499us/step - loss: 89.6615 - mae: 7.0109 - val_loss: 334.0741 - val_mae: 13.8246\n",
      "Epoch 67/2000\n",
      "1988/1988 [==============================] - 1s 487us/step - loss: 85.4823 - mae: 6.8171 - val_loss: 519.8813 - val_mae: 18.7213\n",
      "Epoch 68/2000\n",
      "1988/1988 [==============================] - 1s 496us/step - loss: 91.9484 - mae: 7.0654 - val_loss: 362.5647 - val_mae: 14.5155\n",
      "Epoch 69/2000\n",
      "1988/1988 [==============================] - 1s 492us/step - loss: 80.5700 - mae: 6.7175 - val_loss: 524.9905 - val_mae: 18.9151\n",
      "Epoch 70/2000\n",
      "1988/1988 [==============================] - 1s 486us/step - loss: 85.1318 - mae: 7.0099 - val_loss: 319.9160 - val_mae: 13.4666\n",
      "Epoch 71/2000\n",
      "1988/1988 [==============================] - 1s 495us/step - loss: 89.2037 - mae: 7.1471 - val_loss: 450.6470 - val_mae: 17.0866\n",
      "Epoch 72/2000\n",
      "1988/1988 [==============================] - 1s 506us/step - loss: 77.8651 - mae: 6.6557 - val_loss: 434.9201 - val_mae: 16.7314\n",
      "Epoch 73/2000\n",
      "1988/1988 [==============================] - 1s 520us/step - loss: 87.0150 - mae: 7.0149 - val_loss: 411.5559 - val_mae: 16.1716\n",
      "Epoch 74/2000\n",
      "1988/1988 [==============================] - 1s 553us/step - loss: 71.5687 - mae: 6.4315 - val_loss: 437.7267 - val_mae: 16.6082\n",
      "Epoch 75/2000\n",
      "1988/1988 [==============================] - 1s 636us/step - loss: 76.2813 - mae: 6.5699 - val_loss: 488.9929 - val_mae: 18.0994\n",
      "Epoch 76/2000\n",
      "1988/1988 [==============================] - 1s 683us/step - loss: 77.4556 - mae: 6.6950 - val_loss: 551.3168 - val_mae: 19.7500\n",
      "Epoch 77/2000\n",
      "1988/1988 [==============================] - 1s 698us/step - loss: 70.1020 - mae: 6.2386 - val_loss: 332.0843 - val_mae: 13.8500\n",
      "Epoch 78/2000\n",
      "1988/1988 [==============================] - 1s 723us/step - loss: 71.8630 - mae: 6.3477 - val_loss: 456.2092 - val_mae: 17.5204\n",
      "Epoch 79/2000\n",
      "1988/1988 [==============================] - 1s 709us/step - loss: 76.2262 - mae: 6.5547 - val_loss: 246.1722 - val_mae: 11.8269\n",
      "Epoch 80/2000\n",
      "1988/1988 [==============================] - 1s 747us/step - loss: 65.5197 - mae: 6.0169 - val_loss: 333.3172 - val_mae: 14.5933\n",
      "Epoch 81/2000\n",
      "1988/1988 [==============================] - 1s 731us/step - loss: 70.6916 - mae: 6.2202 - val_loss: 192.9647 - val_mae: 10.5387\n",
      "Epoch 82/2000\n",
      "1988/1988 [==============================] - 1s 717us/step - loss: 64.4685 - mae: 6.0172 - val_loss: 299.8054 - val_mae: 13.6124\n",
      "Epoch 83/2000\n",
      "1988/1988 [==============================] - 1s 647us/step - loss: 65.3749 - mae: 6.0666 - val_loss: 347.1087 - val_mae: 15.0545\n",
      "Epoch 84/2000\n",
      "1988/1988 [==============================] - 1s 602us/step - loss: 65.0321 - mae: 6.1206 - val_loss: 346.4175 - val_mae: 15.1105\n",
      "Epoch 85/2000\n",
      "1988/1988 [==============================] - 1s 597us/step - loss: 62.0113 - mae: 5.9117 - val_loss: 249.8535 - val_mae: 12.1572\n",
      "Epoch 86/2000\n",
      "1988/1988 [==============================] - 1s 550us/step - loss: 65.8043 - mae: 6.0110 - val_loss: 193.1211 - val_mae: 10.5137\n",
      "Epoch 87/2000\n",
      "1988/1988 [==============================] - 1s 547us/step - loss: 61.9743 - mae: 5.9832 - val_loss: 177.1990 - val_mae: 10.4061\n",
      "Epoch 88/2000\n",
      "1988/1988 [==============================] - 1s 537us/step - loss: 64.1324 - mae: 6.0460 - val_loss: 414.3046 - val_mae: 16.9991\n",
      "Epoch 89/2000\n",
      "1988/1988 [==============================] - 1s 534us/step - loss: 58.0759 - mae: 5.7801 - val_loss: 235.1577 - val_mae: 11.7324\n",
      "Epoch 90/2000\n",
      "1988/1988 [==============================] - 1s 519us/step - loss: 57.7171 - mae: 5.7382 - val_loss: 326.8936 - val_mae: 14.6369\n",
      "Epoch 91/2000\n",
      "1988/1988 [==============================] - 1s 513us/step - loss: 61.1176 - mae: 5.8751 - val_loss: 305.3444 - val_mae: 14.0670\n",
      "Epoch 92/2000\n",
      "1988/1988 [==============================] - 1s 523us/step - loss: 62.4772 - mae: 5.9547 - val_loss: 243.8926 - val_mae: 12.0088\n",
      "Epoch 93/2000\n",
      "1988/1988 [==============================] - 1s 507us/step - loss: 54.7274 - mae: 5.6245 - val_loss: 278.0869 - val_mae: 13.1032\n",
      "Epoch 94/2000\n",
      "1988/1988 [==============================] - 1s 504us/step - loss: 58.1266 - mae: 5.8299 - val_loss: 289.0127 - val_mae: 13.5830\n",
      "Epoch 95/2000\n",
      "1988/1988 [==============================] - 1s 526us/step - loss: 53.8634 - mae: 5.5426 - val_loss: 213.4347 - val_mae: 11.1660\n",
      "Epoch 96/2000\n",
      "1988/1988 [==============================] - 1s 544us/step - loss: 52.5331 - mae: 5.5815 - val_loss: 225.1418 - val_mae: 11.4679\n",
      "Epoch 97/2000\n",
      "1988/1988 [==============================] - 1s 540us/step - loss: 51.3087 - mae: 5.3681 - val_loss: 208.2803 - val_mae: 10.9229\n",
      "Epoch 98/2000\n",
      "1988/1988 [==============================] - 1s 545us/step - loss: 55.8467 - mae: 5.6524 - val_loss: 338.1151 - val_mae: 14.6158\n",
      "Epoch 99/2000\n",
      "1988/1988 [==============================] - 1s 570us/step - loss: 57.1066 - mae: 5.7269 - val_loss: 284.3789 - val_mae: 13.0039\n",
      "Epoch 100/2000\n",
      "1988/1988 [==============================] - 1s 577us/step - loss: 59.5325 - mae: 5.7410 - val_loss: 312.5565 - val_mae: 14.2380\n",
      "Epoch 101/2000\n",
      "1988/1988 [==============================] - 1s 582us/step - loss: 51.9113 - mae: 5.3966 - val_loss: 308.6393 - val_mae: 13.8270\n",
      "Epoch 102/2000\n",
      "1988/1988 [==============================] - 1s 590us/step - loss: 54.5796 - mae: 5.5377 - val_loss: 201.8932 - val_mae: 10.7580\n",
      "Epoch 103/2000\n",
      "1988/1988 [==============================] - 1s 593us/step - loss: 54.3720 - mae: 5.4439 - val_loss: 136.9653 - val_mae: 9.0557\n",
      "Epoch 104/2000\n",
      "1988/1988 [==============================] - 1s 563us/step - loss: 51.8216 - mae: 5.3766 - val_loss: 158.5777 - val_mae: 9.5596\n",
      "Epoch 105/2000\n",
      "1988/1988 [==============================] - 1s 587us/step - loss: 48.4590 - mae: 5.2401 - val_loss: 287.9808 - val_mae: 13.4090\n",
      "Epoch 106/2000\n",
      "1988/1988 [==============================] - 1s 588us/step - loss: 47.1057 - mae: 5.1473 - val_loss: 259.8071 - val_mae: 12.6826\n",
      "Epoch 107/2000\n",
      "1988/1988 [==============================] - 1s 566us/step - loss: 51.1686 - mae: 5.2908 - val_loss: 175.4698 - val_mae: 9.9502\n",
      "Epoch 108/2000\n",
      "1988/1988 [==============================] - 1s 569us/step - loss: 51.1334 - mae: 5.4501 - val_loss: 303.7184 - val_mae: 14.1490\n",
      "Epoch 109/2000\n",
      "1988/1988 [==============================] - 1s 561us/step - loss: 50.6910 - mae: 5.2324 - val_loss: 245.5475 - val_mae: 12.3040\n",
      "Epoch 110/2000\n",
      "1988/1988 [==============================] - 1s 559us/step - loss: 48.9682 - mae: 5.1930 - val_loss: 231.8900 - val_mae: 11.9061\n",
      "Epoch 111/2000\n",
      "1988/1988 [==============================] - 1s 582us/step - loss: 48.8225 - mae: 5.2640 - val_loss: 311.0040 - val_mae: 14.1734\n",
      "Epoch 112/2000\n",
      "1988/1988 [==============================] - 1s 568us/step - loss: 50.7465 - mae: 5.2435 - val_loss: 176.6457 - val_mae: 10.0182\n",
      "Epoch 113/2000\n",
      "1988/1988 [==============================] - 1s 572us/step - loss: 49.6757 - mae: 5.0602 - val_loss: 173.8400 - val_mae: 9.9851\n",
      "Epoch 114/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 1s 583us/step - loss: 50.1527 - mae: 5.1560 - val_loss: 375.4095 - val_mae: 16.3549\n",
      "Epoch 115/2000\n",
      "1988/1988 [==============================] - 1s 557us/step - loss: 44.4165 - mae: 4.8292 - val_loss: 225.6193 - val_mae: 11.5775\n",
      "Epoch 116/2000\n",
      "1988/1988 [==============================] - 1s 563us/step - loss: 45.2253 - mae: 4.9219 - val_loss: 205.9886 - val_mae: 10.9435\n",
      "Epoch 117/2000\n",
      "1988/1988 [==============================] - 1s 586us/step - loss: 44.0082 - mae: 4.8160 - val_loss: 303.1219 - val_mae: 13.9025\n",
      "Epoch 118/2000\n",
      "1988/1988 [==============================] - 1s 583us/step - loss: 46.2028 - mae: 4.8641 - val_loss: 171.9231 - val_mae: 9.8976\n",
      "Epoch 119/2000\n",
      "1988/1988 [==============================] - 1s 605us/step - loss: 46.4313 - mae: 4.9181 - val_loss: 227.2868 - val_mae: 11.4344\n",
      "Epoch 120/2000\n",
      "1988/1988 [==============================] - 1s 579us/step - loss: 48.2028 - mae: 4.9997 - val_loss: 331.9642 - val_mae: 14.7107\n",
      "Epoch 121/2000\n",
      "1988/1988 [==============================] - 1s 580us/step - loss: 41.1085 - mae: 4.6199 - val_loss: 184.3002 - val_mae: 10.3201\n",
      "Epoch 122/2000\n",
      "1988/1988 [==============================] - 1s 593us/step - loss: 41.5017 - mae: 4.6708 - val_loss: 278.5190 - val_mae: 13.5693\n",
      "Epoch 123/2000\n",
      "1988/1988 [==============================] - 1s 576us/step - loss: 47.8606 - mae: 4.9965 - val_loss: 218.2145 - val_mae: 11.2792\n",
      "Epoch 124/2000\n",
      "1988/1988 [==============================] - 1s 545us/step - loss: 43.8405 - mae: 4.8819 - val_loss: 160.1344 - val_mae: 9.5470\n",
      "Epoch 125/2000\n",
      "1988/1988 [==============================] - 1s 534us/step - loss: 44.1900 - mae: 4.9000 - val_loss: 219.9879 - val_mae: 11.4675\n",
      "Epoch 126/2000\n",
      "1988/1988 [==============================] - 1s 500us/step - loss: 46.3158 - mae: 4.9341 - val_loss: 324.9983 - val_mae: 15.4445\n",
      "Epoch 127/2000\n",
      "1988/1988 [==============================] - 1s 480us/step - loss: 50.6803 - mae: 5.3646 - val_loss: 96.5297 - val_mae: 7.4498\n",
      "Epoch 128/2000\n",
      "1988/1988 [==============================] - 1s 469us/step - loss: 51.7039 - mae: 5.4201 - val_loss: 95.0225 - val_mae: 7.4644\n",
      "Epoch 129/2000\n",
      "1988/1988 [==============================] - 1s 475us/step - loss: 51.0816 - mae: 5.3588 - val_loss: 274.3009 - val_mae: 13.8572\n",
      "Epoch 130/2000\n",
      "1988/1988 [==============================] - 1s 498us/step - loss: 48.0215 - mae: 5.1692 - val_loss: 137.1502 - val_mae: 8.9255\n",
      "Epoch 131/2000\n",
      "1988/1988 [==============================] - 1s 533us/step - loss: 44.2691 - mae: 4.9618 - val_loss: 171.0856 - val_mae: 10.1731\n",
      "Epoch 132/2000\n",
      "1988/1988 [==============================] - 1s 504us/step - loss: 46.6049 - mae: 5.0608 - val_loss: 210.3356 - val_mae: 11.8359\n",
      "Epoch 133/2000\n",
      "1988/1988 [==============================] - 1s 520us/step - loss: 45.4023 - mae: 4.9298 - val_loss: 118.4317 - val_mae: 8.3232\n",
      "Epoch 134/2000\n",
      "1988/1988 [==============================] - 1s 517us/step - loss: 41.3617 - mae: 4.7325 - val_loss: 178.2717 - val_mae: 10.5197\n",
      "Epoch 135/2000\n",
      "1988/1988 [==============================] - 1s 521us/step - loss: 41.3705 - mae: 4.6958 - val_loss: 88.9041 - val_mae: 7.5285\n",
      "Epoch 136/2000\n",
      "1988/1988 [==============================] - 1s 560us/step - loss: 41.2079 - mae: 4.7786 - val_loss: 92.6932 - val_mae: 7.8983\n",
      "Epoch 137/2000\n",
      "1988/1988 [==============================] - 1s 558us/step - loss: 42.1241 - mae: 4.7296 - val_loss: 160.0875 - val_mae: 9.6926\n",
      "Epoch 138/2000\n",
      "1988/1988 [==============================] - 1s 591us/step - loss: 41.3508 - mae: 4.6637 - val_loss: 160.8056 - val_mae: 9.4413\n",
      "Epoch 139/2000\n",
      "1988/1988 [==============================] - 1s 584us/step - loss: 45.5348 - mae: 4.9394 - val_loss: 332.4623 - val_mae: 15.4913\n",
      "Epoch 140/2000\n",
      "1988/1988 [==============================] - 1s 568us/step - loss: 41.9891 - mae: 4.6659 - val_loss: 315.6770 - val_mae: 14.6302\n",
      "Epoch 141/2000\n",
      "1988/1988 [==============================] - 1s 549us/step - loss: 43.7432 - mae: 4.7705 - val_loss: 348.8569 - val_mae: 15.2446\n",
      "Epoch 142/2000\n",
      "1988/1988 [==============================] - 1s 592us/step - loss: 43.6921 - mae: 4.6936 - val_loss: 247.4786 - val_mae: 11.9886\n",
      "Epoch 143/2000\n",
      "1988/1988 [==============================] - 1s 552us/step - loss: 44.3919 - mae: 4.7948 - val_loss: 304.4584 - val_mae: 13.2261\n",
      "Epoch 144/2000\n",
      "1988/1988 [==============================] - 1s 577us/step - loss: 42.2467 - mae: 4.6785 - val_loss: 434.7783 - val_mae: 16.8546\n",
      "Epoch 145/2000\n",
      "1988/1988 [==============================] - 1s 608us/step - loss: 47.0175 - mae: 4.8753 - val_loss: 245.3968 - val_mae: 11.8422\n",
      "Epoch 146/2000\n",
      "1988/1988 [==============================] - 1s 584us/step - loss: 44.2866 - mae: 4.8346 - val_loss: 424.8714 - val_mae: 17.1236\n",
      "Epoch 147/2000\n",
      "1988/1988 [==============================] - 1s 592us/step - loss: 44.4363 - mae: 4.9032 - val_loss: 266.1065 - val_mae: 12.4084\n",
      "Epoch 148/2000\n",
      "1988/1988 [==============================] - 1s 541us/step - loss: 49.7734 - mae: 5.2465 - val_loss: 143.1104 - val_mae: 9.5545\n",
      "Epoch 149/2000\n",
      "1988/1988 [==============================] - 1s 532us/step - loss: 54.8804 - mae: 5.2951 - val_loss: 144.2806 - val_mae: 9.0889\n",
      "Epoch 150/2000\n",
      "1988/1988 [==============================] - 1s 519us/step - loss: 49.8014 - mae: 5.2188 - val_loss: 80.8947 - val_mae: 7.2379\n",
      "Epoch 151/2000\n",
      "1988/1988 [==============================] - 1s 506us/step - loss: 46.4363 - mae: 5.0270 - val_loss: 43.0021 - val_mae: 5.1331\n",
      "Epoch 152/2000\n",
      "1988/1988 [==============================] - 1s 536us/step - loss: 45.6400 - mae: 4.9588 - val_loss: 43.6715 - val_mae: 5.1890\n",
      "Epoch 153/2000\n",
      "1988/1988 [==============================] - 1s 508us/step - loss: 43.9749 - mae: 4.9292 - val_loss: 44.5923 - val_mae: 5.1592\n",
      "Epoch 154/2000\n",
      "1988/1988 [==============================] - 1s 507us/step - loss: 44.0567 - mae: 4.8683 - val_loss: 74.3972 - val_mae: 7.1566\n",
      "Epoch 155/2000\n",
      "1988/1988 [==============================] - 1s 522us/step - loss: 45.4965 - mae: 4.9400 - val_loss: 44.7306 - val_mae: 5.3970\n",
      "Epoch 156/2000\n",
      "1988/1988 [==============================] - 1s 508us/step - loss: 41.6177 - mae: 4.7286 - val_loss: 140.0593 - val_mae: 10.3558\n",
      "Epoch 157/2000\n",
      "1988/1988 [==============================] - 1s 516us/step - loss: 44.6591 - mae: 4.9365 - val_loss: 76.7612 - val_mae: 7.1214\n",
      "Epoch 158/2000\n",
      "1988/1988 [==============================] - 1s 510us/step - loss: 39.0016 - mae: 4.5221 - val_loss: 132.3705 - val_mae: 9.7908\n",
      "Epoch 159/2000\n",
      "1988/1988 [==============================] - 1s 520us/step - loss: 36.2110 - mae: 4.4232 - val_loss: 53.8601 - val_mae: 5.8348\n",
      "Epoch 160/2000\n",
      "1988/1988 [==============================] - 1s 521us/step - loss: 43.9926 - mae: 4.7671 - val_loss: 54.0472 - val_mae: 5.8784\n",
      "Epoch 161/2000\n",
      "1988/1988 [==============================] - 1s 516us/step - loss: 44.2694 - mae: 4.7548 - val_loss: 47.7953 - val_mae: 5.4795\n",
      "Epoch 162/2000\n",
      "1988/1988 [==============================] - 1s 521us/step - loss: 41.2431 - mae: 4.7010 - val_loss: 130.7401 - val_mae: 9.7477\n",
      "Epoch 163/2000\n",
      "1988/1988 [==============================] - 1s 530us/step - loss: 40.0591 - mae: 4.6877 - val_loss: 62.2890 - val_mae: 6.3657\n",
      "Epoch 164/2000\n",
      "1988/1988 [==============================] - 1s 520us/step - loss: 44.7905 - mae: 4.9826 - val_loss: 59.0185 - val_mae: 6.2667\n",
      "Epoch 165/2000\n",
      "1988/1988 [==============================] - 1s 543us/step - loss: 42.9009 - mae: 4.9222 - val_loss: 107.4516 - val_mae: 8.4070\n",
      "Epoch 166/2000\n",
      "1988/1988 [==============================] - 1s 560us/step - loss: 46.3017 - mae: 5.0152 - val_loss: 165.3655 - val_mae: 11.0090\n",
      "Epoch 167/2000\n",
      "1988/1988 [==============================] - 1s 560us/step - loss: 45.9377 - mae: 5.0406 - val_loss: 74.8607 - val_mae: 6.8723\n",
      "Epoch 168/2000\n",
      "1988/1988 [==============================] - 1s 558us/step - loss: 46.6950 - mae: 5.1302 - val_loss: 139.1662 - val_mae: 9.6927\n",
      "Epoch 169/2000\n",
      "1988/1988 [==============================] - 1s 563us/step - loss: 42.7472 - mae: 4.7961 - val_loss: 53.1346 - val_mae: 5.8139\n",
      "Epoch 170/2000\n",
      "1988/1988 [==============================] - 1s 551us/step - loss: 38.9012 - mae: 4.5868 - val_loss: 67.0158 - val_mae: 6.4326\n",
      "Epoch 171/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 1s 548us/step - loss: 41.1963 - mae: 4.6735 - val_loss: 122.5447 - val_mae: 9.6600\n",
      "Epoch 172/2000\n",
      "1988/1988 [==============================] - 1s 548us/step - loss: 42.6053 - mae: 4.7106 - val_loss: 90.7382 - val_mae: 7.9946\n",
      "Epoch 173/2000\n",
      "1988/1988 [==============================] - 1s 569us/step - loss: 40.9433 - mae: 4.6189 - val_loss: 36.1605 - val_mae: 4.7780\n",
      "Epoch 174/2000\n",
      "1988/1988 [==============================] - 1s 527us/step - loss: 40.5826 - mae: 4.5841 - val_loss: 95.8906 - val_mae: 8.0918\n",
      "Epoch 175/2000\n",
      "1988/1988 [==============================] - 1s 530us/step - loss: 46.7816 - mae: 5.1402 - val_loss: 210.3487 - val_mae: 12.6134\n",
      "Epoch 176/2000\n",
      "1988/1988 [==============================] - 1s 532us/step - loss: 43.3721 - mae: 4.9406 - val_loss: 260.1215 - val_mae: 12.0642\n",
      "Epoch 177/2000\n",
      "1988/1988 [==============================] - 1s 535us/step - loss: 53.2570 - mae: 5.4544 - val_loss: 247.8853 - val_mae: 12.0520\n",
      "Epoch 178/2000\n",
      "1988/1988 [==============================] - 1s 532us/step - loss: 45.1537 - mae: 5.0088 - val_loss: 193.7866 - val_mae: 10.5846\n",
      "Epoch 179/2000\n",
      "1988/1988 [==============================] - 1s 501us/step - loss: 47.3039 - mae: 4.9645 - val_loss: 412.8962 - val_mae: 17.2110\n",
      "Epoch 180/2000\n",
      "1988/1988 [==============================] - 1s 522us/step - loss: 39.0302 - mae: 4.6320 - val_loss: 522.5343 - val_mae: 18.8925\n",
      "Epoch 181/2000\n",
      "1988/1988 [==============================] - 1s 512us/step - loss: 47.6254 - mae: 5.0196 - val_loss: 254.0128 - val_mae: 12.2629\n",
      "Epoch 182/2000\n",
      "1988/1988 [==============================] - 1s 505us/step - loss: 39.5289 - mae: 4.5496 - val_loss: 280.9309 - val_mae: 12.6605\n",
      "Epoch 183/2000\n",
      "1988/1988 [==============================] - 1s 510us/step - loss: 42.9828 - mae: 4.7261 - val_loss: 178.3827 - val_mae: 10.2208\n",
      "Epoch 184/2000\n",
      "1988/1988 [==============================] - 1s 534us/step - loss: 42.8865 - mae: 4.7368 - val_loss: 336.1292 - val_mae: 15.0389\n",
      "Epoch 185/2000\n",
      "1988/1988 [==============================] - 1s 536us/step - loss: 41.4425 - mae: 4.6671 - val_loss: 298.5941 - val_mae: 13.4178\n",
      "Epoch 186/2000\n",
      "1988/1988 [==============================] - 1s 555us/step - loss: 40.7499 - mae: 4.6824 - val_loss: 214.9395 - val_mae: 11.3343\n",
      "Epoch 187/2000\n",
      "1988/1988 [==============================] - 1s 538us/step - loss: 40.2145 - mae: 4.5636 - val_loss: 269.1339 - val_mae: 12.7217\n",
      "Epoch 188/2000\n",
      "1988/1988 [==============================] - 1s 547us/step - loss: 41.1511 - mae: 4.5957 - val_loss: 325.4341 - val_mae: 13.8054\n",
      "Epoch 189/2000\n",
      "1988/1988 [==============================] - 1s 573us/step - loss: 43.9967 - mae: 4.9160 - val_loss: 286.7871 - val_mae: 13.3828\n",
      "Epoch 190/2000\n",
      "1988/1988 [==============================] - 1s 571us/step - loss: 37.9490 - mae: 4.4773 - val_loss: 270.2159 - val_mae: 12.8688\n",
      "Epoch 191/2000\n",
      "1988/1988 [==============================] - 1s 586us/step - loss: 39.9603 - mae: 4.6100 - val_loss: 204.7399 - val_mae: 10.9908\n",
      "Epoch 192/2000\n",
      "1988/1988 [==============================] - 1s 554us/step - loss: 39.4756 - mae: 4.5511 - val_loss: 226.8667 - val_mae: 11.5641\n",
      "Epoch 193/2000\n",
      "1988/1988 [==============================] - 1s 561us/step - loss: 37.2943 - mae: 4.4290 - val_loss: 223.6801 - val_mae: 11.4562\n",
      "Epoch 194/2000\n",
      "1988/1988 [==============================] - 1s 605us/step - loss: 37.9896 - mae: 4.4900 - val_loss: 203.4217 - val_mae: 11.0029\n",
      "Epoch 195/2000\n",
      "1988/1988 [==============================] - 1s 674us/step - loss: 40.1611 - mae: 4.5011 - val_loss: 269.5324 - val_mae: 13.1727\n",
      "Epoch 196/2000\n",
      "1988/1988 [==============================] - 1s 529us/step - loss: 40.7111 - mae: 4.5517 - val_loss: 314.2607 - val_mae: 14.3612\n",
      "Epoch 197/2000\n",
      "1988/1988 [==============================] - 1s 535us/step - loss: 38.3829 - mae: 4.4450 - val_loss: 260.5888 - val_mae: 12.4330\n",
      "Epoch 198/2000\n",
      "1988/1988 [==============================] - 1s 555us/step - loss: 41.7238 - mae: 4.8459 - val_loss: 248.3360 - val_mae: 12.0660\n",
      "Epoch 199/2000\n",
      "1988/1988 [==============================] - 1s 533us/step - loss: 39.4975 - mae: 4.6378 - val_loss: 253.5684 - val_mae: 12.4121\n",
      "Epoch 200/2000\n",
      "1988/1988 [==============================] - 1s 527us/step - loss: 39.0433 - mae: 4.5588 - val_loss: 495.6978 - val_mae: 18.9773\n",
      "Epoch 201/2000\n",
      "1988/1988 [==============================] - 1s 504us/step - loss: 37.3181 - mae: 4.5318 - val_loss: 238.1614 - val_mae: 11.8086\n",
      "Epoch 202/2000\n",
      "1988/1988 [==============================] - 1s 542us/step - loss: 43.2600 - mae: 4.7102 - val_loss: 283.6815 - val_mae: 13.4682\n",
      "Epoch 203/2000\n",
      "1988/1988 [==============================] - 1s 531us/step - loss: 41.2419 - mae: 4.7772 - val_loss: 402.6620 - val_mae: 17.2673\n",
      "Epoch 204/2000\n",
      "1988/1988 [==============================] - 1s 534us/step - loss: 43.3976 - mae: 4.8086 - val_loss: 258.5783 - val_mae: 12.5255\n",
      "Epoch 205/2000\n",
      "1988/1988 [==============================] - 1s 553us/step - loss: 46.3915 - mae: 5.1758 - val_loss: 190.4111 - val_mae: 10.6160\n",
      "Epoch 206/2000\n",
      "1988/1988 [==============================] - 1s 545us/step - loss: 40.2082 - mae: 4.7009 - val_loss: 272.5394 - val_mae: 12.6002\n",
      "Epoch 207/2000\n",
      "1988/1988 [==============================] - 1s 576us/step - loss: 40.9496 - mae: 4.7188 - val_loss: 223.3780 - val_mae: 11.5505\n",
      "Epoch 208/2000\n",
      "1988/1988 [==============================] - 1s 535us/step - loss: 39.6648 - mae: 4.5715 - val_loss: 171.4840 - val_mae: 9.9003\n",
      "Epoch 209/2000\n",
      "1988/1988 [==============================] - 1s 555us/step - loss: 39.6414 - mae: 4.4794 - val_loss: 149.0608 - val_mae: 9.1958\n",
      "Epoch 210/2000\n",
      "1988/1988 [==============================] - 1s 572us/step - loss: 37.1591 - mae: 4.4401 - val_loss: 235.4181 - val_mae: 11.7392\n",
      "Epoch 211/2000\n",
      "1988/1988 [==============================] - 1s 560us/step - loss: 37.1130 - mae: 4.4557 - val_loss: 214.0183 - val_mae: 11.0728\n",
      "Epoch 212/2000\n",
      "1988/1988 [==============================] - 1s 573us/step - loss: 36.7026 - mae: 4.4067 - val_loss: 245.4693 - val_mae: 11.9472\n",
      "Epoch 213/2000\n",
      "1988/1988 [==============================] - 1s 565us/step - loss: 37.6936 - mae: 4.4374 - val_loss: 206.7948 - val_mae: 11.0234\n",
      "Epoch 214/2000\n",
      "1988/1988 [==============================] - 1s 579us/step - loss: 36.7516 - mae: 4.3176 - val_loss: 334.9918 - val_mae: 14.8636\n",
      "Epoch 215/2000\n",
      "1988/1988 [==============================] - 1s 557us/step - loss: 36.3090 - mae: 4.3289 - val_loss: 307.2835 - val_mae: 13.6596\n",
      "Epoch 216/2000\n",
      "1988/1988 [==============================] - 1s 552us/step - loss: 40.2709 - mae: 4.5218 - val_loss: 203.8527 - val_mae: 10.8897\n",
      "Epoch 217/2000\n",
      "1988/1988 [==============================] - 1s 548us/step - loss: 40.0075 - mae: 4.5161 - val_loss: 317.2167 - val_mae: 13.8998\n",
      "Epoch 218/2000\n",
      "1988/1988 [==============================] - 1s 546us/step - loss: 36.9873 - mae: 4.4406 - val_loss: 341.6810 - val_mae: 13.8455\n",
      "Epoch 219/2000\n",
      "1988/1988 [==============================] - 1s 534us/step - loss: 39.6773 - mae: 4.6154 - val_loss: 277.0018 - val_mae: 12.4477\n",
      "Epoch 220/2000\n",
      "1988/1988 [==============================] - 1s 531us/step - loss: 36.9144 - mae: 4.3758 - val_loss: 282.8870 - val_mae: 12.6121\n",
      "Epoch 221/2000\n",
      "1988/1988 [==============================] - 1s 537us/step - loss: 37.8145 - mae: 4.4620 - val_loss: 267.4743 - val_mae: 12.2807\n",
      "Epoch 222/2000\n",
      "1988/1988 [==============================] - 1s 536us/step - loss: 38.2237 - mae: 4.4640 - val_loss: 225.5209 - val_mae: 11.3134\n",
      "Epoch 223/2000\n",
      "1988/1988 [==============================] - 1s 538us/step - loss: 40.0021 - mae: 4.5701 - val_loss: 356.7602 - val_mae: 14.8883\n",
      "Epoch 224/2000\n",
      "1988/1988 [==============================] - 1s 545us/step - loss: 36.3524 - mae: 4.4790 - val_loss: 408.7457 - val_mae: 15.7682\n",
      "Epoch 225/2000\n",
      "1988/1988 [==============================] - 1s 555us/step - loss: 38.0120 - mae: 4.5936 - val_loss: 380.5954 - val_mae: 15.0888\n",
      "Epoch 226/2000\n",
      "1988/1988 [==============================] - 1s 553us/step - loss: 42.0491 - mae: 4.6574 - val_loss: 237.7116 - val_mae: 11.8175\n",
      "Epoch 227/2000\n",
      "1988/1988 [==============================] - 1s 560us/step - loss: 38.3033 - mae: 4.5124 - val_loss: 258.6083 - val_mae: 12.0807\n",
      "Epoch 228/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1988 [==============================] - 1s 556us/step - loss: 33.7538 - mae: 4.2945 - val_loss: 444.4562 - val_mae: 16.9976\n",
      "Epoch 229/2000\n",
      "1988/1988 [==============================] - 1s 565us/step - loss: 36.3439 - mae: 4.4835 - val_loss: 231.6028 - val_mae: 11.4925\n",
      "Epoch 230/2000\n",
      "1988/1988 [==============================] - 1s 554us/step - loss: 37.8056 - mae: 4.4740 - val_loss: 382.3080 - val_mae: 15.6969\n",
      "Epoch 231/2000\n",
      "1988/1988 [==============================] - 1s 555us/step - loss: 38.2651 - mae: 4.4817 - val_loss: 196.1365 - val_mae: 10.9103\n",
      "Epoch 232/2000\n",
      "1988/1988 [==============================] - 1s 546us/step - loss: 39.1257 - mae: 4.5079 - val_loss: 335.0120 - val_mae: 14.0274\n",
      "Epoch 233/2000\n",
      "1988/1988 [==============================] - 1s 550us/step - loss: 35.6050 - mae: 4.3316 - val_loss: 359.7131 - val_mae: 14.4699\n",
      "Epoch 234/2000\n",
      "1988/1988 [==============================] - 1s 538us/step - loss: 33.9041 - mae: 4.2856 - val_loss: 291.1213 - val_mae: 12.9005\n",
      "Epoch 235/2000\n",
      "1988/1988 [==============================] - 1s 521us/step - loss: 35.3918 - mae: 4.2005 - val_loss: 353.8692 - val_mae: 14.4008\n",
      "Epoch 236/2000\n",
      "1988/1988 [==============================] - 1s 528us/step - loss: 32.3561 - mae: 4.1338 - val_loss: 329.1359 - val_mae: 13.7819\n",
      "Epoch 237/2000\n",
      "1988/1988 [==============================] - 1s 530us/step - loss: 35.5274 - mae: 4.2717 - val_loss: 290.9835 - val_mae: 13.2789\n",
      "Epoch 238/2000\n",
      "1988/1988 [==============================] - 1s 530us/step - loss: 35.4754 - mae: 4.3843 - val_loss: 334.7462 - val_mae: 14.3831\n",
      "Epoch 239/2000\n",
      "1988/1988 [==============================] - 1s 514us/step - loss: 38.7975 - mae: 4.5306 - val_loss: 414.5943 - val_mae: 16.1507\n",
      "Epoch 240/2000\n",
      "1988/1988 [==============================] - 1s 529us/step - loss: 38.7840 - mae: 4.4779 - val_loss: 283.1923 - val_mae: 12.6893\n",
      "Epoch 241/2000\n",
      "1988/1988 [==============================] - 1s 515us/step - loss: 36.4811 - mae: 4.3437 - val_loss: 197.6351 - val_mae: 10.7072\n",
      "Epoch 242/2000\n",
      "1988/1988 [==============================] - 1s 531us/step - loss: 37.5694 - mae: 4.4644 - val_loss: 225.9688 - val_mae: 11.3914\n",
      "Epoch 243/2000\n",
      "1988/1988 [==============================] - 1s 524us/step - loss: 36.9226 - mae: 4.3547 - val_loss: 304.6085 - val_mae: 13.5036\n",
      "Epoch 244/2000\n",
      "1988/1988 [==============================] - 1s 539us/step - loss: 38.8566 - mae: 4.5705 - val_loss: 376.6308 - val_mae: 14.7113\n",
      "Epoch 245/2000\n",
      "1988/1988 [==============================] - 1s 527us/step - loss: 36.8163 - mae: 4.4742 - val_loss: 535.9887 - val_mae: 18.4482\n",
      "Epoch 246/2000\n",
      "1988/1988 [==============================] - 1s 552us/step - loss: 35.4010 - mae: 4.2364 - val_loss: 428.6181 - val_mae: 15.5177\n",
      "Epoch 247/2000\n",
      "1988/1988 [==============================] - 1s 536us/step - loss: 39.6084 - mae: 4.4735 - val_loss: 357.2095 - val_mae: 14.2771\n",
      "Epoch 248/2000\n",
      "1988/1988 [==============================] - 1s 547us/step - loss: 41.9306 - mae: 4.6218 - val_loss: 360.9730 - val_mae: 14.2363\n",
      "Epoch 249/2000\n",
      "1988/1988 [==============================] - 1s 538us/step - loss: 41.4207 - mae: 4.6975 - val_loss: 292.0217 - val_mae: 12.9302\n",
      "Epoch 250/2000\n",
      "1988/1988 [==============================] - 1s 536us/step - loss: 35.5165 - mae: 4.3301 - val_loss: 309.0192 - val_mae: 13.3179\n",
      "Epoch 251/2000\n",
      "1988/1988 [==============================] - 1s 546us/step - loss: 38.7407 - mae: 4.3844 - val_loss: 324.2586 - val_mae: 13.6406\n",
      "Epoch 252/2000\n",
      "1988/1988 [==============================] - 1s 552us/step - loss: 39.0829 - mae: 4.4913 - val_loss: 289.7168 - val_mae: 12.9631\n",
      "Epoch 253/2000\n",
      "1988/1988 [==============================] - 1s 541us/step - loss: 36.2639 - mae: 4.3608 - val_loss: 285.7445 - val_mae: 12.8151\n",
      "Epoch 254/2000\n",
      "1988/1988 [==============================] - 1s 547us/step - loss: 37.8051 - mae: 4.4056 - val_loss: 235.9494 - val_mae: 11.5738\n",
      "Epoch 255/2000\n",
      "1988/1988 [==============================] - 1s 540us/step - loss: 38.1993 - mae: 4.3939 - val_loss: 370.2441 - val_mae: 14.9085\n",
      "Epoch 256/2000\n",
      "1988/1988 [==============================] - 1s 541us/step - loss: 36.4978 - mae: 4.3605 - val_loss: 359.1582 - val_mae: 14.8635\n",
      "Epoch 257/2000\n",
      "1988/1988 [==============================] - 1s 538us/step - loss: 34.4355 - mae: 4.1627 - val_loss: 338.8571 - val_mae: 14.0194\n",
      "Epoch 258/2000\n",
      "1988/1988 [==============================] - 1s 545us/step - loss: 39.1181 - mae: 4.5568 - val_loss: 268.4716 - val_mae: 12.6371\n",
      "Epoch 259/2000\n",
      "1988/1988 [==============================] - 1s 545us/step - loss: 33.3571 - mae: 4.1202 - val_loss: 360.5180 - val_mae: 14.8082\n",
      "Epoch 260/2000\n",
      "1988/1988 [==============================] - 1s 548us/step - loss: 36.8110 - mae: 4.3738 - val_loss: 331.9944 - val_mae: 13.9336\n",
      "Epoch 261/2000\n",
      "1988/1988 [==============================] - 1s 551us/step - loss: 36.1956 - mae: 4.2720 - val_loss: 340.8876 - val_mae: 14.0854\n",
      "Epoch 262/2000\n",
      "1988/1988 [==============================] - 1s 549us/step - loss: 34.8206 - mae: 4.2569 - val_loss: 371.3653 - val_mae: 14.7797\n",
      "Epoch 263/2000\n",
      "1988/1988 [==============================] - 1s 540us/step - loss: 37.5106 - mae: 4.5054 - val_loss: 295.2937 - val_mae: 13.0267\n",
      "Epoch 264/2000\n",
      "1988/1988 [==============================] - 1s 541us/step - loss: 40.1389 - mae: 4.5678 - val_loss: 344.7624 - val_mae: 14.4552\n",
      "Epoch 265/2000\n",
      "1988/1988 [==============================] - 1s 543us/step - loss: 35.2584 - mae: 4.2920 - val_loss: 291.6242 - val_mae: 13.0876\n",
      "Epoch 266/2000\n",
      "1988/1988 [==============================] - 1s 541us/step - loss: 34.1070 - mae: 4.2361 - val_loss: 338.4182 - val_mae: 14.4714\n",
      "Epoch 267/2000\n",
      "1988/1988 [==============================] - 1s 547us/step - loss: 35.4879 - mae: 4.3024 - val_loss: 290.6642 - val_mae: 12.9250\n",
      "Epoch 268/2000\n",
      "1988/1988 [==============================] - 1s 521us/step - loss: 41.7968 - mae: 4.6803 - val_loss: 295.4928 - val_mae: 13.0731\n",
      "Epoch 269/2000\n",
      "1988/1988 [==============================] - 1s 530us/step - loss: 37.8686 - mae: 4.4557 - val_loss: 290.2490 - val_mae: 13.0198\n",
      "Epoch 270/2000\n",
      "1988/1988 [==============================] - 1s 523us/step - loss: 34.9682 - mae: 4.3002 - val_loss: 205.2787 - val_mae: 10.8297\n",
      "Epoch 271/2000\n",
      "1988/1988 [==============================] - 1s 544us/step - loss: 34.6773 - mae: 4.3015 - val_loss: 281.8946 - val_mae: 12.6747\n",
      "Epoch 272/2000\n",
      "1988/1988 [==============================] - 1s 520us/step - loss: 38.4078 - mae: 4.5323 - val_loss: 255.2485 - val_mae: 12.0345\n",
      "Epoch 273/2000\n",
      "1988/1988 [==============================] - 1s 531us/step - loss: 34.9143 - mae: 4.2775 - val_loss: 359.1181 - val_mae: 14.7100\n",
      "Epoch 274/2000\n",
      "1988/1988 [==============================] - 1s 525us/step - loss: 35.5012 - mae: 4.2930 - val_loss: 267.6161 - val_mae: 12.3608\n",
      "Epoch 275/2000\n",
      "1988/1988 [==============================] - 1s 523us/step - loss: 36.7924 - mae: 4.3286 - val_loss: 374.7319 - val_mae: 14.8486\n",
      "Epoch 276/2000\n",
      "1988/1988 [==============================] - 1s 532us/step - loss: 37.3240 - mae: 4.4215 - val_loss: 331.1533 - val_mae: 13.7503\n",
      "Epoch 277/2000\n",
      "1988/1988 [==============================] - 1s 523us/step - loss: 35.1214 - mae: 4.2725 - val_loss: 260.9090 - val_mae: 12.2376\n",
      "Epoch 278/2000\n",
      "1988/1988 [==============================] - 1s 526us/step - loss: 38.0865 - mae: 4.4678 - val_loss: 286.6425 - val_mae: 12.7274\n",
      "Epoch 279/2000\n",
      "1988/1988 [==============================] - 1s 524us/step - loss: 38.4279 - mae: 4.4622 - val_loss: 443.7816 - val_mae: 17.0517\n",
      "Epoch 280/2000\n",
      "1988/1988 [==============================] - 1s 535us/step - loss: 37.1370 - mae: 4.3862 - val_loss: 306.9391 - val_mae: 13.6074\n",
      "Epoch 281/2000\n",
      "1988/1988 [==============================] - 1s 537us/step - loss: 37.7986 - mae: 4.3898 - val_loss: 404.6452 - val_mae: 16.0592\n",
      "Epoch 282/2000\n",
      "1988/1988 [==============================] - 1s 544us/step - loss: 36.6583 - mae: 4.3696 - val_loss: 359.2476 - val_mae: 14.6922\n",
      "Epoch 283/2000\n",
      "1988/1988 [==============================] - 1s 545us/step - loss: 38.4345 - mae: 4.4240 - val_loss: 309.1439 - val_mae: 13.5271\n",
      "Epoch 284/2000\n",
      "1988/1988 [==============================] - 1s 545us/step - loss: 39.3809 - mae: 4.5990 - val_loss: 332.2188 - val_mae: 14.1458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/2000\n",
      "1988/1988 [==============================] - 1s 537us/step - loss: 36.3313 - mae: 4.3462 - val_loss: 315.5607 - val_mae: 13.5231\n",
      "Epoch 286/2000\n",
      "1988/1988 [==============================] - 1s 546us/step - loss: 35.8614 - mae: 4.3246 - val_loss: 302.4148 - val_mae: 13.2039\n",
      "Epoch 287/2000\n",
      "1988/1988 [==============================] - 1s 542us/step - loss: 38.3050 - mae: 4.5489 - val_loss: 265.9113 - val_mae: 12.3734\n",
      "Epoch 288/2000\n",
      "1988/1988 [==============================] - 1s 538us/step - loss: 39.1138 - mae: 4.6279 - val_loss: 311.3292 - val_mae: 13.9499\n",
      "Epoch 289/2000\n",
      "1988/1988 [==============================] - 1s 541us/step - loss: 36.7218 - mae: 4.4709 - val_loss: 279.4940 - val_mae: 12.9064\n",
      "Epoch 290/2000\n",
      "1988/1988 [==============================] - 1s 557us/step - loss: 35.9681 - mae: 4.3611 - val_loss: 391.5619 - val_mae: 16.1191\n",
      "Epoch 291/2000\n",
      "1988/1988 [==============================] - 1s 544us/step - loss: 35.1641 - mae: 4.2867 - val_loss: 325.3306 - val_mae: 14.1086\n",
      "Epoch 292/2000\n",
      "1988/1988 [==============================] - 1s 542us/step - loss: 36.5884 - mae: 4.3351 - val_loss: 260.7830 - val_mae: 12.2099\n",
      "Epoch 293/2000\n",
      "1988/1988 [==============================] - 1s 540us/step - loss: 40.9075 - mae: 4.6575 - val_loss: 256.5940 - val_mae: 12.2724\n",
      "Epoch 294/2000\n",
      "1988/1988 [==============================] - 1s 547us/step - loss: 35.4338 - mae: 4.2700 - val_loss: 301.6668 - val_mae: 13.5739\n",
      "Epoch 295/2000\n",
      "1988/1988 [==============================] - 1s 537us/step - loss: 36.7115 - mae: 4.4210 - val_loss: 207.8499 - val_mae: 11.0466\n",
      "Epoch 296/2000\n",
      "1988/1988 [==============================] - 1s 525us/step - loss: 36.8324 - mae: 4.4418 - val_loss: 257.9494 - val_mae: 12.3375\n",
      "Epoch 297/2000\n",
      "1988/1988 [==============================] - 1s 529us/step - loss: 36.8972 - mae: 4.2710 - val_loss: 215.1743 - val_mae: 11.1786\n",
      "Epoch 298/2000\n",
      "1988/1988 [==============================] - 1s 527us/step - loss: 33.5111 - mae: 4.1566 - val_loss: 232.4635 - val_mae: 11.6679\n",
      "Epoch 299/2000\n",
      "1988/1988 [==============================] - 1s 527us/step - loss: 38.7971 - mae: 4.4855 - val_loss: 267.2013 - val_mae: 12.5261\n",
      "Epoch 300/2000\n",
      "1988/1988 [==============================] - 1s 531us/step - loss: 34.5528 - mae: 4.2002 - val_loss: 297.0525 - val_mae: 13.2076\n",
      "Epoch 301/2000\n",
      "1988/1988 [==============================] - 1s 525us/step - loss: 33.8111 - mae: 4.2517 - val_loss: 401.2871 - val_mae: 15.6199\n",
      "Epoch 302/2000\n",
      "1988/1988 [==============================] - 1s 526us/step - loss: 35.6485 - mae: 4.3157 - val_loss: 278.5834 - val_mae: 12.6158\n",
      "Epoch 303/2000\n",
      "1988/1988 [==============================] - 1s 526us/step - loss: 32.8489 - mae: 4.1537 - val_loss: 258.8607 - val_mae: 12.1954\n",
      "Epoch 304/2000\n",
      "1988/1988 [==============================] - 1s 541us/step - loss: 39.0993 - mae: 4.3909 - val_loss: 477.9748 - val_mae: 18.4105\n",
      "Epoch 305/2000\n",
      "1988/1988 [==============================] - 1s 555us/step - loss: 34.6546 - mae: 4.2532 - val_loss: 293.7110 - val_mae: 13.0083\n",
      "Epoch 306/2000\n",
      "1988/1988 [==============================] - 1s 540us/step - loss: 37.7072 - mae: 4.5827 - val_loss: 294.3629 - val_mae: 13.0351\n",
      "Epoch 307/2000\n",
      "1988/1988 [==============================] - 1s 541us/step - loss: 36.5654 - mae: 4.4339 - val_loss: 374.2585 - val_mae: 15.1689\n",
      "Epoch 308/2000\n",
      "1988/1988 [==============================] - 1s 548us/step - loss: 38.5882 - mae: 4.4832 - val_loss: 329.2355 - val_mae: 13.9581\n",
      "Epoch 309/2000\n",
      "1988/1988 [==============================] - 1s 552us/step - loss: 35.1799 - mae: 4.2680 - val_loss: 354.8451 - val_mae: 14.5713\n",
      "Epoch 310/2000\n",
      "1988/1988 [==============================] - 1s 546us/step - loss: 36.9329 - mae: 4.3203 - val_loss: 333.7401 - val_mae: 14.0141\n",
      "Epoch 311/2000\n",
      "1988/1988 [==============================] - 1s 546us/step - loss: 41.6802 - mae: 4.6791 - val_loss: 253.1712 - val_mae: 12.1438\n",
      "Epoch 312/2000\n",
      "1988/1988 [==============================] - 1s 535us/step - loss: 35.3202 - mae: 4.2570 - val_loss: 395.5564 - val_mae: 16.5231\n",
      "Epoch 313/2000\n",
      "1988/1988 [==============================] - 1s 544us/step - loss: 36.0078 - mae: 4.2994 - val_loss: 216.9722 - val_mae: 11.1863\n",
      "Epoch 314/2000\n",
      "1988/1988 [==============================] - 1s 533us/step - loss: 37.1811 - mae: 4.3360 - val_loss: 170.0693 - val_mae: 9.9004\n",
      "Epoch 315/2000\n",
      "1988/1988 [==============================] - 1s 527us/step - loss: 34.7116 - mae: 4.2914 - val_loss: 322.0676 - val_mae: 13.9449\n",
      "Epoch 316/2000\n",
      "1988/1988 [==============================] - 1s 529us/step - loss: 35.5091 - mae: 4.3081 - val_loss: 315.3468 - val_mae: 13.4823\n",
      "Epoch 317/2000\n",
      "1988/1988 [==============================] - 1s 526us/step - loss: 33.6854 - mae: 4.2477 - val_loss: 288.8238 - val_mae: 12.9707\n",
      "Epoch 318/2000\n",
      "1988/1988 [==============================] - 1s 535us/step - loss: 34.7216 - mae: 4.3204 - val_loss: 194.8159 - val_mae: 10.9707\n",
      "Epoch 319/2000\n",
      "1988/1988 [==============================] - 1s 539us/step - loss: 38.9361 - mae: 4.5203 - val_loss: 218.0340 - val_mae: 11.2209\n",
      "Epoch 320/2000\n",
      "1988/1988 [==============================] - 1s 520us/step - loss: 37.2079 - mae: 4.4104 - val_loss: 227.0152 - val_mae: 11.4133\n",
      "Epoch 321/2000\n",
      "1988/1988 [==============================] - 1s 527us/step - loss: 35.4099 - mae: 4.3298 - val_loss: 242.5096 - val_mae: 11.8559\n",
      "Epoch 322/2000\n",
      "1988/1988 [==============================] - 1s 525us/step - loss: 34.0189 - mae: 4.1519 - val_loss: 237.5923 - val_mae: 11.7788\n",
      "Epoch 323/2000\n",
      "1988/1988 [==============================] - 1s 535us/step - loss: 33.4917 - mae: 4.1288 - val_loss: 212.7886 - val_mae: 11.1037\n",
      "Epoch 324/2000\n",
      "1988/1988 [==============================] - 1s 522us/step - loss: 34.4867 - mae: 4.1885 - val_loss: 235.8598 - val_mae: 11.7275\n",
      "Epoch 325/2000\n",
      "1988/1988 [==============================] - 1s 534us/step - loss: 34.2114 - mae: 4.3034 - val_loss: 179.2018 - val_mae: 10.1237\n",
      "Epoch 326/2000\n",
      "1988/1988 [==============================] - 1s 523us/step - loss: 39.2479 - mae: 4.4887 - val_loss: 212.2721 - val_mae: 11.0135\n",
      "Epoch 327/2000\n",
      "1988/1988 [==============================] - 1s 535us/step - loss: 38.5216 - mae: 4.5385 - val_loss: 224.2662 - val_mae: 11.7570\n",
      "Epoch 328/2000\n",
      "1988/1988 [==============================] - 1s 544us/step - loss: 34.9042 - mae: 4.4306 - val_loss: 106.2531 - val_mae: 8.3244\n",
      "Epoch 329/2000\n",
      "1988/1988 [==============================] - 1s 560us/step - loss: 37.9649 - mae: 4.4385 - val_loss: 224.9279 - val_mae: 11.7531\n",
      "Epoch 330/2000\n",
      "1988/1988 [==============================] - 1s 544us/step - loss: 34.4847 - mae: 4.2404 - val_loss: 130.5006 - val_mae: 8.9283\n",
      "Epoch 331/2000\n",
      "1988/1988 [==============================] - 1s 543us/step - loss: 36.0300 - mae: 4.2959 - val_loss: 145.5448 - val_mae: 9.2028\n",
      "Epoch 332/2000\n",
      "1988/1988 [==============================] - 1s 543us/step - loss: 35.4750 - mae: 4.2265 - val_loss: 118.9202 - val_mae: 8.6159\n",
      "Epoch 333/2000\n",
      "1988/1988 [==============================] - 1s 550us/step - loss: 36.2303 - mae: 4.3600 - val_loss: 163.0569 - val_mae: 9.7334\n",
      "Epoch 334/2000\n",
      "1988/1988 [==============================] - 1s 551us/step - loss: 33.2392 - mae: 4.2233 - val_loss: 187.2022 - val_mae: 10.5645\n",
      "Epoch 335/2000\n",
      "1988/1988 [==============================] - 1s 550us/step - loss: 35.4964 - mae: 4.2787 - val_loss: 241.5287 - val_mae: 12.0979\n",
      "Epoch 336/2000\n",
      "1988/1988 [==============================] - 1s 553us/step - loss: 34.3475 - mae: 4.2035 - val_loss: 158.8119 - val_mae: 9.6498\n",
      "Epoch 337/2000\n",
      "1988/1988 [==============================] - 1s 542us/step - loss: 35.2592 - mae: 4.4400 - val_loss: 248.3278 - val_mae: 12.2550\n",
      "Epoch 338/2000\n",
      "1988/1988 [==============================] - 1s 560us/step - loss: 35.2132 - mae: 4.2751 - val_loss: 165.4131 - val_mae: 9.9139\n",
      "Epoch 339/2000\n",
      "1988/1988 [==============================] - 1s 549us/step - loss: 38.0697 - mae: 4.3099 - val_loss: 334.3032 - val_mae: 14.3478\n",
      "Epoch 340/2000\n",
      "1988/1988 [==============================] - 1s 531us/step - loss: 38.1225 - mae: 4.4189 - val_loss: 288.1586 - val_mae: 13.1088\n",
      "Epoch 341/2000\n",
      "1988/1988 [==============================] - 1s 565us/step - loss: 36.0883 - mae: 4.2492 - val_loss: 295.2744 - val_mae: 13.2269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/2000\n",
      "1988/1988 [==============================] - 1s 538us/step - loss: 35.3292 - mae: 4.2285 - val_loss: 405.8436 - val_mae: 16.7501\n",
      "Epoch 343/2000\n",
      "1988/1988 [==============================] - 1s 527us/step - loss: 35.8263 - mae: 4.4686 - val_loss: 301.2835 - val_mae: 13.5519\n",
      "Epoch 344/2000\n",
      "1988/1988 [==============================] - 1s 531us/step - loss: 44.1627 - mae: 4.9487 - val_loss: 225.2003 - val_mae: 11.5638\n",
      "Epoch 345/2000\n",
      "1988/1988 [==============================] - 1s 526us/step - loss: 39.0850 - mae: 4.5082 - val_loss: 306.5046 - val_mae: 13.7442\n",
      "Epoch 346/2000\n",
      "1988/1988 [==============================] - 1s 529us/step - loss: 37.1262 - mae: 4.4278 - val_loss: 191.8509 - val_mae: 10.6048\n",
      "Epoch 347/2000\n",
      "1988/1988 [==============================] - 1s 524us/step - loss: 34.4126 - mae: 4.2637 - val_loss: 237.3012 - val_mae: 11.7625\n",
      "Epoch 348/2000\n",
      "1988/1988 [==============================] - 1s 536us/step - loss: 38.4705 - mae: 4.4489 - val_loss: 306.8544 - val_mae: 13.8758\n",
      "Epoch 349/2000\n",
      "1988/1988 [==============================] - 1s 531us/step - loss: 33.5041 - mae: 4.1600 - val_loss: 239.3186 - val_mae: 11.7225\n",
      "Epoch 350/2000\n",
      "1988/1988 [==============================] - 1s 528us/step - loss: 34.6519 - mae: 4.2673 - val_loss: 259.0906 - val_mae: 12.2107\n",
      "Epoch 351/2000\n",
      "1988/1988 [==============================] - 1s 532us/step - loss: 35.1977 - mae: 4.2364 - val_loss: 388.0920 - val_mae: 15.5668\n",
      "Epoch 352/2000\n",
      "1988/1988 [==============================] - 1s 538us/step - loss: 34.4133 - mae: 4.1877 - val_loss: 226.6711 - val_mae: 11.4545\n",
      "Epoch 353/2000\n",
      "1988/1988 [==============================] - 1s 540us/step - loss: 35.5257 - mae: 4.2999 - val_loss: 280.2821 - val_mae: 12.9958\n",
      "Epoch 354/2000\n",
      "1988/1988 [==============================] - 1s 546us/step - loss: 33.6665 - mae: 4.2410 - val_loss: 210.3577 - val_mae: 11.0338\n",
      "Epoch 355/2000\n",
      "1988/1988 [==============================] - 1s 551us/step - loss: 35.3435 - mae: 4.2266 - val_loss: 227.8531 - val_mae: 11.3938\n",
      "Epoch 356/2000\n",
      "1988/1988 [==============================] - 1s 569us/step - loss: 33.0610 - mae: 4.0903 - val_loss: 223.8536 - val_mae: 11.3214\n",
      "Epoch 357/2000\n",
      "1988/1988 [==============================] - 1s 566us/step - loss: 34.2019 - mae: 4.1282 - val_loss: 149.9995 - val_mae: 10.1980\n",
      "Epoch 358/2000\n",
      "1988/1988 [==============================] - 1s 551us/step - loss: 36.1392 - mae: 4.3423 - val_loss: 218.8967 - val_mae: 11.2808\n",
      "Epoch 359/2000\n",
      "1988/1988 [==============================] - 1s 554us/step - loss: 34.6412 - mae: 4.2435 - val_loss: 150.9438 - val_mae: 9.4429\n",
      "Epoch 360/2000\n",
      "1988/1988 [==============================] - 1s 542us/step - loss: 35.7340 - mae: 4.2808 - val_loss: 308.9578 - val_mae: 13.8276\n",
      "Epoch 361/2000\n",
      "1988/1988 [==============================] - 1s 543us/step - loss: 33.2972 - mae: 4.1704 - val_loss: 256.5658 - val_mae: 12.2765\n",
      "Epoch 362/2000\n",
      "1988/1988 [==============================] - 1s 549us/step - loss: 34.5837 - mae: 4.2638 - val_loss: 312.2487 - val_mae: 13.8625\n",
      "Epoch 363/2000\n",
      "1988/1988 [==============================] - 1s 550us/step - loss: 33.7565 - mae: 4.2307 - val_loss: 201.4089 - val_mae: 10.8262\n",
      "Epoch 364/2000\n",
      "1988/1988 [==============================] - 1s 550us/step - loss: 37.3745 - mae: 4.3288 - val_loss: 173.8401 - val_mae: 10.1405\n",
      "Epoch 365/2000\n",
      "1988/1988 [==============================] - 1s 548us/step - loss: 37.2093 - mae: 4.3392 - val_loss: 354.9502 - val_mae: 15.0824\n",
      "Epoch 366/2000\n",
      "1988/1988 [==============================] - 1s 536us/step - loss: 33.4836 - mae: 4.2752 - val_loss: 150.0796 - val_mae: 9.4106\n",
      "Epoch 367/2000\n",
      "1988/1988 [==============================] - 1s 543us/step - loss: 36.9934 - mae: 4.3169 - val_loss: 196.5648 - val_mae: 10.8838\n",
      "Epoch 368/2000\n",
      "1988/1988 [==============================] - 1s 530us/step - loss: 38.0385 - mae: 4.3846 - val_loss: 194.1291 - val_mae: 10.6104\n",
      "Epoch 369/2000\n",
      "1988/1988 [==============================] - 1s 546us/step - loss: 35.1786 - mae: 4.3071 - val_loss: 244.4602 - val_mae: 12.2876\n",
      "Epoch 370/2000\n",
      "1988/1988 [==============================] - 1s 538us/step - loss: 37.1610 - mae: 4.4388 - val_loss: 178.0513 - val_mae: 10.1177\n",
      "Epoch 371/2000\n",
      "1988/1988 [==============================] - 1s 522us/step - loss: 32.6524 - mae: 4.1176 - val_loss: 219.6241 - val_mae: 11.2396\n",
      "Epoch 372/2000\n",
      "1988/1988 [==============================] - 1s 531us/step - loss: 32.3196 - mae: 4.1206 - val_loss: 245.9948 - val_mae: 11.9502\n",
      "Epoch 373/2000\n",
      "1988/1988 [==============================] - 1s 544us/step - loss: 31.2904 - mae: 3.9892 - val_loss: 236.2561 - val_mae: 11.6960\n",
      "Epoch 00373: early stopping\n",
      "\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 159us/step\n",
      "test loss, test acc: [259.1991469247001, 11.988505363464355]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 12.824052700355997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>true_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191.501175</td>\n",
       "      <td>185.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196.037720</td>\n",
       "      <td>176.979996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185.579681</td>\n",
       "      <td>176.779999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178.730865</td>\n",
       "      <td>172.289993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.928833</td>\n",
       "      <td>174.240005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>233.394836</td>\n",
       "      <td>259.429993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>227.889740</td>\n",
       "      <td>260.140015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>236.216187</td>\n",
       "      <td>262.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>221.815689</td>\n",
       "      <td>261.959991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>234.026443</td>\n",
       "      <td>264.470001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  true_value\n",
       "0    191.501175  185.860001\n",
       "1    196.037720  176.979996\n",
       "2    185.579681  176.779999\n",
       "3    178.730865  172.289993\n",
       "4    175.928833  174.240005\n",
       "..          ...         ...\n",
       "240  233.394836  259.429993\n",
       "241  227.889740  260.140015\n",
       "242  236.216187  262.200012\n",
       "243  221.815689  261.959991\n",
       "244  234.026443  264.470001\n",
       "\n",
       "[245 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_2stacks_true_value, 5, \n",
    "                                                    stock_with_abs_norm, label_value_1d, 64, batch_size, \"val_loss\", 2000)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=7)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test, batch_size=7)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)\n",
    "predictFrame = pd.DataFrame({'prediction': predictions.reshape(X_test.shape[0]), 'true_value': y_test.reshape(X_test.shape[0])})\n",
    "predictFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = predicted_train.reshape(X_train.shape[0]).tolist()\n",
    "b = predicted_valid.reshape(X_valid.shape[0]).tolist()\n",
    "c = predicted_test.reshape(X_test.shape[0]).tolist()\n",
    "d = y_train.reshape(X_train.shape[0]).tolist()\n",
    "e = y_valid.reshape(X_valid.shape[0]).tolist()\n",
    "f = y_test.reshape(X_test.shape[0]).tolist()\n",
    "a.extend(b)\n",
    "a.extend(c)\n",
    "d.extend(e)\n",
    "d.extend(f)\n",
    "predictFrame = pd.DataFrame({'prediction': a, 'true_value': d})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Price', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Best Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim:  6\n",
      "trim:  6\n",
      "trim:  4\n",
      "trim:  4\n",
      "trim:  1\n",
      "trim:  1\n",
      "(1981, 7, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_101 (LSTM)              (7, 64)                   40192     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (7, 20)                   1300      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (7, 1)                    21        \n",
      "=================================================================\n",
      "Total params: 41,513\n",
      "Trainable params: 41,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.551\n",
      "The Validation Accuracy  0.507\n",
      "The Test Accuracy   0.518\n",
      "AUC ROC : 0.481\n",
      "confusion matrix / precision recall scores\n",
      "[[114  21]\n",
      " [ 97  13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.84      0.66       135\n",
      "           1       0.38      0.12      0.18       110\n",
      "\n",
      "    accuracy                           0.52       245\n",
      "   macro avg       0.46      0.48      0.42       245\n",
      "weighted avg       0.47      0.52      0.44       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_1stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_1stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim:  6\n",
      "trim:  6\n",
      "trim:  4\n",
      "trim:  4\n",
      "trim:  1\n",
      "trim:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_104 (LSTM)              (7, 7, 64)                40192     \n",
      "_________________________________________________________________\n",
      "lstm_105 (LSTM)              (7, 64)                   33024     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (7, 64)                   0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (7, 20)                   1300      \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (7, 1)                    21        \n",
      "=================================================================\n",
      "Total params: 74,537\n",
      "Trainable params: 74,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00344: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.947\n",
      "The Validation Accuracy  0.525\n",
      "The Test Accuracy   0.490\n",
      "AUC ROC : 0.492\n",
      "confusion matrix / precision recall scores\n",
      "[[64 71]\n",
      " [54 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.47      0.51       135\n",
      "           1       0.44      0.51      0.47       110\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.49      0.49      0.49       245\n",
      "weighted avg       0.50      0.49      0.49       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_2stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_2stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim:  6\n",
      "trim:  6\n",
      "trim:  4\n",
      "trim:  4\n",
      "trim:  1\n",
      "trim:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_106 (LSTM)              (7, 7, 64)                40192     \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (7, 7, 64)                0         \n",
      "_________________________________________________________________\n",
      "lstm_107 (LSTM)              (7, 7, 64)                33024     \n",
      "_________________________________________________________________\n",
      "lstm_108 (LSTM)              (7, 64)                   33024     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (7, 64)                   0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (7, 20)                   1300      \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (7, 1)                    21        \n",
      "=================================================================\n",
      "Total params: 107,561\n",
      "Trainable params: 107,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00570: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.913\n",
      "The Validation Accuracy  0.442\n",
      "The Test Accuracy   0.531\n",
      "AUC ROC : 0.524\n",
      "confusion matrix / precision recall scores\n",
      "[[80 55]\n",
      " [60 50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.59      0.58       135\n",
      "           1       0.48      0.45      0.47       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.52      0.52      0.52       245\n",
      "weighted avg       0.53      0.53      0.53       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_3stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_3stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim:  6\n",
      "trim:  6\n",
      "trim:  4\n",
      "trim:  4\n",
      "trim:  1\n",
      "trim:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_109 (LSTM)              (7, 7, 64)                40192     \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (7, 7, 64)                0         \n",
      "_________________________________________________________________\n",
      "lstm_110 (LSTM)              (7, 7, 64)                33024     \n",
      "_________________________________________________________________\n",
      "lstm_111 (LSTM)              (7, 7, 64)                33024     \n",
      "_________________________________________________________________\n",
      "lstm_112 (LSTM)              (7, 64)                   33024     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (7, 64)                   0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (7, 20)                   1300      \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (7, 1)                    21        \n",
      "=================================================================\n",
      "Total params: 140,585\n",
      "Trainable params: 140,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00445: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.861\n",
      "The Validation Accuracy  0.475\n",
      "The Test Accuracy   0.469\n",
      "AUC ROC : 0.462\n",
      "confusion matrix / precision recall scores\n",
      "[[72 63]\n",
      " [67 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.53      0.53       135\n",
      "           1       0.41      0.39      0.40       110\n",
      "\n",
      "    accuracy                           0.47       245\n",
      "   macro avg       0.46      0.46      0.46       245\n",
      "weighted avg       0.47      0.47      0.47       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_4stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_4stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Best Stacks (using loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1981, 7, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00242: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.562\n",
      "The Validation Accuracy  0.525\n",
      "The Test Accuracy   0.527\n",
      "AUC ROC : 0.493\n",
      "confusion matrix / precision recall scores\n",
      "[[111  24]\n",
      " [ 92  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.82      0.66       135\n",
      "           1       0.43      0.16      0.24       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.49      0.49      0.45       245\n",
      "weighted avg       0.49      0.53      0.47       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train_loss(buildTrendModel_1stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_1stack_hid64_loss\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00391: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.850\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.514\n",
      "AUC ROC : 0.513\n",
      "confusion matrix / precision recall scores\n",
      "[[71 64]\n",
      " [55 55]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.54       135\n",
      "           1       0.46      0.50      0.48       110\n",
      "\n",
      "    accuracy                           0.51       245\n",
      "   macro avg       0.51      0.51      0.51       245\n",
      "weighted avg       0.52      0.51      0.52       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train_loss(buildTrendModel_2stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_2stack_hid64_loss\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00224: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.539\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.544\n",
      "confusion matrix / precision recall scores\n",
      "[[83 52]\n",
      " [58 52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60       135\n",
      "           1       0.50      0.47      0.49       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.54      0.54      0.54       245\n",
      "weighted avg       0.55      0.55      0.55       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train_loss(buildTrendModel_3stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_3stack_hid64_loss\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00249: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.507\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.555\n",
      "confusion matrix / precision recall scores\n",
      "[[70 65]\n",
      " [45 65]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.52      0.56       135\n",
      "           1       0.50      0.59      0.54       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.55      0.55      0.55       245\n",
      "weighted avg       0.56      0.55      0.55       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train_loss(buildTrendModel_4stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_4stack_hid64_loss\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Best Layer (using loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Predict  1d\n",
      "Layer:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00331: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.718\n",
      "The Validation Accuracy  0.484\n",
      "The Test Accuracy   0.539\n",
      "AUC ROC : 0.517\n",
      "confusion matrix / precision recall scores\n",
      "[[99 36]\n",
      " [77 33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64       135\n",
      "           1       0.48      0.30      0.37       110\n",
      "\n",
      "    accuracy                           0.54       245\n",
      "   macro avg       0.52      0.52      0.50       245\n",
      "weighted avg       0.52      0.54      0.52       245\n",
      "\n",
      "Layer:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00309: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.975\n",
      "The Validation Accuracy  0.475\n",
      "The Test Accuracy   0.514\n",
      "AUC ROC : 0.517\n",
      "confusion matrix / precision recall scores\n",
      "[[66 69]\n",
      " [50 60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.49      0.53       135\n",
      "           1       0.47      0.55      0.50       110\n",
      "\n",
      "    accuracy                           0.51       245\n",
      "   macro avg       0.52      0.52      0.51       245\n",
      "weighted avg       0.52      0.51      0.52       245\n",
      "\n",
      "Layer:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00214: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Start Predict  7d\n",
      "Layer:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00333: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.926\n",
      "The Validation Accuracy  0.558\n",
      "The Test Accuracy   0.600\n",
      "AUC ROC : 0.553\n",
      "confusion matrix / precision recall scores\n",
      "[[115  57]\n",
      " [ 41  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.67      0.70       172\n",
      "           1       0.36      0.44      0.40        73\n",
      "\n",
      "    accuracy                           0.60       245\n",
      "   macro avg       0.55      0.55      0.55       245\n",
      "weighted avg       0.62      0.60      0.61       245\n",
      "\n",
      "Layer:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00311: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.994\n",
      "The Validation Accuracy  0.594\n",
      "The Test Accuracy   0.478\n",
      "AUC ROC : 0.494\n",
      "confusion matrix / precision recall scores\n",
      "[[78 94]\n",
      " [34 39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.45      0.55       172\n",
      "           1       0.29      0.53      0.38        73\n",
      "\n",
      "    accuracy                           0.48       245\n",
      "   macro avg       0.49      0.49      0.46       245\n",
      "weighted avg       0.58      0.48      0.50       245\n",
      "\n",
      "Layer:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00240: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.571\n",
      "The Test Accuracy   0.543\n",
      "AUC ROC : 0.489\n",
      "confusion matrix / precision recall scores\n",
      "[[107  65]\n",
      " [ 47  26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.62      0.66       172\n",
      "           1       0.29      0.36      0.32        73\n",
      "\n",
      "    accuracy                           0.54       245\n",
      "   macro avg       0.49      0.49      0.49       245\n",
      "weighted avg       0.57      0.54      0.56       245\n",
      "\n",
      "Start Predict  30d\n",
      "Layer:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00322: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.947\n",
      "The Validation Accuracy  0.567\n",
      "The Test Accuracy   0.796\n",
      "AUC ROC : 0.547\n",
      "confusion matrix / precision recall scores\n",
      "[[189   4]\n",
      " [ 46   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88       193\n",
      "           1       0.60      0.12      0.19        52\n",
      "\n",
      "    accuracy                           0.80       245\n",
      "   macro avg       0.70      0.55      0.54       245\n",
      "weighted avg       0.76      0.80      0.74       245\n",
      "\n",
      "Layer:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00300: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.993\n",
      "The Validation Accuracy  0.659\n",
      "The Test Accuracy   0.755\n",
      "AUC ROC : 0.606\n",
      "confusion matrix / precision recall scores\n",
      "[[167  26]\n",
      " [ 34  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       193\n",
      "           1       0.41      0.35      0.38        52\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.62      0.61      0.61       245\n",
      "weighted avg       0.74      0.76      0.75       245\n",
      "\n",
      "Layer:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.673\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "hidden_layer_size = [32 , 64, 128]\n",
    "predict_day = [label_abs_1d, label_abs_7d, label_abs_30d]\n",
    "predict_day_str = [\"1d\", \"7d\", \"30d\"]\n",
    "\n",
    "for day in range(3):\n",
    "    print(\"Start Predict \", predict_day_str[day])\n",
    "    for layer_size in hidden_layer_size:\n",
    "        print(\"Layer: \", layer_size)\n",
    "        batch_size = 7\n",
    "        model, X_train, y_train, X_valid, y_valid, X_test, y_test = model_train_loss(buildTrendModel_3stacks, 7, \n",
    "        stock_with_absolute, predict_day[day], layer_size, batch_size)\n",
    "\n",
    "        predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "        predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "        predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "        print()\n",
    "        result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "               \"./LSTM_RESULT/best_layer_with_abs/LSTM_\"+predict_day_str[day]+\"_sequence_3stack_hid\"+str(layer_size)+\"_loss\", model.predict, clf_name=\"LSTM\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test All Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Predict  1d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n",
      "Epoch 00804: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.659\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.531\n",
      "AUC ROC : 0.494\n",
      "confusion matrix / precision recall scores\n",
      "[[115  20]\n",
      " [ 95  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.85      0.67       135\n",
      "           1       0.43      0.14      0.21       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.49      0.49      0.44       245\n",
      "weighted avg       0.49      0.53      0.46       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00357: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.729\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.482\n",
      "AUC ROC : 0.469\n",
      "confusion matrix / precision recall scores\n",
      "[[80 55]\n",
      " [72 38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.59      0.56       135\n",
      "           1       0.41      0.35      0.37       110\n",
      "\n",
      "    accuracy                           0.48       245\n",
      "   macro avg       0.47      0.47      0.47       245\n",
      "weighted avg       0.47      0.48      0.48       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00470: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.687\n",
      "The Validation Accuracy  0.465\n",
      "The Test Accuracy   0.486\n",
      "AUC ROC : 0.465\n",
      "confusion matrix / precision recall scores\n",
      "[[90 45]\n",
      " [81 29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.67      0.59       135\n",
      "           1       0.39      0.26      0.32       110\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.46      0.47      0.45       245\n",
      "weighted avg       0.47      0.49      0.47       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00326: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.617\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.539\n",
      "AUC ROC : 0.503\n",
      "confusion matrix / precision recall scores\n",
      "[[115  20]\n",
      " [ 93  17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.85      0.67       135\n",
      "           1       0.46      0.15      0.23       110\n",
      "\n",
      "    accuracy                           0.54       245\n",
      "   macro avg       0.51      0.50      0.45       245\n",
      "weighted avg       0.51      0.54      0.47       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00328: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.716\n",
      "The Validation Accuracy  0.479\n",
      "The Test Accuracy   0.508\n",
      "AUC ROC : 0.499\n",
      "confusion matrix / precision recall scores\n",
      "[[78 53]\n",
      " [64 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.60      0.57       131\n",
      "           1       0.45      0.40      0.42       107\n",
      "\n",
      "    accuracy                           0.51       238\n",
      "   macro avg       0.50      0.50      0.50       238\n",
      "weighted avg       0.50      0.51      0.50       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.526\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00281: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.903\n",
      "The Validation Accuracy  0.470\n",
      "The Test Accuracy   0.482\n",
      "AUC ROC : 0.482\n",
      "confusion matrix / precision recall scores\n",
      "[[65 70]\n",
      " [57 53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.48      0.51       135\n",
      "           1       0.43      0.48      0.45       110\n",
      "\n",
      "    accuracy                           0.48       245\n",
      "   macro avg       0.48      0.48      0.48       245\n",
      "weighted avg       0.49      0.48      0.48       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00258: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.953\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.559\n",
      "AUC ROC : 0.539\n",
      "confusion matrix / precision recall scores\n",
      "[[96 35]\n",
      " [70 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.65       131\n",
      "           1       0.51      0.35      0.41       107\n",
      "\n",
      "    accuracy                           0.56       238\n",
      "   macro avg       0.55      0.54      0.53       238\n",
      "weighted avg       0.55      0.56      0.54       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00211: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00470: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.939\n",
      "The Validation Accuracy  0.479\n",
      "The Test Accuracy   0.465\n",
      "AUC ROC : 0.463\n",
      "confusion matrix / precision recall scores\n",
      "[[65 70]\n",
      " [61 49]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.48      0.50       135\n",
      "           1       0.41      0.45      0.43       110\n",
      "\n",
      "    accuracy                           0.47       245\n",
      "   macro avg       0.46      0.46      0.46       245\n",
      "weighted avg       0.47      0.47      0.47       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00206: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00254: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.526\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00203: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.526\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.550\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[131   0]\n",
      " [107   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       131\n",
      "           1       0.00      0.00      0.00       107\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start Predict  7d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00511: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.646\n",
      "The Validation Accuracy  0.567\n",
      "The Test Accuracy   0.686\n",
      "AUC ROC : 0.512\n",
      "confusion matrix / precision recall scores\n",
      "[[162  10]\n",
      " [ 67   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.94      0.81       172\n",
      "           1       0.38      0.08      0.13        73\n",
      "\n",
      "    accuracy                           0.69       245\n",
      "   macro avg       0.54      0.51      0.47       245\n",
      "weighted avg       0.61      0.69      0.61       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00258: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.718\n",
      "The Validation Accuracy  0.576\n",
      "The Test Accuracy   0.588\n",
      "AUC ROC : 0.458\n",
      "confusion matrix / precision recall scores\n",
      "[[134  38]\n",
      " [ 63  10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73       172\n",
      "           1       0.21      0.14      0.17        73\n",
      "\n",
      "    accuracy                           0.59       245\n",
      "   macro avg       0.44      0.46      0.45       245\n",
      "weighted avg       0.54      0.59      0.56       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00262: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.880\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.392\n",
      "AUC ROC : 0.362\n",
      "confusion matrix / precision recall scores\n",
      "[[75 97]\n",
      " [52 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.44      0.50       172\n",
      "           1       0.18      0.29      0.22        73\n",
      "\n",
      "    accuracy                           0.39       245\n",
      "   macro avg       0.38      0.36      0.36       245\n",
      "weighted avg       0.47      0.39      0.42       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00253: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.966\n",
      "The Validation Accuracy  0.539\n",
      "The Test Accuracy   0.518\n",
      "AUC ROC : 0.480\n",
      "confusion matrix / precision recall scores\n",
      "[[99 73]\n",
      " [45 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.58      0.63       172\n",
      "           1       0.28      0.38      0.32        73\n",
      "\n",
      "    accuracy                           0.52       245\n",
      "   macro avg       0.48      0.48      0.47       245\n",
      "weighted avg       0.57      0.52      0.54       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00267: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.881\n",
      "The Validation Accuracy  0.539\n",
      "The Test Accuracy   0.529\n",
      "AUC ROC : 0.422\n",
      "confusion matrix / precision recall scores\n",
      "[[115  52]\n",
      " [ 60  11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.67       167\n",
      "           1       0.17      0.15      0.16        71\n",
      "\n",
      "    accuracy                           0.53       238\n",
      "   macro avg       0.42      0.42      0.42       238\n",
      "weighted avg       0.51      0.53      0.52       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00275: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.671\n",
      "The Validation Accuracy  0.548\n",
      "The Test Accuracy   0.673\n",
      "AUC ROC : 0.515\n",
      "confusion matrix / precision recall scores\n",
      "[[156  16]\n",
      " [ 64   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.91      0.80       172\n",
      "           1       0.36      0.12      0.18        73\n",
      "\n",
      "    accuracy                           0.67       245\n",
      "   macro avg       0.53      0.52      0.49       245\n",
      "weighted avg       0.61      0.67      0.61       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00234: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.907\n",
      "The Validation Accuracy  0.599\n",
      "The Test Accuracy   0.588\n",
      "AUC ROC : 0.525\n",
      "confusion matrix / precision recall scores\n",
      "[[117  55]\n",
      " [ 46  27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70       172\n",
      "           1       0.33      0.37      0.35        73\n",
      "\n",
      "    accuracy                           0.59       245\n",
      "   macro avg       0.52      0.53      0.52       245\n",
      "weighted avg       0.60      0.59      0.59       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.970\n",
      "The Validation Accuracy  0.502\n",
      "The Test Accuracy   0.616\n",
      "AUC ROC : 0.518\n",
      "confusion matrix / precision recall scores\n",
      "[[131  41]\n",
      " [ 53  20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.74       172\n",
      "           1       0.33      0.27      0.30        73\n",
      "\n",
      "    accuracy                           0.62       245\n",
      "   macro avg       0.52      0.52      0.52       245\n",
      "weighted avg       0.60      0.62      0.61       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00225: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.986\n",
      "The Validation Accuracy  0.419\n",
      "The Test Accuracy   0.494\n",
      "AUC ROC : 0.474\n",
      "confusion matrix / precision recall scores\n",
      "[[90 82]\n",
      " [42 31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.52      0.59       172\n",
      "           1       0.27      0.42      0.33        73\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.48      0.47      0.46       245\n",
      "weighted avg       0.56      0.49      0.52       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00313: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.948\n",
      "The Validation Accuracy  0.461\n",
      "The Test Accuracy   0.525\n",
      "AUC ROC : 0.467\n",
      "confusion matrix / precision recall scores\n",
      "[[102  65]\n",
      " [ 48  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.61      0.64       167\n",
      "           1       0.26      0.32      0.29        71\n",
      "\n",
      "    accuracy                           0.53       238\n",
      "   macro avg       0.47      0.47      0.47       238\n",
      "weighted avg       0.56      0.53      0.54       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00295: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.579\n",
      "The Validation Accuracy  0.567\n",
      "The Test Accuracy   0.694\n",
      "AUC ROC : 0.494\n",
      "confusion matrix / precision recall scores\n",
      "[[170   2]\n",
      " [ 73   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.99      0.82       172\n",
      "           1       0.00      0.00      0.00        73\n",
      "\n",
      "    accuracy                           0.69       245\n",
      "   macro avg       0.35      0.49      0.41       245\n",
      "weighted avg       0.49      0.69      0.58       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00330: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.978\n",
      "The Validation Accuracy  0.590\n",
      "The Test Accuracy   0.571\n",
      "AUC ROC : 0.478\n",
      "confusion matrix / precision recall scores\n",
      "[[122  50]\n",
      " [ 55  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70       172\n",
      "           1       0.26      0.25      0.26        73\n",
      "\n",
      "    accuracy                           0.57       245\n",
      "   macro avg       0.48      0.48      0.48       245\n",
      "weighted avg       0.56      0.57      0.57       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00239: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.964\n",
      "The Validation Accuracy  0.553\n",
      "The Test Accuracy   0.351\n",
      "AUC ROC : 0.368\n",
      "confusion matrix / precision recall scores\n",
      "[[ 56 116]\n",
      " [ 43  30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.33      0.41       172\n",
      "           1       0.21      0.41      0.27        73\n",
      "\n",
      "    accuracy                           0.35       245\n",
      "   macro avg       0.39      0.37      0.34       245\n",
      "weighted avg       0.46      0.35      0.37       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00290: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.994\n",
      "The Validation Accuracy  0.599\n",
      "The Test Accuracy   0.502\n",
      "AUC ROC : 0.476\n",
      "confusion matrix / precision recall scores\n",
      "[[93 79]\n",
      " [43 30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.54      0.60       172\n",
      "           1       0.28      0.41      0.33        73\n",
      "\n",
      "    accuracy                           0.50       245\n",
      "   macro avg       0.48      0.48      0.47       245\n",
      "weighted avg       0.56      0.50      0.52       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00284: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.581\n",
      "The Test Accuracy   0.588\n",
      "AUC ROC : 0.512\n",
      "confusion matrix / precision recall scores\n",
      "[[117  50]\n",
      " [ 48  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.70       167\n",
      "           1       0.32      0.32      0.32        71\n",
      "\n",
      "    accuracy                           0.59       238\n",
      "   macro avg       0.51      0.51      0.51       238\n",
      "weighted avg       0.59      0.59      0.59       238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start Predict  30d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00206: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.636\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00256: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.920\n",
      "The Validation Accuracy  0.484\n",
      "The Test Accuracy   0.494\n",
      "AUC ROC : 0.412\n",
      "confusion matrix / precision recall scores\n",
      "[[107  86]\n",
      " [ 38  14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.55      0.63       193\n",
      "           1       0.14      0.27      0.18        52\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.44      0.41      0.41       245\n",
      "weighted avg       0.61      0.49      0.54       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00305: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.963\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.759\n",
      "AUC ROC : 0.631\n",
      "confusion matrix / precision recall scores\n",
      "[[165  29]\n",
      " [ 30  21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       194\n",
      "           1       0.42      0.41      0.42        51\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.63      0.63      0.63       245\n",
      "weighted avg       0.76      0.76      0.76       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00450: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.964\n",
      "The Validation Accuracy  0.682\n",
      "The Test Accuracy   0.681\n",
      "AUC ROC : 0.563\n",
      "confusion matrix / precision recall scores\n",
      "[[144  44]\n",
      " [ 32  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.79       188\n",
      "           1       0.29      0.36      0.32        50\n",
      "\n",
      "    accuracy                           0.68       238\n",
      "   macro avg       0.55      0.56      0.56       238\n",
      "weighted avg       0.71      0.68      0.69       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00433: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.701\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.759\n",
      "AUC ROC : 0.482\n",
      "confusion matrix / precision recall scores\n",
      "[[186   7]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.96      0.86       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.39      0.48      0.43       245\n",
      "weighted avg       0.62      0.76      0.68       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00383: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.814\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.633\n",
      "AUC ROC : 0.444\n",
      "confusion matrix / precision recall scores\n",
      "[[149  44]\n",
      " [ 46   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77       193\n",
      "           1       0.12      0.12      0.12        52\n",
      "\n",
      "    accuracy                           0.63       245\n",
      "   macro avg       0.44      0.44      0.44       245\n",
      "weighted avg       0.63      0.63      0.63       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00306: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.820\n",
      "The Validation Accuracy  0.452\n",
      "The Test Accuracy   0.706\n",
      "AUC ROC : 0.596\n",
      "confusion matrix / precision recall scores\n",
      "[[152  41]\n",
      " [ 31  21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       193\n",
      "           1       0.34      0.40      0.37        52\n",
      "\n",
      "    accuracy                           0.71       245\n",
      "   macro avg       0.58      0.60      0.59       245\n",
      "weighted avg       0.73      0.71      0.72       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.630\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.792\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[194   0]\n",
      " [ 51   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       194\n",
      "           1       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.40      0.50      0.44       245\n",
      "weighted avg       0.63      0.79      0.70       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00352: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.899\n",
      "The Validation Accuracy  0.447\n",
      "The Test Accuracy   0.664\n",
      "AUC ROC : 0.428\n",
      "confusion matrix / precision recall scores\n",
      "[[157  31]\n",
      " [ 49   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80       188\n",
      "           1       0.03      0.02      0.02        50\n",
      "\n",
      "    accuracy                           0.66       238\n",
      "   macro avg       0.40      0.43      0.41       238\n",
      "weighted avg       0.61      0.66      0.63       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00231: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00214: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.630\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00202: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.630\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.792\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[194   0]\n",
      " [ 51   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       194\n",
      "           1       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.40      0.50      0.44       245\n",
      "weighted avg       0.63      0.79      0.70       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00202: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.625\n",
      "The Validation Accuracy  0.558\n",
      "The Test Accuracy   0.790\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[188   0]\n",
      " [ 50   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       188\n",
      "           1       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.79       238\n",
      "   macro avg       0.39      0.50      0.44       238\n",
      "weighted avg       0.62      0.79      0.70       238\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "hidden_layer_size = [32 , 64, 128]\n",
    "sequence_day = [1,3,7,15,30]\n",
    "predict_day = [label_abs_1d, label_abs_7d, label_abs_30d]\n",
    "predict_day_str = [\"1d\", \"7d\", \"30d\"]\n",
    "\n",
    "for day in range(3):\n",
    "    print(\"Start Predict \", predict_day_str[day])\n",
    "    print()\n",
    "    for layer_size in hidden_layer_size:\n",
    "        print(\"Layer: \", layer_size)\n",
    "        for past_day in sequence_day:\n",
    "            print(\"Sequence number of day: \", past_day)\n",
    "            model, X_train, y_train, X_valid, y_valid, X_test, y_test = model_train_loss(buildTrendModel_3stacks, past_day, \n",
    "            stock_with_absolute, predict_day[day], layer_size, batch_size)\n",
    "\n",
    "            predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "            predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "            predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "            print()\n",
    "            result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "                   \"./LSTM_RESULT/best_layer_seqnum_with_abs/LSTM_\"+predict_day_str[day]+\"_sequence_3stack_hid\"+str(layer_size)+\"_seq\"+str(past_day)+\"_loss\", model.predict, clf_name=\"LSTM\")\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Predict  1d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n",
      "Epoch 00211: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.681\n",
      "The Validation Accuracy  0.475\n",
      "The Test Accuracy   0.535\n",
      "AUC ROC : 0.510\n",
      "confusion matrix / precision recall scores\n",
      "[[101  34]\n",
      " [ 80  30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64       135\n",
      "           1       0.47      0.27      0.34       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.51      0.51      0.49       245\n",
      "weighted avg       0.52      0.53      0.51       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00213: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.841\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.543\n",
      "AUC ROC : 0.540\n",
      "confusion matrix / precision recall scores\n",
      "[[77 58]\n",
      " [54 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58       135\n",
      "           1       0.49      0.51      0.50       110\n",
      "\n",
      "    accuracy                           0.54       245\n",
      "   macro avg       0.54      0.54      0.54       245\n",
      "weighted avg       0.54      0.54      0.54       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00266: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.911\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.461\n",
      "AUC ROC : 0.466\n",
      "confusion matrix / precision recall scores\n",
      "[[57 78]\n",
      " [54 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.42      0.46       135\n",
      "           1       0.42      0.51      0.46       110\n",
      "\n",
      "    accuracy                           0.46       245\n",
      "   macro avg       0.47      0.47      0.46       245\n",
      "weighted avg       0.47      0.46      0.46       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00248: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.964\n",
      "The Validation Accuracy  0.576\n",
      "The Test Accuracy   0.559\n",
      "AUC ROC : 0.543\n",
      "confusion matrix / precision recall scores\n",
      "[[95 40]\n",
      " [68 42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.70      0.64       135\n",
      "           1       0.51      0.38      0.44       110\n",
      "\n",
      "    accuracy                           0.56       245\n",
      "   macro avg       0.55      0.54      0.54       245\n",
      "weighted avg       0.55      0.56      0.55       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00220: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.931\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.521\n",
      "AUC ROC : 0.518\n",
      "confusion matrix / precision recall scores\n",
      "[[72 59]\n",
      " [55 52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.55      0.56       131\n",
      "           1       0.47      0.49      0.48       107\n",
      "\n",
      "    accuracy                           0.52       238\n",
      "   macro avg       0.52      0.52      0.52       238\n",
      "weighted avg       0.52      0.52      0.52       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00270: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.808\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.535\n",
      "AUC ROC : 0.515\n",
      "confusion matrix / precision recall scores\n",
      "[[96 39]\n",
      " [75 35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.71      0.63       135\n",
      "           1       0.47      0.32      0.38       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.52      0.51      0.50       245\n",
      "weighted avg       0.52      0.53      0.52       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00222: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.940\n",
      "The Validation Accuracy  0.465\n",
      "The Test Accuracy   0.494\n",
      "AUC ROC : 0.491\n",
      "confusion matrix / precision recall scores\n",
      "[[70 65]\n",
      " [59 51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.52      0.53       135\n",
      "           1       0.44      0.46      0.45       110\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.49      0.49      0.49       245\n",
      "weighted avg       0.50      0.49      0.49       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00215: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.531\n",
      "AUC ROC : 0.527\n",
      "confusion matrix / precision recall scores\n",
      "[[76 59]\n",
      " [56 54]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57       135\n",
      "           1       0.48      0.49      0.48       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.53      0.53      0.53       245\n",
      "weighted avg       0.53      0.53      0.53       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00234: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.990\n",
      "The Validation Accuracy  0.447\n",
      "The Test Accuracy   0.506\n",
      "AUC ROC : 0.506\n",
      "confusion matrix / precision recall scores\n",
      "[[68 67]\n",
      " [54 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53       135\n",
      "           1       0.46      0.51      0.48       110\n",
      "\n",
      "    accuracy                           0.51       245\n",
      "   macro avg       0.51      0.51      0.50       245\n",
      "weighted avg       0.51      0.51      0.51       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00210: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.999\n",
      "The Validation Accuracy  0.498\n",
      "The Test Accuracy   0.483\n",
      "AUC ROC : 0.477\n",
      "confusion matrix / precision recall scores\n",
      "[[71 60]\n",
      " [63 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.54      0.54       131\n",
      "           1       0.42      0.41      0.42       107\n",
      "\n",
      "    accuracy                           0.48       238\n",
      "   macro avg       0.48      0.48      0.48       238\n",
      "weighted avg       0.48      0.48      0.48       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00227: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.784\n",
      "The Validation Accuracy  0.516\n",
      "The Test Accuracy   0.547\n",
      "AUC ROC : 0.516\n",
      "confusion matrix / precision recall scores\n",
      "[[111  24]\n",
      " [ 87  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.82      0.67       135\n",
      "           1       0.49      0.21      0.29       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.52      0.52      0.48       245\n",
      "weighted avg       0.53      0.55      0.50       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00219: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.526\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00236: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.993\n",
      "The Validation Accuracy  0.544\n",
      "The Test Accuracy   0.437\n",
      "AUC ROC : 0.433\n",
      "confusion matrix / precision recall scores\n",
      "[[63 72]\n",
      " [66 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.47      0.48       135\n",
      "           1       0.38      0.40      0.39       110\n",
      "\n",
      "    accuracy                           0.44       245\n",
      "   macro avg       0.43      0.43      0.43       245\n",
      "weighted avg       0.44      0.44      0.44       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00243: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.999\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.555\n",
      "AUC ROC : 0.556\n",
      "confusion matrix / precision recall scores\n",
      "[[74 61]\n",
      " [48 62]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.55      0.58       135\n",
      "           1       0.50      0.56      0.53       110\n",
      "\n",
      "    accuracy                           0.56       245\n",
      "   macro avg       0.56      0.56      0.55       245\n",
      "weighted avg       0.56      0.56      0.56       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00228: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  1.000\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.517\n",
      "AUC ROC : 0.520\n",
      "confusion matrix / precision recall scores\n",
      "[[64 67]\n",
      " [48 59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.49      0.53       131\n",
      "           1       0.47      0.55      0.51       107\n",
      "\n",
      "    accuracy                           0.52       238\n",
      "   macro avg       0.52      0.52      0.52       238\n",
      "weighted avg       0.53      0.52      0.52       238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start Predict  7d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00207: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.711\n",
      "The Validation Accuracy  0.558\n",
      "The Test Accuracy   0.584\n",
      "AUC ROC : 0.510\n",
      "confusion matrix / precision recall scores\n",
      "[[119  53]\n",
      " [ 49  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       172\n",
      "           1       0.31      0.33      0.32        73\n",
      "\n",
      "    accuracy                           0.58       245\n",
      "   macro avg       0.51      0.51      0.51       245\n",
      "weighted avg       0.59      0.58      0.59       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00212: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.820\n",
      "The Validation Accuracy  0.544\n",
      "The Test Accuracy   0.624\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[139  33]\n",
      " [ 59  14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.81      0.75       172\n",
      "           1       0.30      0.19      0.23        73\n",
      "\n",
      "    accuracy                           0.62       245\n",
      "   macro avg       0.50      0.50      0.49       245\n",
      "weighted avg       0.58      0.62      0.60       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00209: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.981\n",
      "The Validation Accuracy  0.479\n",
      "The Test Accuracy   0.612\n",
      "AUC ROC : 0.503\n",
      "confusion matrix / precision recall scores\n",
      "[[133  39]\n",
      " [ 56  17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.74       172\n",
      "           1       0.30      0.23      0.26        73\n",
      "\n",
      "    accuracy                           0.61       245\n",
      "   macro avg       0.50      0.50      0.50       245\n",
      "weighted avg       0.58      0.61      0.60       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00216: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.989\n",
      "The Validation Accuracy  0.581\n",
      "The Test Accuracy   0.596\n",
      "AUC ROC : 0.519\n",
      "confusion matrix / precision recall scores\n",
      "[[122  50]\n",
      " [ 49  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71       172\n",
      "           1       0.32      0.33      0.33        73\n",
      "\n",
      "    accuracy                           0.60       245\n",
      "   macro avg       0.52      0.52      0.52       245\n",
      "weighted avg       0.60      0.60      0.60       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00203: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.991\n",
      "The Validation Accuracy  0.516\n",
      "The Test Accuracy   0.634\n",
      "AUC ROC : 0.553\n",
      "confusion matrix / precision recall scores\n",
      "[[126  41]\n",
      " [ 46  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       167\n",
      "           1       0.38      0.35      0.36        71\n",
      "\n",
      "    accuracy                           0.63       238\n",
      "   macro avg       0.56      0.55      0.55       238\n",
      "weighted avg       0.63      0.63      0.63       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00206: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.808\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.633\n",
      "AUC ROC : 0.486\n",
      "confusion matrix / precision recall scores\n",
      "[[146  26]\n",
      " [ 64   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.85      0.76       172\n",
      "           1       0.26      0.12      0.17        73\n",
      "\n",
      "    accuracy                           0.63       245\n",
      "   macro avg       0.48      0.49      0.47       245\n",
      "weighted avg       0.56      0.63      0.59       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00208: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.942\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.592\n",
      "AUC ROC : 0.516\n",
      "confusion matrix / precision recall scores\n",
      "[[121  51]\n",
      " [ 49  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71       172\n",
      "           1       0.32      0.33      0.32        73\n",
      "\n",
      "    accuracy                           0.59       245\n",
      "   macro avg       0.52      0.52      0.52       245\n",
      "weighted avg       0.60      0.59      0.59       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00203: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.553\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.523\n",
      "confusion matrix / precision recall scores\n",
      "[[102  70]\n",
      " [ 40  33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       172\n",
      "           1       0.32      0.45      0.38        73\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.52      0.52      0.51       245\n",
      "weighted avg       0.60      0.55      0.57       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.493\n",
      "The Test Accuracy   0.600\n",
      "AUC ROC : 0.553\n",
      "confusion matrix / precision recall scores\n",
      "[[115  57]\n",
      " [ 41  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.67      0.70       172\n",
      "           1       0.36      0.44      0.40        73\n",
      "\n",
      "    accuracy                           0.60       245\n",
      "   macro avg       0.55      0.55      0.55       245\n",
      "weighted avg       0.62      0.60      0.61       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.544\n",
      "The Test Accuracy   0.567\n",
      "AUC ROC : 0.534\n",
      "confusion matrix / precision recall scores\n",
      "[[103  64]\n",
      " [ 39  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.62      0.67       167\n",
      "           1       0.33      0.45      0.38        71\n",
      "\n",
      "    accuracy                           0.57       238\n",
      "   macro avg       0.53      0.53      0.52       238\n",
      "weighted avg       0.61      0.57      0.58       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00206: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.904\n",
      "The Validation Accuracy  0.539\n",
      "The Test Accuracy   0.637\n",
      "AUC ROC : 0.540\n",
      "confusion matrix / precision recall scores\n",
      "[[134  38]\n",
      " [ 51  22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75       172\n",
      "           1       0.37      0.30      0.33        73\n",
      "\n",
      "    accuracy                           0.64       245\n",
      "   macro avg       0.55      0.54      0.54       245\n",
      "weighted avg       0.62      0.64      0.63       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00221: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.995\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.547\n",
      "AUC ROC : 0.520\n",
      "confusion matrix / precision recall scores\n",
      "[[101  71]\n",
      " [ 40  33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       172\n",
      "           1       0.32      0.45      0.37        73\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.52      0.52      0.51       245\n",
      "weighted avg       0.60      0.55      0.56       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00212: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.999\n",
      "The Validation Accuracy  0.590\n",
      "The Test Accuracy   0.531\n",
      "AUC ROC : 0.508\n",
      "confusion matrix / precision recall scores\n",
      "[[97 75]\n",
      " [40 33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.56      0.63       172\n",
      "           1       0.31      0.45      0.36        73\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.51      0.51      0.50       245\n",
      "weighted avg       0.59      0.53      0.55       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00213: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  1.000\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.567\n",
      "AUC ROC : 0.526\n",
      "confusion matrix / precision recall scores\n",
      "[[108  64]\n",
      " [ 42  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67       172\n",
      "           1       0.33      0.42      0.37        73\n",
      "\n",
      "    accuracy                           0.57       245\n",
      "   macro avg       0.52      0.53      0.52       245\n",
      "weighted avg       0.60      0.57      0.58       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00223: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.548\n",
      "The Test Accuracy   0.546\n",
      "AUC ROC : 0.450\n",
      "confusion matrix / precision recall scores\n",
      "[[115  52]\n",
      " [ 56  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       167\n",
      "           1       0.22      0.21      0.22        71\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.45      0.45      0.45       238\n",
      "weighted avg       0.54      0.55      0.54       238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start Predict  30d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00212: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.749\n",
      "The Validation Accuracy  0.585\n",
      "The Test Accuracy   0.796\n",
      "AUC ROC : 0.554\n",
      "confusion matrix / precision recall scores\n",
      "[[188   5]\n",
      " [ 45   7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88       193\n",
      "           1       0.58      0.13      0.22        52\n",
      "\n",
      "    accuracy                           0.80       245\n",
      "   macro avg       0.70      0.55      0.55       245\n",
      "weighted avg       0.76      0.80      0.74       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00230: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.926\n",
      "The Validation Accuracy  0.493\n",
      "The Test Accuracy   0.576\n",
      "AUC ROC : 0.443\n",
      "confusion matrix / precision recall scores\n",
      "[[130  63]\n",
      " [ 41  11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.67      0.71       193\n",
      "           1       0.15      0.21      0.17        52\n",
      "\n",
      "    accuracy                           0.58       245\n",
      "   macro avg       0.45      0.44      0.44       245\n",
      "weighted avg       0.63      0.58      0.60       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00226: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.996\n",
      "The Validation Accuracy  0.553\n",
      "The Test Accuracy   0.759\n",
      "AUC ROC : 0.538\n",
      "confusion matrix / precision recall scores\n",
      "[[178  15]\n",
      " [ 44   8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.86       193\n",
      "           1       0.35      0.15      0.21        52\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.57      0.54      0.54       245\n",
      "weighted avg       0.71      0.76      0.72       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00225: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.996\n",
      "The Validation Accuracy  0.608\n",
      "The Test Accuracy   0.514\n",
      "AUC ROC : 0.368\n",
      "confusion matrix / precision recall scores\n",
      "[[120  74]\n",
      " [ 45   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.62      0.67       194\n",
      "           1       0.07      0.12      0.09        51\n",
      "\n",
      "    accuracy                           0.51       245\n",
      "   macro avg       0.40      0.37      0.38       245\n",
      "weighted avg       0.59      0.51      0.55       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00234: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.991\n",
      "The Validation Accuracy  0.438\n",
      "The Test Accuracy   0.655\n",
      "AUC ROC : 0.620\n",
      "confusion matrix / precision recall scores\n",
      "[[128  60]\n",
      " [ 22  28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.68      0.76       188\n",
      "           1       0.32      0.56      0.41        50\n",
      "\n",
      "    accuracy                           0.66       238\n",
      "   macro avg       0.59      0.62      0.58       238\n",
      "weighted avg       0.74      0.66      0.68       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00216: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.815\n",
      "The Validation Accuracy  0.553\n",
      "The Test Accuracy   0.763\n",
      "AUC ROC : 0.499\n",
      "confusion matrix / precision recall scores\n",
      "[[185   8]\n",
      " [ 50   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.96      0.86       193\n",
      "           1       0.20      0.04      0.06        52\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.49      0.50      0.46       245\n",
      "weighted avg       0.66      0.76      0.69       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00231: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.977\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.641\n",
      "AUC ROC : 0.533\n",
      "confusion matrix / precision recall scores\n",
      "[[139  54]\n",
      " [ 34  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76       193\n",
      "           1       0.25      0.35      0.29        52\n",
      "\n",
      "    accuracy                           0.64       245\n",
      "   macro avg       0.53      0.53      0.52       245\n",
      "weighted avg       0.69      0.64      0.66       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00207: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  1.000\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.771\n",
      "AUC ROC : 0.560\n",
      "confusion matrix / precision recall scores\n",
      "[[179  14]\n",
      " [ 42  10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.86       193\n",
      "           1       0.42      0.19      0.26        52\n",
      "\n",
      "    accuracy                           0.77       245\n",
      "   macro avg       0.61      0.56      0.56       245\n",
      "weighted avg       0.73      0.77      0.74       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00248: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.576\n",
      "The Test Accuracy   0.727\n",
      "AUC ROC : 0.524\n",
      "confusion matrix / precision recall scores\n",
      "[[169  25]\n",
      " [ 42   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       194\n",
      "           1       0.26      0.18      0.21        51\n",
      "\n",
      "    accuracy                           0.73       245\n",
      "   macro avg       0.53      0.52      0.52       245\n",
      "weighted avg       0.69      0.73      0.70       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00230: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.999\n",
      "The Validation Accuracy  0.525\n",
      "The Test Accuracy   0.672\n",
      "AUC ROC : 0.492\n",
      "confusion matrix / precision recall scores\n",
      "[[151  37]\n",
      " [ 41   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79       188\n",
      "           1       0.20      0.18      0.19        50\n",
      "\n",
      "    accuracy                           0.67       238\n",
      "   macro avg       0.49      0.49      0.49       238\n",
      "weighted avg       0.66      0.67      0.67       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00267: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.995\n",
      "The Validation Accuracy  0.604\n",
      "The Test Accuracy   0.706\n",
      "AUC ROC : 0.448\n",
      "confusion matrix / precision recall scores\n",
      "[[173  20]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.71       245\n",
      "   macro avg       0.38      0.45      0.41       245\n",
      "weighted avg       0.61      0.71      0.65       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00296: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.630\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.792\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[194   0]\n",
      " [ 51   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       194\n",
      "           1       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.40      0.50      0.44       245\n",
      "weighted avg       0.63      0.79      0.70       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00320: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.484\n",
      "The Test Accuracy   0.685\n",
      "AUC ROC : 0.617\n",
      "confusion matrix / precision recall scores\n",
      "[[138  50]\n",
      " [ 25  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.79       188\n",
      "           1       0.33      0.50      0.40        50\n",
      "\n",
      "    accuracy                           0.68       238\n",
      "   macro avg       0.59      0.62      0.59       238\n",
      "weighted avg       0.74      0.68      0.71       238\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "hidden_layer_size = [32 , 64, 128]\n",
    "sequence_day = [1,3,7,15,30]\n",
    "predict_day = [label_abs_1d, label_abs_7d, label_abs_30d]\n",
    "predict_day_str = [\"1d\", \"7d\", \"30d\"]\n",
    "\n",
    "for day in range(3):\n",
    "    print(\"Start Predict \", predict_day_str[day])\n",
    "    print()\n",
    "    for layer_size in hidden_layer_size:\n",
    "        print(\"Layer: \", layer_size)\n",
    "        for past_day in sequence_day:\n",
    "            print(\"Sequence number of day: \", past_day)\n",
    "            model, X_train, y_train, X_valid, y_valid, X_test, y_test = model_train_loss(buildTrendModel_3stacks, past_day, \n",
    "            stock_without_absolute, predict_day[day], layer_size, batch_size)\n",
    "\n",
    "            predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "            predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "            predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "            print()\n",
    "            result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "                   \"./LSTM_RESULT/best_layer_seqnum_without_abs/LSTM_\"+predict_day_str[day]+\"_sequence_3stack_hid\"+str(layer_size)+\"_seq\"+str(past_day)+\"_loss\", model.predict, clf_name=\"LSTM\")\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 7\n",
    "sequence_day = 7 #???\n",
    "hidden_layer_size = [32 , 64, 128]\n",
    "predict_day = [label_abs_1d, label_abs_7d, label_abs_30d]\n",
    "predict_day_str = [\"1d\", \"7d\", \"30d\"]\n",
    "\n",
    "for day in range(3):\n",
    "    print(\"Start Predict \", predict_day_str[day])\n",
    "    for layer_size in hidden_layer_size:\n",
    "        print(\"Layer: \", layer_size)\n",
    "        batch_size = 7\n",
    "        model, X_train, y_train, X_valid, y_valid, X_test, y_test = model_train_loss(buildTrendModel_3stacks, sequence_day, \n",
    "        stock_without_absolute, predict_day[day], layer_size, batch_size)\n",
    "\n",
    "        predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "        predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "        predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "        print()\n",
    "        result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "               \"./LSTM_RESULT/best_layer_with_abs/LSTM_\"+predict_day_str[day]+\"_sequence_3stack_hid\"+str(layer_size)+\"_loss\", model.predict, clf_name=\"LSTM\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = [32 , 64, 128]\n",
    "past_days = [1,2,4,7,14,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(1, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(1, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 1, 50)             28600     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 1, 75)             37800     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 1, 75)             0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1992 samples, validate on 222 samples\n",
      "Epoch 1/1000\n",
      "1992/1992 [==============================] - 1s 672us/step - loss: 0.6927 - accuracy: 0.5146 - val_loss: 0.6900 - val_accuracy: 0.5586\n",
      "Epoch 2/1000\n",
      "1992/1992 [==============================] - 0s 87us/step - loss: 0.6921 - accuracy: 0.5231 - val_loss: 0.6890 - val_accuracy: 0.5586\n",
      "Epoch 3/1000\n",
      "1992/1992 [==============================] - 0s 76us/step - loss: 0.6922 - accuracy: 0.5231 - val_loss: 0.6893 - val_accuracy: 0.5586\n",
      "Epoch 4/1000\n",
      "1992/1992 [==============================] - 0s 84us/step - loss: 0.6922 - accuracy: 0.5231 - val_loss: 0.6882 - val_accuracy: 0.5586\n",
      "Epoch 5/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6919 - accuracy: 0.5231 - val_loss: 0.6877 - val_accuracy: 0.5586\n",
      "Epoch 6/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6919 - accuracy: 0.5226 - val_loss: 0.6881 - val_accuracy: 0.5586\n",
      "Epoch 7/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6920 - accuracy: 0.5236 - val_loss: 0.6867 - val_accuracy: 0.5586\n",
      "Epoch 8/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6912 - accuracy: 0.5246 - val_loss: 0.6888 - val_accuracy: 0.5541\n",
      "Epoch 9/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6911 - accuracy: 0.5326 - val_loss: 0.6860 - val_accuracy: 0.5541\n",
      "Epoch 10/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6912 - accuracy: 0.5377 - val_loss: 0.6904 - val_accuracy: 0.5360\n",
      "Epoch 11/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6912 - accuracy: 0.5266 - val_loss: 0.6870 - val_accuracy: 0.5631\n",
      "Epoch 12/1000\n",
      "1992/1992 [==============================] - 0s 76us/step - loss: 0.6911 - accuracy: 0.5261 - val_loss: 0.6882 - val_accuracy: 0.5541\n",
      "Epoch 13/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6909 - accuracy: 0.5291 - val_loss: 0.6885 - val_accuracy: 0.5631\n",
      "Epoch 14/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6907 - accuracy: 0.5296 - val_loss: 0.6852 - val_accuracy: 0.5586\n",
      "Epoch 15/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6908 - accuracy: 0.5281 - val_loss: 0.6884 - val_accuracy: 0.5766\n",
      "Epoch 16/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6901 - accuracy: 0.5236 - val_loss: 0.6898 - val_accuracy: 0.5721\n",
      "Epoch 17/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6908 - accuracy: 0.5311 - val_loss: 0.6859 - val_accuracy: 0.5586\n",
      "Epoch 18/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6907 - accuracy: 0.5256 - val_loss: 0.6873 - val_accuracy: 0.5766\n",
      "Epoch 19/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6903 - accuracy: 0.5361 - val_loss: 0.6933 - val_accuracy: 0.5270\n",
      "Epoch 20/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6911 - accuracy: 0.5241 - val_loss: 0.6867 - val_accuracy: 0.5766\n",
      "Epoch 21/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6901 - accuracy: 0.5301 - val_loss: 0.6878 - val_accuracy: 0.5676\n",
      "Epoch 22/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6886 - accuracy: 0.5377 - val_loss: 0.6856 - val_accuracy: 0.5811\n",
      "Epoch 23/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6894 - accuracy: 0.5437 - val_loss: 0.6867 - val_accuracy: 0.5721\n",
      "Epoch 24/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6895 - accuracy: 0.5341 - val_loss: 0.6865 - val_accuracy: 0.5721\n",
      "Epoch 25/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6889 - accuracy: 0.5402 - val_loss: 0.6867 - val_accuracy: 0.5721\n",
      "Epoch 26/1000\n",
      "1992/1992 [==============================] - 0s 70us/step - loss: 0.6896 - accuracy: 0.5472 - val_loss: 0.6925 - val_accuracy: 0.5315\n",
      "Epoch 27/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6868 - accuracy: 0.5412 - val_loss: 0.6892 - val_accuracy: 0.5586\n",
      "Epoch 28/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6889 - accuracy: 0.5296 - val_loss: 0.6892 - val_accuracy: 0.5721\n",
      "Epoch 29/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6890 - accuracy: 0.5397 - val_loss: 0.6893 - val_accuracy: 0.5631\n",
      "Epoch 30/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6865 - accuracy: 0.5442 - val_loss: 0.6978 - val_accuracy: 0.5135\n",
      "Epoch 31/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6872 - accuracy: 0.5387 - val_loss: 0.6912 - val_accuracy: 0.5676\n",
      "Epoch 32/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6874 - accuracy: 0.5417 - val_loss: 0.6940 - val_accuracy: 0.5541\n",
      "Epoch 33/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6860 - accuracy: 0.5658 - val_loss: 0.6967 - val_accuracy: 0.5586\n",
      "Epoch 34/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6908 - accuracy: 0.5366 - val_loss: 0.6974 - val_accuracy: 0.5000\n",
      "Epoch 35/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6871 - accuracy: 0.5457 - val_loss: 0.6960 - val_accuracy: 0.5090\n",
      "Epoch 36/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6854 - accuracy: 0.5582 - val_loss: 0.6964 - val_accuracy: 0.4910\n",
      "Epoch 37/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6848 - accuracy: 0.5577 - val_loss: 0.7026 - val_accuracy: 0.4910\n",
      "Epoch 38/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6893 - accuracy: 0.5397 - val_loss: 0.7232 - val_accuracy: 0.4234\n",
      "Epoch 39/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6890 - accuracy: 0.5261 - val_loss: 0.6910 - val_accuracy: 0.5856\n",
      "Epoch 40/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6863 - accuracy: 0.5437 - val_loss: 0.6940 - val_accuracy: 0.5270\n",
      "Epoch 41/1000\n",
      "1992/1992 [==============================] - 0s 71us/step - loss: 0.6850 - accuracy: 0.5472 - val_loss: 0.6963 - val_accuracy: 0.5631\n",
      "Epoch 42/1000\n",
      "1992/1992 [==============================] - 0s 71us/step - loss: 0.6858 - accuracy: 0.5356 - val_loss: 0.6986 - val_accuracy: 0.5811\n",
      "Epoch 43/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6846 - accuracy: 0.5577 - val_loss: 0.7040 - val_accuracy: 0.4865\n",
      "Epoch 44/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6849 - accuracy: 0.5377 - val_loss: 0.7025 - val_accuracy: 0.5541\n",
      "Epoch 45/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6874 - accuracy: 0.5502 - val_loss: 0.6983 - val_accuracy: 0.5856\n",
      "Epoch 46/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6859 - accuracy: 0.5572 - val_loss: 0.6999 - val_accuracy: 0.5856\n",
      "Epoch 47/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6896 - accuracy: 0.5341 - val_loss: 0.6942 - val_accuracy: 0.5901\n",
      "Epoch 48/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6833 - accuracy: 0.5502 - val_loss: 0.7024 - val_accuracy: 0.4820\n",
      "Epoch 49/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6824 - accuracy: 0.5693 - val_loss: 0.7131 - val_accuracy: 0.4685\n",
      "Epoch 50/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6842 - accuracy: 0.5336 - val_loss: 0.7026 - val_accuracy: 0.4910\n",
      "Epoch 51/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6844 - accuracy: 0.5487 - val_loss: 0.7070 - val_accuracy: 0.4640\n",
      "Epoch 52/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6816 - accuracy: 0.5638 - val_loss: 0.7017 - val_accuracy: 0.5315\n",
      "Epoch 53/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6807 - accuracy: 0.5532 - val_loss: 0.7121 - val_accuracy: 0.4955\n",
      "Epoch 54/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6795 - accuracy: 0.5643 - val_loss: 0.7163 - val_accuracy: 0.4730\n",
      "Epoch 55/1000\n",
      "1992/1992 [==============================] - 0s 72us/step - loss: 0.6789 - accuracy: 0.5638 - val_loss: 0.7157 - val_accuracy: 0.4865\n",
      "Epoch 56/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6779 - accuracy: 0.5658 - val_loss: 0.7176 - val_accuracy: 0.4775\n",
      "Epoch 57/1000\n",
      "1992/1992 [==============================] - 0s 79us/step - loss: 0.6837 - accuracy: 0.5356 - val_loss: 0.7125 - val_accuracy: 0.5045\n",
      "Epoch 58/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6811 - accuracy: 0.5643 - val_loss: 0.7087 - val_accuracy: 0.4955\n",
      "Epoch 59/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6785 - accuracy: 0.5628 - val_loss: 0.7109 - val_accuracy: 0.5135\n",
      "Epoch 60/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6794 - accuracy: 0.5482 - val_loss: 0.7119 - val_accuracy: 0.5045\n",
      "Epoch 61/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6782 - accuracy: 0.5633 - val_loss: 0.7140 - val_accuracy: 0.5586\n",
      "Epoch 62/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6796 - accuracy: 0.5693 - val_loss: 0.7178 - val_accuracy: 0.4640\n",
      "Epoch 63/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6782 - accuracy: 0.5653 - val_loss: 0.7081 - val_accuracy: 0.5270\n",
      "Epoch 64/1000\n",
      "1992/1992 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.55 - 0s 66us/step - loss: 0.6758 - accuracy: 0.5572 - val_loss: 0.7181 - val_accuracy: 0.5090\n",
      "Epoch 65/1000\n",
      "1992/1992 [==============================] - 0s 76us/step - loss: 0.6805 - accuracy: 0.5683 - val_loss: 0.7083 - val_accuracy: 0.5225\n",
      "Epoch 66/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6775 - accuracy: 0.5678 - val_loss: 0.7230 - val_accuracy: 0.4505\n",
      "Epoch 67/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6778 - accuracy: 0.5517 - val_loss: 0.7158 - val_accuracy: 0.4775\n",
      "Epoch 68/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6811 - accuracy: 0.5587 - val_loss: 0.7080 - val_accuracy: 0.5045\n",
      "Epoch 69/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6768 - accuracy: 0.5622 - val_loss: 0.7215 - val_accuracy: 0.4459\n",
      "Epoch 70/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6764 - accuracy: 0.5617 - val_loss: 0.7196 - val_accuracy: 0.5090\n",
      "Epoch 71/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6766 - accuracy: 0.5633 - val_loss: 0.7122 - val_accuracy: 0.5135\n",
      "Epoch 72/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6761 - accuracy: 0.5552 - val_loss: 0.7191 - val_accuracy: 0.5270\n",
      "Epoch 73/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6764 - accuracy: 0.5688 - val_loss: 0.7139 - val_accuracy: 0.5225\n",
      "Epoch 74/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6740 - accuracy: 0.5602 - val_loss: 0.7216 - val_accuracy: 0.5405\n",
      "Epoch 75/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6754 - accuracy: 0.5693 - val_loss: 0.7096 - val_accuracy: 0.5045\n",
      "Epoch 76/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6747 - accuracy: 0.5668 - val_loss: 0.7186 - val_accuracy: 0.5360\n",
      "Epoch 77/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6702 - accuracy: 0.5798 - val_loss: 0.7370 - val_accuracy: 0.4640\n",
      "Epoch 78/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6742 - accuracy: 0.5673 - val_loss: 0.7131 - val_accuracy: 0.5360\n",
      "Epoch 79/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6750 - accuracy: 0.5587 - val_loss: 0.7141 - val_accuracy: 0.5225\n",
      "Epoch 80/1000\n",
      "1992/1992 [==============================] - 0s 74us/step - loss: 0.6740 - accuracy: 0.5668 - val_loss: 0.7125 - val_accuracy: 0.5315\n",
      "Epoch 81/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6707 - accuracy: 0.5653 - val_loss: 0.7374 - val_accuracy: 0.4505\n",
      "Epoch 82/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6734 - accuracy: 0.5688 - val_loss: 0.7255 - val_accuracy: 0.5405\n",
      "Epoch 83/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6763 - accuracy: 0.5567 - val_loss: 0.7110 - val_accuracy: 0.5270\n",
      "Epoch 84/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6661 - accuracy: 0.5808 - val_loss: 0.7432 - val_accuracy: 0.4640\n",
      "Epoch 85/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6698 - accuracy: 0.5582 - val_loss: 0.7202 - val_accuracy: 0.4640\n",
      "Epoch 86/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6717 - accuracy: 0.5733 - val_loss: 0.7174 - val_accuracy: 0.5450\n",
      "Epoch 87/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6700 - accuracy: 0.5773 - val_loss: 0.7176 - val_accuracy: 0.4955\n",
      "Epoch 88/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6689 - accuracy: 0.5763 - val_loss: 0.7167 - val_accuracy: 0.5225\n",
      "Epoch 89/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6654 - accuracy: 0.5783 - val_loss: 0.7213 - val_accuracy: 0.4910\n",
      "Epoch 90/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6640 - accuracy: 0.5753 - val_loss: 0.7435 - val_accuracy: 0.5495\n",
      "Epoch 91/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6682 - accuracy: 0.5713 - val_loss: 0.7318 - val_accuracy: 0.4369\n",
      "Epoch 92/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6653 - accuracy: 0.5663 - val_loss: 0.7276 - val_accuracy: 0.5180\n",
      "Epoch 93/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6667 - accuracy: 0.5818 - val_loss: 0.7204 - val_accuracy: 0.5360\n",
      "Epoch 94/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6632 - accuracy: 0.5838 - val_loss: 0.7387 - val_accuracy: 0.5180\n",
      "Epoch 95/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6719 - accuracy: 0.5547 - val_loss: 0.7116 - val_accuracy: 0.5315\n",
      "Epoch 96/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6632 - accuracy: 0.5873 - val_loss: 0.7278 - val_accuracy: 0.4865\n",
      "Epoch 97/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6651 - accuracy: 0.5798 - val_loss: 0.7198 - val_accuracy: 0.5225\n",
      "Epoch 98/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6632 - accuracy: 0.5838 - val_loss: 0.7297 - val_accuracy: 0.5180\n",
      "Epoch 99/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6614 - accuracy: 0.5803 - val_loss: 0.7279 - val_accuracy: 0.5450\n",
      "Epoch 100/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6618 - accuracy: 0.5949 - val_loss: 0.7454 - val_accuracy: 0.4414\n",
      "Epoch 101/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6656 - accuracy: 0.5753 - val_loss: 0.7161 - val_accuracy: 0.5495\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6610 - accuracy: 0.5863 - val_loss: 0.7390 - val_accuracy: 0.5270\n",
      "Epoch 103/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6605 - accuracy: 0.5813 - val_loss: 0.7369 - val_accuracy: 0.4595\n",
      "Epoch 104/1000\n",
      "1992/1992 [==============================] - 0s 69us/step - loss: 0.6604 - accuracy: 0.5848 - val_loss: 0.7326 - val_accuracy: 0.5360\n",
      "Epoch 105/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6601 - accuracy: 0.5949 - val_loss: 0.7259 - val_accuracy: 0.4955\n",
      "Epoch 106/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6611 - accuracy: 0.5863 - val_loss: 0.7187 - val_accuracy: 0.5315\n",
      "Epoch 107/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6657 - accuracy: 0.5713 - val_loss: 0.7192 - val_accuracy: 0.5180\n",
      "Epoch 108/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6565 - accuracy: 0.5959 - val_loss: 0.7592 - val_accuracy: 0.4730\n",
      "Epoch 109/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6577 - accuracy: 0.5914 - val_loss: 0.7507 - val_accuracy: 0.5270\n",
      "Epoch 110/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6536 - accuracy: 0.5964 - val_loss: 0.7417 - val_accuracy: 0.5180\n",
      "Epoch 111/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6545 - accuracy: 0.5984 - val_loss: 0.7448 - val_accuracy: 0.4865\n",
      "Epoch 112/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6555 - accuracy: 0.5863 - val_loss: 0.7509 - val_accuracy: 0.5360\n",
      "Epoch 113/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6575 - accuracy: 0.5868 - val_loss: 0.7547 - val_accuracy: 0.5045\n",
      "Epoch 114/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6642 - accuracy: 0.5798 - val_loss: 0.7130 - val_accuracy: 0.5225\n",
      "Epoch 115/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6573 - accuracy: 0.5959 - val_loss: 0.7538 - val_accuracy: 0.5045\n",
      "Epoch 116/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6512 - accuracy: 0.5934 - val_loss: 0.7384 - val_accuracy: 0.5405\n",
      "Epoch 117/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6613 - accuracy: 0.5848 - val_loss: 0.7428 - val_accuracy: 0.4820\n",
      "Epoch 118/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6620 - accuracy: 0.5818 - val_loss: 0.7342 - val_accuracy: 0.5225\n",
      "Epoch 119/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6545 - accuracy: 0.5939 - val_loss: 0.7390 - val_accuracy: 0.5225\n",
      "Epoch 120/1000\n",
      "1992/1992 [==============================] - 0s 69us/step - loss: 0.6556 - accuracy: 0.5979 - val_loss: 0.7444 - val_accuracy: 0.5045\n",
      "Epoch 121/1000\n",
      "1992/1992 [==============================] - 0s 69us/step - loss: 0.6503 - accuracy: 0.6019 - val_loss: 0.7497 - val_accuracy: 0.5225\n",
      "Epoch 122/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6552 - accuracy: 0.5964 - val_loss: 0.7327 - val_accuracy: 0.5541\n",
      "Epoch 123/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6564 - accuracy: 0.5879 - val_loss: 0.7505 - val_accuracy: 0.4955\n",
      "Epoch 124/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6565 - accuracy: 0.5989 - val_loss: 0.7390 - val_accuracy: 0.5045\n",
      "Epoch 125/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6529 - accuracy: 0.5924 - val_loss: 0.7382 - val_accuracy: 0.5090\n",
      "Epoch 126/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6472 - accuracy: 0.6145 - val_loss: 0.7779 - val_accuracy: 0.5180\n",
      "Epoch 127/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6533 - accuracy: 0.5984 - val_loss: 0.7587 - val_accuracy: 0.5225\n",
      "Epoch 128/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6474 - accuracy: 0.6094 - val_loss: 0.7508 - val_accuracy: 0.5315\n",
      "Epoch 129/1000\n",
      "1992/1992 [==============================] - 0s 72us/step - loss: 0.6517 - accuracy: 0.5934 - val_loss: 0.7539 - val_accuracy: 0.5000\n",
      "Epoch 130/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6508 - accuracy: 0.5929 - val_loss: 0.7695 - val_accuracy: 0.5045\n",
      "Epoch 131/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6501 - accuracy: 0.6029 - val_loss: 0.7406 - val_accuracy: 0.5045\n",
      "Epoch 132/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6498 - accuracy: 0.5994 - val_loss: 0.7387 - val_accuracy: 0.5135\n",
      "Epoch 133/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6553 - accuracy: 0.5873 - val_loss: 0.7631 - val_accuracy: 0.4865\n",
      "Epoch 134/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6546 - accuracy: 0.5868 - val_loss: 0.7362 - val_accuracy: 0.5000\n",
      "Epoch 135/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6653 - accuracy: 0.5758 - val_loss: 0.7165 - val_accuracy: 0.4955\n",
      "Epoch 136/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6538 - accuracy: 0.6004 - val_loss: 0.7423 - val_accuracy: 0.4910\n",
      "Epoch 137/1000\n",
      "1992/1992 [==============================] - 0s 71us/step - loss: 0.6474 - accuracy: 0.6019 - val_loss: 0.7507 - val_accuracy: 0.5000\n",
      "Epoch 138/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6474 - accuracy: 0.6084 - val_loss: 0.7550 - val_accuracy: 0.4955\n",
      "Epoch 139/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6443 - accuracy: 0.6029 - val_loss: 0.7653 - val_accuracy: 0.5360\n",
      "Epoch 140/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6454 - accuracy: 0.6004 - val_loss: 0.7384 - val_accuracy: 0.5090\n",
      "Epoch 141/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6501 - accuracy: 0.6019 - val_loss: 0.7672 - val_accuracy: 0.4505\n",
      "Epoch 142/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6435 - accuracy: 0.5999 - val_loss: 0.7440 - val_accuracy: 0.5090\n",
      "Epoch 143/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6486 - accuracy: 0.5929 - val_loss: 0.7686 - val_accuracy: 0.5090\n",
      "Epoch 144/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6421 - accuracy: 0.6135 - val_loss: 0.7930 - val_accuracy: 0.4865\n",
      "Epoch 145/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6478 - accuracy: 0.5924 - val_loss: 0.7442 - val_accuracy: 0.5090\n",
      "Epoch 146/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6561 - accuracy: 0.5884 - val_loss: 0.7432 - val_accuracy: 0.5045\n",
      "Epoch 147/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6426 - accuracy: 0.6165 - val_loss: 0.7678 - val_accuracy: 0.5180\n",
      "Epoch 148/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6483 - accuracy: 0.6054 - val_loss: 0.7547 - val_accuracy: 0.5135\n",
      "Epoch 149/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6426 - accuracy: 0.6064 - val_loss: 0.7625 - val_accuracy: 0.5045\n",
      "Epoch 150/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6435 - accuracy: 0.5969 - val_loss: 0.7482 - val_accuracy: 0.5135\n",
      "Epoch 151/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6415 - accuracy: 0.6124 - val_loss: 0.7544 - val_accuracy: 0.5180\n",
      "Epoch 152/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6424 - accuracy: 0.6104 - val_loss: 0.7831 - val_accuracy: 0.4955\n",
      "Epoch 153/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6446 - accuracy: 0.6059 - val_loss: 0.7689 - val_accuracy: 0.4685\n",
      "Epoch 154/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6464 - accuracy: 0.6064 - val_loss: 0.7492 - val_accuracy: 0.5045\n",
      "Epoch 155/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6425 - accuracy: 0.6054 - val_loss: 0.7478 - val_accuracy: 0.5180\n",
      "Epoch 156/1000\n",
      "1992/1992 [==============================] - 0s 72us/step - loss: 0.6416 - accuracy: 0.6044 - val_loss: 0.7819 - val_accuracy: 0.5225\n",
      "Epoch 157/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6387 - accuracy: 0.6170 - val_loss: 0.7849 - val_accuracy: 0.5270\n",
      "Epoch 158/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6424 - accuracy: 0.6049 - val_loss: 0.7858 - val_accuracy: 0.5000\n",
      "Epoch 159/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6400 - accuracy: 0.6044 - val_loss: 0.7672 - val_accuracy: 0.5270\n",
      "Epoch 160/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6401 - accuracy: 0.6049 - val_loss: 0.8022 - val_accuracy: 0.4910\n",
      "Epoch 161/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6412 - accuracy: 0.6160 - val_loss: 0.7493 - val_accuracy: 0.5180\n",
      "Epoch 162/1000\n",
      "1992/1992 [==============================] - 0s 76us/step - loss: 0.6389 - accuracy: 0.6150 - val_loss: 0.8286 - val_accuracy: 0.4820\n",
      "Epoch 163/1000\n",
      "1992/1992 [==============================] - 0s 82us/step - loss: 0.6408 - accuracy: 0.5989 - val_loss: 0.7817 - val_accuracy: 0.5450\n",
      "Epoch 164/1000\n",
      "1992/1992 [==============================] - 0s 79us/step - loss: 0.6349 - accuracy: 0.6180 - val_loss: 0.7751 - val_accuracy: 0.5000\n",
      "Epoch 165/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6452 - accuracy: 0.6034 - val_loss: 0.7482 - val_accuracy: 0.4640\n",
      "Epoch 166/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6404 - accuracy: 0.6029 - val_loss: 0.7665 - val_accuracy: 0.5405\n",
      "Epoch 167/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6431 - accuracy: 0.5974 - val_loss: 0.7735 - val_accuracy: 0.5225\n",
      "Epoch 168/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6345 - accuracy: 0.6145 - val_loss: 0.7886 - val_accuracy: 0.4910\n",
      "Epoch 169/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6355 - accuracy: 0.6135 - val_loss: 0.7796 - val_accuracy: 0.5225\n",
      "Epoch 170/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6349 - accuracy: 0.6104 - val_loss: 0.7967 - val_accuracy: 0.5000\n",
      "Epoch 171/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6340 - accuracy: 0.6044 - val_loss: 0.7949 - val_accuracy: 0.5270\n",
      "Epoch 172/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6323 - accuracy: 0.6155 - val_loss: 0.8035 - val_accuracy: 0.5000\n",
      "Epoch 173/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6347 - accuracy: 0.6064 - val_loss: 0.8007 - val_accuracy: 0.5135\n",
      "Epoch 174/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6373 - accuracy: 0.6064 - val_loss: 0.7750 - val_accuracy: 0.5315\n",
      "Epoch 175/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6400 - accuracy: 0.6014 - val_loss: 0.8038 - val_accuracy: 0.4865\n",
      "Epoch 176/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6370 - accuracy: 0.5994 - val_loss: 0.7609 - val_accuracy: 0.5360\n",
      "Epoch 177/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6338 - accuracy: 0.6215 - val_loss: 0.7797 - val_accuracy: 0.5225\n",
      "Epoch 178/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6328 - accuracy: 0.6175 - val_loss: 0.7945 - val_accuracy: 0.4775\n",
      "Epoch 179/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6386 - accuracy: 0.5889 - val_loss: 0.7480 - val_accuracy: 0.5270\n",
      "Epoch 180/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6404 - accuracy: 0.6064 - val_loss: 0.7837 - val_accuracy: 0.4865\n",
      "Epoch 181/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6351 - accuracy: 0.6059 - val_loss: 0.7851 - val_accuracy: 0.4955\n",
      "Epoch 182/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6294 - accuracy: 0.6094 - val_loss: 0.7707 - val_accuracy: 0.5270\n",
      "Epoch 183/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6325 - accuracy: 0.6104 - val_loss: 0.7797 - val_accuracy: 0.5045\n",
      "Epoch 184/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6279 - accuracy: 0.6109 - val_loss: 0.7672 - val_accuracy: 0.4550\n",
      "Epoch 185/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6355 - accuracy: 0.6155 - val_loss: 0.7885 - val_accuracy: 0.5045\n",
      "Epoch 186/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6295 - accuracy: 0.6245 - val_loss: 0.7766 - val_accuracy: 0.5045\n",
      "Epoch 187/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6270 - accuracy: 0.6295 - val_loss: 0.8240 - val_accuracy: 0.5000\n",
      "Epoch 188/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6317 - accuracy: 0.6140 - val_loss: 0.8110 - val_accuracy: 0.4820\n",
      "Epoch 189/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6270 - accuracy: 0.6175 - val_loss: 0.7992 - val_accuracy: 0.4955\n",
      "Epoch 190/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6298 - accuracy: 0.6195 - val_loss: 0.8207 - val_accuracy: 0.4865\n",
      "Epoch 191/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6243 - accuracy: 0.6270 - val_loss: 0.8311 - val_accuracy: 0.4685\n",
      "Epoch 192/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6307 - accuracy: 0.6260 - val_loss: 0.8166 - val_accuracy: 0.5090\n",
      "Epoch 193/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6291 - accuracy: 0.6205 - val_loss: 0.8536 - val_accuracy: 0.4550\n",
      "Epoch 194/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6380 - accuracy: 0.6185 - val_loss: 0.7622 - val_accuracy: 0.5135\n",
      "Epoch 195/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6327 - accuracy: 0.6215 - val_loss: 0.8067 - val_accuracy: 0.5135\n",
      "Epoch 196/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6261 - accuracy: 0.6295 - val_loss: 0.7621 - val_accuracy: 0.4955\n",
      "Epoch 197/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6217 - accuracy: 0.6220 - val_loss: 0.8431 - val_accuracy: 0.4775\n",
      "Epoch 198/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6290 - accuracy: 0.6160 - val_loss: 0.7951 - val_accuracy: 0.4955\n",
      "Epoch 199/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6277 - accuracy: 0.6089 - val_loss: 0.8024 - val_accuracy: 0.4910\n",
      "Epoch 200/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6329 - accuracy: 0.6150 - val_loss: 0.7714 - val_accuracy: 0.4775\n",
      "Epoch 201/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6224 - accuracy: 0.6320 - val_loss: 0.8259 - val_accuracy: 0.5045\n",
      "Epoch 202/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6274 - accuracy: 0.6220 - val_loss: 0.7602 - val_accuracy: 0.4595\n",
      "Epoch 203/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6240 - accuracy: 0.6245 - val_loss: 0.8787 - val_accuracy: 0.5000\n",
      "Epoch 204/1000\n",
      "1992/1992 [==============================] - 0s 97us/step - loss: 0.6271 - accuracy: 0.6064 - val_loss: 0.8038 - val_accuracy: 0.5180\n",
      "Epoch 205/1000\n",
      "1992/1992 [==============================] - 0s 88us/step - loss: 0.6254 - accuracy: 0.6225 - val_loss: 0.8270 - val_accuracy: 0.4550\n",
      "Epoch 206/1000\n",
      "1992/1992 [==============================] - 0s 83us/step - loss: 0.6280 - accuracy: 0.6135 - val_loss: 0.7928 - val_accuracy: 0.4955\n",
      "Epoch 207/1000\n",
      "1992/1992 [==============================] - 0s 85us/step - loss: 0.6195 - accuracy: 0.6205 - val_loss: 0.7941 - val_accuracy: 0.4730\n",
      "Epoch 208/1000\n",
      "1992/1992 [==============================] - 0s 75us/step - loss: 0.6163 - accuracy: 0.6330 - val_loss: 0.8188 - val_accuracy: 0.4775\n",
      "Epoch 209/1000\n",
      "1992/1992 [==============================] - 0s 75us/step - loss: 0.6217 - accuracy: 0.6099 - val_loss: 0.8215 - val_accuracy: 0.5045\n",
      "Epoch 210/1000\n",
      "1992/1992 [==============================] - 0s 77us/step - loss: 0.6323 - accuracy: 0.6094 - val_loss: 0.7974 - val_accuracy: 0.5045\n",
      "Epoch 211/1000\n",
      "1992/1992 [==============================] - 0s 94us/step - loss: 0.6188 - accuracy: 0.6175 - val_loss: 0.8645 - val_accuracy: 0.4865\n",
      "Epoch 212/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992/1992 [==============================] - 0s 69us/step - loss: 0.6227 - accuracy: 0.6205 - val_loss: 0.8381 - val_accuracy: 0.5360\n",
      "Epoch 213/1000\n",
      "1992/1992 [==============================] - 0s 70us/step - loss: 0.6167 - accuracy: 0.6235 - val_loss: 0.8567 - val_accuracy: 0.4640\n",
      "Epoch 214/1000\n",
      "1992/1992 [==============================] - 0s 79us/step - loss: 0.6206 - accuracy: 0.6265 - val_loss: 0.8102 - val_accuracy: 0.5000\n",
      "Epoch 00214: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a424eceb8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAST_DAYS = 1\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "246/246 [==============================] - 0s 49us/step\n",
      "test loss, test acc: [0.881428475787, 0.5325203537940979]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (246, 1)\n",
      "rmse: 0.5213076995516022\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Without Absolute Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(7, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(7, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 7, 50)             28600     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 7, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 7, 75)             37800     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 7, 75)             0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1987 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1987/1987 [==============================] - 2s 1ms/step - loss: 0.6939 - accuracy: 0.5048 - val_loss: 0.6880 - val_accuracy: 0.5747\n",
      "Epoch 2/1000\n",
      "1987/1987 [==============================] - 1s 271us/step - loss: 0.6935 - accuracy: 0.5118 - val_loss: 0.6922 - val_accuracy: 0.5837\n",
      "Epoch 3/1000\n",
      "1987/1987 [==============================] - 1s 308us/step - loss: 0.6933 - accuracy: 0.5239 - val_loss: 0.6900 - val_accuracy: 0.5747\n",
      "Epoch 4/1000\n",
      "1987/1987 [==============================] - 1s 254us/step - loss: 0.6929 - accuracy: 0.5133 - val_loss: 0.6856 - val_accuracy: 0.5747\n",
      "Epoch 5/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6922 - accuracy: 0.5219 - val_loss: 0.6874 - val_accuracy: 0.5747\n",
      "Epoch 6/1000\n",
      "1987/1987 [==============================] - 0s 249us/step - loss: 0.6933 - accuracy: 0.5038 - val_loss: 0.6869 - val_accuracy: 0.5747\n",
      "Epoch 7/1000\n",
      "1987/1987 [==============================] - 1s 262us/step - loss: 0.6936 - accuracy: 0.5229 - val_loss: 0.6902 - val_accuracy: 0.5747\n",
      "Epoch 8/1000\n",
      "1987/1987 [==============================] - 0s 249us/step - loss: 0.6922 - accuracy: 0.5219 - val_loss: 0.6854 - val_accuracy: 0.5747\n",
      "Epoch 9/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6928 - accuracy: 0.5219 - val_loss: 0.6875 - val_accuracy: 0.5747\n",
      "Epoch 10/1000\n",
      "1987/1987 [==============================] - 0s 234us/step - loss: 0.6922 - accuracy: 0.5219 - val_loss: 0.6868 - val_accuracy: 0.5747\n",
      "Epoch 11/1000\n",
      "1987/1987 [==============================] - 0s 246us/step - loss: 0.6922 - accuracy: 0.5219 - val_loss: 0.6859 - val_accuracy: 0.5747\n",
      "Epoch 12/1000\n",
      "1987/1987 [==============================] - 0s 245us/step - loss: 0.6929 - accuracy: 0.5184 - val_loss: 0.6875 - val_accuracy: 0.5747\n",
      "Epoch 13/1000\n",
      "1987/1987 [==============================] - 0s 243us/step - loss: 0.6924 - accuracy: 0.5219 - val_loss: 0.6873 - val_accuracy: 0.5747\n",
      "Epoch 14/1000\n",
      "1987/1987 [==============================] - 1s 256us/step - loss: 0.6919 - accuracy: 0.5239 - val_loss: 0.6890 - val_accuracy: 0.5792\n",
      "Epoch 15/1000\n",
      "1987/1987 [==============================] - 1s 262us/step - loss: 0.6913 - accuracy: 0.5335 - val_loss: 0.6858 - val_accuracy: 0.5747\n",
      "Epoch 16/1000\n",
      "1987/1987 [==============================] - 0s 235us/step - loss: 0.6917 - accuracy: 0.5234 - val_loss: 0.6874 - val_accuracy: 0.5656\n",
      "Epoch 17/1000\n",
      "1987/1987 [==============================] - 0s 241us/step - loss: 0.6924 - accuracy: 0.5204 - val_loss: 0.6859 - val_accuracy: 0.5747\n",
      "Epoch 18/1000\n",
      "1987/1987 [==============================] - 0s 230us/step - loss: 0.6926 - accuracy: 0.5224 - val_loss: 0.6904 - val_accuracy: 0.5747\n",
      "Epoch 19/1000\n",
      "1987/1987 [==============================] - 0s 243us/step - loss: 0.6920 - accuracy: 0.5234 - val_loss: 0.6884 - val_accuracy: 0.5792\n",
      "Epoch 20/1000\n",
      "1987/1987 [==============================] - 0s 244us/step - loss: 0.6916 - accuracy: 0.5264 - val_loss: 0.6858 - val_accuracy: 0.5747\n",
      "Epoch 21/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6916 - accuracy: 0.5345 - val_loss: 0.6866 - val_accuracy: 0.5701\n",
      "Epoch 22/1000\n",
      "1987/1987 [==============================] - 0s 249us/step - loss: 0.6915 - accuracy: 0.5330 - val_loss: 0.6927 - val_accuracy: 0.5294\n",
      "Epoch 23/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6919 - accuracy: 0.5204 - val_loss: 0.6874 - val_accuracy: 0.5656\n",
      "Epoch 24/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6911 - accuracy: 0.5350 - val_loss: 0.6888 - val_accuracy: 0.5611\n",
      "Epoch 25/1000\n",
      "1987/1987 [==============================] - 0s 245us/step - loss: 0.6918 - accuracy: 0.5315 - val_loss: 0.6880 - val_accuracy: 0.5656\n",
      "Epoch 26/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6915 - accuracy: 0.5249 - val_loss: 0.6873 - val_accuracy: 0.5747\n",
      "Epoch 27/1000\n",
      "1987/1987 [==============================] - 0s 247us/step - loss: 0.6912 - accuracy: 0.5299 - val_loss: 0.6894 - val_accuracy: 0.5701\n",
      "Epoch 28/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6860 - val_accuracy: 0.5747\n",
      "Epoch 29/1000\n",
      "1987/1987 [==============================] - 0s 247us/step - loss: 0.6909 - accuracy: 0.5375 - val_loss: 0.6864 - val_accuracy: 0.5656\n",
      "Epoch 30/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6917 - accuracy: 0.5335 - val_loss: 0.6860 - val_accuracy: 0.5656\n",
      "Epoch 31/1000\n",
      "1987/1987 [==============================] - 1s 256us/step - loss: 0.6906 - accuracy: 0.5390 - val_loss: 0.6894 - val_accuracy: 0.5656\n",
      "Epoch 32/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.6909 - accuracy: 0.5405 - val_loss: 0.6860 - val_accuracy: 0.5701\n",
      "Epoch 33/1000\n",
      "1987/1987 [==============================] - 1s 257us/step - loss: 0.6907 - accuracy: 0.5310 - val_loss: 0.6894 - val_accuracy: 0.5747\n",
      "Epoch 34/1000\n",
      "1987/1987 [==============================] - 1s 252us/step - loss: 0.6905 - accuracy: 0.5284 - val_loss: 0.6885 - val_accuracy: 0.5747\n",
      "Epoch 35/1000\n",
      "1987/1987 [==============================] - 1s 254us/step - loss: 0.6914 - accuracy: 0.5310 - val_loss: 0.6894 - val_accuracy: 0.5747\n",
      "Epoch 36/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6900 - accuracy: 0.5410 - val_loss: 0.6893 - val_accuracy: 0.5792\n",
      "Epoch 37/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6899 - accuracy: 0.5380 - val_loss: 0.6846 - val_accuracy: 0.5747\n",
      "Epoch 38/1000\n",
      "1987/1987 [==============================] - 1s 265us/step - loss: 0.6909 - accuracy: 0.5289 - val_loss: 0.6891 - val_accuracy: 0.5611\n",
      "Epoch 39/1000\n",
      "1987/1987 [==============================] - 1s 285us/step - loss: 0.6897 - accuracy: 0.5360 - val_loss: 0.6879 - val_accuracy: 0.5566\n",
      "Epoch 40/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.6900 - accuracy: 0.5370 - val_loss: 0.6860 - val_accuracy: 0.5747\n",
      "Epoch 41/1000\n",
      "1987/1987 [==============================] - 1s 278us/step - loss: 0.6910 - accuracy: 0.5320 - val_loss: 0.6890 - val_accuracy: 0.5747\n",
      "Epoch 42/1000\n",
      "1987/1987 [==============================] - 1s 273us/step - loss: 0.6898 - accuracy: 0.5395 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 43/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6895 - accuracy: 0.5460 - val_loss: 0.6875 - val_accuracy: 0.5747\n",
      "Epoch 44/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6890 - accuracy: 0.5325 - val_loss: 0.6888 - val_accuracy: 0.5701\n",
      "Epoch 45/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6883 - accuracy: 0.5415 - val_loss: 0.6930 - val_accuracy: 0.5566\n",
      "Epoch 46/1000\n",
      "1987/1987 [==============================] - 0s 245us/step - loss: 0.6891 - accuracy: 0.5355 - val_loss: 0.6918 - val_accuracy: 0.5566\n",
      "Epoch 47/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.6894 - accuracy: 0.5390 - val_loss: 0.6925 - val_accuracy: 0.5656\n",
      "Epoch 48/1000\n",
      "1987/1987 [==============================] - 0s 239us/step - loss: 0.6883 - accuracy: 0.5430 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 49/1000\n",
      "1987/1987 [==============================] - 0s 243us/step - loss: 0.6892 - accuracy: 0.5425 - val_loss: 0.6897 - val_accuracy: 0.5747\n",
      "Epoch 50/1000\n",
      "1987/1987 [==============================] - 0s 248us/step - loss: 0.6868 - accuracy: 0.5370 - val_loss: 0.6940 - val_accuracy: 0.5249\n",
      "Epoch 51/1000\n",
      "1987/1987 [==============================] - 0s 233us/step - loss: 0.6870 - accuracy: 0.5365 - val_loss: 0.6906 - val_accuracy: 0.5747\n",
      "Epoch 52/1000\n",
      "1987/1987 [==============================] - 0s 238us/step - loss: 0.6873 - accuracy: 0.5415 - val_loss: 0.6901 - val_accuracy: 0.5747\n",
      "Epoch 53/1000\n",
      "1987/1987 [==============================] - 0s 243us/step - loss: 0.6903 - accuracy: 0.5320 - val_loss: 0.6862 - val_accuracy: 0.5747\n",
      "Epoch 54/1000\n",
      "1987/1987 [==============================] - 0s 236us/step - loss: 0.6891 - accuracy: 0.5330 - val_loss: 0.6938 - val_accuracy: 0.5792\n",
      "Epoch 55/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6884 - accuracy: 0.5450 - val_loss: 0.6904 - val_accuracy: 0.5747\n",
      "Epoch 56/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6885 - accuracy: 0.5310 - val_loss: 0.6906 - val_accuracy: 0.5747\n",
      "Epoch 57/1000\n",
      "1987/1987 [==============================] - 1s 313us/step - loss: 0.6865 - accuracy: 0.5471 - val_loss: 0.6964 - val_accuracy: 0.5385\n",
      "Epoch 58/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6907 - accuracy: 0.5360 - val_loss: 0.6871 - val_accuracy: 0.5701\n",
      "Epoch 59/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6864 - accuracy: 0.5420 - val_loss: 0.6922 - val_accuracy: 0.5475\n",
      "Epoch 60/1000\n",
      "1987/1987 [==============================] - 1s 254us/step - loss: 0.6879 - accuracy: 0.5350 - val_loss: 0.6853 - val_accuracy: 0.5837\n",
      "Epoch 61/1000\n",
      "1987/1987 [==============================] - 1s 288us/step - loss: 0.6891 - accuracy: 0.5325 - val_loss: 0.6891 - val_accuracy: 0.5701\n",
      "Epoch 62/1000\n",
      "1987/1987 [==============================] - 1s 278us/step - loss: 0.6870 - accuracy: 0.5460 - val_loss: 0.6931 - val_accuracy: 0.5475\n",
      "Epoch 63/1000\n",
      "1987/1987 [==============================] - 1s 255us/step - loss: 0.6864 - accuracy: 0.5420 - val_loss: 0.6890 - val_accuracy: 0.5611\n",
      "Epoch 64/1000\n",
      "1987/1987 [==============================] - 0s 249us/step - loss: 0.6866 - accuracy: 0.5395 - val_loss: 0.6917 - val_accuracy: 0.5747\n",
      "Epoch 65/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6858 - accuracy: 0.5471 - val_loss: 0.6913 - val_accuracy: 0.5611\n",
      "Epoch 66/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6884 - accuracy: 0.5330 - val_loss: 0.6907 - val_accuracy: 0.5747\n",
      "Epoch 67/1000\n",
      "1987/1987 [==============================] - 0s 244us/step - loss: 0.6849 - accuracy: 0.5481 - val_loss: 0.6930 - val_accuracy: 0.5701\n",
      "Epoch 68/1000\n",
      "1987/1987 [==============================] - 0s 238us/step - loss: 0.6892 - accuracy: 0.5345 - val_loss: 0.6908 - val_accuracy: 0.5701\n",
      "Epoch 69/1000\n",
      "1987/1987 [==============================] - 0s 247us/step - loss: 0.6861 - accuracy: 0.5496 - val_loss: 0.6915 - val_accuracy: 0.5747\n",
      "Epoch 70/1000\n",
      "1987/1987 [==============================] - 0s 234us/step - loss: 0.6853 - accuracy: 0.5541 - val_loss: 0.7078 - val_accuracy: 0.4887\n",
      "Epoch 71/1000\n",
      "1987/1987 [==============================] - 0s 246us/step - loss: 0.6929 - accuracy: 0.5264 - val_loss: 0.6876 - val_accuracy: 0.5656\n",
      "Epoch 72/1000\n",
      "1987/1987 [==============================] - 0s 244us/step - loss: 0.6876 - accuracy: 0.5435 - val_loss: 0.6881 - val_accuracy: 0.5747\n",
      "Epoch 73/1000\n",
      "1987/1987 [==============================] - 1s 262us/step - loss: 0.6865 - accuracy: 0.5425 - val_loss: 0.6887 - val_accuracy: 0.5701\n",
      "Epoch 74/1000\n",
      "1987/1987 [==============================] - 1s 315us/step - loss: 0.6878 - accuracy: 0.5395 - val_loss: 0.6914 - val_accuracy: 0.5701\n",
      "Epoch 75/1000\n",
      "1987/1987 [==============================] - 1s 319us/step - loss: 0.6837 - accuracy: 0.5481 - val_loss: 0.6975 - val_accuracy: 0.5430\n",
      "Epoch 76/1000\n",
      "1987/1987 [==============================] - 1s 310us/step - loss: 0.6856 - accuracy: 0.5632 - val_loss: 0.6939 - val_accuracy: 0.5701\n",
      "Epoch 77/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6843 - accuracy: 0.5551 - val_loss: 0.6907 - val_accuracy: 0.5656\n",
      "Epoch 78/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6842 - accuracy: 0.5455 - val_loss: 0.6923 - val_accuracy: 0.5611\n",
      "Epoch 79/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.6836 - accuracy: 0.5541 - val_loss: 0.6913 - val_accuracy: 0.5611\n",
      "Epoch 80/1000\n",
      "1987/1987 [==============================] - 1s 268us/step - loss: 0.6843 - accuracy: 0.5511 - val_loss: 0.6939 - val_accuracy: 0.5747\n",
      "Epoch 81/1000\n",
      "1987/1987 [==============================] - 1s 293us/step - loss: 0.6863 - accuracy: 0.5435 - val_loss: 0.6936 - val_accuracy: 0.5656\n",
      "Epoch 82/1000\n",
      "1987/1987 [==============================] - 1s 255us/step - loss: 0.6851 - accuracy: 0.5415 - val_loss: 0.6984 - val_accuracy: 0.5294\n",
      "Epoch 83/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6864 - accuracy: 0.5486 - val_loss: 0.6930 - val_accuracy: 0.5747\n",
      "Epoch 84/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6843 - accuracy: 0.5445 - val_loss: 0.6992 - val_accuracy: 0.5249\n",
      "Epoch 85/1000\n",
      "1987/1987 [==============================] - 1s 272us/step - loss: 0.6845 - accuracy: 0.5440 - val_loss: 0.6947 - val_accuracy: 0.5611\n",
      "Epoch 86/1000\n",
      "1987/1987 [==============================] - 1s 275us/step - loss: 0.6837 - accuracy: 0.5526 - val_loss: 0.6998 - val_accuracy: 0.5339\n",
      "Epoch 87/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6844 - accuracy: 0.5491 - val_loss: 0.6950 - val_accuracy: 0.5656\n",
      "Epoch 88/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6834 - accuracy: 0.5420 - val_loss: 0.6962 - val_accuracy: 0.5701\n",
      "Epoch 89/1000\n",
      "1987/1987 [==============================] - 1s 296us/step - loss: 0.6809 - accuracy: 0.5551 - val_loss: 0.7071 - val_accuracy: 0.5430\n",
      "Epoch 90/1000\n",
      "1987/1987 [==============================] - 1s 293us/step - loss: 0.6837 - accuracy: 0.5476 - val_loss: 0.6963 - val_accuracy: 0.5747\n",
      "Epoch 91/1000\n",
      "1987/1987 [==============================] - 1s 271us/step - loss: 0.6812 - accuracy: 0.5657 - val_loss: 0.6962 - val_accuracy: 0.5430\n",
      "Epoch 92/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6796 - accuracy: 0.5556 - val_loss: 0.7010 - val_accuracy: 0.5611\n",
      "Epoch 93/1000\n",
      "1987/1987 [==============================] - 1s 256us/step - loss: 0.6803 - accuracy: 0.5531 - val_loss: 0.7016 - val_accuracy: 0.5113\n",
      "Epoch 94/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6859 - accuracy: 0.5435 - val_loss: 0.6929 - val_accuracy: 0.5520\n",
      "Epoch 95/1000\n",
      "1987/1987 [==============================] - 1s 258us/step - loss: 0.6834 - accuracy: 0.5486 - val_loss: 0.6977 - val_accuracy: 0.5611\n",
      "Epoch 96/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.6816 - accuracy: 0.5506 - val_loss: 0.6999 - val_accuracy: 0.5339\n",
      "Epoch 97/1000\n",
      "1987/1987 [==============================] - 1s 271us/step - loss: 0.6833 - accuracy: 0.5546 - val_loss: 0.6938 - val_accuracy: 0.5747\n",
      "Epoch 98/1000\n",
      "1987/1987 [==============================] - 1s 282us/step - loss: 0.6795 - accuracy: 0.5481 - val_loss: 0.7066 - val_accuracy: 0.4751\n",
      "Epoch 99/1000\n",
      "1987/1987 [==============================] - 1s 323us/step - loss: 0.6780 - accuracy: 0.5632 - val_loss: 0.7000 - val_accuracy: 0.5520\n",
      "Epoch 100/1000\n",
      "1987/1987 [==============================] - 1s 323us/step - loss: 0.6804 - accuracy: 0.5591 - val_loss: 0.7055 - val_accuracy: 0.5475\n",
      "Epoch 101/1000\n",
      "1987/1987 [==============================] - 1s 281us/step - loss: 0.6805 - accuracy: 0.5511 - val_loss: 0.7008 - val_accuracy: 0.5294\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987/1987 [==============================] - 1s 317us/step - loss: 0.6771 - accuracy: 0.5682 - val_loss: 0.7039 - val_accuracy: 0.5339\n",
      "Epoch 103/1000\n",
      "1987/1987 [==============================] - 1s 297us/step - loss: 0.6804 - accuracy: 0.5606 - val_loss: 0.7029 - val_accuracy: 0.5294\n",
      "Epoch 104/1000\n",
      "1987/1987 [==============================] - 1s 276us/step - loss: 0.6791 - accuracy: 0.5425 - val_loss: 0.7009 - val_accuracy: 0.5475\n",
      "Epoch 105/1000\n",
      "1987/1987 [==============================] - 1s 383us/step - loss: 0.6755 - accuracy: 0.5697 - val_loss: 0.7020 - val_accuracy: 0.5339\n",
      "Epoch 106/1000\n",
      "1987/1987 [==============================] - 1s 291us/step - loss: 0.6730 - accuracy: 0.5757 - val_loss: 0.7042 - val_accuracy: 0.5113\n",
      "Epoch 107/1000\n",
      "1987/1987 [==============================] - 1s 309us/step - loss: 0.6764 - accuracy: 0.5637 - val_loss: 0.7016 - val_accuracy: 0.5475\n",
      "Epoch 108/1000\n",
      "1987/1987 [==============================] - 1s 392us/step - loss: 0.6759 - accuracy: 0.5687 - val_loss: 0.7094 - val_accuracy: 0.5294\n",
      "Epoch 109/1000\n",
      "1987/1987 [==============================] - 1s 318us/step - loss: 0.6742 - accuracy: 0.5642 - val_loss: 0.7005 - val_accuracy: 0.5566\n",
      "Epoch 110/1000\n",
      "1987/1987 [==============================] - 1s 352us/step - loss: 0.6727 - accuracy: 0.5783 - val_loss: 0.7059 - val_accuracy: 0.5249\n",
      "Epoch 111/1000\n",
      "1987/1987 [==============================] - 1s 372us/step - loss: 0.6718 - accuracy: 0.5702 - val_loss: 0.7041 - val_accuracy: 0.5520\n",
      "Epoch 112/1000\n",
      "1987/1987 [==============================] - 1s 318us/step - loss: 0.6710 - accuracy: 0.5838 - val_loss: 0.7078 - val_accuracy: 0.5204\n",
      "Epoch 113/1000\n",
      "1987/1987 [==============================] - 1s 315us/step - loss: 0.6701 - accuracy: 0.5757 - val_loss: 0.7063 - val_accuracy: 0.5611\n",
      "Epoch 114/1000\n",
      "1987/1987 [==============================] - 1s 299us/step - loss: 0.6671 - accuracy: 0.5762 - val_loss: 0.7130 - val_accuracy: 0.4796\n",
      "Epoch 115/1000\n",
      "1987/1987 [==============================] - 1s 304us/step - loss: 0.6745 - accuracy: 0.5692 - val_loss: 0.7022 - val_accuracy: 0.5113\n",
      "Epoch 116/1000\n",
      "1987/1987 [==============================] - 1s 298us/step - loss: 0.6685 - accuracy: 0.5707 - val_loss: 0.7177 - val_accuracy: 0.5023\n",
      "Epoch 117/1000\n",
      "1987/1987 [==============================] - 1s 296us/step - loss: 0.6670 - accuracy: 0.5742 - val_loss: 0.7131 - val_accuracy: 0.5158\n",
      "Epoch 118/1000\n",
      "1987/1987 [==============================] - 1s 269us/step - loss: 0.6615 - accuracy: 0.6014 - val_loss: 0.7068 - val_accuracy: 0.5204\n",
      "Epoch 119/1000\n",
      "1987/1987 [==============================] - 1s 268us/step - loss: 0.6592 - accuracy: 0.6024 - val_loss: 0.7113 - val_accuracy: 0.4842\n",
      "Epoch 120/1000\n",
      "1987/1987 [==============================] - 1s 277us/step - loss: 0.6663 - accuracy: 0.5762 - val_loss: 0.7055 - val_accuracy: 0.5520\n",
      "Epoch 121/1000\n",
      "1987/1987 [==============================] - 1s 274us/step - loss: 0.6604 - accuracy: 0.5808 - val_loss: 0.7129 - val_accuracy: 0.5294\n",
      "Epoch 122/1000\n",
      "1987/1987 [==============================] - 1s 287us/step - loss: 0.6611 - accuracy: 0.5833 - val_loss: 0.6978 - val_accuracy: 0.5747\n",
      "Epoch 123/1000\n",
      "1987/1987 [==============================] - 1s 318us/step - loss: 0.6578 - accuracy: 0.5898 - val_loss: 0.7219 - val_accuracy: 0.5430\n",
      "Epoch 124/1000\n",
      "1987/1987 [==============================] - 1s 305us/step - loss: 0.6559 - accuracy: 0.5863 - val_loss: 0.7266 - val_accuracy: 0.4977\n",
      "Epoch 125/1000\n",
      "1987/1987 [==============================] - 1s 314us/step - loss: 0.6537 - accuracy: 0.5944 - val_loss: 0.7056 - val_accuracy: 0.5113\n",
      "Epoch 126/1000\n",
      "1987/1987 [==============================] - 1s 330us/step - loss: 0.6595 - accuracy: 0.5908 - val_loss: 0.7267 - val_accuracy: 0.5068\n",
      "Epoch 127/1000\n",
      "1987/1987 [==============================] - 1s 353us/step - loss: 0.6639 - accuracy: 0.5954 - val_loss: 0.7060 - val_accuracy: 0.5385\n",
      "Epoch 128/1000\n",
      "1987/1987 [==============================] - 1s 435us/step - loss: 0.6582 - accuracy: 0.5949 - val_loss: 0.7344 - val_accuracy: 0.5068\n",
      "Epoch 129/1000\n",
      "1987/1987 [==============================] - 1s 418us/step - loss: 0.6593 - accuracy: 0.5843 - val_loss: 0.7163 - val_accuracy: 0.5158\n",
      "Epoch 130/1000\n",
      "1987/1987 [==============================] - 1s 458us/step - loss: 0.6513 - accuracy: 0.6095 - val_loss: 0.7445 - val_accuracy: 0.5023\n",
      "Epoch 131/1000\n",
      "1987/1987 [==============================] - 1s 495us/step - loss: 0.6495 - accuracy: 0.5959 - val_loss: 0.7089 - val_accuracy: 0.5339\n",
      "Epoch 132/1000\n",
      "1987/1987 [==============================] - 1s 514us/step - loss: 0.6533 - accuracy: 0.5959 - val_loss: 0.7307 - val_accuracy: 0.5520\n",
      "Epoch 133/1000\n",
      "1987/1987 [==============================] - 1s 546us/step - loss: 0.6494 - accuracy: 0.6029 - val_loss: 0.7280 - val_accuracy: 0.4796\n",
      "Epoch 134/1000\n",
      "1987/1987 [==============================] - 1s 565us/step - loss: 0.6490 - accuracy: 0.6019 - val_loss: 0.7198 - val_accuracy: 0.5204\n",
      "Epoch 135/1000\n",
      "1987/1987 [==============================] - 1s 571us/step - loss: 0.6427 - accuracy: 0.6150 - val_loss: 0.7347 - val_accuracy: 0.5339\n",
      "Epoch 136/1000\n",
      "1987/1987 [==============================] - 1s 597us/step - loss: 0.6441 - accuracy: 0.6095 - val_loss: 0.7246 - val_accuracy: 0.4977\n",
      "Epoch 137/1000\n",
      "1987/1987 [==============================] - 1s 574us/step - loss: 0.6463 - accuracy: 0.6105 - val_loss: 0.7256 - val_accuracy: 0.5339\n",
      "Epoch 138/1000\n",
      "1987/1987 [==============================] - 1s 541us/step - loss: 0.6451 - accuracy: 0.6150 - val_loss: 0.7231 - val_accuracy: 0.4796\n",
      "Epoch 139/1000\n",
      "1987/1987 [==============================] - 1s 513us/step - loss: 0.6357 - accuracy: 0.6145 - val_loss: 0.7450 - val_accuracy: 0.5023\n",
      "Epoch 140/1000\n",
      "1987/1987 [==============================] - 1s 467us/step - loss: 0.6414 - accuracy: 0.6230 - val_loss: 0.7301 - val_accuracy: 0.5294\n",
      "Epoch 141/1000\n",
      "1987/1987 [==============================] - 1s 456us/step - loss: 0.6368 - accuracy: 0.6135 - val_loss: 0.7060 - val_accuracy: 0.5520\n",
      "Epoch 142/1000\n",
      "1987/1987 [==============================] - 1s 401us/step - loss: 0.6359 - accuracy: 0.6246 - val_loss: 0.7442 - val_accuracy: 0.5249\n",
      "Epoch 143/1000\n",
      "1987/1987 [==============================] - 1s 372us/step - loss: 0.6365 - accuracy: 0.6150 - val_loss: 0.7349 - val_accuracy: 0.5113\n",
      "Epoch 144/1000\n",
      "1987/1987 [==============================] - 1s 353us/step - loss: 0.6290 - accuracy: 0.6236 - val_loss: 0.7176 - val_accuracy: 0.5385\n",
      "Epoch 145/1000\n",
      "1987/1987 [==============================] - 1s 339us/step - loss: 0.6199 - accuracy: 0.6341 - val_loss: 0.7299 - val_accuracy: 0.5385\n",
      "Epoch 146/1000\n",
      "1987/1987 [==============================] - 1s 313us/step - loss: 0.6294 - accuracy: 0.6256 - val_loss: 0.7362 - val_accuracy: 0.5023\n",
      "Epoch 147/1000\n",
      "1987/1987 [==============================] - 1s 305us/step - loss: 0.6749 - accuracy: 0.5747 - val_loss: 0.6967 - val_accuracy: 0.5249\n",
      "Epoch 148/1000\n",
      "1987/1987 [==============================] - 1s 292us/step - loss: 0.6640 - accuracy: 0.5898 - val_loss: 0.7097 - val_accuracy: 0.5701\n",
      "Epoch 149/1000\n",
      "1987/1987 [==============================] - 1s 267us/step - loss: 0.6534 - accuracy: 0.6064 - val_loss: 0.7277 - val_accuracy: 0.5249\n",
      "Epoch 150/1000\n",
      "1987/1987 [==============================] - 1s 262us/step - loss: 0.6397 - accuracy: 0.6019 - val_loss: 0.7434 - val_accuracy: 0.5520\n",
      "Epoch 151/1000\n",
      "1987/1987 [==============================] - 1s 269us/step - loss: 0.6296 - accuracy: 0.6266 - val_loss: 0.7221 - val_accuracy: 0.5158\n",
      "Epoch 152/1000\n",
      "1987/1987 [==============================] - 1s 258us/step - loss: 0.6261 - accuracy: 0.6407 - val_loss: 0.7466 - val_accuracy: 0.5339\n",
      "Epoch 153/1000\n",
      "1987/1987 [==============================] - 1s 263us/step - loss: 0.6315 - accuracy: 0.6155 - val_loss: 0.7268 - val_accuracy: 0.5566\n",
      "Epoch 154/1000\n",
      "1987/1987 [==============================] - 1s 255us/step - loss: 0.6250 - accuracy: 0.6281 - val_loss: 0.7461 - val_accuracy: 0.5339\n",
      "Epoch 155/1000\n",
      "1987/1987 [==============================] - 1s 269us/step - loss: 0.6178 - accuracy: 0.6376 - val_loss: 0.7653 - val_accuracy: 0.5068\n",
      "Epoch 156/1000\n",
      "1987/1987 [==============================] - 1s 257us/step - loss: 0.6077 - accuracy: 0.6482 - val_loss: 0.8326 - val_accuracy: 0.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6502 - accuracy: 0.6170 - val_loss: 0.7057 - val_accuracy: 0.5204\n",
      "Epoch 158/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6180 - accuracy: 0.6402 - val_loss: 0.7523 - val_accuracy: 0.5747\n",
      "Epoch 159/1000\n",
      "1987/1987 [==============================] - 1s 269us/step - loss: 0.6173 - accuracy: 0.6381 - val_loss: 0.7351 - val_accuracy: 0.5249\n",
      "Epoch 160/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6044 - accuracy: 0.6573 - val_loss: 0.7496 - val_accuracy: 0.5294\n",
      "Epoch 161/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6015 - accuracy: 0.6527 - val_loss: 0.7290 - val_accuracy: 0.5475\n",
      "Epoch 162/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6005 - accuracy: 0.6623 - val_loss: 0.7611 - val_accuracy: 0.5385\n",
      "Epoch 163/1000\n",
      "1987/1987 [==============================] - 1s 255us/step - loss: 0.5928 - accuracy: 0.6507 - val_loss: 0.7640 - val_accuracy: 0.5204\n",
      "Epoch 164/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.5900 - accuracy: 0.6633 - val_loss: 0.7849 - val_accuracy: 0.5385\n",
      "Epoch 165/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.5992 - accuracy: 0.6412 - val_loss: 0.7840 - val_accuracy: 0.5158\n",
      "Epoch 166/1000\n",
      "1987/1987 [==============================] - 1s 268us/step - loss: 0.5856 - accuracy: 0.6678 - val_loss: 0.7561 - val_accuracy: 0.5294\n",
      "Epoch 167/1000\n",
      "1987/1987 [==============================] - 1s 257us/step - loss: 0.5907 - accuracy: 0.6643 - val_loss: 0.7681 - val_accuracy: 0.5204\n",
      "Epoch 168/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6370 - accuracy: 0.6230 - val_loss: 0.7061 - val_accuracy: 0.5339\n",
      "Epoch 169/1000\n",
      "1987/1987 [==============================] - 1s 274us/step - loss: 0.6157 - accuracy: 0.6402 - val_loss: 0.7293 - val_accuracy: 0.5204\n",
      "Epoch 170/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.5858 - accuracy: 0.6648 - val_loss: 0.7769 - val_accuracy: 0.5339\n",
      "Epoch 171/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.5838 - accuracy: 0.6714 - val_loss: 0.7894 - val_accuracy: 0.5294\n",
      "Epoch 172/1000\n",
      "1987/1987 [==============================] - 1s 288us/step - loss: 0.5770 - accuracy: 0.6744 - val_loss: 0.7553 - val_accuracy: 0.5158\n",
      "Epoch 173/1000\n",
      "1987/1987 [==============================] - 1s 287us/step - loss: 0.6000 - accuracy: 0.6568 - val_loss: 0.7708 - val_accuracy: 0.5113\n",
      "Epoch 174/1000\n",
      "1987/1987 [==============================] - 1s 291us/step - loss: 0.5916 - accuracy: 0.6638 - val_loss: 0.7655 - val_accuracy: 0.5204\n",
      "Epoch 175/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.5843 - accuracy: 0.6709 - val_loss: 0.7819 - val_accuracy: 0.5023\n",
      "Epoch 176/1000\n",
      "1987/1987 [==============================] - 1s 316us/step - loss: 0.5836 - accuracy: 0.6563 - val_loss: 0.7969 - val_accuracy: 0.5113\n",
      "Epoch 177/1000\n",
      "1987/1987 [==============================] - 1s 306us/step - loss: 0.5960 - accuracy: 0.6699 - val_loss: 0.7996 - val_accuracy: 0.5339\n",
      "Epoch 178/1000\n",
      "1987/1987 [==============================] - 1s 322us/step - loss: 0.5671 - accuracy: 0.6779 - val_loss: 0.8114 - val_accuracy: 0.5294\n",
      "Epoch 179/1000\n",
      "1987/1987 [==============================] - 1s 339us/step - loss: 0.5632 - accuracy: 0.6819 - val_loss: 0.7744 - val_accuracy: 0.5023\n",
      "Epoch 180/1000\n",
      "1987/1987 [==============================] - 1s 330us/step - loss: 0.5684 - accuracy: 0.6704 - val_loss: 0.7744 - val_accuracy: 0.4842\n",
      "Epoch 181/1000\n",
      "1987/1987 [==============================] - 1s 323us/step - loss: 0.5738 - accuracy: 0.6739 - val_loss: 0.7621 - val_accuracy: 0.4887\n",
      "Epoch 182/1000\n",
      "1987/1987 [==============================] - 1s 340us/step - loss: 0.5655 - accuracy: 0.6875 - val_loss: 0.8070 - val_accuracy: 0.5113\n",
      "Epoch 183/1000\n",
      "1987/1987 [==============================] - 1s 326us/step - loss: 0.5811 - accuracy: 0.6653 - val_loss: 0.8564 - val_accuracy: 0.5158\n",
      "Epoch 184/1000\n",
      "1987/1987 [==============================] - 1s 349us/step - loss: 0.5624 - accuracy: 0.6860 - val_loss: 0.7865 - val_accuracy: 0.5113\n",
      "Epoch 185/1000\n",
      "1987/1987 [==============================] - 1s 369us/step - loss: 0.5463 - accuracy: 0.6935 - val_loss: 0.8398 - val_accuracy: 0.5204\n",
      "Epoch 186/1000\n",
      "1987/1987 [==============================] - 1s 325us/step - loss: 0.5649 - accuracy: 0.6860 - val_loss: 0.8380 - val_accuracy: 0.4842\n",
      "Epoch 187/1000\n",
      "1987/1987 [==============================] - 1s 312us/step - loss: 0.5365 - accuracy: 0.7021 - val_loss: 0.8407 - val_accuracy: 0.5339\n",
      "Epoch 188/1000\n",
      "1987/1987 [==============================] - 1s 304us/step - loss: 0.5467 - accuracy: 0.6975 - val_loss: 0.8346 - val_accuracy: 0.5113\n",
      "Epoch 189/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.5455 - accuracy: 0.6945 - val_loss: 0.8739 - val_accuracy: 0.5249\n",
      "Epoch 190/1000\n",
      "1987/1987 [==============================] - 1s 298us/step - loss: 0.5477 - accuracy: 0.6895 - val_loss: 0.7875 - val_accuracy: 0.5113\n",
      "Epoch 191/1000\n",
      "1987/1987 [==============================] - 1s 283us/step - loss: 0.5285 - accuracy: 0.7046 - val_loss: 0.8775 - val_accuracy: 0.5158\n",
      "Epoch 192/1000\n",
      "1987/1987 [==============================] - 1s 289us/step - loss: 0.5510 - accuracy: 0.6895 - val_loss: 0.8845 - val_accuracy: 0.4977\n",
      "Epoch 193/1000\n",
      "1987/1987 [==============================] - 1s 273us/step - loss: 0.5318 - accuracy: 0.7051 - val_loss: 0.9135 - val_accuracy: 0.5249\n",
      "Epoch 194/1000\n",
      "1987/1987 [==============================] - 1s 303us/step - loss: 0.5249 - accuracy: 0.7141 - val_loss: 0.8852 - val_accuracy: 0.4932\n",
      "Epoch 195/1000\n",
      "1987/1987 [==============================] - 1s 281us/step - loss: 0.5222 - accuracy: 0.7116 - val_loss: 0.8575 - val_accuracy: 0.5068\n",
      "Epoch 196/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.5338 - accuracy: 0.7076 - val_loss: 0.8968 - val_accuracy: 0.4751\n",
      "Epoch 197/1000\n",
      "1987/1987 [==============================] - 1s 278us/step - loss: 0.5217 - accuracy: 0.7162 - val_loss: 0.8780 - val_accuracy: 0.5023\n",
      "Epoch 198/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.5301 - accuracy: 0.7011 - val_loss: 0.8311 - val_accuracy: 0.5158\n",
      "Epoch 199/1000\n",
      "1987/1987 [==============================] - 1s 281us/step - loss: 0.5198 - accuracy: 0.7121 - val_loss: 0.9137 - val_accuracy: 0.4661\n",
      "Epoch 200/1000\n",
      "1987/1987 [==============================] - 1s 279us/step - loss: 0.5176 - accuracy: 0.7151 - val_loss: 0.8322 - val_accuracy: 0.5158\n",
      "Epoch 201/1000\n",
      "1987/1987 [==============================] - 1s 281us/step - loss: 0.5060 - accuracy: 0.7217 - val_loss: 0.8682 - val_accuracy: 0.4977\n",
      "Epoch 202/1000\n",
      "1987/1987 [==============================] - 1s 265us/step - loss: 0.5121 - accuracy: 0.7146 - val_loss: 0.8476 - val_accuracy: 0.4751\n",
      "Epoch 203/1000\n",
      "1987/1987 [==============================] - 1s 275us/step - loss: 0.5638 - accuracy: 0.6945 - val_loss: 0.7979 - val_accuracy: 0.5068\n",
      "Epoch 204/1000\n",
      "1987/1987 [==============================] - 1s 293us/step - loss: 0.5269 - accuracy: 0.7071 - val_loss: 0.8703 - val_accuracy: 0.4977\n",
      "Epoch 205/1000\n",
      "1987/1987 [==============================] - 1s 296us/step - loss: 0.5042 - accuracy: 0.7247 - val_loss: 0.9526 - val_accuracy: 0.4842\n",
      "Epoch 206/1000\n",
      "1987/1987 [==============================] - 1s 271us/step - loss: 0.5003 - accuracy: 0.7262 - val_loss: 0.9595 - val_accuracy: 0.4932\n",
      "Epoch 207/1000\n",
      "1987/1987 [==============================] - 1s 282us/step - loss: 0.4970 - accuracy: 0.7363 - val_loss: 0.9908 - val_accuracy: 0.4887\n",
      "Epoch 208/1000\n",
      "1987/1987 [==============================] - 1s 265us/step - loss: 0.5295 - accuracy: 0.7212 - val_loss: 0.9481 - val_accuracy: 0.4751\n",
      "Epoch 209/1000\n",
      "1987/1987 [==============================] - 1s 283us/step - loss: 0.5070 - accuracy: 0.7262 - val_loss: 0.9140 - val_accuracy: 0.4842\n",
      "Epoch 210/1000\n",
      "1987/1987 [==============================] - 1s 284us/step - loss: 0.4940 - accuracy: 0.7333 - val_loss: 0.9265 - val_accuracy: 0.5158\n",
      "Epoch 211/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.4856 - accuracy: 0.7408 - val_loss: 0.9935 - val_accuracy: 0.4932\n",
      "Epoch 212/1000\n",
      "1987/1987 [==============================] - 1s 300us/step - loss: 0.5655 - accuracy: 0.6834 - val_loss: 0.7787 - val_accuracy: 0.5113\n",
      "Epoch 213/1000\n",
      "1987/1987 [==============================] - 1s 293us/step - loss: 0.5318 - accuracy: 0.7121 - val_loss: 0.8526 - val_accuracy: 0.4842\n",
      "Epoch 214/1000\n",
      "1987/1987 [==============================] - 1s 305us/step - loss: 0.5062 - accuracy: 0.7323 - val_loss: 0.8638 - val_accuracy: 0.4842\n",
      "Epoch 215/1000\n",
      "1987/1987 [==============================] - 1s 285us/step - loss: 0.4975 - accuracy: 0.7277 - val_loss: 0.9237 - val_accuracy: 0.4615\n",
      "Epoch 216/1000\n",
      "1987/1987 [==============================] - 1s 301us/step - loss: 0.5225 - accuracy: 0.7217 - val_loss: 0.9193 - val_accuracy: 0.4977\n",
      "Epoch 217/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.5070 - accuracy: 0.7187 - val_loss: 0.8652 - val_accuracy: 0.4977\n",
      "Epoch 218/1000\n",
      "1987/1987 [==============================] - 1s 296us/step - loss: 0.5012 - accuracy: 0.7242 - val_loss: 1.0037 - val_accuracy: 0.4977\n",
      "Epoch 219/1000\n",
      "1987/1987 [==============================] - 1s 298us/step - loss: 0.4877 - accuracy: 0.7343 - val_loss: 0.9405 - val_accuracy: 0.4842\n",
      "Epoch 220/1000\n",
      "1987/1987 [==============================] - 1s 297us/step - loss: 0.4842 - accuracy: 0.7338 - val_loss: 0.9529 - val_accuracy: 0.4887\n",
      "Epoch 221/1000\n",
      "1987/1987 [==============================] - 1s 304us/step - loss: 0.4911 - accuracy: 0.7237 - val_loss: 0.9158 - val_accuracy: 0.5113\n",
      "Epoch 222/1000\n",
      "1987/1987 [==============================] - 1s 338us/step - loss: 0.4770 - accuracy: 0.7393 - val_loss: 1.0016 - val_accuracy: 0.4661\n",
      "Epoch 223/1000\n",
      "1987/1987 [==============================] - 1s 408us/step - loss: 0.4632 - accuracy: 0.7443 - val_loss: 1.0417 - val_accuracy: 0.4796\n",
      "Epoch 224/1000\n",
      "1987/1987 [==============================] - 1s 330us/step - loss: 0.4619 - accuracy: 0.7458 - val_loss: 1.0757 - val_accuracy: 0.4570\n",
      "Epoch 225/1000\n",
      "1987/1987 [==============================] - 1s 338us/step - loss: 0.4670 - accuracy: 0.7464 - val_loss: 0.9633 - val_accuracy: 0.4977\n",
      "Epoch 226/1000\n",
      "1987/1987 [==============================] - 1s 309us/step - loss: 0.4681 - accuracy: 0.7620 - val_loss: 1.0417 - val_accuracy: 0.4796\n",
      "Epoch 227/1000\n",
      "1987/1987 [==============================] - 1s 304us/step - loss: 0.4579 - accuracy: 0.7428 - val_loss: 1.0274 - val_accuracy: 0.5113\n",
      "Epoch 228/1000\n",
      "1987/1987 [==============================] - 1s 305us/step - loss: 0.4567 - accuracy: 0.7529 - val_loss: 1.1008 - val_accuracy: 0.4887\n",
      "Epoch 229/1000\n",
      "1987/1987 [==============================] - 1s 323us/step - loss: 0.4355 - accuracy: 0.7635 - val_loss: 1.1809 - val_accuracy: 0.4887\n",
      "Epoch 230/1000\n",
      "1987/1987 [==============================] - 1s 300us/step - loss: 0.4510 - accuracy: 0.7464 - val_loss: 1.1243 - val_accuracy: 0.4977\n",
      "Epoch 231/1000\n",
      "1987/1987 [==============================] - 1s 312us/step - loss: 0.4368 - accuracy: 0.7645 - val_loss: 1.1382 - val_accuracy: 0.4887\n",
      "Epoch 232/1000\n",
      "1987/1987 [==============================] - 1s 311us/step - loss: 0.4485 - accuracy: 0.7630 - val_loss: 1.0484 - val_accuracy: 0.4796\n",
      "Epoch 233/1000\n",
      "1987/1987 [==============================] - 1s 295us/step - loss: 0.4341 - accuracy: 0.7670 - val_loss: 1.1743 - val_accuracy: 0.4796\n",
      "Epoch 234/1000\n",
      "1987/1987 [==============================] - 1s 297us/step - loss: 0.4244 - accuracy: 0.7740 - val_loss: 1.1755 - val_accuracy: 0.5113\n",
      "Epoch 235/1000\n",
      "1987/1987 [==============================] - 1s 289us/step - loss: 0.4162 - accuracy: 0.7765 - val_loss: 1.2933 - val_accuracy: 0.4887\n",
      "Epoch 236/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.4156 - accuracy: 0.7735 - val_loss: 1.3154 - val_accuracy: 0.4977\n",
      "Epoch 237/1000\n",
      "1987/1987 [==============================] - 1s 283us/step - loss: 0.4201 - accuracy: 0.7715 - val_loss: 1.1477 - val_accuracy: 0.5204\n",
      "Epoch 00237: early stopping\n",
      "7 day\n",
      "\n",
      "# Evaluate on test data\n",
      "246/246 [==============================] - 0s 122us/step\n",
      "test loss, test acc: [1.2456587872854092, 0.577235758304596]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (246, 1)\n",
      "rmse: 0.5626497492768694\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 7\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "print(\"7 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(15, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(15, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 15, 50)            28600     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 15, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 15, 75)            37800     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 15, 75)            0         \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 2s 1ms/step - loss: 0.6941 - accuracy: 0.4965 - val_loss: 0.6907 - val_accuracy: 0.5656\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.6943 - accuracy: 0.5121 - val_loss: 0.6942 - val_accuracy: 0.4570\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.6917 - accuracy: 0.5222 - val_loss: 0.6857 - val_accuracy: 0.5701\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.6920 - accuracy: 0.5197 - val_loss: 0.6884 - val_accuracy: 0.5792\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.6924 - accuracy: 0.5086 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.6920 - accuracy: 0.5268 - val_loss: 0.6915 - val_accuracy: 0.5520\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 432us/step - loss: 0.6925 - accuracy: 0.5283 - val_loss: 0.6865 - val_accuracy: 0.5747\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.6921 - accuracy: 0.5242 - val_loss: 0.6894 - val_accuracy: 0.5928\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.6912 - accuracy: 0.5263 - val_loss: 0.6892 - val_accuracy: 0.5701\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.6913 - accuracy: 0.5126 - val_loss: 0.6908 - val_accuracy: 0.5611\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 490us/step - loss: 0.6920 - accuracy: 0.5283 - val_loss: 0.6892 - val_accuracy: 0.5656\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.6914 - accuracy: 0.5222 - val_loss: 0.6882 - val_accuracy: 0.5882\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.6905 - accuracy: 0.5258 - val_loss: 0.6902 - val_accuracy: 0.5882\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.6907 - accuracy: 0.5338 - val_loss: 0.6931 - val_accuracy: 0.5113\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.6907 - accuracy: 0.5273 - val_loss: 0.6917 - val_accuracy: 0.5611\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.6911 - accuracy: 0.5379 - val_loss: 0.6895 - val_accuracy: 0.5385\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.6899 - accuracy: 0.5247 - val_loss: 0.6911 - val_accuracy: 0.5294\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 455us/step - loss: 0.6913 - accuracy: 0.5227 - val_loss: 0.6907 - val_accuracy: 0.5294\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.6907 - accuracy: 0.5278 - val_loss: 0.6887 - val_accuracy: 0.5928\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.6898 - accuracy: 0.5293 - val_loss: 0.6924 - val_accuracy: 0.5113\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 475us/step - loss: 0.6905 - accuracy: 0.5273 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 500us/step - loss: 0.6909 - accuracy: 0.5207 - val_loss: 0.6880 - val_accuracy: 0.5656\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.6906 - accuracy: 0.5283 - val_loss: 0.6868 - val_accuracy: 0.5837\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.6913 - accuracy: 0.5076 - val_loss: 0.6873 - val_accuracy: 0.5792\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.6920 - accuracy: 0.5313 - val_loss: 0.6922 - val_accuracy: 0.5294\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.6893 - accuracy: 0.5298 - val_loss: 0.6932 - val_accuracy: 0.5747\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.6900 - accuracy: 0.5313 - val_loss: 0.6892 - val_accuracy: 0.5385\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.6898 - accuracy: 0.5298 - val_loss: 0.6909 - val_accuracy: 0.5566\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 482us/step - loss: 0.6898 - accuracy: 0.5338 - val_loss: 0.6910 - val_accuracy: 0.5294\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 513us/step - loss: 0.6893 - accuracy: 0.5399 - val_loss: 0.6893 - val_accuracy: 0.5611\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.6885 - accuracy: 0.5581 - val_loss: 0.6997 - val_accuracy: 0.4525\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 481us/step - loss: 0.6896 - accuracy: 0.5364 - val_loss: 0.6901 - val_accuracy: 0.5339\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 505us/step - loss: 0.6886 - accuracy: 0.5379 - val_loss: 0.6886 - val_accuracy: 0.5385\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6870 - accuracy: 0.5470 - val_loss: 0.6921 - val_accuracy: 0.5204\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6877 - accuracy: 0.5480 - val_loss: 0.6906 - val_accuracy: 0.5294\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.6927 - accuracy: 0.5202 - val_loss: 0.6858 - val_accuracy: 0.5701\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.6899 - accuracy: 0.5253 - val_loss: 0.6962 - val_accuracy: 0.4706\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.6907 - accuracy: 0.5197 - val_loss: 0.6902 - val_accuracy: 0.5792\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 452us/step - loss: 0.6899 - accuracy: 0.5253 - val_loss: 0.6890 - val_accuracy: 0.5837\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.6911 - accuracy: 0.5258 - val_loss: 0.6930 - val_accuracy: 0.5204\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6922 - accuracy: 0.5293 - val_loss: 0.6907 - val_accuracy: 0.5701\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.6895 - accuracy: 0.5364 - val_loss: 0.6908 - val_accuracy: 0.5701\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.6890 - accuracy: 0.5419 - val_loss: 0.6938 - val_accuracy: 0.5339\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6870 - accuracy: 0.5510 - val_loss: 0.6957 - val_accuracy: 0.5158\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6894 - accuracy: 0.5288 - val_loss: 0.6891 - val_accuracy: 0.5294\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 492us/step - loss: 0.6901 - accuracy: 0.5283 - val_loss: 0.6901 - val_accuracy: 0.5520\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6871 - accuracy: 0.5343 - val_loss: 0.6993 - val_accuracy: 0.4887\n",
      "Epoch 48/1000\n",
      "1980/1980 [==============================] - 1s 495us/step - loss: 0.6866 - accuracy: 0.5480 - val_loss: 0.6917 - val_accuracy: 0.5023\n",
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 1s 493us/step - loss: 0.6878 - accuracy: 0.5338 - val_loss: 0.6926 - val_accuracy: 0.5385\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 1s 475us/step - loss: 0.6842 - accuracy: 0.5505 - val_loss: 0.6994 - val_accuracy: 0.5113\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.6861 - accuracy: 0.5510 - val_loss: 0.6885 - val_accuracy: 0.5656\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 0.6867 - accuracy: 0.5434 - val_loss: 0.7049 - val_accuracy: 0.4661\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6864 - accuracy: 0.5444 - val_loss: 0.6892 - val_accuracy: 0.5385\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 1s 518us/step - loss: 0.6831 - accuracy: 0.5545 - val_loss: 0.7116 - val_accuracy: 0.4706\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 1s 511us/step - loss: 0.6861 - accuracy: 0.5601 - val_loss: 0.6894 - val_accuracy: 0.5701\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.6873 - accuracy: 0.5288 - val_loss: 0.6924 - val_accuracy: 0.5339\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.6860 - accuracy: 0.5470 - val_loss: 0.7078 - val_accuracy: 0.4842\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 1s 444us/step - loss: 0.6856 - accuracy: 0.5551 - val_loss: 0.7040 - val_accuracy: 0.4706\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 1s 448us/step - loss: 0.6852 - accuracy: 0.5530 - val_loss: 0.6897 - val_accuracy: 0.5339\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.6815 - accuracy: 0.5646 - val_loss: 0.7045 - val_accuracy: 0.5204\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.6800 - accuracy: 0.5581 - val_loss: 0.6944 - val_accuracy: 0.5520\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.6785 - accuracy: 0.5753 - val_loss: 0.7008 - val_accuracy: 0.5113\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.6768 - accuracy: 0.5631 - val_loss: 0.7177 - val_accuracy: 0.4480\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 1s 444us/step - loss: 0.6807 - accuracy: 0.5566 - val_loss: 0.7287 - val_accuracy: 0.5611\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.6833 - accuracy: 0.5490 - val_loss: 0.7013 - val_accuracy: 0.5068\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.6798 - accuracy: 0.5702 - val_loss: 0.7112 - val_accuracy: 0.5339\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.6805 - accuracy: 0.5525 - val_loss: 0.7184 - val_accuracy: 0.5023\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 505us/step - loss: 0.6779 - accuracy: 0.5621 - val_loss: 0.7071 - val_accuracy: 0.5430\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 481us/step - loss: 0.6799 - accuracy: 0.5662 - val_loss: 0.7033 - val_accuracy: 0.5294\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.6741 - accuracy: 0.5722 - val_loss: 0.7113 - val_accuracy: 0.5566\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.6767 - accuracy: 0.5672 - val_loss: 0.7077 - val_accuracy: 0.5113\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.6734 - accuracy: 0.5616 - val_loss: 0.7263 - val_accuracy: 0.5430\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 547us/step - loss: 0.6683 - accuracy: 0.5859 - val_loss: 0.7191 - val_accuracy: 0.5158\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.6801 - accuracy: 0.5490 - val_loss: 0.7092 - val_accuracy: 0.4751\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 488us/step - loss: 0.6749 - accuracy: 0.5687 - val_loss: 0.7071 - val_accuracy: 0.5566\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 495us/step - loss: 0.6719 - accuracy: 0.5808 - val_loss: 0.7090 - val_accuracy: 0.5430\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6733 - accuracy: 0.5667 - val_loss: 0.7180 - val_accuracy: 0.4842\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6661 - accuracy: 0.5808 - val_loss: 0.7226 - val_accuracy: 0.5023\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6702 - accuracy: 0.5914 - val_loss: 0.7085 - val_accuracy: 0.5158\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.6703 - accuracy: 0.5838 - val_loss: 0.7029 - val_accuracy: 0.5249\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 1s 450us/step - loss: 0.6700 - accuracy: 0.5808 - val_loss: 0.7221 - val_accuracy: 0.5475\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.6688 - accuracy: 0.5894 - val_loss: 0.7136 - val_accuracy: 0.4887\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 1s 448us/step - loss: 0.6622 - accuracy: 0.5955 - val_loss: 0.7403 - val_accuracy: 0.5339\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 1s 455us/step - loss: 0.6650 - accuracy: 0.5854 - val_loss: 0.7229 - val_accuracy: 0.5023\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6615 - accuracy: 0.5934 - val_loss: 0.7357 - val_accuracy: 0.5339\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.6590 - accuracy: 0.6015 - val_loss: 0.7600 - val_accuracy: 0.5113\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6562 - accuracy: 0.6071 - val_loss: 0.7328 - val_accuracy: 0.5520\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6595 - accuracy: 0.6061 - val_loss: 0.7304 - val_accuracy: 0.5385\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 473us/step - loss: 0.6597 - accuracy: 0.6040 - val_loss: 0.7203 - val_accuracy: 0.4796\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 517us/step - loss: 0.6652 - accuracy: 0.5899 - val_loss: 0.7160 - val_accuracy: 0.5294\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6605 - accuracy: 0.6020 - val_loss: 0.7332 - val_accuracy: 0.5385\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.6574 - accuracy: 0.6076 - val_loss: 0.7156 - val_accuracy: 0.5249\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 1s 485us/step - loss: 0.6668 - accuracy: 0.5929 - val_loss: 0.7310 - val_accuracy: 0.5294\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6610 - accuracy: 0.5965 - val_loss: 0.7023 - val_accuracy: 0.5475\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.6518 - accuracy: 0.6091 - val_loss: 0.7414 - val_accuracy: 0.4977\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6526 - accuracy: 0.6056 - val_loss: 0.7251 - val_accuracy: 0.5294\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.6548 - accuracy: 0.6035 - val_loss: 0.7231 - val_accuracy: 0.4932\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 1s 496us/step - loss: 0.6505 - accuracy: 0.6035 - val_loss: 0.7600 - val_accuracy: 0.5430\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.6540 - accuracy: 0.6056 - val_loss: 0.7244 - val_accuracy: 0.4977\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.6508 - accuracy: 0.6192 - val_loss: 0.7182 - val_accuracy: 0.5430\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6477 - accuracy: 0.6076 - val_loss: 0.7389 - val_accuracy: 0.5339\n",
      "Epoch 103/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.6428 - accuracy: 0.6232 - val_loss: 0.7570 - val_accuracy: 0.4842\n",
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.6520 - accuracy: 0.6081 - val_loss: 0.7199 - val_accuracy: 0.5339\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 452us/step - loss: 0.6478 - accuracy: 0.6253 - val_loss: 0.7652 - val_accuracy: 0.5339\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.6468 - accuracy: 0.6101 - val_loss: 0.7471 - val_accuracy: 0.5249\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 493us/step - loss: 0.6412 - accuracy: 0.6141 - val_loss: 0.7549 - val_accuracy: 0.5294\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.6401 - accuracy: 0.6192 - val_loss: 0.7186 - val_accuracy: 0.5520\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.6460 - accuracy: 0.6177 - val_loss: 0.7654 - val_accuracy: 0.5249\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.6375 - accuracy: 0.6384 - val_loss: 0.7439 - val_accuracy: 0.4751\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6449 - accuracy: 0.6253 - val_loss: 0.7351 - val_accuracy: 0.5611\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6349 - accuracy: 0.6394 - val_loss: 0.7293 - val_accuracy: 0.5339\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6383 - accuracy: 0.6318 - val_loss: 0.7678 - val_accuracy: 0.5385\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 482us/step - loss: 0.6347 - accuracy: 0.6288 - val_loss: 0.7701 - val_accuracy: 0.5158\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6376 - accuracy: 0.6202 - val_loss: 0.7609 - val_accuracy: 0.5339\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.6321 - accuracy: 0.6359 - val_loss: 0.7363 - val_accuracy: 0.5430\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6367 - accuracy: 0.6293 - val_loss: 0.7323 - val_accuracy: 0.5294\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 485us/step - loss: 0.6306 - accuracy: 0.6379 - val_loss: 0.7450 - val_accuracy: 0.5520\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 486us/step - loss: 0.6365 - accuracy: 0.6202 - val_loss: 0.7438 - val_accuracy: 0.5430\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6385 - accuracy: 0.6227 - val_loss: 0.7425 - val_accuracy: 0.5158\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 492us/step - loss: 0.6315 - accuracy: 0.6303 - val_loss: 0.7962 - val_accuracy: 0.5430\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 0.6330 - accuracy: 0.6212 - val_loss: 0.7482 - val_accuracy: 0.5294\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.6352 - accuracy: 0.6212 - val_loss: 0.7437 - val_accuracy: 0.5430\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 503us/step - loss: 0.6275 - accuracy: 0.6414 - val_loss: 0.7705 - val_accuracy: 0.5385\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6212 - accuracy: 0.6475 - val_loss: 0.8013 - val_accuracy: 0.5158\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.6192 - accuracy: 0.6434 - val_loss: 0.8084 - val_accuracy: 0.5204\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 445us/step - loss: 0.6296 - accuracy: 0.6389 - val_loss: 0.7367 - val_accuracy: 0.5294\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.6252 - accuracy: 0.6389 - val_loss: 0.7620 - val_accuracy: 0.5475\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.6172 - accuracy: 0.6500 - val_loss: 0.8043 - val_accuracy: 0.4977\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.6165 - accuracy: 0.6495 - val_loss: 0.7752 - val_accuracy: 0.5068\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.6097 - accuracy: 0.6576 - val_loss: 0.8002 - val_accuracy: 0.5113\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6100 - accuracy: 0.6561 - val_loss: 0.8088 - val_accuracy: 0.5023\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.6204 - accuracy: 0.6399 - val_loss: 0.7900 - val_accuracy: 0.5249\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6271 - accuracy: 0.6288 - val_loss: 0.7815 - val_accuracy: 0.4706\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.6192 - accuracy: 0.6389 - val_loss: 0.7843 - val_accuracy: 0.5023\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6292 - accuracy: 0.6424 - val_loss: 0.7324 - val_accuracy: 0.5430\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6234 - accuracy: 0.6505 - val_loss: 0.7743 - val_accuracy: 0.5385\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6202 - accuracy: 0.6384 - val_loss: 0.7451 - val_accuracy: 0.5339\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 1s 487us/step - loss: 0.6182 - accuracy: 0.6500 - val_loss: 0.7643 - val_accuracy: 0.5475\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.6090 - accuracy: 0.6616 - val_loss: 0.7686 - val_accuracy: 0.4977\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.6099 - accuracy: 0.6566 - val_loss: 0.8167 - val_accuracy: 0.5158\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 1s 537us/step - loss: 0.6062 - accuracy: 0.6571 - val_loss: 0.8124 - val_accuracy: 0.4977\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 498us/step - loss: 0.6046 - accuracy: 0.6611 - val_loss: 0.7692 - val_accuracy: 0.5158\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 487us/step - loss: 0.6214 - accuracy: 0.6434 - val_loss: 0.7957 - val_accuracy: 0.5294\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 490us/step - loss: 0.6113 - accuracy: 0.6490 - val_loss: 0.8056 - val_accuracy: 0.4977\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.6022 - accuracy: 0.6545 - val_loss: 0.7709 - val_accuracy: 0.5068\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6032 - accuracy: 0.6636 - val_loss: 0.8103 - val_accuracy: 0.4887\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6036 - accuracy: 0.6606 - val_loss: 0.8415 - val_accuracy: 0.5294\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.5914 - accuracy: 0.6616 - val_loss: 0.8308 - val_accuracy: 0.5430\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.5818 - accuracy: 0.6672 - val_loss: 0.8704 - val_accuracy: 0.5475\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 450us/step - loss: 0.5893 - accuracy: 0.6793 - val_loss: 0.8384 - val_accuracy: 0.5113\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.5832 - accuracy: 0.6742 - val_loss: 0.8534 - val_accuracy: 0.5701\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.5843 - accuracy: 0.6763 - val_loss: 0.8657 - val_accuracy: 0.4932\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.5960 - accuracy: 0.6530 - val_loss: 0.8467 - val_accuracy: 0.5204\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.5866 - accuracy: 0.6712 - val_loss: 0.8279 - val_accuracy: 0.5339\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.5739 - accuracy: 0.6955 - val_loss: 0.8605 - val_accuracy: 0.5430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.5880 - accuracy: 0.6707 - val_loss: 0.8288 - val_accuracy: 0.5385\n",
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.5827 - accuracy: 0.6702 - val_loss: 0.8477 - val_accuracy: 0.5249\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.5923 - accuracy: 0.6662 - val_loss: 0.8114 - val_accuracy: 0.5475\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 510us/step - loss: 0.5843 - accuracy: 0.6712 - val_loss: 0.9197 - val_accuracy: 0.5339\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.5781 - accuracy: 0.6864 - val_loss: 0.8542 - val_accuracy: 0.5566\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.5720 - accuracy: 0.6803 - val_loss: 0.8914 - val_accuracy: 0.5113\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.5756 - accuracy: 0.6763 - val_loss: 0.8331 - val_accuracy: 0.5249\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.5826 - accuracy: 0.6763 - val_loss: 0.9312 - val_accuracy: 0.5204\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 0.5610 - accuracy: 0.6924 - val_loss: 0.8937 - val_accuracy: 0.5339\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.5686 - accuracy: 0.6818 - val_loss: 0.8336 - val_accuracy: 0.4842\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.5733 - accuracy: 0.6813 - val_loss: 0.8587 - val_accuracy: 0.5249\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.5834 - accuracy: 0.6778 - val_loss: 0.8762 - val_accuracy: 0.5430\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.5546 - accuracy: 0.7015 - val_loss: 0.9457 - val_accuracy: 0.5249\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 455us/step - loss: 0.5466 - accuracy: 0.7061 - val_loss: 0.9469 - val_accuracy: 0.5294\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 1s 448us/step - loss: 0.5492 - accuracy: 0.6960 - val_loss: 1.0247 - val_accuracy: 0.5385\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.5464 - accuracy: 0.7025 - val_loss: 0.9551 - val_accuracy: 0.5068\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.5509 - accuracy: 0.7005 - val_loss: 0.9934 - val_accuracy: 0.5566\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.5479 - accuracy: 0.7111 - val_loss: 0.9013 - val_accuracy: 0.5249\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.5351 - accuracy: 0.7030 - val_loss: 0.9897 - val_accuracy: 0.5158\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.5615 - accuracy: 0.7010 - val_loss: 0.9220 - val_accuracy: 0.4706\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.5431 - accuracy: 0.6955 - val_loss: 0.9971 - val_accuracy: 0.5294\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 519us/step - loss: 0.5323 - accuracy: 0.7152 - val_loss: 0.9556 - val_accuracy: 0.5385\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.5323 - accuracy: 0.7030 - val_loss: 0.9186 - val_accuracy: 0.5566\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.5362 - accuracy: 0.7005 - val_loss: 0.9990 - val_accuracy: 0.5294\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 448us/step - loss: 0.5366 - accuracy: 0.7066 - val_loss: 1.0113 - val_accuracy: 0.5385\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.5322 - accuracy: 0.7141 - val_loss: 0.9777 - val_accuracy: 0.5113\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.5233 - accuracy: 0.7247 - val_loss: 0.9631 - val_accuracy: 0.4977\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.5341 - accuracy: 0.7061 - val_loss: 1.0024 - val_accuracy: 0.5023\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.5039 - accuracy: 0.7298 - val_loss: 1.0680 - val_accuracy: 0.4932\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 0.5224 - accuracy: 0.7222 - val_loss: 1.0401 - val_accuracy: 0.5113\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.5283 - accuracy: 0.7152 - val_loss: 0.9962 - val_accuracy: 0.4887\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.5173 - accuracy: 0.7202 - val_loss: 1.0055 - val_accuracy: 0.4977\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.4975 - accuracy: 0.7495 - val_loss: 1.1952 - val_accuracy: 0.5339\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.5176 - accuracy: 0.7141 - val_loss: 1.0140 - val_accuracy: 0.5068\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.5027 - accuracy: 0.7298 - val_loss: 1.1324 - val_accuracy: 0.4932\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.5151 - accuracy: 0.7308 - val_loss: 1.0446 - val_accuracy: 0.5113\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.5043 - accuracy: 0.7278 - val_loss: 1.0746 - val_accuracy: 0.4842\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.5115 - accuracy: 0.7333 - val_loss: 1.1027 - val_accuracy: 0.5158\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.5009 - accuracy: 0.7348 - val_loss: 1.0993 - val_accuracy: 0.5113\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.5117 - accuracy: 0.7212 - val_loss: 1.0057 - val_accuracy: 0.4887\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 1s 513us/step - loss: 0.4866 - accuracy: 0.7460 - val_loss: 1.1617 - val_accuracy: 0.5430\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.4932 - accuracy: 0.7374 - val_loss: 1.1371 - val_accuracy: 0.5068\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.4731 - accuracy: 0.7384 - val_loss: 1.1227 - val_accuracy: 0.5204\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.4864 - accuracy: 0.7338 - val_loss: 1.1708 - val_accuracy: 0.5158\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.4841 - accuracy: 0.7389 - val_loss: 1.1601 - val_accuracy: 0.5113\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.4743 - accuracy: 0.7444 - val_loss: 1.2004 - val_accuracy: 0.4887\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.4718 - accuracy: 0.7419 - val_loss: 1.2425 - val_accuracy: 0.5023\n",
      "Epoch 00203: early stopping\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 145us/step\n",
      "test loss, test acc: [1.1414791734851137, 0.5224489569664001]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.5823113730505857\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(30, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(30, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 30, 50)            28600     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 30, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 30, 75)            37800     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 30, 75)            0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1968 samples, validate on 219 samples\n",
      "Epoch 1/1000\n",
      "1968/1968 [==============================] - 3s 2ms/step - loss: 0.6964 - accuracy: 0.5102 - val_loss: 0.6887 - val_accuracy: 0.5571\n",
      "Epoch 2/1000\n",
      "1968/1968 [==============================] - 2s 935us/step - loss: 0.6929 - accuracy: 0.5107 - val_loss: 0.6914 - val_accuracy: 0.5525\n",
      "Epoch 3/1000\n",
      "1968/1968 [==============================] - 2s 971us/step - loss: 0.6921 - accuracy: 0.5305 - val_loss: 0.6900 - val_accuracy: 0.5571\n",
      "Epoch 4/1000\n",
      "1968/1968 [==============================] - 2s 980us/step - loss: 0.6923 - accuracy: 0.5290 - val_loss: 0.6945 - val_accuracy: 0.5114\n",
      "Epoch 5/1000\n",
      "1968/1968 [==============================] - 2s 999us/step - loss: 0.6917 - accuracy: 0.5310 - val_loss: 0.6908 - val_accuracy: 0.5571\n",
      "Epoch 6/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6918 - accuracy: 0.5193 - val_loss: 0.6901 - val_accuracy: 0.5571\n",
      "Epoch 7/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6925 - accuracy: 0.5264 - val_loss: 0.6901 - val_accuracy: 0.5571\n",
      "Epoch 8/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6908 - accuracy: 0.5356 - val_loss: 0.6966 - val_accuracy: 0.5068\n",
      "Epoch 9/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6924 - accuracy: 0.5208 - val_loss: 0.6925 - val_accuracy: 0.5114\n",
      "Epoch 10/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6916 - accuracy: 0.5320 - val_loss: 0.6921 - val_accuracy: 0.5388\n",
      "Epoch 11/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6919 - accuracy: 0.5432 - val_loss: 0.6926 - val_accuracy: 0.5114\n",
      "Epoch 12/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6907 - accuracy: 0.5366 - val_loss: 0.6950 - val_accuracy: 0.5160\n",
      "Epoch 13/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6900 - accuracy: 0.5330 - val_loss: 0.6893 - val_accuracy: 0.5571\n",
      "Epoch 14/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6922 - accuracy: 0.5285 - val_loss: 0.6909 - val_accuracy: 0.5479\n",
      "Epoch 15/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6909 - accuracy: 0.5285 - val_loss: 0.6934 - val_accuracy: 0.5205\n",
      "Epoch 16/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6908 - accuracy: 0.5386 - val_loss: 0.6939 - val_accuracy: 0.5160\n",
      "Epoch 17/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6911 - accuracy: 0.5335 - val_loss: 0.6925 - val_accuracy: 0.5388\n",
      "Epoch 18/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6898 - accuracy: 0.5386 - val_loss: 0.6958 - val_accuracy: 0.5114\n",
      "Epoch 19/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6918 - accuracy: 0.5351 - val_loss: 0.6928 - val_accuracy: 0.5388\n",
      "Epoch 20/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6908 - accuracy: 0.5351 - val_loss: 0.6947 - val_accuracy: 0.5205\n",
      "Epoch 21/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6905 - accuracy: 0.5381 - val_loss: 0.6973 - val_accuracy: 0.5297\n",
      "Epoch 22/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6895 - accuracy: 0.5417 - val_loss: 0.6956 - val_accuracy: 0.5297\n",
      "Epoch 23/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6902 - accuracy: 0.5437 - val_loss: 0.6981 - val_accuracy: 0.4977\n",
      "Epoch 24/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6899 - accuracy: 0.5356 - val_loss: 0.6916 - val_accuracy: 0.5479\n",
      "Epoch 25/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6897 - accuracy: 0.5467 - val_loss: 0.6976 - val_accuracy: 0.5068\n",
      "Epoch 26/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6883 - accuracy: 0.5417 - val_loss: 0.6960 - val_accuracy: 0.5251\n",
      "Epoch 27/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6893 - accuracy: 0.5356 - val_loss: 0.6968 - val_accuracy: 0.5068\n",
      "Epoch 28/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6892 - accuracy: 0.5320 - val_loss: 0.6911 - val_accuracy: 0.5479\n",
      "Epoch 29/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6883 - accuracy: 0.5457 - val_loss: 0.7012 - val_accuracy: 0.5160\n",
      "Epoch 30/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6904 - accuracy: 0.5335 - val_loss: 0.6915 - val_accuracy: 0.5342\n",
      "Epoch 31/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6894 - accuracy: 0.5386 - val_loss: 0.6929 - val_accuracy: 0.5434\n",
      "Epoch 32/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6880 - accuracy: 0.5356 - val_loss: 0.6918 - val_accuracy: 0.5342\n",
      "Epoch 33/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6888 - accuracy: 0.5351 - val_loss: 0.6931 - val_accuracy: 0.5525\n",
      "Epoch 34/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6871 - accuracy: 0.5493 - val_loss: 0.6943 - val_accuracy: 0.5571\n",
      "Epoch 35/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6890 - accuracy: 0.5305 - val_loss: 0.6951 - val_accuracy: 0.4566\n",
      "Epoch 36/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6868 - accuracy: 0.5442 - val_loss: 0.6962 - val_accuracy: 0.4703\n",
      "Epoch 37/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6849 - accuracy: 0.5534 - val_loss: 0.7060 - val_accuracy: 0.4932\n",
      "Epoch 38/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6869 - accuracy: 0.5462 - val_loss: 0.7021 - val_accuracy: 0.4886\n",
      "Epoch 39/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6873 - accuracy: 0.5432 - val_loss: 0.6952 - val_accuracy: 0.5160\n",
      "Epoch 40/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6852 - accuracy: 0.5462 - val_loss: 0.6943 - val_accuracy: 0.5205\n",
      "Epoch 41/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6844 - accuracy: 0.5508 - val_loss: 0.7043 - val_accuracy: 0.4886\n",
      "Epoch 42/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6843 - accuracy: 0.5447 - val_loss: 0.7025 - val_accuracy: 0.4612\n",
      "Epoch 43/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6828 - accuracy: 0.5783 - val_loss: 0.6957 - val_accuracy: 0.5297\n",
      "Epoch 44/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6851 - accuracy: 0.5508 - val_loss: 0.7011 - val_accuracy: 0.4566\n",
      "Epoch 45/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6821 - accuracy: 0.5574 - val_loss: 0.7116 - val_accuracy: 0.4155\n",
      "Epoch 46/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6802 - accuracy: 0.5645 - val_loss: 0.6954 - val_accuracy: 0.5114\n",
      "Epoch 47/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6819 - accuracy: 0.5655 - val_loss: 0.6938 - val_accuracy: 0.4795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6812 - accuracy: 0.5549 - val_loss: 0.6952 - val_accuracy: 0.5114\n",
      "Epoch 49/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6771 - accuracy: 0.5681 - val_loss: 0.7124 - val_accuracy: 0.4840\n",
      "Epoch 50/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6822 - accuracy: 0.5534 - val_loss: 0.6886 - val_accuracy: 0.5205\n",
      "Epoch 51/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6781 - accuracy: 0.5757 - val_loss: 0.7014 - val_accuracy: 0.4840\n",
      "Epoch 52/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6784 - accuracy: 0.5630 - val_loss: 0.6873 - val_accuracy: 0.5251\n",
      "Epoch 53/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6788 - accuracy: 0.5549 - val_loss: 0.7087 - val_accuracy: 0.4886\n",
      "Epoch 54/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6770 - accuracy: 0.5696 - val_loss: 0.7097 - val_accuracy: 0.4384\n",
      "Epoch 55/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6750 - accuracy: 0.5843 - val_loss: 0.7035 - val_accuracy: 0.4977\n",
      "Epoch 56/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6767 - accuracy: 0.5610 - val_loss: 0.6976 - val_accuracy: 0.4977\n",
      "Epoch 57/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6703 - accuracy: 0.5818 - val_loss: 0.6998 - val_accuracy: 0.5023\n",
      "Epoch 58/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6750 - accuracy: 0.5727 - val_loss: 0.7002 - val_accuracy: 0.4932\n",
      "Epoch 59/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6694 - accuracy: 0.5823 - val_loss: 0.7209 - val_accuracy: 0.4521\n",
      "Epoch 60/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6731 - accuracy: 0.5833 - val_loss: 0.6865 - val_accuracy: 0.5068\n",
      "Epoch 61/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6686 - accuracy: 0.5996 - val_loss: 0.7200 - val_accuracy: 0.4886\n",
      "Epoch 62/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6703 - accuracy: 0.5864 - val_loss: 0.6955 - val_accuracy: 0.5388\n",
      "Epoch 63/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6646 - accuracy: 0.5981 - val_loss: 0.7042 - val_accuracy: 0.5068\n",
      "Epoch 64/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6648 - accuracy: 0.5879 - val_loss: 0.7040 - val_accuracy: 0.5114\n",
      "Epoch 65/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6617 - accuracy: 0.5915 - val_loss: 0.7338 - val_accuracy: 0.4566\n",
      "Epoch 66/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6614 - accuracy: 0.5971 - val_loss: 0.7173 - val_accuracy: 0.5023\n",
      "Epoch 67/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6597 - accuracy: 0.6062 - val_loss: 0.7427 - val_accuracy: 0.4475\n",
      "Epoch 68/1000\n",
      "1968/1968 [==============================] - 2s 1000us/step - loss: 0.6689 - accuracy: 0.5696 - val_loss: 0.7242 - val_accuracy: 0.4749\n",
      "Epoch 69/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6624 - accuracy: 0.5955 - val_loss: 0.7447 - val_accuracy: 0.4703\n",
      "Epoch 70/1000\n",
      "1968/1968 [==============================] - 2s 979us/step - loss: 0.6595 - accuracy: 0.6113 - val_loss: 0.7332 - val_accuracy: 0.4703\n",
      "Epoch 71/1000\n",
      "1968/1968 [==============================] - 2s 965us/step - loss: 0.6566 - accuracy: 0.6006 - val_loss: 0.7200 - val_accuracy: 0.4886\n",
      "Epoch 72/1000\n",
      "1968/1968 [==============================] - 2s 978us/step - loss: 0.6591 - accuracy: 0.5986 - val_loss: 0.7038 - val_accuracy: 0.4932\n",
      "Epoch 73/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6596 - accuracy: 0.6062 - val_loss: 0.7226 - val_accuracy: 0.5205\n",
      "Epoch 74/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6577 - accuracy: 0.6021 - val_loss: 0.7293 - val_accuracy: 0.4566\n",
      "Epoch 75/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6487 - accuracy: 0.6082 - val_loss: 0.7454 - val_accuracy: 0.5388\n",
      "Epoch 76/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6491 - accuracy: 0.6199 - val_loss: 0.7116 - val_accuracy: 0.5342\n",
      "Epoch 77/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6566 - accuracy: 0.6001 - val_loss: 0.7369 - val_accuracy: 0.4566\n",
      "Epoch 78/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6472 - accuracy: 0.6159 - val_loss: 0.7841 - val_accuracy: 0.4658\n",
      "Epoch 79/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6474 - accuracy: 0.6189 - val_loss: 0.7350 - val_accuracy: 0.4521\n",
      "Epoch 80/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6458 - accuracy: 0.6189 - val_loss: 0.7497 - val_accuracy: 0.4566\n",
      "Epoch 81/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6414 - accuracy: 0.6184 - val_loss: 0.7503 - val_accuracy: 0.4658\n",
      "Epoch 82/1000\n",
      "1968/1968 [==============================] - 2s 998us/step - loss: 0.6421 - accuracy: 0.6270 - val_loss: 0.7394 - val_accuracy: 0.4566\n",
      "Epoch 83/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6464 - accuracy: 0.6245 - val_loss: 0.7216 - val_accuracy: 0.4977\n",
      "Epoch 84/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6439 - accuracy: 0.6174 - val_loss: 0.7657 - val_accuracy: 0.4566\n",
      "Epoch 85/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6377 - accuracy: 0.6331 - val_loss: 0.7803 - val_accuracy: 0.4612\n",
      "Epoch 86/1000\n",
      "1968/1968 [==============================] - 2s 997us/step - loss: 0.6481 - accuracy: 0.6098 - val_loss: 0.7596 - val_accuracy: 0.4977\n",
      "Epoch 87/1000\n",
      "1968/1968 [==============================] - 2s 994us/step - loss: 0.6480 - accuracy: 0.6103 - val_loss: 0.7196 - val_accuracy: 0.4795\n",
      "Epoch 88/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6352 - accuracy: 0.6265 - val_loss: 0.7703 - val_accuracy: 0.4338\n",
      "Epoch 89/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6316 - accuracy: 0.6382 - val_loss: 0.7378 - val_accuracy: 0.4932\n",
      "Epoch 90/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6383 - accuracy: 0.6225 - val_loss: 0.7937 - val_accuracy: 0.4292\n",
      "Epoch 91/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6327 - accuracy: 0.6296 - val_loss: 0.7882 - val_accuracy: 0.4977\n",
      "Epoch 92/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6289 - accuracy: 0.6402 - val_loss: 0.7675 - val_accuracy: 0.4475\n",
      "Epoch 93/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6261 - accuracy: 0.6397 - val_loss: 0.7801 - val_accuracy: 0.4429\n",
      "Epoch 94/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6222 - accuracy: 0.6438 - val_loss: 0.7831 - val_accuracy: 0.4201\n",
      "Epoch 95/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6264 - accuracy: 0.6296 - val_loss: 0.8144 - val_accuracy: 0.4840\n",
      "Epoch 96/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6386 - accuracy: 0.6260 - val_loss: 0.7591 - val_accuracy: 0.4932\n",
      "Epoch 97/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6276 - accuracy: 0.6423 - val_loss: 0.7873 - val_accuracy: 0.4521\n",
      "Epoch 98/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6190 - accuracy: 0.6494 - val_loss: 0.8212 - val_accuracy: 0.4566\n",
      "Epoch 99/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6253 - accuracy: 0.6362 - val_loss: 0.7562 - val_accuracy: 0.4795\n",
      "Epoch 100/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6138 - accuracy: 0.6555 - val_loss: 0.7366 - val_accuracy: 0.5251\n",
      "Epoch 101/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6223 - accuracy: 0.6275 - val_loss: 0.7956 - val_accuracy: 0.5114\n",
      "Epoch 102/1000\n",
      "1968/1968 [==============================] - 2s 998us/step - loss: 0.6195 - accuracy: 0.6443 - val_loss: 0.7762 - val_accuracy: 0.4658\n",
      "Epoch 103/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6095 - accuracy: 0.6560 - val_loss: 0.8428 - val_accuracy: 0.4658\n",
      "Epoch 104/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6053 - accuracy: 0.6535 - val_loss: 0.8140 - val_accuracy: 0.4886\n",
      "Epoch 105/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6077 - accuracy: 0.6484 - val_loss: 0.7971 - val_accuracy: 0.4977\n",
      "Epoch 106/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6025 - accuracy: 0.6641 - val_loss: 0.8145 - val_accuracy: 0.4658\n",
      "Epoch 107/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6059 - accuracy: 0.6677 - val_loss: 0.8043 - val_accuracy: 0.4795\n",
      "Epoch 108/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6054 - accuracy: 0.6540 - val_loss: 0.7970 - val_accuracy: 0.4840\n",
      "Epoch 109/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6021 - accuracy: 0.6712 - val_loss: 0.8477 - val_accuracy: 0.5023\n",
      "Epoch 110/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5989 - accuracy: 0.6529 - val_loss: 0.7818 - val_accuracy: 0.5479\n",
      "Epoch 111/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6004 - accuracy: 0.6596 - val_loss: 0.8084 - val_accuracy: 0.4886\n",
      "Epoch 112/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6012 - accuracy: 0.6570 - val_loss: 0.8466 - val_accuracy: 0.4703\n",
      "Epoch 113/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6011 - accuracy: 0.6641 - val_loss: 0.8250 - val_accuracy: 0.4566\n",
      "Epoch 114/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5916 - accuracy: 0.6672 - val_loss: 0.7930 - val_accuracy: 0.4932\n",
      "Epoch 115/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5828 - accuracy: 0.6692 - val_loss: 0.8328 - val_accuracy: 0.5023\n",
      "Epoch 116/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5819 - accuracy: 0.6794 - val_loss: 0.8095 - val_accuracy: 0.4840\n",
      "Epoch 117/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5862 - accuracy: 0.6662 - val_loss: 0.8077 - val_accuracy: 0.4795\n",
      "Epoch 118/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5839 - accuracy: 0.6743 - val_loss: 0.9048 - val_accuracy: 0.4795\n",
      "Epoch 119/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5924 - accuracy: 0.6631 - val_loss: 0.8298 - val_accuracy: 0.4749\n",
      "Epoch 120/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5765 - accuracy: 0.6677 - val_loss: 0.8380 - val_accuracy: 0.4886\n",
      "Epoch 121/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5773 - accuracy: 0.6753 - val_loss: 0.8758 - val_accuracy: 0.4932\n",
      "Epoch 122/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5846 - accuracy: 0.6738 - val_loss: 0.8326 - val_accuracy: 0.4749\n",
      "Epoch 123/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5784 - accuracy: 0.6748 - val_loss: 0.8607 - val_accuracy: 0.4886\n",
      "Epoch 124/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5600 - accuracy: 0.6870 - val_loss: 0.9330 - val_accuracy: 0.4749\n",
      "Epoch 125/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5842 - accuracy: 0.6748 - val_loss: 0.8137 - val_accuracy: 0.4703\n",
      "Epoch 126/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5662 - accuracy: 0.6733 - val_loss: 0.9027 - val_accuracy: 0.4658\n",
      "Epoch 127/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5790 - accuracy: 0.6799 - val_loss: 0.8248 - val_accuracy: 0.4521\n",
      "Epoch 128/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5845 - accuracy: 0.6707 - val_loss: 0.9306 - val_accuracy: 0.4703\n",
      "Epoch 129/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5646 - accuracy: 0.6773 - val_loss: 0.8901 - val_accuracy: 0.4932\n",
      "Epoch 130/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5464 - accuracy: 0.6997 - val_loss: 0.9503 - val_accuracy: 0.4521\n",
      "Epoch 131/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5674 - accuracy: 0.6829 - val_loss: 0.8231 - val_accuracy: 0.5388\n",
      "Epoch 132/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5515 - accuracy: 0.6961 - val_loss: 0.9397 - val_accuracy: 0.4247\n",
      "Epoch 133/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5428 - accuracy: 0.6972 - val_loss: 0.9245 - val_accuracy: 0.4749\n",
      "Epoch 134/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5517 - accuracy: 0.6916 - val_loss: 0.9447 - val_accuracy: 0.4703\n",
      "Epoch 135/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5444 - accuracy: 0.6997 - val_loss: 0.9995 - val_accuracy: 0.5023\n",
      "Epoch 136/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5343 - accuracy: 0.7068 - val_loss: 0.9600 - val_accuracy: 0.4886\n",
      "Epoch 137/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5505 - accuracy: 0.6961 - val_loss: 0.8754 - val_accuracy: 0.4566\n",
      "Epoch 138/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5483 - accuracy: 0.6916 - val_loss: 0.9924 - val_accuracy: 0.4384\n",
      "Epoch 139/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5230 - accuracy: 0.7149 - val_loss: 1.0418 - val_accuracy: 0.4292\n",
      "Epoch 140/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5189 - accuracy: 0.7226 - val_loss: 0.9388 - val_accuracy: 0.4566\n",
      "Epoch 141/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5312 - accuracy: 0.7088 - val_loss: 1.0179 - val_accuracy: 0.4840\n",
      "Epoch 142/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5234 - accuracy: 0.7190 - val_loss: 1.0417 - val_accuracy: 0.4703\n",
      "Epoch 143/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5182 - accuracy: 0.7078 - val_loss: 1.0251 - val_accuracy: 0.4840\n",
      "Epoch 144/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5099 - accuracy: 0.7134 - val_loss: 1.0255 - val_accuracy: 0.5205\n",
      "Epoch 145/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5078 - accuracy: 0.7348 - val_loss: 1.0012 - val_accuracy: 0.5068\n",
      "Epoch 146/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5138 - accuracy: 0.7221 - val_loss: 1.0020 - val_accuracy: 0.4749\n",
      "Epoch 147/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5172 - accuracy: 0.7154 - val_loss: 0.9164 - val_accuracy: 0.5114\n",
      "Epoch 148/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5003 - accuracy: 0.7368 - val_loss: 0.9936 - val_accuracy: 0.4840\n",
      "Epoch 149/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5037 - accuracy: 0.7231 - val_loss: 1.0146 - val_accuracy: 0.4521\n",
      "Epoch 150/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5039 - accuracy: 0.7119 - val_loss: 1.0491 - val_accuracy: 0.4566\n",
      "Epoch 151/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5151 - accuracy: 0.7195 - val_loss: 1.0643 - val_accuracy: 0.4886\n",
      "Epoch 152/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4863 - accuracy: 0.7368 - val_loss: 1.0480 - val_accuracy: 0.4795\n",
      "Epoch 153/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4849 - accuracy: 0.7419 - val_loss: 1.1389 - val_accuracy: 0.4658\n",
      "Epoch 154/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5022 - accuracy: 0.7271 - val_loss: 1.0881 - val_accuracy: 0.4703\n",
      "Epoch 155/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5041 - accuracy: 0.7302 - val_loss: 0.9929 - val_accuracy: 0.4795\n",
      "Epoch 156/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5067 - accuracy: 0.7266 - val_loss: 0.9939 - val_accuracy: 0.4612\n",
      "Epoch 157/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4880 - accuracy: 0.7368 - val_loss: 1.0701 - val_accuracy: 0.4795\n",
      "Epoch 158/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4812 - accuracy: 0.7449 - val_loss: 1.0531 - val_accuracy: 0.5297\n",
      "Epoch 159/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4714 - accuracy: 0.7434 - val_loss: 1.1627 - val_accuracy: 0.4247\n",
      "Epoch 160/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4797 - accuracy: 0.7419 - val_loss: 1.0976 - val_accuracy: 0.4886\n",
      "Epoch 161/1000\n",
      "1968/1968 [==============================] - 2s 998us/step - loss: 0.4768 - accuracy: 0.7434 - val_loss: 1.0153 - val_accuracy: 0.4703\n",
      "Epoch 162/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4597 - accuracy: 0.7586 - val_loss: 1.0393 - val_accuracy: 0.5205\n",
      "Epoch 163/1000\n",
      "1968/1968 [==============================] - 2s 991us/step - loss: 0.4739 - accuracy: 0.7388 - val_loss: 1.1401 - val_accuracy: 0.4932\n",
      "Epoch 164/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4579 - accuracy: 0.7571 - val_loss: 1.1024 - val_accuracy: 0.4795\n",
      "Epoch 165/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4373 - accuracy: 0.7703 - val_loss: 1.1491 - val_accuracy: 0.5023\n",
      "Epoch 166/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4459 - accuracy: 0.7607 - val_loss: 1.0578 - val_accuracy: 0.5434\n",
      "Epoch 167/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4596 - accuracy: 0.7576 - val_loss: 1.0483 - val_accuracy: 0.5023\n",
      "Epoch 168/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4587 - accuracy: 0.7627 - val_loss: 1.1779 - val_accuracy: 0.4475\n",
      "Epoch 169/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4340 - accuracy: 0.7724 - val_loss: 1.1747 - val_accuracy: 0.5160\n",
      "Epoch 170/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4159 - accuracy: 0.7739 - val_loss: 1.1933 - val_accuracy: 0.4703\n",
      "Epoch 171/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4285 - accuracy: 0.7703 - val_loss: 1.1129 - val_accuracy: 0.5160\n",
      "Epoch 172/1000\n",
      "1968/1968 [==============================] - 2s 996us/step - loss: 0.4268 - accuracy: 0.7734 - val_loss: 1.1134 - val_accuracy: 0.5023\n",
      "Epoch 173/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4331 - accuracy: 0.7658 - val_loss: 1.1759 - val_accuracy: 0.5114\n",
      "Epoch 174/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4188 - accuracy: 0.7810 - val_loss: 1.2701 - val_accuracy: 0.5023\n",
      "Epoch 175/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4299 - accuracy: 0.7703 - val_loss: 1.2755 - val_accuracy: 0.5068\n",
      "Epoch 176/1000\n",
      "1968/1968 [==============================] - 2s 992us/step - loss: 0.4512 - accuracy: 0.7541 - val_loss: 1.2346 - val_accuracy: 0.4932\n",
      "Epoch 177/1000\n",
      "1968/1968 [==============================] - 2s 989us/step - loss: 0.4184 - accuracy: 0.7830 - val_loss: 1.2015 - val_accuracy: 0.5160\n",
      "Epoch 178/1000\n",
      "1968/1968 [==============================] - 2s 996us/step - loss: 0.4205 - accuracy: 0.7876 - val_loss: 1.1858 - val_accuracy: 0.5434\n",
      "Epoch 179/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4322 - accuracy: 0.7779 - val_loss: 1.3149 - val_accuracy: 0.4886\n",
      "Epoch 180/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4115 - accuracy: 0.7891 - val_loss: 1.1450 - val_accuracy: 0.5160\n",
      "Epoch 181/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4042 - accuracy: 0.7825 - val_loss: 1.2264 - val_accuracy: 0.5297\n",
      "Epoch 182/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3996 - accuracy: 0.7937 - val_loss: 1.3582 - val_accuracy: 0.4977\n",
      "Epoch 183/1000\n",
      "1968/1968 [==============================] - 2s 996us/step - loss: 0.3979 - accuracy: 0.7912 - val_loss: 1.2519 - val_accuracy: 0.5160\n",
      "Epoch 184/1000\n",
      "1968/1968 [==============================] - 2s 990us/step - loss: 0.3922 - accuracy: 0.7967 - val_loss: 1.3490 - val_accuracy: 0.5297\n",
      "Epoch 185/1000\n",
      "1968/1968 [==============================] - 2s 984us/step - loss: 0.3855 - accuracy: 0.7962 - val_loss: 1.2943 - val_accuracy: 0.5251\n",
      "Epoch 186/1000\n",
      "1968/1968 [==============================] - 2s 970us/step - loss: 0.4000 - accuracy: 0.7947 - val_loss: 1.2458 - val_accuracy: 0.5205\n",
      "Epoch 187/1000\n",
      "1968/1968 [==============================] - 2s 988us/step - loss: 0.3943 - accuracy: 0.7835 - val_loss: 1.4841 - val_accuracy: 0.4977\n",
      "Epoch 188/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4011 - accuracy: 0.7978 - val_loss: 1.3315 - val_accuracy: 0.5023\n",
      "Epoch 189/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3690 - accuracy: 0.8084 - val_loss: 1.3771 - val_accuracy: 0.5068\n",
      "Epoch 190/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3649 - accuracy: 0.8064 - val_loss: 1.3293 - val_accuracy: 0.5205\n",
      "Epoch 191/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3764 - accuracy: 0.8044 - val_loss: 1.3573 - val_accuracy: 0.4977\n",
      "Epoch 192/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3851 - accuracy: 0.7937 - val_loss: 1.3711 - val_accuracy: 0.4886\n",
      "Epoch 193/1000\n",
      "1968/1968 [==============================] - 2s 991us/step - loss: 0.3518 - accuracy: 0.8252 - val_loss: 1.3506 - val_accuracy: 0.5297\n",
      "Epoch 194/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3274 - accuracy: 0.8415 - val_loss: 1.5998 - val_accuracy: 0.5068\n",
      "Epoch 195/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3483 - accuracy: 0.8140 - val_loss: 1.5770 - val_accuracy: 0.4932\n",
      "Epoch 196/1000\n",
      "1968/1968 [==============================] - 2s 993us/step - loss: 0.3466 - accuracy: 0.8196 - val_loss: 1.4790 - val_accuracy: 0.4977\n",
      "Epoch 197/1000\n",
      "1968/1968 [==============================] - 2s 963us/step - loss: 0.3679 - accuracy: 0.8155 - val_loss: 1.6357 - val_accuracy: 0.5068\n",
      "Epoch 198/1000\n",
      "1968/1968 [==============================] - 2s 977us/step - loss: 0.3516 - accuracy: 0.8252 - val_loss: 1.5999 - val_accuracy: 0.4977\n",
      "Epoch 199/1000\n",
      "1968/1968 [==============================] - 2s 999us/step - loss: 0.3412 - accuracy: 0.8242 - val_loss: 1.4876 - val_accuracy: 0.5251\n",
      "Epoch 200/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3396 - accuracy: 0.8206 - val_loss: 1.4762 - val_accuracy: 0.5388\n",
      "Epoch 201/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3594 - accuracy: 0.8262 - val_loss: 1.4757 - val_accuracy: 0.5114\n",
      "Epoch 202/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3229 - accuracy: 0.8318 - val_loss: 1.6784 - val_accuracy: 0.5160\n",
      "Epoch 203/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3526 - accuracy: 0.8140 - val_loss: 1.3941 - val_accuracy: 0.5205\n",
      "Epoch 204/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3559 - accuracy: 0.8110 - val_loss: 1.4461 - val_accuracy: 0.5297\n",
      "Epoch 205/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3244 - accuracy: 0.8354 - val_loss: 1.5590 - val_accuracy: 0.5342\n",
      "Epoch 206/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3079 - accuracy: 0.8425 - val_loss: 1.6024 - val_accuracy: 0.4840\n",
      "Epoch 207/1000\n",
      "1968/1968 [==============================] - 2s 967us/step - loss: 0.3327 - accuracy: 0.8445 - val_loss: 1.4888 - val_accuracy: 0.4977\n",
      "Epoch 208/1000\n",
      "1968/1968 [==============================] - 2s 989us/step - loss: 0.3302 - accuracy: 0.8349 - val_loss: 1.6240 - val_accuracy: 0.5388\n",
      "Epoch 209/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2904 - accuracy: 0.8598 - val_loss: 1.6600 - val_accuracy: 0.5205\n",
      "Epoch 210/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2974 - accuracy: 0.8537 - val_loss: 1.7274 - val_accuracy: 0.5342\n",
      "Epoch 211/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2783 - accuracy: 0.8633 - val_loss: 1.5896 - val_accuracy: 0.5205\n",
      "Epoch 212/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2854 - accuracy: 0.8674 - val_loss: 1.6866 - val_accuracy: 0.5342\n",
      "Epoch 213/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2762 - accuracy: 0.8694 - val_loss: 1.7907 - val_accuracy: 0.5068\n",
      "Epoch 214/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2707 - accuracy: 0.8755 - val_loss: 1.7822 - val_accuracy: 0.4977\n",
      "Epoch 215/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2693 - accuracy: 0.8786 - val_loss: 1.8179 - val_accuracy: 0.4977\n",
      "Epoch 216/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2868 - accuracy: 0.8659 - val_loss: 1.6669 - val_accuracy: 0.5114\n",
      "Epoch 217/1000\n",
      "1968/1968 [==============================] - 2s 982us/step - loss: 0.2781 - accuracy: 0.8618 - val_loss: 1.7804 - val_accuracy: 0.5297\n",
      "Epoch 218/1000\n",
      "1968/1968 [==============================] - 2s 976us/step - loss: 0.2990 - accuracy: 0.8521 - val_loss: 1.7877 - val_accuracy: 0.4977\n",
      "Epoch 219/1000\n",
      "1968/1968 [==============================] - 2s 967us/step - loss: 0.2665 - accuracy: 0.8699 - val_loss: 1.7481 - val_accuracy: 0.5662\n",
      "Epoch 220/1000\n",
      "1968/1968 [==============================] - 2s 975us/step - loss: 0.2736 - accuracy: 0.8694 - val_loss: 1.7509 - val_accuracy: 0.5114\n",
      "Epoch 221/1000\n",
      "1968/1968 [==============================] - 2s 982us/step - loss: 0.2900 - accuracy: 0.8577 - val_loss: 1.6904 - val_accuracy: 0.5297\n",
      "Epoch 222/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2688 - accuracy: 0.8720 - val_loss: 1.6633 - val_accuracy: 0.5708\n",
      "Epoch 223/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2661 - accuracy: 0.8740 - val_loss: 1.7691 - val_accuracy: 0.4795\n",
      "Epoch 224/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2615 - accuracy: 0.8775 - val_loss: 1.8142 - val_accuracy: 0.5205\n",
      "Epoch 225/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2551 - accuracy: 0.8831 - val_loss: 1.8624 - val_accuracy: 0.5023\n",
      "Epoch 226/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2606 - accuracy: 0.8755 - val_loss: 1.8634 - val_accuracy: 0.5616\n",
      "Epoch 227/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2847 - accuracy: 0.8633 - val_loss: 1.8080 - val_accuracy: 0.5160\n",
      "Epoch 228/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2975 - accuracy: 0.8577 - val_loss: 1.8082 - val_accuracy: 0.4840\n",
      "Epoch 229/1000\n",
      "1968/1968 [==============================] - 2s 971us/step - loss: 0.2702 - accuracy: 0.8750 - val_loss: 1.8521 - val_accuracy: 0.5068\n",
      "Epoch 230/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2984 - accuracy: 0.8532 - val_loss: 2.0011 - val_accuracy: 0.4749\n",
      "Epoch 231/1000\n",
      "1968/1968 [==============================] - 2s 990us/step - loss: 0.2355 - accuracy: 0.8938 - val_loss: 1.8958 - val_accuracy: 0.5160\n",
      "Epoch 232/1000\n",
      "1968/1968 [==============================] - 2s 994us/step - loss: 0.2433 - accuracy: 0.8933 - val_loss: 1.9126 - val_accuracy: 0.4840\n",
      "Epoch 233/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2105 - accuracy: 0.9090 - val_loss: 1.9795 - val_accuracy: 0.4977\n",
      "Epoch 234/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2161 - accuracy: 0.9055 - val_loss: 1.9267 - val_accuracy: 0.5205\n",
      "Epoch 235/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2206 - accuracy: 0.8999 - val_loss: 1.9807 - val_accuracy: 0.4795\n",
      "Epoch 236/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2243 - accuracy: 0.8963 - val_loss: 2.0644 - val_accuracy: 0.5023\n",
      "Epoch 237/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2138 - accuracy: 0.9065 - val_loss: 2.0891 - val_accuracy: 0.5114\n",
      "Epoch 238/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2024 - accuracy: 0.9141 - val_loss: 2.0806 - val_accuracy: 0.5023\n",
      "Epoch 239/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2073 - accuracy: 0.9090 - val_loss: 2.1486 - val_accuracy: 0.5160\n",
      "Epoch 240/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2003 - accuracy: 0.9157 - val_loss: 2.1830 - val_accuracy: 0.4658\n",
      "Epoch 241/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2008 - accuracy: 0.9085 - val_loss: 2.1165 - val_accuracy: 0.5023\n",
      "Epoch 242/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2086 - accuracy: 0.9065 - val_loss: 2.4454 - val_accuracy: 0.4475\n",
      "Epoch 243/1000\n",
      "1968/1968 [==============================] - 2s 998us/step - loss: 0.2189 - accuracy: 0.8958 - val_loss: 2.2649 - val_accuracy: 0.4977\n",
      "Epoch 244/1000\n",
      "1968/1968 [==============================] - 2s 999us/step - loss: 0.1900 - accuracy: 0.9126 - val_loss: 2.0695 - val_accuracy: 0.5160\n",
      "Epoch 245/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1982 - accuracy: 0.9116 - val_loss: 2.2297 - val_accuracy: 0.4886\n",
      "Epoch 246/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1891 - accuracy: 0.9182 - val_loss: 2.2463 - val_accuracy: 0.4886\n",
      "Epoch 247/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1886 - accuracy: 0.9126 - val_loss: 2.2135 - val_accuracy: 0.5068\n",
      "Epoch 248/1000\n",
      "1968/1968 [==============================] - 2s 989us/step - loss: 0.1721 - accuracy: 0.9243 - val_loss: 2.3821 - val_accuracy: 0.4658\n",
      "Epoch 249/1000\n",
      "1968/1968 [==============================] - 2s 985us/step - loss: 0.2092 - accuracy: 0.9045 - val_loss: 2.2001 - val_accuracy: 0.4977\n",
      "Epoch 250/1000\n",
      "1968/1968 [==============================] - 2s 992us/step - loss: 0.2243 - accuracy: 0.9004 - val_loss: 2.3416 - val_accuracy: 0.4658\n",
      "Epoch 251/1000\n",
      "1968/1968 [==============================] - 2s 995us/step - loss: 0.2734 - accuracy: 0.8750 - val_loss: 1.9938 - val_accuracy: 0.4932\n",
      "Epoch 252/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2247 - accuracy: 0.9045 - val_loss: 2.2144 - val_accuracy: 0.4795\n",
      "Epoch 253/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2035 - accuracy: 0.9162 - val_loss: 2.2341 - val_accuracy: 0.4932\n",
      "Epoch 254/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1762 - accuracy: 0.9172 - val_loss: 2.0819 - val_accuracy: 0.5251\n",
      "Epoch 255/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1841 - accuracy: 0.9212 - val_loss: 2.2314 - val_accuracy: 0.4703\n",
      "Epoch 256/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1652 - accuracy: 0.9319 - val_loss: 2.2624 - val_accuracy: 0.4612\n",
      "Epoch 257/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1415 - accuracy: 0.9441 - val_loss: 2.3877 - val_accuracy: 0.4932\n",
      "Epoch 258/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1642 - accuracy: 0.9319 - val_loss: 2.5571 - val_accuracy: 0.4566\n",
      "Epoch 259/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1745 - accuracy: 0.9284 - val_loss: 2.4668 - val_accuracy: 0.4749\n",
      "Epoch 260/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2187 - accuracy: 0.8989 - val_loss: 2.2726 - val_accuracy: 0.4932\n",
      "Epoch 00260: early stopping\n",
      "30 day\n",
      "\n",
      "# Evaluate on test data\n",
      "244/244 [==============================] - 0s 407us/step\n",
      "test loss, test acc: [2.709462697388696, 0.4549180269241333]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (244, 1)\n",
      "rmse: 0.6776633592510598\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 30\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "print(\"30 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(30, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(30, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_48 (LSTM)               (None, 30, 50)            28600     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 30, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 30, 75)            37800     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 30, 75)            0         \n",
      "_________________________________________________________________\n",
      "lstm_50 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1968 samples, validate on 219 samples\n",
      "Epoch 1/1000\n",
      "1968/1968 [==============================] - 3s 2ms/step - loss: 0.6957 - accuracy: 0.4990 - val_loss: 0.6897 - val_accuracy: 0.5571\n",
      "Epoch 2/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.6920 - accuracy: 0.5188 - val_loss: 0.6897 - val_accuracy: 0.5571\n",
      "Epoch 3/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.6936 - accuracy: 0.5340 - val_loss: 0.6938 - val_accuracy: 0.5068\n",
      "Epoch 4/1000\n",
      "1968/1968 [==============================] - 2s 866us/step - loss: 0.6917 - accuracy: 0.5361 - val_loss: 0.6904 - val_accuracy: 0.5571\n",
      "Epoch 5/1000\n",
      "1968/1968 [==============================] - 2s 881us/step - loss: 0.6917 - accuracy: 0.5427 - val_loss: 0.6929 - val_accuracy: 0.5205\n",
      "Epoch 6/1000\n",
      "1968/1968 [==============================] - 2s 870us/step - loss: 0.6908 - accuracy: 0.5249 - val_loss: 0.6921 - val_accuracy: 0.5388\n",
      "Epoch 7/1000\n",
      "1968/1968 [==============================] - 2s 896us/step - loss: 0.6927 - accuracy: 0.5274 - val_loss: 0.6928 - val_accuracy: 0.5160\n",
      "Epoch 8/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.6907 - accuracy: 0.5310 - val_loss: 0.6910 - val_accuracy: 0.5525\n",
      "Epoch 9/1000\n",
      "1968/1968 [==============================] - 2s 842us/step - loss: 0.6907 - accuracy: 0.5274 - val_loss: 0.6926 - val_accuracy: 0.5205\n",
      "Epoch 10/1000\n",
      "1968/1968 [==============================] - 2s 850us/step - loss: 0.6904 - accuracy: 0.5422 - val_loss: 0.6958 - val_accuracy: 0.5251\n",
      "Epoch 11/1000\n",
      "1968/1968 [==============================] - 2s 865us/step - loss: 0.6908 - accuracy: 0.5346 - val_loss: 0.6998 - val_accuracy: 0.5160\n",
      "Epoch 12/1000\n",
      "1968/1968 [==============================] - 2s 840us/step - loss: 0.6913 - accuracy: 0.5366 - val_loss: 0.6957 - val_accuracy: 0.5114\n",
      "Epoch 13/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.6927 - accuracy: 0.5152 - val_loss: 0.6926 - val_accuracy: 0.5251\n",
      "Epoch 14/1000\n",
      "1968/1968 [==============================] - 2s 897us/step - loss: 0.6906 - accuracy: 0.5371 - val_loss: 0.6942 - val_accuracy: 0.5160\n",
      "Epoch 15/1000\n",
      "1968/1968 [==============================] - 2s 882us/step - loss: 0.6901 - accuracy: 0.5330 - val_loss: 0.7002 - val_accuracy: 0.4977\n",
      "Epoch 16/1000\n",
      "1968/1968 [==============================] - 2s 863us/step - loss: 0.6903 - accuracy: 0.5381 - val_loss: 0.6946 - val_accuracy: 0.5160\n",
      "Epoch 17/1000\n",
      "1968/1968 [==============================] - 2s 883us/step - loss: 0.6907 - accuracy: 0.5295 - val_loss: 0.6967 - val_accuracy: 0.5251\n",
      "Epoch 18/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.6884 - accuracy: 0.5483 - val_loss: 0.7020 - val_accuracy: 0.5068\n",
      "Epoch 19/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.6889 - accuracy: 0.5305 - val_loss: 0.7013 - val_accuracy: 0.5479\n",
      "Epoch 20/1000\n",
      "1968/1968 [==============================] - 2s 840us/step - loss: 0.6921 - accuracy: 0.5340 - val_loss: 0.6919 - val_accuracy: 0.5479\n",
      "Epoch 21/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.6917 - accuracy: 0.5340 - val_loss: 0.6941 - val_accuracy: 0.5342\n",
      "Epoch 22/1000\n",
      "1968/1968 [==============================] - 2s 815us/step - loss: 0.6904 - accuracy: 0.5361 - val_loss: 0.7016 - val_accuracy: 0.5068\n",
      "Epoch 23/1000\n",
      "1968/1968 [==============================] - 2s 813us/step - loss: 0.6896 - accuracy: 0.5432 - val_loss: 0.6987 - val_accuracy: 0.5114\n",
      "Epoch 24/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.6886 - accuracy: 0.5437 - val_loss: 0.7042 - val_accuracy: 0.4977\n",
      "Epoch 25/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6903 - accuracy: 0.5335 - val_loss: 0.6928 - val_accuracy: 0.5525\n",
      "Epoch 26/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6931 - accuracy: 0.5310 - val_loss: 0.6930 - val_accuracy: 0.5205\n",
      "Epoch 27/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6905 - accuracy: 0.5401 - val_loss: 0.6978 - val_accuracy: 0.5342\n",
      "Epoch 28/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6891 - accuracy: 0.5442 - val_loss: 0.6926 - val_accuracy: 0.5479\n",
      "Epoch 29/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.6890 - accuracy: 0.5488 - val_loss: 0.6983 - val_accuracy: 0.5160\n",
      "Epoch 30/1000\n",
      "1968/1968 [==============================] - 2s 843us/step - loss: 0.6893 - accuracy: 0.5356 - val_loss: 0.6953 - val_accuracy: 0.5205\n",
      "Epoch 31/1000\n",
      "1968/1968 [==============================] - 2s 815us/step - loss: 0.6883 - accuracy: 0.5407 - val_loss: 0.7001 - val_accuracy: 0.5160\n",
      "Epoch 32/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6910 - accuracy: 0.5229 - val_loss: 0.6902 - val_accuracy: 0.5525\n",
      "Epoch 33/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6928 - accuracy: 0.5295 - val_loss: 0.6921 - val_accuracy: 0.5297\n",
      "Epoch 34/1000\n",
      "1968/1968 [==============================] - 2s 822us/step - loss: 0.6882 - accuracy: 0.5386 - val_loss: 0.6957 - val_accuracy: 0.5251\n",
      "Epoch 35/1000\n",
      "1968/1968 [==============================] - 2s 826us/step - loss: 0.6885 - accuracy: 0.5330 - val_loss: 0.6991 - val_accuracy: 0.4612\n",
      "Epoch 36/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.6888 - accuracy: 0.5371 - val_loss: 0.6971 - val_accuracy: 0.5068\n",
      "Epoch 37/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6877 - accuracy: 0.5422 - val_loss: 0.7019 - val_accuracy: 0.5114\n",
      "Epoch 38/1000\n",
      "1968/1968 [==============================] - 2s 826us/step - loss: 0.6908 - accuracy: 0.5437 - val_loss: 0.6937 - val_accuracy: 0.5297\n",
      "Epoch 39/1000\n",
      "1968/1968 [==============================] - 2s 832us/step - loss: 0.6879 - accuracy: 0.5473 - val_loss: 0.6965 - val_accuracy: 0.5251\n",
      "Epoch 40/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6869 - accuracy: 0.5401 - val_loss: 0.7019 - val_accuracy: 0.4795\n",
      "Epoch 41/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6852 - accuracy: 0.5523 - val_loss: 0.7042 - val_accuracy: 0.4384\n",
      "Epoch 42/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.6894 - accuracy: 0.5340 - val_loss: 0.6938 - val_accuracy: 0.5434\n",
      "Epoch 43/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.6874 - accuracy: 0.5417 - val_loss: 0.6981 - val_accuracy: 0.5068\n",
      "Epoch 44/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6860 - accuracy: 0.5518 - val_loss: 0.6995 - val_accuracy: 0.5114\n",
      "Epoch 45/1000\n",
      "1968/1968 [==============================] - 2s 832us/step - loss: 0.6861 - accuracy: 0.5401 - val_loss: 0.7031 - val_accuracy: 0.4703\n",
      "Epoch 46/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6840 - accuracy: 0.5473 - val_loss: 0.7039 - val_accuracy: 0.5388\n",
      "Epoch 47/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6885 - accuracy: 0.5386 - val_loss: 0.6990 - val_accuracy: 0.4932\n",
      "Epoch 48/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.6880 - accuracy: 0.5462 - val_loss: 0.6973 - val_accuracy: 0.5114\n",
      "Epoch 49/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.6864 - accuracy: 0.5432 - val_loss: 0.6995 - val_accuracy: 0.5251\n",
      "Epoch 50/1000\n",
      "1968/1968 [==============================] - 2s 818us/step - loss: 0.6850 - accuracy: 0.5376 - val_loss: 0.7047 - val_accuracy: 0.4977\n",
      "Epoch 51/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.6833 - accuracy: 0.5320 - val_loss: 0.7008 - val_accuracy: 0.5068\n",
      "Epoch 52/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.6834 - accuracy: 0.5574 - val_loss: 0.7074 - val_accuracy: 0.5023\n",
      "Epoch 53/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6839 - accuracy: 0.5544 - val_loss: 0.7040 - val_accuracy: 0.4840\n",
      "Epoch 54/1000\n",
      "1968/1968 [==============================] - 2s 826us/step - loss: 0.6837 - accuracy: 0.5473 - val_loss: 0.6960 - val_accuracy: 0.4658\n",
      "Epoch 55/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6840 - accuracy: 0.5569 - val_loss: 0.6965 - val_accuracy: 0.4840\n",
      "Epoch 56/1000\n",
      "1968/1968 [==============================] - 2s 832us/step - loss: 0.6846 - accuracy: 0.5462 - val_loss: 0.6992 - val_accuracy: 0.5205\n",
      "Epoch 57/1000\n",
      "1968/1968 [==============================] - 2s 823us/step - loss: 0.6877 - accuracy: 0.5330 - val_loss: 0.6917 - val_accuracy: 0.5388\n",
      "Epoch 58/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6873 - accuracy: 0.5432 - val_loss: 0.6978 - val_accuracy: 0.5205\n",
      "Epoch 59/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.6839 - accuracy: 0.5401 - val_loss: 0.7047 - val_accuracy: 0.5205\n",
      "Epoch 60/1000\n",
      "1968/1968 [==============================] - 2s 849us/step - loss: 0.6874 - accuracy: 0.5432 - val_loss: 0.7043 - val_accuracy: 0.4658\n",
      "Epoch 61/1000\n",
      "1968/1968 [==============================] - 2s 869us/step - loss: 0.6824 - accuracy: 0.5447 - val_loss: 0.7136 - val_accuracy: 0.4658\n",
      "Epoch 62/1000\n",
      "1968/1968 [==============================] - 2s 889us/step - loss: 0.6816 - accuracy: 0.5610 - val_loss: 0.6970 - val_accuracy: 0.5205\n",
      "Epoch 63/1000\n",
      "1968/1968 [==============================] - 2s 935us/step - loss: 0.6789 - accuracy: 0.5676 - val_loss: 0.7144 - val_accuracy: 0.4201\n",
      "Epoch 64/1000\n",
      "1968/1968 [==============================] - 2s 964us/step - loss: 0.6797 - accuracy: 0.5645 - val_loss: 0.6968 - val_accuracy: 0.4886\n",
      "Epoch 65/1000\n",
      "1968/1968 [==============================] - 2s 984us/step - loss: 0.6803 - accuracy: 0.5493 - val_loss: 0.7025 - val_accuracy: 0.5068\n",
      "Epoch 66/1000\n",
      "1968/1968 [==============================] - 2s 992us/step - loss: 0.6770 - accuracy: 0.5696 - val_loss: 0.7111 - val_accuracy: 0.4566\n",
      "Epoch 67/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6763 - accuracy: 0.5803 - val_loss: 0.7042 - val_accuracy: 0.4886\n",
      "Epoch 68/1000\n",
      "1968/1968 [==============================] - 2s 970us/step - loss: 0.6719 - accuracy: 0.5899 - val_loss: 0.7130 - val_accuracy: 0.4612\n",
      "Epoch 69/1000\n",
      "1968/1968 [==============================] - 2s 914us/step - loss: 0.6792 - accuracy: 0.5737 - val_loss: 0.6979 - val_accuracy: 0.4886\n",
      "Epoch 70/1000\n",
      "1968/1968 [==============================] - 2s 879us/step - loss: 0.6761 - accuracy: 0.5783 - val_loss: 0.7202 - val_accuracy: 0.4566\n",
      "Epoch 71/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.6731 - accuracy: 0.5747 - val_loss: 0.7134 - val_accuracy: 0.4612\n",
      "Epoch 72/1000\n",
      "1968/1968 [==============================] - 2s 863us/step - loss: 0.6751 - accuracy: 0.5772 - val_loss: 0.7136 - val_accuracy: 0.4795\n",
      "Epoch 73/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.6703 - accuracy: 0.5716 - val_loss: 0.7072 - val_accuracy: 0.4566\n",
      "Epoch 74/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6690 - accuracy: 0.5869 - val_loss: 0.7113 - val_accuracy: 0.5251\n",
      "Epoch 75/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.6719 - accuracy: 0.5996 - val_loss: 0.7335 - val_accuracy: 0.5023\n",
      "Epoch 76/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.6756 - accuracy: 0.5732 - val_loss: 0.7142 - val_accuracy: 0.5023\n",
      "Epoch 77/1000\n",
      "1968/1968 [==============================] - 2s 853us/step - loss: 0.6651 - accuracy: 0.6052 - val_loss: 0.7420 - val_accuracy: 0.5023\n",
      "Epoch 78/1000\n",
      "1968/1968 [==============================] - 2s 894us/step - loss: 0.6790 - accuracy: 0.5783 - val_loss: 0.6990 - val_accuracy: 0.5662\n",
      "Epoch 79/1000\n",
      "1968/1968 [==============================] - 2s 890us/step - loss: 0.6723 - accuracy: 0.5874 - val_loss: 0.7113 - val_accuracy: 0.5023\n",
      "Epoch 80/1000\n",
      "1968/1968 [==============================] - 2s 864us/step - loss: 0.6623 - accuracy: 0.6103 - val_loss: 0.7242 - val_accuracy: 0.4749\n",
      "Epoch 81/1000\n",
      "1968/1968 [==============================] - 2s 876us/step - loss: 0.6725 - accuracy: 0.5813 - val_loss: 0.7095 - val_accuracy: 0.5023\n",
      "Epoch 82/1000\n",
      "1968/1968 [==============================] - 2s 880us/step - loss: 0.6593 - accuracy: 0.6092 - val_loss: 0.7471 - val_accuracy: 0.4658\n",
      "Epoch 83/1000\n",
      "1968/1968 [==============================] - 2s 870us/step - loss: 0.6641 - accuracy: 0.6026 - val_loss: 0.7309 - val_accuracy: 0.4886\n",
      "Epoch 84/1000\n",
      "1968/1968 [==============================] - 2s 876us/step - loss: 0.6647 - accuracy: 0.5879 - val_loss: 0.7386 - val_accuracy: 0.4292\n",
      "Epoch 85/1000\n",
      "1968/1968 [==============================] - 2s 852us/step - loss: 0.6645 - accuracy: 0.5889 - val_loss: 0.7220 - val_accuracy: 0.4703\n",
      "Epoch 86/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.6530 - accuracy: 0.6113 - val_loss: 0.7448 - val_accuracy: 0.4247\n",
      "Epoch 87/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.6541 - accuracy: 0.6108 - val_loss: 0.7581 - val_accuracy: 0.4110\n",
      "Epoch 88/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6551 - accuracy: 0.5991 - val_loss: 0.7543 - val_accuracy: 0.4566\n",
      "Epoch 89/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6584 - accuracy: 0.6047 - val_loss: 0.7577 - val_accuracy: 0.4749\n",
      "Epoch 90/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6496 - accuracy: 0.6067 - val_loss: 0.7514 - val_accuracy: 0.4749\n",
      "Epoch 91/1000\n",
      "1968/1968 [==============================] - 2s 838us/step - loss: 0.6613 - accuracy: 0.6011 - val_loss: 0.7440 - val_accuracy: 0.4703\n",
      "Epoch 92/1000\n",
      "1968/1968 [==============================] - 2s 842us/step - loss: 0.6550 - accuracy: 0.6087 - val_loss: 0.7517 - val_accuracy: 0.4521\n",
      "Epoch 93/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.6444 - accuracy: 0.6260 - val_loss: 0.7499 - val_accuracy: 0.4475\n",
      "Epoch 94/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.6412 - accuracy: 0.6225 - val_loss: 0.7596 - val_accuracy: 0.4749\n",
      "Epoch 95/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.6441 - accuracy: 0.6148 - val_loss: 0.7537 - val_accuracy: 0.5205\n",
      "Epoch 96/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.6456 - accuracy: 0.6138 - val_loss: 0.7738 - val_accuracy: 0.4429\n",
      "Epoch 97/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.6426 - accuracy: 0.6286 - val_loss: 0.7509 - val_accuracy: 0.4658\n",
      "Epoch 98/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.6538 - accuracy: 0.6108 - val_loss: 0.7741 - val_accuracy: 0.4384\n",
      "Epoch 99/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.6481 - accuracy: 0.6047 - val_loss: 0.7479 - val_accuracy: 0.4612\n",
      "Epoch 100/1000\n",
      "1968/1968 [==============================] - 2s 835us/step - loss: 0.6384 - accuracy: 0.6367 - val_loss: 0.7766 - val_accuracy: 0.4932\n",
      "Epoch 101/1000\n",
      "1968/1968 [==============================] - 2s 818us/step - loss: 0.6424 - accuracy: 0.6286 - val_loss: 0.7432 - val_accuracy: 0.4749\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6328 - accuracy: 0.6341 - val_loss: 0.7608 - val_accuracy: 0.4521\n",
      "Epoch 103/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.6336 - accuracy: 0.6347 - val_loss: 0.7830 - val_accuracy: 0.4886\n",
      "Epoch 104/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.6333 - accuracy: 0.6336 - val_loss: 0.7324 - val_accuracy: 0.5160\n",
      "Epoch 105/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.6385 - accuracy: 0.6225 - val_loss: 0.7615 - val_accuracy: 0.4886\n",
      "Epoch 106/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6383 - accuracy: 0.6357 - val_loss: 0.7714 - val_accuracy: 0.4795\n",
      "Epoch 107/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6506 - accuracy: 0.6286 - val_loss: 0.7743 - val_accuracy: 0.4612\n",
      "Epoch 108/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6287 - accuracy: 0.6357 - val_loss: 0.8073 - val_accuracy: 0.4475\n",
      "Epoch 109/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.6311 - accuracy: 0.6372 - val_loss: 0.7970 - val_accuracy: 0.4521\n",
      "Epoch 110/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6269 - accuracy: 0.6286 - val_loss: 0.7552 - val_accuracy: 0.4886\n",
      "Epoch 111/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.6376 - accuracy: 0.6240 - val_loss: 0.7952 - val_accuracy: 0.4840\n",
      "Epoch 112/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.6187 - accuracy: 0.6367 - val_loss: 0.7958 - val_accuracy: 0.4703\n",
      "Epoch 113/1000\n",
      "1968/1968 [==============================] - 2s 853us/step - loss: 0.6376 - accuracy: 0.6240 - val_loss: 0.7887 - val_accuracy: 0.4475\n",
      "Epoch 114/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.6245 - accuracy: 0.6418 - val_loss: 0.8041 - val_accuracy: 0.4795\n",
      "Epoch 115/1000\n",
      "1968/1968 [==============================] - 2s 852us/step - loss: 0.6135 - accuracy: 0.6529 - val_loss: 0.7895 - val_accuracy: 0.4886\n",
      "Epoch 116/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6151 - accuracy: 0.6504 - val_loss: 0.7952 - val_accuracy: 0.4658\n",
      "Epoch 117/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6197 - accuracy: 0.6402 - val_loss: 0.8233 - val_accuracy: 0.4521\n",
      "Epoch 118/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6059 - accuracy: 0.6529 - val_loss: 0.8384 - val_accuracy: 0.4749\n",
      "Epoch 119/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6201 - accuracy: 0.6408 - val_loss: 0.7981 - val_accuracy: 0.5068\n",
      "Epoch 120/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6164 - accuracy: 0.6504 - val_loss: 0.8099 - val_accuracy: 0.4521\n",
      "Epoch 121/1000\n",
      "1968/1968 [==============================] - 2s 826us/step - loss: 0.6093 - accuracy: 0.6550 - val_loss: 0.8149 - val_accuracy: 0.4384\n",
      "Epoch 122/1000\n",
      "1968/1968 [==============================] - 2s 838us/step - loss: 0.6116 - accuracy: 0.6423 - val_loss: 0.8271 - val_accuracy: 0.4932\n",
      "Epoch 123/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6140 - accuracy: 0.6443 - val_loss: 0.7971 - val_accuracy: 0.4703\n",
      "Epoch 124/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.6054 - accuracy: 0.6611 - val_loss: 0.8287 - val_accuracy: 0.4749\n",
      "Epoch 125/1000\n",
      "1968/1968 [==============================] - 2s 893us/step - loss: 0.6060 - accuracy: 0.6529 - val_loss: 0.8287 - val_accuracy: 0.4795\n",
      "Epoch 126/1000\n",
      "1968/1968 [==============================] - 2s 862us/step - loss: 0.6053 - accuracy: 0.6555 - val_loss: 0.8565 - val_accuracy: 0.4795\n",
      "Epoch 127/1000\n",
      "1968/1968 [==============================] - 2s 897us/step - loss: 0.6076 - accuracy: 0.6519 - val_loss: 0.8782 - val_accuracy: 0.4749\n",
      "Epoch 128/1000\n",
      "1968/1968 [==============================] - 2s 938us/step - loss: 0.6048 - accuracy: 0.6545 - val_loss: 0.8109 - val_accuracy: 0.4566\n",
      "Epoch 129/1000\n",
      "1968/1968 [==============================] - 2s 907us/step - loss: 0.6093 - accuracy: 0.6550 - val_loss: 0.8554 - val_accuracy: 0.4840\n",
      "Epoch 130/1000\n",
      "1968/1968 [==============================] - 2s 950us/step - loss: 0.5966 - accuracy: 0.6646 - val_loss: 0.8412 - val_accuracy: 0.4749\n",
      "Epoch 131/1000\n",
      "1968/1968 [==============================] - 2s 936us/step - loss: 0.5914 - accuracy: 0.6560 - val_loss: 0.9026 - val_accuracy: 0.4566\n",
      "Epoch 132/1000\n",
      "1968/1968 [==============================] - 2s 932us/step - loss: 0.6171 - accuracy: 0.6504 - val_loss: 0.8117 - val_accuracy: 0.4429\n",
      "Epoch 133/1000\n",
      "1968/1968 [==============================] - 2s 914us/step - loss: 0.6329 - accuracy: 0.6326 - val_loss: 0.8166 - val_accuracy: 0.4795\n",
      "Epoch 134/1000\n",
      "1968/1968 [==============================] - 2s 893us/step - loss: 0.6044 - accuracy: 0.6590 - val_loss: 0.8339 - val_accuracy: 0.4703\n",
      "Epoch 135/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.5940 - accuracy: 0.6606 - val_loss: 0.8725 - val_accuracy: 0.4566\n",
      "Epoch 136/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.5825 - accuracy: 0.6778 - val_loss: 0.8268 - val_accuracy: 0.4886\n",
      "Epoch 137/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.5957 - accuracy: 0.6641 - val_loss: 0.9111 - val_accuracy: 0.4566\n",
      "Epoch 138/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.5994 - accuracy: 0.6580 - val_loss: 0.8543 - val_accuracy: 0.4612\n",
      "Epoch 139/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.5775 - accuracy: 0.6738 - val_loss: 0.9325 - val_accuracy: 0.4703\n",
      "Epoch 140/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.5930 - accuracy: 0.6723 - val_loss: 0.8519 - val_accuracy: 0.4612\n",
      "Epoch 141/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.5842 - accuracy: 0.6707 - val_loss: 0.9138 - val_accuracy: 0.4795\n",
      "Epoch 142/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.5753 - accuracy: 0.6728 - val_loss: 0.8915 - val_accuracy: 0.4795\n",
      "Epoch 143/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.5846 - accuracy: 0.6677 - val_loss: 0.9122 - val_accuracy: 0.4749\n",
      "Epoch 144/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.5847 - accuracy: 0.6631 - val_loss: 0.8950 - val_accuracy: 0.4521\n",
      "Epoch 145/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.5615 - accuracy: 0.6865 - val_loss: 0.9262 - val_accuracy: 0.4749\n",
      "Epoch 146/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.5760 - accuracy: 0.6789 - val_loss: 0.9022 - val_accuracy: 0.4749\n",
      "Epoch 147/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.5751 - accuracy: 0.6789 - val_loss: 0.8634 - val_accuracy: 0.4658\n",
      "Epoch 148/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.5692 - accuracy: 0.6951 - val_loss: 0.9611 - val_accuracy: 0.4749\n",
      "Epoch 149/1000\n",
      "1968/1968 [==============================] - 2s 866us/step - loss: 0.5595 - accuracy: 0.6855 - val_loss: 0.9122 - val_accuracy: 0.4886\n",
      "Epoch 150/1000\n",
      "1968/1968 [==============================] - 2s 862us/step - loss: 0.5566 - accuracy: 0.6982 - val_loss: 0.9267 - val_accuracy: 0.5205\n",
      "Epoch 151/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.5674 - accuracy: 0.6819 - val_loss: 0.9543 - val_accuracy: 0.4886\n",
      "Epoch 152/1000\n",
      "1968/1968 [==============================] - 2s 862us/step - loss: 0.5711 - accuracy: 0.6753 - val_loss: 0.9196 - val_accuracy: 0.5068\n",
      "Epoch 153/1000\n",
      "1968/1968 [==============================] - 2s 852us/step - loss: 0.5670 - accuracy: 0.6839 - val_loss: 0.9617 - val_accuracy: 0.4384\n",
      "Epoch 154/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.5568 - accuracy: 0.6870 - val_loss: 0.9904 - val_accuracy: 0.4566\n",
      "Epoch 155/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.5616 - accuracy: 0.6855 - val_loss: 0.9484 - val_accuracy: 0.4566\n",
      "Epoch 156/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.5611 - accuracy: 0.6921 - val_loss: 0.9207 - val_accuracy: 0.4612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.5395 - accuracy: 0.7088 - val_loss: 0.9561 - val_accuracy: 0.4977\n",
      "Epoch 158/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.5673 - accuracy: 0.6839 - val_loss: 0.8926 - val_accuracy: 0.4977\n",
      "Epoch 159/1000\n",
      "1968/1968 [==============================] - 2s 835us/step - loss: 0.5560 - accuracy: 0.6905 - val_loss: 0.9696 - val_accuracy: 0.4886\n",
      "Epoch 160/1000\n",
      "1968/1968 [==============================] - 2s 822us/step - loss: 0.5541 - accuracy: 0.6987 - val_loss: 0.9650 - val_accuracy: 0.4977\n",
      "Epoch 161/1000\n",
      "1968/1968 [==============================] - 2s 800us/step - loss: 0.5534 - accuracy: 0.6855 - val_loss: 0.9668 - val_accuracy: 0.4886\n",
      "Epoch 162/1000\n",
      "1968/1968 [==============================] - 2s 808us/step - loss: 0.5436 - accuracy: 0.6900 - val_loss: 0.9495 - val_accuracy: 0.5251\n",
      "Epoch 163/1000\n",
      "1968/1968 [==============================] - 2s 810us/step - loss: 0.5314 - accuracy: 0.7170 - val_loss: 1.0137 - val_accuracy: 0.4658\n",
      "Epoch 164/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.5379 - accuracy: 0.6911 - val_loss: 0.9847 - val_accuracy: 0.4795\n",
      "Epoch 165/1000\n",
      "1968/1968 [==============================] - 2s 823us/step - loss: 0.5304 - accuracy: 0.7012 - val_loss: 1.0037 - val_accuracy: 0.4795\n",
      "Epoch 166/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.5193 - accuracy: 0.7134 - val_loss: 1.0028 - val_accuracy: 0.4840\n",
      "Epoch 167/1000\n",
      "1968/1968 [==============================] - 2s 872us/step - loss: 0.5388 - accuracy: 0.7002 - val_loss: 0.9625 - val_accuracy: 0.4977\n",
      "Epoch 168/1000\n",
      "1968/1968 [==============================] - 2s 913us/step - loss: 0.5602 - accuracy: 0.6865 - val_loss: 0.9463 - val_accuracy: 0.5068\n",
      "Epoch 169/1000\n",
      "1968/1968 [==============================] - 2s 888us/step - loss: 0.5309 - accuracy: 0.7017 - val_loss: 0.9803 - val_accuracy: 0.4612\n",
      "Epoch 170/1000\n",
      "1968/1968 [==============================] - 2s 941us/step - loss: 0.5229 - accuracy: 0.7124 - val_loss: 0.9674 - val_accuracy: 0.4932\n",
      "Epoch 171/1000\n",
      "1968/1968 [==============================] - 2s 913us/step - loss: 0.5392 - accuracy: 0.7068 - val_loss: 0.9828 - val_accuracy: 0.4749\n",
      "Epoch 172/1000\n",
      "1968/1968 [==============================] - 2s 909us/step - loss: 0.5209 - accuracy: 0.7139 - val_loss: 0.9817 - val_accuracy: 0.4886\n",
      "Epoch 173/1000\n",
      "1968/1968 [==============================] - 2s 896us/step - loss: 0.5189 - accuracy: 0.7165 - val_loss: 1.0129 - val_accuracy: 0.4612\n",
      "Epoch 174/1000\n",
      "1968/1968 [==============================] - 2s 932us/step - loss: 0.5154 - accuracy: 0.7185 - val_loss: 1.0282 - val_accuracy: 0.4840\n",
      "Epoch 175/1000\n",
      "1968/1968 [==============================] - 2s 899us/step - loss: 0.5181 - accuracy: 0.7215 - val_loss: 0.9806 - val_accuracy: 0.4932\n",
      "Epoch 176/1000\n",
      "1968/1968 [==============================] - 2s 844us/step - loss: 0.5184 - accuracy: 0.7109 - val_loss: 1.0228 - val_accuracy: 0.4566\n",
      "Epoch 177/1000\n",
      "1968/1968 [==============================] - 2s 842us/step - loss: 0.5091 - accuracy: 0.7165 - val_loss: 1.0438 - val_accuracy: 0.4749\n",
      "Epoch 178/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.4922 - accuracy: 0.7363 - val_loss: 1.0520 - val_accuracy: 0.4795\n",
      "Epoch 179/1000\n",
      "1968/1968 [==============================] - 2s 835us/step - loss: 0.4962 - accuracy: 0.7393 - val_loss: 0.9950 - val_accuracy: 0.4795\n",
      "Epoch 180/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.5087 - accuracy: 0.7322 - val_loss: 1.0440 - val_accuracy: 0.5023\n",
      "Epoch 181/1000\n",
      "1968/1968 [==============================] - 2s 868us/step - loss: 0.5088 - accuracy: 0.7190 - val_loss: 0.9913 - val_accuracy: 0.4977\n",
      "Epoch 182/1000\n",
      "1968/1968 [==============================] - 2s 860us/step - loss: 0.5132 - accuracy: 0.7231 - val_loss: 1.0882 - val_accuracy: 0.4795\n",
      "Epoch 183/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.5034 - accuracy: 0.7282 - val_loss: 1.0575 - val_accuracy: 0.5114\n",
      "Epoch 184/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.5079 - accuracy: 0.7205 - val_loss: 1.0389 - val_accuracy: 0.4840\n",
      "Epoch 185/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.4786 - accuracy: 0.7383 - val_loss: 1.0879 - val_accuracy: 0.4840\n",
      "Epoch 186/1000\n",
      "1968/1968 [==============================] - 2s 848us/step - loss: 0.5008 - accuracy: 0.7246 - val_loss: 1.0731 - val_accuracy: 0.4795\n",
      "Epoch 187/1000\n",
      "1968/1968 [==============================] - 2s 850us/step - loss: 0.5093 - accuracy: 0.7205 - val_loss: 1.1027 - val_accuracy: 0.4749\n",
      "Epoch 188/1000\n",
      "1968/1968 [==============================] - 2s 908us/step - loss: 0.5045 - accuracy: 0.7287 - val_loss: 1.0590 - val_accuracy: 0.4612\n",
      "Epoch 189/1000\n",
      "1968/1968 [==============================] - 2s 912us/step - loss: 0.4958 - accuracy: 0.7327 - val_loss: 1.0680 - val_accuracy: 0.4977\n",
      "Epoch 190/1000\n",
      "1968/1968 [==============================] - 2s 860us/step - loss: 0.4621 - accuracy: 0.7536 - val_loss: 1.1250 - val_accuracy: 0.5251\n",
      "Epoch 191/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.4863 - accuracy: 0.7261 - val_loss: 1.0800 - val_accuracy: 0.4886\n",
      "Epoch 192/1000\n",
      "1968/1968 [==============================] - 2s 816us/step - loss: 0.4656 - accuracy: 0.7480 - val_loss: 1.0563 - val_accuracy: 0.4658\n",
      "Epoch 193/1000\n",
      "1968/1968 [==============================] - 2s 823us/step - loss: 0.4861 - accuracy: 0.7266 - val_loss: 1.1135 - val_accuracy: 0.5068\n",
      "Epoch 194/1000\n",
      "1968/1968 [==============================] - 2s 817us/step - loss: 0.4739 - accuracy: 0.7470 - val_loss: 1.0596 - val_accuracy: 0.5068\n",
      "Epoch 195/1000\n",
      "1968/1968 [==============================] - 2s 807us/step - loss: 0.4802 - accuracy: 0.7363 - val_loss: 1.1580 - val_accuracy: 0.5114\n",
      "Epoch 196/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.4934 - accuracy: 0.7409 - val_loss: 1.0704 - val_accuracy: 0.4521\n",
      "Epoch 197/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.4698 - accuracy: 0.7510 - val_loss: 1.0495 - val_accuracy: 0.5571\n",
      "Epoch 198/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.4607 - accuracy: 0.7414 - val_loss: 1.1433 - val_accuracy: 0.4658\n",
      "Epoch 199/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.4575 - accuracy: 0.7520 - val_loss: 1.1522 - val_accuracy: 0.4749\n",
      "Epoch 200/1000\n",
      "1968/1968 [==============================] - 2s 848us/step - loss: 0.4566 - accuracy: 0.7612 - val_loss: 1.1552 - val_accuracy: 0.4840\n",
      "Epoch 201/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.4474 - accuracy: 0.7663 - val_loss: 1.1861 - val_accuracy: 0.4886\n",
      "Epoch 202/1000\n",
      "1968/1968 [==============================] - 2s 848us/step - loss: 0.4544 - accuracy: 0.7566 - val_loss: 1.1831 - val_accuracy: 0.4977\n",
      "Epoch 203/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.4423 - accuracy: 0.7678 - val_loss: 1.2241 - val_accuracy: 0.4795\n",
      "Epoch 204/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.4455 - accuracy: 0.7520 - val_loss: 1.1502 - val_accuracy: 0.4612\n",
      "Epoch 205/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.4485 - accuracy: 0.7622 - val_loss: 1.2228 - val_accuracy: 0.4795\n",
      "Epoch 206/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.4438 - accuracy: 0.7617 - val_loss: 1.2404 - val_accuracy: 0.4886\n",
      "Epoch 207/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.4425 - accuracy: 0.7586 - val_loss: 1.2200 - val_accuracy: 0.4932\n",
      "Epoch 208/1000\n",
      "1968/1968 [==============================] - 2s 873us/step - loss: 0.4388 - accuracy: 0.7647 - val_loss: 1.2535 - val_accuracy: 0.4749\n",
      "Epoch 209/1000\n",
      "1968/1968 [==============================] - 2s 879us/step - loss: 0.4409 - accuracy: 0.7652 - val_loss: 1.1861 - val_accuracy: 0.4795\n",
      "Epoch 210/1000\n",
      "1968/1968 [==============================] - 2s 888us/step - loss: 0.4670 - accuracy: 0.7475 - val_loss: 1.1739 - val_accuracy: 0.4977\n",
      "Epoch 211/1000\n",
      "1968/1968 [==============================] - 2s 906us/step - loss: 0.4666 - accuracy: 0.7597 - val_loss: 1.2140 - val_accuracy: 0.4749\n",
      "Epoch 212/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.4641 - accuracy: 0.7637 - val_loss: 1.1837 - val_accuracy: 0.5023\n",
      "Epoch 213/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.4519 - accuracy: 0.7591 - val_loss: 1.1677 - val_accuracy: 0.5068\n",
      "Epoch 214/1000\n",
      "1968/1968 [==============================] - 2s 869us/step - loss: 0.4333 - accuracy: 0.7718 - val_loss: 1.1875 - val_accuracy: 0.4977\n",
      "Epoch 215/1000\n",
      "1968/1968 [==============================] - 2s 910us/step - loss: 0.4319 - accuracy: 0.7698 - val_loss: 1.1692 - val_accuracy: 0.4977\n",
      "Epoch 216/1000\n",
      "1968/1968 [==============================] - 2s 871us/step - loss: 0.4186 - accuracy: 0.7800 - val_loss: 1.2620 - val_accuracy: 0.5160\n",
      "Epoch 217/1000\n",
      "1968/1968 [==============================] - 2s 863us/step - loss: 0.4256 - accuracy: 0.7749 - val_loss: 1.1831 - val_accuracy: 0.5114\n",
      "Epoch 218/1000\n",
      "1968/1968 [==============================] - 2s 903us/step - loss: 0.4326 - accuracy: 0.7693 - val_loss: 1.2151 - val_accuracy: 0.4749\n",
      "Epoch 219/1000\n",
      "1968/1968 [==============================] - 2s 908us/step - loss: 0.4350 - accuracy: 0.7637 - val_loss: 1.1359 - val_accuracy: 0.4977\n",
      "Epoch 220/1000\n",
      "1968/1968 [==============================] - 2s 899us/step - loss: 0.4252 - accuracy: 0.7724 - val_loss: 1.2898 - val_accuracy: 0.4703\n",
      "Epoch 221/1000\n",
      "1968/1968 [==============================] - 2s 901us/step - loss: 0.4148 - accuracy: 0.7820 - val_loss: 1.2089 - val_accuracy: 0.4977\n",
      "Epoch 222/1000\n",
      "1968/1968 [==============================] - 2s 880us/step - loss: 0.3973 - accuracy: 0.7856 - val_loss: 1.3104 - val_accuracy: 0.4795\n",
      "Epoch 223/1000\n",
      "1968/1968 [==============================] - 2s 864us/step - loss: 0.3982 - accuracy: 0.7805 - val_loss: 1.2954 - val_accuracy: 0.4749\n",
      "Epoch 224/1000\n",
      "1968/1968 [==============================] - 2s 856us/step - loss: 0.3971 - accuracy: 0.7866 - val_loss: 1.3459 - val_accuracy: 0.4703\n",
      "Epoch 225/1000\n",
      "1968/1968 [==============================] - 2s 863us/step - loss: 0.4033 - accuracy: 0.7856 - val_loss: 1.2934 - val_accuracy: 0.4977\n",
      "Epoch 226/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.4017 - accuracy: 0.7942 - val_loss: 1.3117 - val_accuracy: 0.4749\n",
      "Epoch 227/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.3925 - accuracy: 0.7973 - val_loss: 1.4476 - val_accuracy: 0.4932\n",
      "Epoch 228/1000\n",
      "1968/1968 [==============================] - 2s 853us/step - loss: 0.3867 - accuracy: 0.7952 - val_loss: 1.3695 - val_accuracy: 0.5114\n",
      "Epoch 229/1000\n",
      "1968/1968 [==============================] - 2s 886us/step - loss: 0.3911 - accuracy: 0.7993 - val_loss: 1.3519 - val_accuracy: 0.5023\n",
      "Epoch 230/1000\n",
      "1968/1968 [==============================] - 2s 885us/step - loss: 0.4129 - accuracy: 0.7907 - val_loss: 1.3149 - val_accuracy: 0.4658\n",
      "Epoch 231/1000\n",
      "1968/1968 [==============================] - 2s 930us/step - loss: 0.4155 - accuracy: 0.7795 - val_loss: 1.3394 - val_accuracy: 0.4795\n",
      "Epoch 232/1000\n",
      "1968/1968 [==============================] - 2s 888us/step - loss: 0.4097 - accuracy: 0.7724 - val_loss: 1.3735 - val_accuracy: 0.4795\n",
      "Epoch 233/1000\n",
      "1968/1968 [==============================] - 2s 906us/step - loss: 0.3986 - accuracy: 0.7917 - val_loss: 1.2873 - val_accuracy: 0.4977\n",
      "Epoch 234/1000\n",
      "1968/1968 [==============================] - 2s 895us/step - loss: 0.3942 - accuracy: 0.7840 - val_loss: 1.3897 - val_accuracy: 0.4795\n",
      "Epoch 235/1000\n",
      "1968/1968 [==============================] - 2s 887us/step - loss: 0.3668 - accuracy: 0.8125 - val_loss: 1.4711 - val_accuracy: 0.4886\n",
      "Epoch 236/1000\n",
      "1968/1968 [==============================] - 2s 882us/step - loss: 0.3993 - accuracy: 0.7983 - val_loss: 1.3417 - val_accuracy: 0.4795\n",
      "Epoch 237/1000\n",
      "1968/1968 [==============================] - 2s 886us/step - loss: 0.3944 - accuracy: 0.7901 - val_loss: 1.4417 - val_accuracy: 0.4703\n",
      "Epoch 238/1000\n",
      "1968/1968 [==============================] - 2s 886us/step - loss: 0.3786 - accuracy: 0.8018 - val_loss: 1.3497 - val_accuracy: 0.5068\n",
      "Epoch 239/1000\n",
      "1968/1968 [==============================] - 2s 905us/step - loss: 0.3731 - accuracy: 0.8105 - val_loss: 1.4542 - val_accuracy: 0.4795\n",
      "Epoch 240/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.3559 - accuracy: 0.8105 - val_loss: 1.4862 - val_accuracy: 0.5205\n",
      "Epoch 241/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.3544 - accuracy: 0.8084 - val_loss: 1.5261 - val_accuracy: 0.4749\n",
      "Epoch 242/1000\n",
      "1968/1968 [==============================] - 2s 893us/step - loss: 0.3403 - accuracy: 0.8298 - val_loss: 1.5243 - val_accuracy: 0.4749\n",
      "Epoch 243/1000\n",
      "1968/1968 [==============================] - 2s 878us/step - loss: 0.3637 - accuracy: 0.8237 - val_loss: 1.5587 - val_accuracy: 0.5068\n",
      "Epoch 244/1000\n",
      "1968/1968 [==============================] - 2s 890us/step - loss: 0.3672 - accuracy: 0.8150 - val_loss: 1.5526 - val_accuracy: 0.4566\n",
      "Epoch 245/1000\n",
      "1968/1968 [==============================] - 2s 865us/step - loss: 0.3953 - accuracy: 0.7962 - val_loss: 1.3736 - val_accuracy: 0.5205\n",
      "Epoch 246/1000\n",
      "1968/1968 [==============================] - 2s 848us/step - loss: 0.4112 - accuracy: 0.7744 - val_loss: 1.4752 - val_accuracy: 0.5068\n",
      "Epoch 247/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.3599 - accuracy: 0.8125 - val_loss: 1.5454 - val_accuracy: 0.4795\n",
      "Epoch 248/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.3650 - accuracy: 0.8125 - val_loss: 1.5486 - val_accuracy: 0.5342\n",
      "Epoch 249/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.3738 - accuracy: 0.8034 - val_loss: 1.5095 - val_accuracy: 0.4795\n",
      "Epoch 250/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.3648 - accuracy: 0.8130 - val_loss: 1.4750 - val_accuracy: 0.4292\n",
      "Epoch 251/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3594 - accuracy: 0.8211 - val_loss: 1.4926 - val_accuracy: 0.4932\n",
      "Epoch 252/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3393 - accuracy: 0.8323 - val_loss: 1.5573 - val_accuracy: 0.4658\n",
      "Epoch 253/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.3314 - accuracy: 0.8272 - val_loss: 1.4927 - val_accuracy: 0.5251\n",
      "Epoch 254/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3300 - accuracy: 0.8277 - val_loss: 1.6011 - val_accuracy: 0.4977\n",
      "Epoch 255/1000\n",
      "1968/1968 [==============================] - 2s 850us/step - loss: 0.3300 - accuracy: 0.8277 - val_loss: 1.5259 - val_accuracy: 0.5023\n",
      "Epoch 256/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.3508 - accuracy: 0.8201 - val_loss: 1.6204 - val_accuracy: 0.5023\n",
      "Epoch 257/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.3512 - accuracy: 0.8186 - val_loss: 1.5873 - val_accuracy: 0.4977\n",
      "Epoch 258/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3646 - accuracy: 0.8110 - val_loss: 1.4678 - val_accuracy: 0.5068\n",
      "Epoch 259/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.3414 - accuracy: 0.8303 - val_loss: 1.5475 - val_accuracy: 0.4840\n",
      "Epoch 260/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.3326 - accuracy: 0.8328 - val_loss: 1.5628 - val_accuracy: 0.5023\n",
      "Epoch 261/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.3253 - accuracy: 0.8369 - val_loss: 1.7393 - val_accuracy: 0.4840\n",
      "Epoch 262/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.3390 - accuracy: 0.8277 - val_loss: 1.6044 - val_accuracy: 0.5251\n",
      "Epoch 263/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.3314 - accuracy: 0.8293 - val_loss: 1.6872 - val_accuracy: 0.4795\n",
      "Epoch 264/1000\n",
      "1968/1968 [==============================] - 2s 864us/step - loss: 0.3379 - accuracy: 0.8267 - val_loss: 1.5901 - val_accuracy: 0.5251\n",
      "Epoch 265/1000\n",
      "1968/1968 [==============================] - 2s 861us/step - loss: 0.3317 - accuracy: 0.8308 - val_loss: 1.6206 - val_accuracy: 0.5297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.3159 - accuracy: 0.8389 - val_loss: 1.5660 - val_accuracy: 0.5023\n",
      "Epoch 267/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.3217 - accuracy: 0.8420 - val_loss: 1.6273 - val_accuracy: 0.4886\n",
      "Epoch 268/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.3070 - accuracy: 0.8491 - val_loss: 1.5968 - val_accuracy: 0.5434\n",
      "Epoch 269/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3025 - accuracy: 0.8511 - val_loss: 1.5670 - val_accuracy: 0.5297\n",
      "Epoch 270/1000\n",
      "1968/1968 [==============================] - 2s 818us/step - loss: 0.2864 - accuracy: 0.8572 - val_loss: 1.7966 - val_accuracy: 0.4703\n",
      "Epoch 271/1000\n",
      "1968/1968 [==============================] - 2s 805us/step - loss: 0.2970 - accuracy: 0.8592 - val_loss: 1.7786 - val_accuracy: 0.5205\n",
      "Epoch 272/1000\n",
      "1968/1968 [==============================] - 2s 809us/step - loss: 0.3225 - accuracy: 0.8379 - val_loss: 1.6613 - val_accuracy: 0.5160\n",
      "Epoch 273/1000\n",
      "1968/1968 [==============================] - 2s 819us/step - loss: 0.3350 - accuracy: 0.8277 - val_loss: 1.6824 - val_accuracy: 0.5297\n",
      "Epoch 274/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.3459 - accuracy: 0.8318 - val_loss: 1.6363 - val_accuracy: 0.5023\n",
      "Epoch 275/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.3125 - accuracy: 0.8481 - val_loss: 1.7157 - val_accuracy: 0.5068\n",
      "Epoch 276/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.3332 - accuracy: 0.8384 - val_loss: 1.6753 - val_accuracy: 0.5068\n",
      "Epoch 277/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.3535 - accuracy: 0.8343 - val_loss: 1.6126 - val_accuracy: 0.4977\n",
      "Epoch 278/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.3524 - accuracy: 0.8171 - val_loss: 1.6800 - val_accuracy: 0.4840\n",
      "Epoch 279/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.3147 - accuracy: 0.8374 - val_loss: 1.7155 - val_accuracy: 0.4886\n",
      "Epoch 280/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.3001 - accuracy: 0.8577 - val_loss: 1.6451 - val_accuracy: 0.5160\n",
      "Epoch 281/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.2821 - accuracy: 0.8587 - val_loss: 1.7062 - val_accuracy: 0.5251\n",
      "Epoch 282/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.2832 - accuracy: 0.8643 - val_loss: 1.6802 - val_accuracy: 0.5388\n",
      "Epoch 283/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.2844 - accuracy: 0.8552 - val_loss: 1.6742 - val_accuracy: 0.5023\n",
      "Epoch 284/1000\n",
      "1968/1968 [==============================] - 2s 809us/step - loss: 0.2825 - accuracy: 0.8572 - val_loss: 1.7338 - val_accuracy: 0.5388\n",
      "Epoch 285/1000\n",
      "1968/1968 [==============================] - 2s 808us/step - loss: 0.2944 - accuracy: 0.8537 - val_loss: 1.6898 - val_accuracy: 0.5205\n",
      "Epoch 286/1000\n",
      "1968/1968 [==============================] - 2s 814us/step - loss: 0.2878 - accuracy: 0.8506 - val_loss: 1.8315 - val_accuracy: 0.5114\n",
      "Epoch 287/1000\n",
      "1968/1968 [==============================] - 2s 818us/step - loss: 0.2620 - accuracy: 0.8684 - val_loss: 1.8584 - val_accuracy: 0.5023\n",
      "Epoch 00287: early stopping\n",
      "30 day\n",
      "\n",
      "# Evaluate on test data\n",
      "244/244 [==============================] - 0s 355us/step\n",
      "test loss, test acc: [1.9669188026522026, 0.5409836173057556]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (244, 1)\n",
      "rmse: 0.6229645494364865\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "PAST_DAYS = 30\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_accuracy\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "print(\"30 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(15, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_44 (LSTM)               (None, 15, 128)           113152    \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 15, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_46 (LSTM)               (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_47 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 306,593\n",
      "Trainable params: 306,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 3s 2ms/step - loss: 0.6967 - accuracy: 0.4929 - val_loss: 0.6863 - val_accuracy: 0.5701\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 676us/step - loss: 0.6934 - accuracy: 0.5071 - val_loss: 0.6869 - val_accuracy: 0.5701\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6933 - accuracy: 0.5091 - val_loss: 0.6855 - val_accuracy: 0.5701\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 695us/step - loss: 0.6920 - accuracy: 0.5096 - val_loss: 0.6940 - val_accuracy: 0.4118\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 672us/step - loss: 0.6924 - accuracy: 0.5121 - val_loss: 0.6872 - val_accuracy: 0.5792\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.6927 - accuracy: 0.5187 - val_loss: 0.6858 - val_accuracy: 0.5701\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 671us/step - loss: 0.6924 - accuracy: 0.5212 - val_loss: 0.6907 - val_accuracy: 0.5747\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 671us/step - loss: 0.6928 - accuracy: 0.5207 - val_loss: 0.6901 - val_accuracy: 0.5747\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 686us/step - loss: 0.6923 - accuracy: 0.5237 - val_loss: 0.6897 - val_accuracy: 0.5747\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6920 - accuracy: 0.5227 - val_loss: 0.6896 - val_accuracy: 0.5928\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6924 - accuracy: 0.5237 - val_loss: 0.6884 - val_accuracy: 0.5701\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.6920 - accuracy: 0.5263 - val_loss: 0.6882 - val_accuracy: 0.5701\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 695us/step - loss: 0.6919 - accuracy: 0.5222 - val_loss: 0.6871 - val_accuracy: 0.5701\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6925 - accuracy: 0.5258 - val_loss: 0.6878 - val_accuracy: 0.5792\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6918 - accuracy: 0.5146 - val_loss: 0.6914 - val_accuracy: 0.5520\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6917 - accuracy: 0.5258 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6908 - accuracy: 0.5298 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6917 - accuracy: 0.5227 - val_loss: 0.6913 - val_accuracy: 0.5566\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6917 - accuracy: 0.5253 - val_loss: 0.6893 - val_accuracy: 0.5792\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6915 - accuracy: 0.5202 - val_loss: 0.6885 - val_accuracy: 0.5701\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 717us/step - loss: 0.6926 - accuracy: 0.5253 - val_loss: 0.6888 - val_accuracy: 0.5701\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 692us/step - loss: 0.6921 - accuracy: 0.5247 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6921 - accuracy: 0.5283 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6915 - accuracy: 0.5273 - val_loss: 0.6880 - val_accuracy: 0.5882\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6914 - accuracy: 0.5207 - val_loss: 0.6893 - val_accuracy: 0.5475\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6910 - accuracy: 0.5237 - val_loss: 0.6917 - val_accuracy: 0.5339\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.6921 - accuracy: 0.5157 - val_loss: 0.6872 - val_accuracy: 0.5701\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.6920 - accuracy: 0.5182 - val_loss: 0.6915 - val_accuracy: 0.5249\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 716us/step - loss: 0.6919 - accuracy: 0.5278 - val_loss: 0.6887 - val_accuracy: 0.5701\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6907 - accuracy: 0.5288 - val_loss: 0.6894 - val_accuracy: 0.5747\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6916 - accuracy: 0.5222 - val_loss: 0.6881 - val_accuracy: 0.5701\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6913 - accuracy: 0.5263 - val_loss: 0.6894 - val_accuracy: 0.5747\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6912 - accuracy: 0.5273 - val_loss: 0.6886 - val_accuracy: 0.5882\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 755us/step - loss: 0.6907 - accuracy: 0.5268 - val_loss: 0.6940 - val_accuracy: 0.4751\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 733us/step - loss: 0.6914 - accuracy: 0.5116 - val_loss: 0.6916 - val_accuracy: 0.5339\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 724us/step - loss: 0.6918 - accuracy: 0.5242 - val_loss: 0.6902 - val_accuracy: 0.5928\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6906 - accuracy: 0.5283 - val_loss: 0.6893 - val_accuracy: 0.5882\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.6899 - accuracy: 0.5323 - val_loss: 0.6920 - val_accuracy: 0.5068\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.6908 - accuracy: 0.5258 - val_loss: 0.6895 - val_accuracy: 0.5747\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 728us/step - loss: 0.6894 - accuracy: 0.5222 - val_loss: 0.6993 - val_accuracy: 0.4163\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.6902 - accuracy: 0.5146 - val_loss: 0.6934 - val_accuracy: 0.5701\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6913 - accuracy: 0.5187 - val_loss: 0.6886 - val_accuracy: 0.5701\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 715us/step - loss: 0.6915 - accuracy: 0.5268 - val_loss: 0.6901 - val_accuracy: 0.6018\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 724us/step - loss: 0.6906 - accuracy: 0.5308 - val_loss: 0.6873 - val_accuracy: 0.5882\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.6891 - accuracy: 0.5258 - val_loss: 0.6928 - val_accuracy: 0.5249\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 729us/step - loss: 0.6905 - accuracy: 0.5328 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 743us/step - loss: 0.6909 - accuracy: 0.5242 - val_loss: 0.6901 - val_accuracy: 0.5701\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.6896 - accuracy: 0.5172 - val_loss: 0.6929 - val_accuracy: 0.5747\n",
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 2s 787us/step - loss: 0.6900 - accuracy: 0.5258 - val_loss: 0.6902 - val_accuracy: 0.5339\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 2s 838us/step - loss: 0.6908 - accuracy: 0.5348 - val_loss: 0.6905 - val_accuracy: 0.5928\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.6909 - accuracy: 0.5530 - val_loss: 0.6899 - val_accuracy: 0.5747\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 2s 849us/step - loss: 0.6902 - accuracy: 0.5318 - val_loss: 0.6912 - val_accuracy: 0.5973\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 2s 790us/step - loss: 0.6892 - accuracy: 0.5354 - val_loss: 0.6953 - val_accuracy: 0.4932\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 2s 834us/step - loss: 0.6905 - accuracy: 0.5237 - val_loss: 0.6871 - val_accuracy: 0.5701\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 2s 762us/step - loss: 0.6888 - accuracy: 0.5429 - val_loss: 0.6945 - val_accuracy: 0.5294\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 2s 816us/step - loss: 0.6893 - accuracy: 0.5394 - val_loss: 0.6905 - val_accuracy: 0.5475\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 2s 882us/step - loss: 0.6891 - accuracy: 0.5399 - val_loss: 0.6972 - val_accuracy: 0.5204\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 2s 1ms/step - loss: 0.6896 - accuracy: 0.5308 - val_loss: 0.6889 - val_accuracy: 0.5656\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 3s 1ms/step - loss: 0.6891 - accuracy: 0.5414 - val_loss: 0.6979 - val_accuracy: 0.4887\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 3s 1ms/step - loss: 0.6882 - accuracy: 0.5419 - val_loss: 0.6949 - val_accuracy: 0.5385\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 3s 1ms/step - loss: 0.6888 - accuracy: 0.5333 - val_loss: 0.6963 - val_accuracy: 0.5294\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 3s 2ms/step - loss: 0.6867 - accuracy: 0.5596 - val_loss: 0.6944 - val_accuracy: 0.5339\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 2s 1ms/step - loss: 0.6876 - accuracy: 0.5545 - val_loss: 0.6891 - val_accuracy: 0.5792\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 2s 906us/step - loss: 0.6898 - accuracy: 0.5253 - val_loss: 0.6964 - val_accuracy: 0.4842\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 2s 808us/step - loss: 0.6859 - accuracy: 0.5399 - val_loss: 0.6941 - val_accuracy: 0.5656\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 2s 772us/step - loss: 0.6848 - accuracy: 0.5515 - val_loss: 0.6853 - val_accuracy: 0.5701\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 727us/step - loss: 0.6844 - accuracy: 0.5465 - val_loss: 0.7038 - val_accuracy: 0.4525\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6856 - accuracy: 0.5535 - val_loss: 0.6971 - val_accuracy: 0.5475\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6845 - accuracy: 0.5631 - val_loss: 0.7012 - val_accuracy: 0.5294\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 691us/step - loss: 0.6839 - accuracy: 0.5465 - val_loss: 0.6907 - val_accuracy: 0.5792\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.6833 - accuracy: 0.5535 - val_loss: 0.6905 - val_accuracy: 0.5792\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 686us/step - loss: 0.6802 - accuracy: 0.5636 - val_loss: 0.7041 - val_accuracy: 0.5204\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6789 - accuracy: 0.5561 - val_loss: 0.7000 - val_accuracy: 0.5385\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.6762 - accuracy: 0.5692 - val_loss: 0.6909 - val_accuracy: 0.5520\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 725us/step - loss: 0.6822 - accuracy: 0.5586 - val_loss: 0.7038 - val_accuracy: 0.5204\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 738us/step - loss: 0.6769 - accuracy: 0.5722 - val_loss: 0.6981 - val_accuracy: 0.5385\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 742us/step - loss: 0.6757 - accuracy: 0.5722 - val_loss: 0.6992 - val_accuracy: 0.5113\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 2s 759us/step - loss: 0.6741 - accuracy: 0.5667 - val_loss: 0.6949 - val_accuracy: 0.5475\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 754us/step - loss: 0.6827 - accuracy: 0.5556 - val_loss: 0.6997 - val_accuracy: 0.5430\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 2s 826us/step - loss: 0.6767 - accuracy: 0.5646 - val_loss: 0.7106 - val_accuracy: 0.4842\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 2s 808us/step - loss: 0.6749 - accuracy: 0.5722 - val_loss: 0.6996 - val_accuracy: 0.5113\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 2s 796us/step - loss: 0.6736 - accuracy: 0.5657 - val_loss: 0.6885 - val_accuracy: 0.5294\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 2s 814us/step - loss: 0.6699 - accuracy: 0.5753 - val_loss: 0.6967 - val_accuracy: 0.5430\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 2s 814us/step - loss: 0.6711 - accuracy: 0.5727 - val_loss: 0.6993 - val_accuracy: 0.5113\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 2s 813us/step - loss: 0.6723 - accuracy: 0.5783 - val_loss: 0.6890 - val_accuracy: 0.4977\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 2s 808us/step - loss: 0.6668 - accuracy: 0.5854 - val_loss: 0.6995 - val_accuracy: 0.5385\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 2s 806us/step - loss: 0.6686 - accuracy: 0.5758 - val_loss: 0.6931 - val_accuracy: 0.5701\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 2s 772us/step - loss: 0.6641 - accuracy: 0.5904 - val_loss: 0.7157 - val_accuracy: 0.5158\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 2s 781us/step - loss: 0.6571 - accuracy: 0.5924 - val_loss: 0.6901 - val_accuracy: 0.6063\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 754us/step - loss: 0.6624 - accuracy: 0.5874 - val_loss: 0.6888 - val_accuracy: 0.5566\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.6526 - accuracy: 0.6040 - val_loss: 0.7049 - val_accuracy: 0.5339\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 2s 772us/step - loss: 0.6645 - accuracy: 0.6061 - val_loss: 0.7067 - val_accuracy: 0.5204\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 2s 787us/step - loss: 0.6519 - accuracy: 0.6146 - val_loss: 0.7190 - val_accuracy: 0.5294\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 2s 794us/step - loss: 0.6692 - accuracy: 0.5753 - val_loss: 0.7172 - val_accuracy: 0.5204\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 2s 764us/step - loss: 0.6568 - accuracy: 0.6091 - val_loss: 0.6953 - val_accuracy: 0.5475\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 2s 760us/step - loss: 0.6513 - accuracy: 0.6217 - val_loss: 0.7083 - val_accuracy: 0.5249\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 2s 803us/step - loss: 0.6484 - accuracy: 0.6146 - val_loss: 0.7122 - val_accuracy: 0.5385\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 2s 780us/step - loss: 0.6487 - accuracy: 0.6217 - val_loss: 0.7172 - val_accuracy: 0.5204\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 2s 783us/step - loss: 0.6450 - accuracy: 0.6288 - val_loss: 0.7222 - val_accuracy: 0.5385\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 2s 812us/step - loss: 0.6432 - accuracy: 0.6207 - val_loss: 0.7317 - val_accuracy: 0.5113\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 2s 771us/step - loss: 0.6451 - accuracy: 0.6237 - val_loss: 0.7108 - val_accuracy: 0.5430\n",
      "Epoch 102/1000\n",
      "1980/1980 [==============================] - 2s 768us/step - loss: 0.6426 - accuracy: 0.6323 - val_loss: 0.7251 - val_accuracy: 0.5294\n",
      "Epoch 103/1000\n",
      "1980/1980 [==============================] - 2s 777us/step - loss: 0.6353 - accuracy: 0.6293 - val_loss: 0.7245 - val_accuracy: 0.5430\n",
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 2s 765us/step - loss: 0.6354 - accuracy: 0.6338 - val_loss: 0.7351 - val_accuracy: 0.5158\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.6415 - accuracy: 0.6258 - val_loss: 0.7089 - val_accuracy: 0.5294\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.6411 - accuracy: 0.6217 - val_loss: 0.7175 - val_accuracy: 0.5068\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.6369 - accuracy: 0.6434 - val_loss: 0.7257 - val_accuracy: 0.5385\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 720us/step - loss: 0.6293 - accuracy: 0.6374 - val_loss: 0.7175 - val_accuracy: 0.5158\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 718us/step - loss: 0.6277 - accuracy: 0.6359 - val_loss: 0.7400 - val_accuracy: 0.5339\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.6277 - accuracy: 0.6455 - val_loss: 0.7567 - val_accuracy: 0.5249\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 720us/step - loss: 0.6243 - accuracy: 0.6460 - val_loss: 0.7501 - val_accuracy: 0.5249\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 714us/step - loss: 0.6186 - accuracy: 0.6530 - val_loss: 0.7276 - val_accuracy: 0.5294\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.6251 - accuracy: 0.6525 - val_loss: 0.7655 - val_accuracy: 0.5158\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.6204 - accuracy: 0.6535 - val_loss: 0.7249 - val_accuracy: 0.5339\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 738us/step - loss: 0.6180 - accuracy: 0.6535 - val_loss: 0.7499 - val_accuracy: 0.5385\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 736us/step - loss: 0.6175 - accuracy: 0.6470 - val_loss: 0.7494 - val_accuracy: 0.5158\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.6095 - accuracy: 0.6606 - val_loss: 0.7806 - val_accuracy: 0.4887\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 736us/step - loss: 0.6151 - accuracy: 0.6520 - val_loss: 0.7326 - val_accuracy: 0.5430\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 735us/step - loss: 0.6169 - accuracy: 0.6586 - val_loss: 0.7501 - val_accuracy: 0.5068\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.6062 - accuracy: 0.6611 - val_loss: 0.7766 - val_accuracy: 0.5204\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.6068 - accuracy: 0.6611 - val_loss: 0.7383 - val_accuracy: 0.5339\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 738us/step - loss: 0.6081 - accuracy: 0.6616 - val_loss: 0.7738 - val_accuracy: 0.5158\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 745us/step - loss: 0.6111 - accuracy: 0.6626 - val_loss: 0.7623 - val_accuracy: 0.5113\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 729us/step - loss: 0.6002 - accuracy: 0.6636 - val_loss: 0.7923 - val_accuracy: 0.5113\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 724us/step - loss: 0.5951 - accuracy: 0.6758 - val_loss: 0.7731 - val_accuracy: 0.5294\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.5968 - accuracy: 0.6621 - val_loss: 0.8039 - val_accuracy: 0.4977\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.6023 - accuracy: 0.6722 - val_loss: 0.7574 - val_accuracy: 0.5339\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 732us/step - loss: 0.5998 - accuracy: 0.6687 - val_loss: 0.7869 - val_accuracy: 0.5113\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.5872 - accuracy: 0.6828 - val_loss: 0.8159 - val_accuracy: 0.5204\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 2s 775us/step - loss: 0.5906 - accuracy: 0.6793 - val_loss: 0.8422 - val_accuracy: 0.5249\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 757us/step - loss: 0.5912 - accuracy: 0.6727 - val_loss: 0.7782 - val_accuracy: 0.4977\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.5903 - accuracy: 0.6808 - val_loss: 0.7907 - val_accuracy: 0.4977\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 2s 765us/step - loss: 0.5813 - accuracy: 0.6773 - val_loss: 0.8088 - val_accuracy: 0.5068\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 2s 788us/step - loss: 0.5855 - accuracy: 0.6793 - val_loss: 0.8396 - val_accuracy: 0.4796\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 2s 794us/step - loss: 0.5847 - accuracy: 0.6894 - val_loss: 0.8020 - val_accuracy: 0.5023\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 2s 812us/step - loss: 0.5806 - accuracy: 0.6914 - val_loss: 0.8455 - val_accuracy: 0.5113\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 2s 787us/step - loss: 0.5910 - accuracy: 0.6717 - val_loss: 0.7862 - val_accuracy: 0.5068\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 2s 758us/step - loss: 0.5747 - accuracy: 0.6909 - val_loss: 0.8529 - val_accuracy: 0.5158\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 2s 848us/step - loss: 0.5778 - accuracy: 0.6798 - val_loss: 0.7885 - val_accuracy: 0.4932\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.5684 - accuracy: 0.6924 - val_loss: 0.8988 - val_accuracy: 0.5113\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 2s 788us/step - loss: 0.5805 - accuracy: 0.6773 - val_loss: 0.8753 - val_accuracy: 0.4842\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 2s 770us/step - loss: 0.5778 - accuracy: 0.6833 - val_loss: 0.8230 - val_accuracy: 0.5430\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.5720 - accuracy: 0.6919 - val_loss: 0.8867 - val_accuracy: 0.5249\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.5622 - accuracy: 0.6924 - val_loss: 0.8770 - val_accuracy: 0.4796\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.5606 - accuracy: 0.7035 - val_loss: 0.8475 - val_accuracy: 0.5204\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 751us/step - loss: 0.5538 - accuracy: 0.7071 - val_loss: 0.8763 - val_accuracy: 0.5339\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 725us/step - loss: 0.5488 - accuracy: 0.7061 - val_loss: 0.8389 - val_accuracy: 0.5294\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 735us/step - loss: 0.5618 - accuracy: 0.6919 - val_loss: 0.9009 - val_accuracy: 0.4842\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.5583 - accuracy: 0.6944 - val_loss: 0.9129 - val_accuracy: 0.5158\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.5505 - accuracy: 0.7015 - val_loss: 0.8595 - val_accuracy: 0.4842\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 743us/step - loss: 0.5479 - accuracy: 0.7096 - val_loss: 0.9260 - val_accuracy: 0.5294\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 757us/step - loss: 0.5532 - accuracy: 0.6965 - val_loss: 0.8989 - val_accuracy: 0.5023\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 2s 761us/step - loss: 0.5508 - accuracy: 0.7116 - val_loss: 0.8962 - val_accuracy: 0.5204\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 2s 761us/step - loss: 0.5360 - accuracy: 0.7101 - val_loss: 0.9402 - val_accuracy: 0.5430\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 2s 800us/step - loss: 0.5438 - accuracy: 0.7081 - val_loss: 0.9361 - val_accuracy: 0.5294\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 2s 788us/step - loss: 0.5257 - accuracy: 0.7202 - val_loss: 1.0042 - val_accuracy: 0.5068\n",
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 2s 761us/step - loss: 0.5389 - accuracy: 0.7187 - val_loss: 0.8843 - val_accuracy: 0.4977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.5280 - accuracy: 0.7222 - val_loss: 1.0194 - val_accuracy: 0.5023\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.5174 - accuracy: 0.7313 - val_loss: 0.9869 - val_accuracy: 0.5113\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.5119 - accuracy: 0.7343 - val_loss: 1.0292 - val_accuracy: 0.5113\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 2s 781us/step - loss: 0.5231 - accuracy: 0.7247 - val_loss: 0.9317 - val_accuracy: 0.5113\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.5065 - accuracy: 0.7328 - val_loss: 1.0171 - val_accuracy: 0.5068\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.5268 - accuracy: 0.7146 - val_loss: 0.9725 - val_accuracy: 0.4932\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 732us/step - loss: 0.5293 - accuracy: 0.7177 - val_loss: 0.9744 - val_accuracy: 0.5113\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.5053 - accuracy: 0.7293 - val_loss: 1.0547 - val_accuracy: 0.5113\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 711us/step - loss: 0.4992 - accuracy: 0.7369 - val_loss: 1.0429 - val_accuracy: 0.5204\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.4891 - accuracy: 0.7374 - val_loss: 1.0528 - val_accuracy: 0.5158\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.4953 - accuracy: 0.7328 - val_loss: 1.0722 - val_accuracy: 0.4751\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 724us/step - loss: 0.4933 - accuracy: 0.7318 - val_loss: 1.0180 - val_accuracy: 0.5023\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.5007 - accuracy: 0.7374 - val_loss: 1.0650 - val_accuracy: 0.4932\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 2s 774us/step - loss: 0.4999 - accuracy: 0.7323 - val_loss: 1.0369 - val_accuracy: 0.4661\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 753us/step - loss: 0.5043 - accuracy: 0.7374 - val_loss: 1.0463 - val_accuracy: 0.5113\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.4903 - accuracy: 0.7434 - val_loss: 1.0553 - val_accuracy: 0.4751\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 727us/step - loss: 0.4693 - accuracy: 0.7556 - val_loss: 1.1027 - val_accuracy: 0.5023\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 730us/step - loss: 0.4793 - accuracy: 0.7525 - val_loss: 1.0004 - val_accuracy: 0.4932\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 745us/step - loss: 0.4713 - accuracy: 0.7510 - val_loss: 1.0791 - val_accuracy: 0.5339\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.4620 - accuracy: 0.7566 - val_loss: 1.1467 - val_accuracy: 0.5068\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 734us/step - loss: 0.4881 - accuracy: 0.7460 - val_loss: 1.0704 - val_accuracy: 0.4932\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 723us/step - loss: 0.4895 - accuracy: 0.7414 - val_loss: 1.1188 - val_accuracy: 0.5158\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 716us/step - loss: 0.4768 - accuracy: 0.7540 - val_loss: 1.0692 - val_accuracy: 0.5158\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.4779 - accuracy: 0.7540 - val_loss: 1.0650 - val_accuracy: 0.4977\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.4549 - accuracy: 0.7556 - val_loss: 1.1927 - val_accuracy: 0.4977\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.4569 - accuracy: 0.7611 - val_loss: 1.2231 - val_accuracy: 0.4977\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.4480 - accuracy: 0.7732 - val_loss: 1.1786 - val_accuracy: 0.5158\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.4408 - accuracy: 0.7712 - val_loss: 1.2422 - val_accuracy: 0.4887\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 715us/step - loss: 0.4475 - accuracy: 0.7732 - val_loss: 1.2549 - val_accuracy: 0.4932\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 725us/step - loss: 0.4480 - accuracy: 0.7657 - val_loss: 1.1641 - val_accuracy: 0.5158\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 2s 782us/step - loss: 0.4363 - accuracy: 0.7742 - val_loss: 1.1419 - val_accuracy: 0.5113\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 2s 843us/step - loss: 0.4151 - accuracy: 0.7965 - val_loss: 1.2176 - val_accuracy: 0.4751\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 2s 799us/step - loss: 0.4112 - accuracy: 0.7995 - val_loss: 1.1642 - val_accuracy: 0.4887\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 2s 780us/step - loss: 0.4193 - accuracy: 0.7874 - val_loss: 1.2339 - val_accuracy: 0.4887\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 2s 776us/step - loss: 0.4034 - accuracy: 0.8061 - val_loss: 1.2567 - val_accuracy: 0.5294\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 2s 780us/step - loss: 0.4117 - accuracy: 0.7975 - val_loss: 1.2610 - val_accuracy: 0.4932\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 2s 814us/step - loss: 0.3873 - accuracy: 0.8066 - val_loss: 1.2896 - val_accuracy: 0.5249\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 2s 796us/step - loss: 0.4106 - accuracy: 0.7889 - val_loss: 1.2751 - val_accuracy: 0.5023\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 2s 794us/step - loss: 0.4027 - accuracy: 0.8005 - val_loss: 1.3872 - val_accuracy: 0.4887\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 2s 762us/step - loss: 0.4306 - accuracy: 0.7803 - val_loss: 1.2440 - val_accuracy: 0.4977\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 2s 776us/step - loss: 0.3951 - accuracy: 0.8091 - val_loss: 1.3066 - val_accuracy: 0.4977\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 735us/step - loss: 0.3726 - accuracy: 0.8111 - val_loss: 1.3649 - val_accuracy: 0.4842\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 730us/step - loss: 0.3659 - accuracy: 0.8253 - val_loss: 1.3630 - val_accuracy: 0.5023\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.3674 - accuracy: 0.8232 - val_loss: 1.2867 - val_accuracy: 0.5158\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 727us/step - loss: 0.3876 - accuracy: 0.8081 - val_loss: 1.3318 - val_accuracy: 0.4887\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.3699 - accuracy: 0.8131 - val_loss: 1.4252 - val_accuracy: 0.4932\n",
      "Epoch 204/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.3577 - accuracy: 0.8263 - val_loss: 1.4640 - val_accuracy: 0.5023\n",
      "Epoch 205/1000\n",
      "1980/1980 [==============================] - 1s 734us/step - loss: 0.3496 - accuracy: 0.8247 - val_loss: 1.5660 - val_accuracy: 0.5158\n",
      "Epoch 206/1000\n",
      "1980/1980 [==============================] - 1s 739us/step - loss: 0.3567 - accuracy: 0.8167 - val_loss: 1.3442 - val_accuracy: 0.5068\n",
      "Epoch 207/1000\n",
      "1980/1980 [==============================] - 2s 763us/step - loss: 0.3357 - accuracy: 0.8348 - val_loss: 1.6185 - val_accuracy: 0.4977\n",
      "Epoch 208/1000\n",
      "1980/1980 [==============================] - 2s 794us/step - loss: 0.3263 - accuracy: 0.8409 - val_loss: 1.7237 - val_accuracy: 0.4796\n",
      "Epoch 209/1000\n",
      "1980/1980 [==============================] - 2s 807us/step - loss: 0.3685 - accuracy: 0.8146 - val_loss: 1.6410 - val_accuracy: 0.4751\n",
      "Epoch 210/1000\n",
      "1980/1980 [==============================] - 2s 797us/step - loss: 0.3596 - accuracy: 0.8222 - val_loss: 1.4877 - val_accuracy: 0.4887\n",
      "Epoch 211/1000\n",
      "1980/1980 [==============================] - 2s 785us/step - loss: 0.3335 - accuracy: 0.8369 - val_loss: 1.5569 - val_accuracy: 0.4977\n",
      "Epoch 212/1000\n",
      "1980/1980 [==============================] - 2s 765us/step - loss: 0.3153 - accuracy: 0.8444 - val_loss: 1.6393 - val_accuracy: 0.4932\n",
      "Epoch 213/1000\n",
      "1980/1980 [==============================] - 2s 786us/step - loss: 0.3041 - accuracy: 0.8601 - val_loss: 1.5607 - val_accuracy: 0.5023\n",
      "Epoch 214/1000\n",
      "1980/1980 [==============================] - 2s 770us/step - loss: 0.3020 - accuracy: 0.8520 - val_loss: 1.6371 - val_accuracy: 0.5158\n",
      "Epoch 215/1000\n",
      "1980/1980 [==============================] - 2s 773us/step - loss: 0.3234 - accuracy: 0.8455 - val_loss: 1.6779 - val_accuracy: 0.4842\n",
      "Epoch 216/1000\n",
      "1980/1980 [==============================] - 1s 747us/step - loss: 0.3357 - accuracy: 0.8333 - val_loss: 1.6805 - val_accuracy: 0.5023\n",
      "Epoch 217/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.3017 - accuracy: 0.8540 - val_loss: 1.6009 - val_accuracy: 0.4887\n",
      "Epoch 218/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.3033 - accuracy: 0.8530 - val_loss: 1.6027 - val_accuracy: 0.5158\n",
      "Epoch 219/1000\n",
      "1980/1980 [==============================] - 1s 753us/step - loss: 0.2820 - accuracy: 0.8682 - val_loss: 1.6346 - val_accuracy: 0.5249\n",
      "Epoch 220/1000\n",
      "1980/1980 [==============================] - 1s 734us/step - loss: 0.2812 - accuracy: 0.8641 - val_loss: 1.8202 - val_accuracy: 0.4977\n",
      "Epoch 221/1000\n",
      "1980/1980 [==============================] - 1s 727us/step - loss: 0.2884 - accuracy: 0.8631 - val_loss: 1.6566 - val_accuracy: 0.5339\n",
      "Epoch 222/1000\n",
      "1980/1980 [==============================] - 1s 729us/step - loss: 0.2882 - accuracy: 0.8596 - val_loss: 1.7409 - val_accuracy: 0.4887\n",
      "Epoch 223/1000\n",
      "1980/1980 [==============================] - 1s 743us/step - loss: 0.3056 - accuracy: 0.8576 - val_loss: 1.6516 - val_accuracy: 0.5430\n",
      "Epoch 224/1000\n",
      "1980/1980 [==============================] - 2s 799us/step - loss: 0.2890 - accuracy: 0.8621 - val_loss: 1.6045 - val_accuracy: 0.5385\n",
      "Epoch 225/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.3049 - accuracy: 0.8515 - val_loss: 1.6774 - val_accuracy: 0.5023\n",
      "Epoch 226/1000\n",
      "1980/1980 [==============================] - 1s 752us/step - loss: 0.2614 - accuracy: 0.8758 - val_loss: 1.7667 - val_accuracy: 0.4977\n",
      "Epoch 227/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.2736 - accuracy: 0.8763 - val_loss: 1.9110 - val_accuracy: 0.5158\n",
      "Epoch 228/1000\n",
      "1980/1980 [==============================] - 2s 766us/step - loss: 0.3029 - accuracy: 0.8530 - val_loss: 1.8039 - val_accuracy: 0.5339\n",
      "Epoch 229/1000\n",
      "1980/1980 [==============================] - 2s 766us/step - loss: 0.2755 - accuracy: 0.8778 - val_loss: 1.7083 - val_accuracy: 0.5249\n",
      "Epoch 230/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.2649 - accuracy: 0.8803 - val_loss: 1.9422 - val_accuracy: 0.4977\n",
      "Epoch 231/1000\n",
      "1980/1980 [==============================] - 2s 760us/step - loss: 0.2383 - accuracy: 0.8869 - val_loss: 1.9374 - val_accuracy: 0.5113\n",
      "Epoch 232/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.2887 - accuracy: 0.8601 - val_loss: 1.7558 - val_accuracy: 0.5204\n",
      "Epoch 233/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.2505 - accuracy: 0.8768 - val_loss: 1.9668 - val_accuracy: 0.5204\n",
      "Epoch 234/1000\n",
      "1980/1980 [==============================] - 2s 771us/step - loss: 0.2320 - accuracy: 0.8864 - val_loss: 2.0264 - val_accuracy: 0.4887\n",
      "Epoch 235/1000\n",
      "1980/1980 [==============================] - 1s 752us/step - loss: 0.2204 - accuracy: 0.8990 - val_loss: 1.8920 - val_accuracy: 0.5294\n",
      "Epoch 236/1000\n",
      "1980/1980 [==============================] - 1s 747us/step - loss: 0.2420 - accuracy: 0.8955 - val_loss: 2.2158 - val_accuracy: 0.4932\n",
      "Epoch 237/1000\n",
      "1980/1980 [==============================] - 1s 736us/step - loss: 0.2312 - accuracy: 0.8975 - val_loss: 2.1214 - val_accuracy: 0.5068\n",
      "Epoch 238/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.2259 - accuracy: 0.8970 - val_loss: 1.9555 - val_accuracy: 0.5204\n",
      "Epoch 239/1000\n",
      "1980/1980 [==============================] - 1s 726us/step - loss: 0.1962 - accuracy: 0.9086 - val_loss: 2.2549 - val_accuracy: 0.4977\n",
      "Epoch 240/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.2060 - accuracy: 0.9035 - val_loss: 2.0961 - val_accuracy: 0.5249\n",
      "Epoch 241/1000\n",
      "1980/1980 [==============================] - 1s 720us/step - loss: 0.2255 - accuracy: 0.8955 - val_loss: 2.0969 - val_accuracy: 0.5158\n",
      "Epoch 242/1000\n",
      "1980/1980 [==============================] - 1s 716us/step - loss: 0.2218 - accuracy: 0.8965 - val_loss: 2.3155 - val_accuracy: 0.4932\n",
      "Epoch 243/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.2112 - accuracy: 0.9025 - val_loss: 2.0053 - val_accuracy: 0.5385\n",
      "Epoch 244/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.1976 - accuracy: 0.9106 - val_loss: 2.1227 - val_accuracy: 0.5158\n",
      "Epoch 245/1000\n",
      "1980/1980 [==============================] - 1s 711us/step - loss: 0.2173 - accuracy: 0.8985 - val_loss: 2.2244 - val_accuracy: 0.5023\n",
      "Epoch 246/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.2345 - accuracy: 0.8914 - val_loss: 2.1127 - val_accuracy: 0.4977\n",
      "Epoch 247/1000\n",
      "1980/1980 [==============================] - 1s 731us/step - loss: 0.2382 - accuracy: 0.8904 - val_loss: 1.9043 - val_accuracy: 0.5339\n",
      "Epoch 248/1000\n",
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.1885 - accuracy: 0.9111 - val_loss: 2.1699 - val_accuracy: 0.5249\n",
      "Epoch 249/1000\n",
      "1980/1980 [==============================] - 1s 749us/step - loss: 0.2150 - accuracy: 0.9066 - val_loss: 2.1473 - val_accuracy: 0.5385\n",
      "Epoch 250/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.1700 - accuracy: 0.9247 - val_loss: 2.3778 - val_accuracy: 0.5339\n",
      "Epoch 251/1000\n",
      "1980/1980 [==============================] - 1s 752us/step - loss: 0.1882 - accuracy: 0.9177 - val_loss: 2.3431 - val_accuracy: 0.5158\n",
      "Epoch 252/1000\n",
      "1980/1980 [==============================] - 2s 761us/step - loss: 0.2008 - accuracy: 0.9096 - val_loss: 2.1108 - val_accuracy: 0.5158\n",
      "Epoch 253/1000\n",
      "1980/1980 [==============================] - 1s 735us/step - loss: 0.2066 - accuracy: 0.9025 - val_loss: 2.2538 - val_accuracy: 0.5385\n",
      "Epoch 254/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.2114 - accuracy: 0.9071 - val_loss: 2.3005 - val_accuracy: 0.5068\n",
      "Epoch 255/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.1731 - accuracy: 0.9202 - val_loss: 2.3615 - val_accuracy: 0.5339\n",
      "Epoch 256/1000\n",
      "1980/1980 [==============================] - 1s 720us/step - loss: 0.1731 - accuracy: 0.9227 - val_loss: 2.3164 - val_accuracy: 0.5430\n",
      "Epoch 257/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.2212 - accuracy: 0.9010 - val_loss: 2.3155 - val_accuracy: 0.4932\n",
      "Epoch 258/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.2113 - accuracy: 0.9126 - val_loss: 2.1804 - val_accuracy: 0.5520\n",
      "Epoch 259/1000\n",
      "1980/1980 [==============================] - 1s 733us/step - loss: 0.1680 - accuracy: 0.9258 - val_loss: 2.3547 - val_accuracy: 0.5158\n",
      "Epoch 260/1000\n",
      "1980/1980 [==============================] - 1s 718us/step - loss: 0.1623 - accuracy: 0.9359 - val_loss: 2.3483 - val_accuracy: 0.5430\n",
      "Epoch 261/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.1562 - accuracy: 0.9308 - val_loss: 2.3043 - val_accuracy: 0.5158\n",
      "Epoch 262/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.1305 - accuracy: 0.9434 - val_loss: 2.6284 - val_accuracy: 0.5158\n",
      "Epoch 263/1000\n",
      "1980/1980 [==============================] - 1s 725us/step - loss: 0.1295 - accuracy: 0.9439 - val_loss: 2.5280 - val_accuracy: 0.5520\n",
      "Epoch 264/1000\n",
      "1980/1980 [==============================] - 1s 715us/step - loss: 0.1200 - accuracy: 0.9505 - val_loss: 2.5515 - val_accuracy: 0.5249\n",
      "Epoch 265/1000\n",
      "1980/1980 [==============================] - 1s 717us/step - loss: 0.1347 - accuracy: 0.9434 - val_loss: 2.4661 - val_accuracy: 0.5385\n",
      "Epoch 266/1000\n",
      "1980/1980 [==============================] - 1s 749us/step - loss: 0.1246 - accuracy: 0.9470 - val_loss: 2.5362 - val_accuracy: 0.5294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00266: early stopping\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 203us/step\n",
      "test loss, test acc: [2.2715577407759064, 0.5142857432365417]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.6361083635720066\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(15, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_55 (LSTM)               (None, 15, 128)           113152    \n",
      "_________________________________________________________________\n",
      "lstm_56 (LSTM)               (None, 15, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 306,593\n",
      "Trainable params: 306,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 3s 1ms/step - loss: 0.6950 - accuracy: 0.5116 - val_loss: 0.6833 - val_accuracy: 0.5701\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6953 - accuracy: 0.5217 - val_loss: 0.6939 - val_accuracy: 0.4661\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 674us/step - loss: 0.6927 - accuracy: 0.5268 - val_loss: 0.6898 - val_accuracy: 0.5701\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.6935 - accuracy: 0.5141 - val_loss: 0.6856 - val_accuracy: 0.5701\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6926 - accuracy: 0.5182 - val_loss: 0.6901 - val_accuracy: 0.5701\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 686us/step - loss: 0.6919 - accuracy: 0.5202 - val_loss: 0.6928 - val_accuracy: 0.5339\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 673us/step - loss: 0.6924 - accuracy: 0.5202 - val_loss: 0.6882 - val_accuracy: 0.5701\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 670us/step - loss: 0.6925 - accuracy: 0.5182 - val_loss: 0.6888 - val_accuracy: 0.5747\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.6924 - accuracy: 0.5232 - val_loss: 0.6885 - val_accuracy: 0.5747\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6915 - accuracy: 0.5298 - val_loss: 0.6892 - val_accuracy: 0.5747\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6921 - accuracy: 0.5141 - val_loss: 0.6864 - val_accuracy: 0.5747\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 731us/step - loss: 0.6922 - accuracy: 0.5167 - val_loss: 0.6870 - val_accuracy: 0.5701\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6913 - accuracy: 0.5268 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6910 - accuracy: 0.5278 - val_loss: 0.6926 - val_accuracy: 0.5158\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6926 - accuracy: 0.5040 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.6916 - accuracy: 0.5207 - val_loss: 0.6895 - val_accuracy: 0.5792\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6917 - accuracy: 0.5303 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6916 - accuracy: 0.5237 - val_loss: 0.6891 - val_accuracy: 0.5701\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.6918 - accuracy: 0.5288 - val_loss: 0.6873 - val_accuracy: 0.5882\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6920 - accuracy: 0.5258 - val_loss: 0.6873 - val_accuracy: 0.5837\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6922 - accuracy: 0.5232 - val_loss: 0.6893 - val_accuracy: 0.5701\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.6911 - accuracy: 0.5268 - val_loss: 0.6894 - val_accuracy: 0.5837\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6912 - accuracy: 0.5293 - val_loss: 0.6907 - val_accuracy: 0.5656\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6919 - accuracy: 0.5237 - val_loss: 0.6878 - val_accuracy: 0.5701\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6923 - accuracy: 0.5258 - val_loss: 0.6889 - val_accuracy: 0.5701\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6909 - accuracy: 0.5227 - val_loss: 0.6916 - val_accuracy: 0.5973\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6918 - accuracy: 0.5187 - val_loss: 0.6903 - val_accuracy: 0.5792\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 718us/step - loss: 0.6923 - accuracy: 0.5232 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6917 - accuracy: 0.5283 - val_loss: 0.6875 - val_accuracy: 0.5928\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6916 - accuracy: 0.5258 - val_loss: 0.6890 - val_accuracy: 0.5611\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6918 - accuracy: 0.5338 - val_loss: 0.6876 - val_accuracy: 0.5837\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6906 - accuracy: 0.5278 - val_loss: 0.6869 - val_accuracy: 0.5701\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 675us/step - loss: 0.6908 - accuracy: 0.5288 - val_loss: 0.6904 - val_accuracy: 0.5566\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6905 - accuracy: 0.5207 - val_loss: 0.6882 - val_accuracy: 0.5882\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6899 - accuracy: 0.5258 - val_loss: 0.6885 - val_accuracy: 0.5747\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6905 - accuracy: 0.5348 - val_loss: 0.6900 - val_accuracy: 0.5430\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6902 - accuracy: 0.5258 - val_loss: 0.6906 - val_accuracy: 0.5385\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6904 - accuracy: 0.5525 - val_loss: 0.6970 - val_accuracy: 0.5701\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6979 - accuracy: 0.5010 - val_loss: 0.6918 - val_accuracy: 0.5701\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6953 - accuracy: 0.5131 - val_loss: 0.6842 - val_accuracy: 0.5701\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6967 - accuracy: 0.5061 - val_loss: 0.6941 - val_accuracy: 0.4299\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.6943 - accuracy: 0.5177 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.6921 - accuracy: 0.5253 - val_loss: 0.6852 - val_accuracy: 0.5701\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6932 - accuracy: 0.5227 - val_loss: 0.6881 - val_accuracy: 0.5701\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.6943 - accuracy: 0.5040 - val_loss: 0.6858 - val_accuracy: 0.5701\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 708us/step - loss: 0.6933 - accuracy: 0.5192 - val_loss: 0.6872 - val_accuracy: 0.5701\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6914 - accuracy: 0.5217 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6945 - accuracy: 0.4975 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6921 - accuracy: 0.5146 - val_loss: 0.6919 - val_accuracy: 0.5701\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 1s 691us/step - loss: 0.6916 - accuracy: 0.5268 - val_loss: 0.6879 - val_accuracy: 0.5701\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6913 - accuracy: 0.5298 - val_loss: 0.6910 - val_accuracy: 0.5475\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6927 - accuracy: 0.5081 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 1s 714us/step - loss: 0.6925 - accuracy: 0.5247 - val_loss: 0.6858 - val_accuracy: 0.5701\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6919 - accuracy: 0.5232 - val_loss: 0.6934 - val_accuracy: 0.4208\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6926 - accuracy: 0.5076 - val_loss: 0.6859 - val_accuracy: 0.5701\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.6927 - accuracy: 0.5222 - val_loss: 0.6881 - val_accuracy: 0.5701\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6933 - accuracy: 0.5212 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6928 - accuracy: 0.5227 - val_loss: 0.6861 - val_accuracy: 0.5701\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6928 - accuracy: 0.5066 - val_loss: 0.6860 - val_accuracy: 0.5701\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6931 - accuracy: 0.5247 - val_loss: 0.6869 - val_accuracy: 0.5701\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.6927 - accuracy: 0.5202 - val_loss: 0.6879 - val_accuracy: 0.5701\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 1s 672us/step - loss: 0.6920 - accuracy: 0.5212 - val_loss: 0.6877 - val_accuracy: 0.5701\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6931 - accuracy: 0.5247 - val_loss: 0.6865 - val_accuracy: 0.5701\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6926 - accuracy: 0.5242 - val_loss: 0.6879 - val_accuracy: 0.5701\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6922 - accuracy: 0.5242 - val_loss: 0.6869 - val_accuracy: 0.5701\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6924 - accuracy: 0.5242 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.6918 - accuracy: 0.5222 - val_loss: 0.6865 - val_accuracy: 0.5701\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 691us/step - loss: 0.6921 - accuracy: 0.5242 - val_loss: 0.6917 - val_accuracy: 0.5611\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6918 - accuracy: 0.5212 - val_loss: 0.6884 - val_accuracy: 0.5701\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6916 - accuracy: 0.5288 - val_loss: 0.6895 - val_accuracy: 0.5973\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6912 - accuracy: 0.5222 - val_loss: 0.6884 - val_accuracy: 0.5701\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6911 - accuracy: 0.5232 - val_loss: 0.6930 - val_accuracy: 0.4887\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6917 - accuracy: 0.5263 - val_loss: 0.6905 - val_accuracy: 0.5656\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6909 - accuracy: 0.5237 - val_loss: 0.6916 - val_accuracy: 0.5294\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6906 - accuracy: 0.5293 - val_loss: 0.6953 - val_accuracy: 0.5113\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6908 - accuracy: 0.5308 - val_loss: 0.6903 - val_accuracy: 0.5430\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6895 - accuracy: 0.5323 - val_loss: 0.6897 - val_accuracy: 0.5837\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6891 - accuracy: 0.5273 - val_loss: 0.6889 - val_accuracy: 0.5792\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.6924 - accuracy: 0.5237 - val_loss: 0.6858 - val_accuracy: 0.5837\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6916 - accuracy: 0.5197 - val_loss: 0.6868 - val_accuracy: 0.5837\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 1s 684us/step - loss: 0.6894 - accuracy: 0.5419 - val_loss: 0.6930 - val_accuracy: 0.5204\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6914 - accuracy: 0.5354 - val_loss: 0.6922 - val_accuracy: 0.5339\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6909 - accuracy: 0.5293 - val_loss: 0.6903 - val_accuracy: 0.5747\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6902 - accuracy: 0.5283 - val_loss: 0.6942 - val_accuracy: 0.5294\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 1s 718us/step - loss: 0.6891 - accuracy: 0.5429 - val_loss: 0.6933 - val_accuracy: 0.5294\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 1s 708us/step - loss: 0.6918 - accuracy: 0.5126 - val_loss: 0.6894 - val_accuracy: 0.5339\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6885 - accuracy: 0.5323 - val_loss: 0.7009 - val_accuracy: 0.4570\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6892 - accuracy: 0.5303 - val_loss: 0.6927 - val_accuracy: 0.5113\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6886 - accuracy: 0.5389 - val_loss: 0.6914 - val_accuracy: 0.5294\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.6905 - accuracy: 0.5273 - val_loss: 0.6893 - val_accuracy: 0.5701\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6896 - accuracy: 0.5288 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6900 - accuracy: 0.5237 - val_loss: 0.6955 - val_accuracy: 0.4977\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.6886 - accuracy: 0.5389 - val_loss: 0.6933 - val_accuracy: 0.5113\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6892 - accuracy: 0.5242 - val_loss: 0.6935 - val_accuracy: 0.5339\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 1s 671us/step - loss: 0.6882 - accuracy: 0.5323 - val_loss: 0.7009 - val_accuracy: 0.5158\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.6890 - accuracy: 0.5338 - val_loss: 0.6976 - val_accuracy: 0.5068\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.6905 - accuracy: 0.5197 - val_loss: 0.6891 - val_accuracy: 0.5294\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6878 - accuracy: 0.5333 - val_loss: 0.6966 - val_accuracy: 0.5113\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6890 - accuracy: 0.5308 - val_loss: 0.6929 - val_accuracy: 0.5249\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6873 - accuracy: 0.5404 - val_loss: 0.6896 - val_accuracy: 0.5973\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6864 - accuracy: 0.5354 - val_loss: 0.6997 - val_accuracy: 0.4932\n",
      "Epoch 102/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6868 - accuracy: 0.5510 - val_loss: 0.6892 - val_accuracy: 0.5566\n",
      "Epoch 103/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6855 - accuracy: 0.5394 - val_loss: 0.6987 - val_accuracy: 0.5249\n",
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6867 - accuracy: 0.5404 - val_loss: 0.6899 - val_accuracy: 0.5701\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6845 - accuracy: 0.5465 - val_loss: 0.6929 - val_accuracy: 0.5023\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6896 - accuracy: 0.5278 - val_loss: 0.6889 - val_accuracy: 0.5520\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6869 - accuracy: 0.5505 - val_loss: 0.6906 - val_accuracy: 0.5158\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6860 - accuracy: 0.5389 - val_loss: 0.6952 - val_accuracy: 0.5566\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6818 - accuracy: 0.5515 - val_loss: 0.7088 - val_accuracy: 0.5158\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 677us/step - loss: 0.6834 - accuracy: 0.5449 - val_loss: 0.6841 - val_accuracy: 0.5566\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6852 - accuracy: 0.5581 - val_loss: 0.6976 - val_accuracy: 0.5656\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6838 - accuracy: 0.5379 - val_loss: 0.6925 - val_accuracy: 0.5520\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6830 - accuracy: 0.5566 - val_loss: 0.7038 - val_accuracy: 0.5158\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 684us/step - loss: 0.6825 - accuracy: 0.5505 - val_loss: 0.6925 - val_accuracy: 0.4977\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6831 - accuracy: 0.5591 - val_loss: 0.7003 - val_accuracy: 0.5837\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 711us/step - loss: 0.6875 - accuracy: 0.5444 - val_loss: 0.6841 - val_accuracy: 0.5611\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6800 - accuracy: 0.5505 - val_loss: 0.7002 - val_accuracy: 0.5158\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6775 - accuracy: 0.5540 - val_loss: 0.6872 - val_accuracy: 0.5656\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6785 - accuracy: 0.5525 - val_loss: 0.7072 - val_accuracy: 0.5249\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 669us/step - loss: 0.6815 - accuracy: 0.5530 - val_loss: 0.6895 - val_accuracy: 0.5385\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6743 - accuracy: 0.5697 - val_loss: 0.6981 - val_accuracy: 0.5294\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6817 - accuracy: 0.5566 - val_loss: 0.7006 - val_accuracy: 0.4796\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.6749 - accuracy: 0.5641 - val_loss: 0.6867 - val_accuracy: 0.5611\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6718 - accuracy: 0.5808 - val_loss: 0.7167 - val_accuracy: 0.5204\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.6731 - accuracy: 0.5672 - val_loss: 0.6919 - val_accuracy: 0.5747\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.6681 - accuracy: 0.5808 - val_loss: 0.7019 - val_accuracy: 0.5385\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.6798 - accuracy: 0.5702 - val_loss: 0.6907 - val_accuracy: 0.5339\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.6731 - accuracy: 0.5783 - val_loss: 0.7015 - val_accuracy: 0.5339\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 687us/step - loss: 0.6703 - accuracy: 0.5712 - val_loss: 0.6947 - val_accuracy: 0.5385\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6711 - accuracy: 0.5621 - val_loss: 0.7021 - val_accuracy: 0.5294\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.6662 - accuracy: 0.5939 - val_loss: 0.7027 - val_accuracy: 0.5204\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6697 - accuracy: 0.5783 - val_loss: 0.6991 - val_accuracy: 0.5566\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6614 - accuracy: 0.5919 - val_loss: 0.7129 - val_accuracy: 0.5023\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 1s 679us/step - loss: 0.6628 - accuracy: 0.5929 - val_loss: 0.7079 - val_accuracy: 0.5701\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6712 - accuracy: 0.5874 - val_loss: 0.6977 - val_accuracy: 0.5566\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.6616 - accuracy: 0.5864 - val_loss: 0.7000 - val_accuracy: 0.5701\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6588 - accuracy: 0.5894 - val_loss: 0.6954 - val_accuracy: 0.5973\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 1s 686us/step - loss: 0.6688 - accuracy: 0.5854 - val_loss: 0.6896 - val_accuracy: 0.5520\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6674 - accuracy: 0.5864 - val_loss: 0.7034 - val_accuracy: 0.5747\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6535 - accuracy: 0.5960 - val_loss: 0.7098 - val_accuracy: 0.5339\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6565 - accuracy: 0.5939 - val_loss: 0.7112 - val_accuracy: 0.5475\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6509 - accuracy: 0.6071 - val_loss: 0.7537 - val_accuracy: 0.4842\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6486 - accuracy: 0.6192 - val_loss: 0.7316 - val_accuracy: 0.5339\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.6473 - accuracy: 0.6237 - val_loss: 0.7208 - val_accuracy: 0.5701\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6465 - accuracy: 0.6187 - val_loss: 0.7355 - val_accuracy: 0.5701\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6397 - accuracy: 0.6308 - val_loss: 0.7472 - val_accuracy: 0.4751\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6451 - accuracy: 0.6141 - val_loss: 0.7264 - val_accuracy: 0.5385\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6429 - accuracy: 0.6212 - val_loss: 0.7229 - val_accuracy: 0.5611\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 675us/step - loss: 0.6425 - accuracy: 0.6348 - val_loss: 0.7306 - val_accuracy: 0.5747\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6479 - accuracy: 0.6182 - val_loss: 0.7179 - val_accuracy: 0.5475\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6342 - accuracy: 0.6313 - val_loss: 0.7212 - val_accuracy: 0.5701\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6273 - accuracy: 0.6328 - val_loss: 0.7538 - val_accuracy: 0.5204\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6298 - accuracy: 0.6369 - val_loss: 0.7654 - val_accuracy: 0.5430\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.6235 - accuracy: 0.6394 - val_loss: 0.7585 - val_accuracy: 0.5520\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 1s 689us/step - loss: 0.6260 - accuracy: 0.6429 - val_loss: 0.7407 - val_accuracy: 0.5294\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6313 - accuracy: 0.6399 - val_loss: 0.7527 - val_accuracy: 0.5249\n",
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6274 - accuracy: 0.6449 - val_loss: 0.7424 - val_accuracy: 0.5339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 684us/step - loss: 0.6205 - accuracy: 0.6439 - val_loss: 0.7506 - val_accuracy: 0.5249\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.6163 - accuracy: 0.6540 - val_loss: 0.7758 - val_accuracy: 0.5249\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6176 - accuracy: 0.6485 - val_loss: 0.7511 - val_accuracy: 0.5339\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6161 - accuracy: 0.6571 - val_loss: 0.7716 - val_accuracy: 0.5475\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6133 - accuracy: 0.6500 - val_loss: 0.7491 - val_accuracy: 0.5430\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6108 - accuracy: 0.6591 - val_loss: 0.7591 - val_accuracy: 0.5294\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.6063 - accuracy: 0.6626 - val_loss: 0.7908 - val_accuracy: 0.5385\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6018 - accuracy: 0.6601 - val_loss: 0.7598 - val_accuracy: 0.5520\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6020 - accuracy: 0.6652 - val_loss: 0.7488 - val_accuracy: 0.5747\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6055 - accuracy: 0.6727 - val_loss: 0.8003 - val_accuracy: 0.5294\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6023 - accuracy: 0.6682 - val_loss: 0.8029 - val_accuracy: 0.5249\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.5879 - accuracy: 0.6646 - val_loss: 0.8314 - val_accuracy: 0.5566\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.5971 - accuracy: 0.6616 - val_loss: 0.8081 - val_accuracy: 0.5520\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.5963 - accuracy: 0.6626 - val_loss: 0.8169 - val_accuracy: 0.5385\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.5834 - accuracy: 0.6742 - val_loss: 0.8450 - val_accuracy: 0.5520\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 677us/step - loss: 0.5781 - accuracy: 0.6763 - val_loss: 0.8439 - val_accuracy: 0.5520\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 687us/step - loss: 0.5766 - accuracy: 0.6717 - val_loss: 0.8659 - val_accuracy: 0.5566\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 708us/step - loss: 0.5810 - accuracy: 0.6773 - val_loss: 0.8380 - val_accuracy: 0.5520\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.5807 - accuracy: 0.6768 - val_loss: 0.8130 - val_accuracy: 0.5430\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.5785 - accuracy: 0.6768 - val_loss: 0.8621 - val_accuracy: 0.5701\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.5685 - accuracy: 0.6889 - val_loss: 0.8105 - val_accuracy: 0.5294\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.5550 - accuracy: 0.6864 - val_loss: 0.8542 - val_accuracy: 0.5520\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.5738 - accuracy: 0.6854 - val_loss: 0.8616 - val_accuracy: 0.5656\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.5556 - accuracy: 0.6965 - val_loss: 0.8819 - val_accuracy: 0.5520\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 711us/step - loss: 0.5704 - accuracy: 0.6944 - val_loss: 0.8780 - val_accuracy: 0.5520\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.5719 - accuracy: 0.6869 - val_loss: 0.8727 - val_accuracy: 0.5339\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 692us/step - loss: 0.5510 - accuracy: 0.6894 - val_loss: 0.9056 - val_accuracy: 0.5430\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.5461 - accuracy: 0.7005 - val_loss: 0.9128 - val_accuracy: 0.5656\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.5659 - accuracy: 0.6773 - val_loss: 0.8803 - val_accuracy: 0.5204\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.5416 - accuracy: 0.7025 - val_loss: 0.9363 - val_accuracy: 0.5656\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.5383 - accuracy: 0.7081 - val_loss: 0.9504 - val_accuracy: 0.5430\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.5308 - accuracy: 0.7096 - val_loss: 0.9477 - val_accuracy: 0.5475\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.5353 - accuracy: 0.7081 - val_loss: 0.9392 - val_accuracy: 0.5430\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.5576 - accuracy: 0.6889 - val_loss: 0.9063 - val_accuracy: 0.5158\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 1s 677us/step - loss: 0.5276 - accuracy: 0.7172 - val_loss: 0.9742 - val_accuracy: 0.5475\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.5279 - accuracy: 0.7066 - val_loss: 0.9867 - val_accuracy: 0.5339\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.5242 - accuracy: 0.7061 - val_loss: 0.9264 - val_accuracy: 0.5656\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.5308 - accuracy: 0.7056 - val_loss: 1.0330 - val_accuracy: 0.5385\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.5170 - accuracy: 0.7172 - val_loss: 1.0706 - val_accuracy: 0.5385\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.5019 - accuracy: 0.7278 - val_loss: 1.0464 - val_accuracy: 0.5430\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.4933 - accuracy: 0.7313 - val_loss: 1.0084 - val_accuracy: 0.5385\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.5262 - accuracy: 0.7076 - val_loss: 1.0643 - val_accuracy: 0.5204\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 691us/step - loss: 0.5232 - accuracy: 0.7106 - val_loss: 1.0392 - val_accuracy: 0.5520\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 676us/step - loss: 0.5070 - accuracy: 0.7278 - val_loss: 0.9728 - val_accuracy: 0.5566\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 714us/step - loss: 0.4925 - accuracy: 0.7222 - val_loss: 1.1698 - val_accuracy: 0.5385\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 695us/step - loss: 0.4989 - accuracy: 0.7242 - val_loss: 1.0610 - val_accuracy: 0.5611\n",
      "Epoch 204/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.4845 - accuracy: 0.7354 - val_loss: 1.0922 - val_accuracy: 0.5566\n",
      "Epoch 205/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.4858 - accuracy: 0.7470 - val_loss: 1.1857 - val_accuracy: 0.5339\n",
      "Epoch 206/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.4794 - accuracy: 0.7434 - val_loss: 1.1001 - val_accuracy: 0.5656\n",
      "Epoch 207/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.4962 - accuracy: 0.7303 - val_loss: 1.1714 - val_accuracy: 0.5701\n",
      "Epoch 208/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.4662 - accuracy: 0.7495 - val_loss: 1.1227 - val_accuracy: 0.5611\n",
      "Epoch 209/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.4789 - accuracy: 0.7460 - val_loss: 1.1989 - val_accuracy: 0.5611\n",
      "Epoch 210/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.4607 - accuracy: 0.7606 - val_loss: 1.2102 - val_accuracy: 0.5113\n",
      "Epoch 211/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.4651 - accuracy: 0.7460 - val_loss: 1.1555 - val_accuracy: 0.5611\n",
      "Epoch 212/1000\n",
      "1980/1980 [==============================] - 1s 687us/step - loss: 0.4480 - accuracy: 0.7606 - val_loss: 1.2356 - val_accuracy: 0.5656\n",
      "Epoch 213/1000\n",
      "1980/1980 [==============================] - 1s 708us/step - loss: 0.4445 - accuracy: 0.7722 - val_loss: 1.2333 - val_accuracy: 0.5385\n",
      "Epoch 214/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.4656 - accuracy: 0.7571 - val_loss: 1.1043 - val_accuracy: 0.5747\n",
      "Epoch 215/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.4660 - accuracy: 0.7545 - val_loss: 1.2337 - val_accuracy: 0.5339\n",
      "Epoch 216/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.4625 - accuracy: 0.7586 - val_loss: 1.2626 - val_accuracy: 0.5566\n",
      "Epoch 217/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.4430 - accuracy: 0.7616 - val_loss: 1.2500 - val_accuracy: 0.5611\n",
      "Epoch 218/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.4238 - accuracy: 0.7768 - val_loss: 1.1823 - val_accuracy: 0.5520\n",
      "Epoch 219/1000\n",
      "1980/1980 [==============================] - 1s 692us/step - loss: 0.4425 - accuracy: 0.7667 - val_loss: 1.2763 - val_accuracy: 0.5475\n",
      "Epoch 220/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.4366 - accuracy: 0.7646 - val_loss: 1.2646 - val_accuracy: 0.5882\n",
      "Epoch 221/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.4241 - accuracy: 0.7707 - val_loss: 1.3088 - val_accuracy: 0.5475\n",
      "Epoch 222/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.4241 - accuracy: 0.7702 - val_loss: 1.3572 - val_accuracy: 0.5385\n",
      "Epoch 223/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.4181 - accuracy: 0.7768 - val_loss: 1.3453 - val_accuracy: 0.5611\n",
      "Epoch 224/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.4174 - accuracy: 0.7687 - val_loss: 1.2781 - val_accuracy: 0.5611\n",
      "Epoch 225/1000\n",
      "1980/1980 [==============================] - 1s 684us/step - loss: 0.3968 - accuracy: 0.7904 - val_loss: 1.3586 - val_accuracy: 0.5566\n",
      "Epoch 226/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.3844 - accuracy: 0.7899 - val_loss: 1.4924 - val_accuracy: 0.5339\n",
      "Epoch 00226: early stopping\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 193us/step\n",
      "test loss, test acc: [1.3268592109485549, 0.5142857432365417]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.6006114553838006\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_accuracy\", patience=200, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(15, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_67 (LSTM)               (None, 15, 128)           113152    \n",
      "_________________________________________________________________\n",
      "lstm_68 (LSTM)               (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_69 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 175,009\n",
      "Trainable params: 175,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 2s 935us/step - loss: 0.6975 - accuracy: 0.5152 - val_loss: 0.6924 - val_accuracy: 0.4977\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6938 - accuracy: 0.5111 - val_loss: 0.6939 - val_accuracy: 0.4796\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6933 - accuracy: 0.5157 - val_loss: 0.6883 - val_accuracy: 0.5747\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6914 - accuracy: 0.5197 - val_loss: 0.6905 - val_accuracy: 0.5385\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.6909 - accuracy: 0.5268 - val_loss: 0.6890 - val_accuracy: 0.5656\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6920 - accuracy: 0.5202 - val_loss: 0.6883 - val_accuracy: 0.5747\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6931 - accuracy: 0.5101 - val_loss: 0.6920 - val_accuracy: 0.5475\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6923 - accuracy: 0.5162 - val_loss: 0.7005 - val_accuracy: 0.4344\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6933 - accuracy: 0.5187 - val_loss: 0.6857 - val_accuracy: 0.5701\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6915 - accuracy: 0.5217 - val_loss: 0.6927 - val_accuracy: 0.5520\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.6928 - accuracy: 0.5298 - val_loss: 0.6887 - val_accuracy: 0.5701\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6913 - accuracy: 0.5293 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6922 - accuracy: 0.5141 - val_loss: 0.6926 - val_accuracy: 0.5475\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6921 - accuracy: 0.5157 - val_loss: 0.6911 - val_accuracy: 0.5701\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6911 - accuracy: 0.5232 - val_loss: 0.6916 - val_accuracy: 0.5566\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6929 - accuracy: 0.5157 - val_loss: 0.6860 - val_accuracy: 0.5701\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6913 - accuracy: 0.5217 - val_loss: 0.6899 - val_accuracy: 0.5973\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6909 - accuracy: 0.5338 - val_loss: 0.6892 - val_accuracy: 0.5928\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6916 - accuracy: 0.5258 - val_loss: 0.6872 - val_accuracy: 0.5792\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6904 - accuracy: 0.5263 - val_loss: 0.6930 - val_accuracy: 0.5475\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.6903 - accuracy: 0.5333 - val_loss: 0.6878 - val_accuracy: 0.5882\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.6910 - accuracy: 0.5268 - val_loss: 0.6883 - val_accuracy: 0.5928\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.6915 - accuracy: 0.5141 - val_loss: 0.6908 - val_accuracy: 0.5656\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.6912 - accuracy: 0.5121 - val_loss: 0.6901 - val_accuracy: 0.5385\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.6924 - accuracy: 0.5283 - val_loss: 0.6881 - val_accuracy: 0.5928\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.6915 - accuracy: 0.5263 - val_loss: 0.6930 - val_accuracy: 0.5385\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.6913 - accuracy: 0.5237 - val_loss: 0.6899 - val_accuracy: 0.5656\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.6912 - accuracy: 0.5146 - val_loss: 0.6929 - val_accuracy: 0.5792\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.6910 - accuracy: 0.5278 - val_loss: 0.6901 - val_accuracy: 0.5747\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6903 - accuracy: 0.5278 - val_loss: 0.6915 - val_accuracy: 0.5882\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6901 - accuracy: 0.5273 - val_loss: 0.6907 - val_accuracy: 0.5611\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6902 - accuracy: 0.5258 - val_loss: 0.6922 - val_accuracy: 0.5249\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.6908 - accuracy: 0.5258 - val_loss: 0.6922 - val_accuracy: 0.5158\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6905 - accuracy: 0.5187 - val_loss: 0.6953 - val_accuracy: 0.5385\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.6899 - accuracy: 0.5308 - val_loss: 0.6924 - val_accuracy: 0.5204\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.6910 - accuracy: 0.5364 - val_loss: 0.6873 - val_accuracy: 0.5928\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.6905 - accuracy: 0.5232 - val_loss: 0.6873 - val_accuracy: 0.5882\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.6911 - accuracy: 0.5323 - val_loss: 0.6927 - val_accuracy: 0.5294\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.6904 - accuracy: 0.5354 - val_loss: 0.6889 - val_accuracy: 0.5792\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.6899 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5249\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6917 - accuracy: 0.5182 - val_loss: 0.6985 - val_accuracy: 0.4389\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.6885 - accuracy: 0.5343 - val_loss: 0.6900 - val_accuracy: 0.5928\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.6886 - accuracy: 0.5313 - val_loss: 0.6958 - val_accuracy: 0.5158\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6929 - val_accuracy: 0.5882\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6905 - accuracy: 0.5354 - val_loss: 0.6933 - val_accuracy: 0.5520\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6900 - accuracy: 0.5247 - val_loss: 0.6918 - val_accuracy: 0.5204\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6889 - accuracy: 0.5273 - val_loss: 0.6956 - val_accuracy: 0.5882\n",
      "Epoch 48/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6886 - accuracy: 0.5338 - val_loss: 0.6891 - val_accuracy: 0.5475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6900 - accuracy: 0.5298 - val_loss: 0.6921 - val_accuracy: 0.5113\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6915 - accuracy: 0.5308 - val_loss: 0.6953 - val_accuracy: 0.5882\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6892 - accuracy: 0.5232 - val_loss: 0.6926 - val_accuracy: 0.5837\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6890 - accuracy: 0.5283 - val_loss: 0.6890 - val_accuracy: 0.5520\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6908 - accuracy: 0.5364 - val_loss: 0.6853 - val_accuracy: 0.5928\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6883 - accuracy: 0.5354 - val_loss: 0.6976 - val_accuracy: 0.5294\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6881 - accuracy: 0.5429 - val_loss: 0.6932 - val_accuracy: 0.5747\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6892 - accuracy: 0.5293 - val_loss: 0.6923 - val_accuracy: 0.5339\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6893 - accuracy: 0.5379 - val_loss: 0.6939 - val_accuracy: 0.5339\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6864 - accuracy: 0.5505 - val_loss: 0.6919 - val_accuracy: 0.5701\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6842 - accuracy: 0.5525 - val_loss: 0.6908 - val_accuracy: 0.5430\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6876 - accuracy: 0.5369 - val_loss: 0.6878 - val_accuracy: 0.5611\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.6891 - accuracy: 0.5369 - val_loss: 0.6914 - val_accuracy: 0.5701\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6917 - accuracy: 0.5136 - val_loss: 0.6876 - val_accuracy: 0.5837\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6872 - accuracy: 0.5333 - val_loss: 0.6986 - val_accuracy: 0.5294\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6866 - accuracy: 0.5379 - val_loss: 0.6940 - val_accuracy: 0.5656\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.6849 - accuracy: 0.5520 - val_loss: 0.7049 - val_accuracy: 0.5566\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6886 - accuracy: 0.5374 - val_loss: 0.6883 - val_accuracy: 0.5520\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6867 - accuracy: 0.5551 - val_loss: 0.6968 - val_accuracy: 0.5520\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6846 - accuracy: 0.5551 - val_loss: 0.6915 - val_accuracy: 0.5520\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6869 - accuracy: 0.5348 - val_loss: 0.6947 - val_accuracy: 0.5294\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6876 - accuracy: 0.5384 - val_loss: 0.6898 - val_accuracy: 0.5701\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6854 - accuracy: 0.5424 - val_loss: 0.6975 - val_accuracy: 0.5113\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6854 - accuracy: 0.5389 - val_loss: 0.6876 - val_accuracy: 0.5566\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6836 - accuracy: 0.5535 - val_loss: 0.6971 - val_accuracy: 0.5520\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6812 - accuracy: 0.5747 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6835 - accuracy: 0.5626 - val_loss: 0.6968 - val_accuracy: 0.5294\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6838 - accuracy: 0.5520 - val_loss: 0.6983 - val_accuracy: 0.5068\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6817 - accuracy: 0.5712 - val_loss: 0.6985 - val_accuracy: 0.5430\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6810 - accuracy: 0.5758 - val_loss: 0.6959 - val_accuracy: 0.5520\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6824 - accuracy: 0.5616 - val_loss: 0.6930 - val_accuracy: 0.5339\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6784 - accuracy: 0.5823 - val_loss: 0.7026 - val_accuracy: 0.5475\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6821 - accuracy: 0.5545 - val_loss: 0.6876 - val_accuracy: 0.5520\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6774 - accuracy: 0.5682 - val_loss: 0.7030 - val_accuracy: 0.5656\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6787 - accuracy: 0.5707 - val_loss: 0.6956 - val_accuracy: 0.5792\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6758 - accuracy: 0.5793 - val_loss: 0.7073 - val_accuracy: 0.5068\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6802 - accuracy: 0.5586 - val_loss: 0.6947 - val_accuracy: 0.5294\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6792 - accuracy: 0.5742 - val_loss: 0.6879 - val_accuracy: 0.5611\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6739 - accuracy: 0.5808 - val_loss: 0.7037 - val_accuracy: 0.5520\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6726 - accuracy: 0.5899 - val_loss: 0.6997 - val_accuracy: 0.5701\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6732 - accuracy: 0.5742 - val_loss: 0.6979 - val_accuracy: 0.5475\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6732 - accuracy: 0.5823 - val_loss: 0.7138 - val_accuracy: 0.5430\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6712 - accuracy: 0.5697 - val_loss: 0.6985 - val_accuracy: 0.5430\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6676 - accuracy: 0.5919 - val_loss: 0.6969 - val_accuracy: 0.5928\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6721 - accuracy: 0.5828 - val_loss: 0.7159 - val_accuracy: 0.5339\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6721 - accuracy: 0.5747 - val_loss: 0.7092 - val_accuracy: 0.5385\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.6703 - accuracy: 0.5879 - val_loss: 0.7166 - val_accuracy: 0.5339\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6677 - accuracy: 0.5924 - val_loss: 0.7042 - val_accuracy: 0.5656\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6653 - accuracy: 0.5990 - val_loss: 0.7138 - val_accuracy: 0.5520\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6663 - accuracy: 0.6020 - val_loss: 0.7065 - val_accuracy: 0.5430\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6668 - accuracy: 0.5848 - val_loss: 0.7117 - val_accuracy: 0.5113\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6619 - accuracy: 0.5929 - val_loss: 0.7077 - val_accuracy: 0.5520\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6671 - accuracy: 0.5788 - val_loss: 0.7036 - val_accuracy: 0.5339\n",
      "Epoch 102/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6598 - accuracy: 0.5970 - val_loss: 0.7196 - val_accuracy: 0.5204\n",
      "Epoch 103/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6722 - accuracy: 0.5692 - val_loss: 0.6995 - val_accuracy: 0.5430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6580 - accuracy: 0.6066 - val_loss: 0.7340 - val_accuracy: 0.5294\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.6626 - accuracy: 0.5929 - val_loss: 0.7023 - val_accuracy: 0.5430\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6601 - accuracy: 0.5980 - val_loss: 0.7202 - val_accuracy: 0.5339\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6595 - accuracy: 0.5909 - val_loss: 0.7251 - val_accuracy: 0.5385\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6509 - accuracy: 0.6121 - val_loss: 0.7326 - val_accuracy: 0.5566\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6518 - accuracy: 0.6126 - val_loss: 0.7391 - val_accuracy: 0.5385\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6545 - accuracy: 0.6081 - val_loss: 0.7235 - val_accuracy: 0.5339\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6535 - accuracy: 0.6091 - val_loss: 0.7186 - val_accuracy: 0.5792\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6428 - accuracy: 0.6298 - val_loss: 0.7363 - val_accuracy: 0.5249\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6437 - accuracy: 0.6167 - val_loss: 0.7358 - val_accuracy: 0.5475\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6441 - accuracy: 0.6157 - val_loss: 0.7318 - val_accuracy: 0.5475\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6507 - accuracy: 0.6131 - val_loss: 0.7265 - val_accuracy: 0.5566\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6409 - accuracy: 0.6298 - val_loss: 0.7357 - val_accuracy: 0.5611\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6510 - accuracy: 0.6136 - val_loss: 0.7088 - val_accuracy: 0.5475\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6493 - accuracy: 0.6212 - val_loss: 0.7412 - val_accuracy: 0.5430\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6394 - accuracy: 0.6308 - val_loss: 0.7442 - val_accuracy: 0.5068\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6408 - accuracy: 0.6283 - val_loss: 0.7483 - val_accuracy: 0.5475\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6408 - accuracy: 0.6318 - val_loss: 0.7547 - val_accuracy: 0.5204\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6372 - accuracy: 0.6227 - val_loss: 0.7393 - val_accuracy: 0.5249\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6436 - accuracy: 0.6298 - val_loss: 0.7296 - val_accuracy: 0.5249\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6324 - accuracy: 0.6364 - val_loss: 0.7313 - val_accuracy: 0.5430\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6307 - accuracy: 0.6384 - val_loss: 0.7551 - val_accuracy: 0.5068\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6317 - accuracy: 0.6399 - val_loss: 0.7361 - val_accuracy: 0.4977\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6276 - accuracy: 0.6348 - val_loss: 0.7648 - val_accuracy: 0.5158\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.6213 - accuracy: 0.6571 - val_loss: 0.7614 - val_accuracy: 0.5068\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.6212 - accuracy: 0.6465 - val_loss: 0.7639 - val_accuracy: 0.5249\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.6168 - accuracy: 0.6525 - val_loss: 0.7690 - val_accuracy: 0.5158\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 442us/step - loss: 0.6211 - accuracy: 0.6429 - val_loss: 0.7939 - val_accuracy: 0.4977\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.6181 - accuracy: 0.6455 - val_loss: 0.7792 - val_accuracy: 0.5385\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6187 - accuracy: 0.6333 - val_loss: 0.7603 - val_accuracy: 0.5294\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6082 - accuracy: 0.6586 - val_loss: 0.7662 - val_accuracy: 0.5339\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6055 - accuracy: 0.6530 - val_loss: 0.7660 - val_accuracy: 0.5339\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6048 - accuracy: 0.6515 - val_loss: 0.7772 - val_accuracy: 0.5566\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6061 - accuracy: 0.6606 - val_loss: 0.7850 - val_accuracy: 0.5249\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.6056 - accuracy: 0.6677 - val_loss: 0.7646 - val_accuracy: 0.5158\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 0.5923 - accuracy: 0.6722 - val_loss: 0.7948 - val_accuracy: 0.5294\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.6023 - accuracy: 0.6722 - val_loss: 0.8319 - val_accuracy: 0.5068\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.6118 - accuracy: 0.6394 - val_loss: 0.7552 - val_accuracy: 0.5566\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.5944 - accuracy: 0.6753 - val_loss: 0.7646 - val_accuracy: 0.5339\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 476us/step - loss: 0.5861 - accuracy: 0.6864 - val_loss: 0.8209 - val_accuracy: 0.5023\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.5798 - accuracy: 0.6763 - val_loss: 0.8689 - val_accuracy: 0.5158\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.5784 - accuracy: 0.6823 - val_loss: 0.7959 - val_accuracy: 0.5068\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.5750 - accuracy: 0.6914 - val_loss: 0.8604 - val_accuracy: 0.5294\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.5970 - accuracy: 0.6631 - val_loss: 0.7804 - val_accuracy: 0.4977\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.5700 - accuracy: 0.6924 - val_loss: 0.8448 - val_accuracy: 0.5249\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.5632 - accuracy: 0.6990 - val_loss: 0.8540 - val_accuracy: 0.4751\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.5647 - accuracy: 0.6960 - val_loss: 0.8295 - val_accuracy: 0.5113\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.5665 - accuracy: 0.7040 - val_loss: 0.8717 - val_accuracy: 0.5204\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 0.5579 - accuracy: 0.7035 - val_loss: 0.8904 - val_accuracy: 0.5068\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.5447 - accuracy: 0.7101 - val_loss: 0.8972 - val_accuracy: 0.5068\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.5401 - accuracy: 0.7101 - val_loss: 0.8638 - val_accuracy: 0.5385\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 0.5440 - accuracy: 0.7061 - val_loss: 0.9389 - val_accuracy: 0.5068\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.5445 - accuracy: 0.7076 - val_loss: 0.9119 - val_accuracy: 0.5566\n",
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.5669 - accuracy: 0.6854 - val_loss: 0.8544 - val_accuracy: 0.4706\n",
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.5458 - accuracy: 0.7040 - val_loss: 0.9288 - val_accuracy: 0.5158\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5421 - accuracy: 0.7091 - val_loss: 0.8208 - val_accuracy: 0.5249\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5381 - accuracy: 0.7066 - val_loss: 0.9520 - val_accuracy: 0.4977\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.5218 - accuracy: 0.7172 - val_loss: 1.0018 - val_accuracy: 0.5158\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.5190 - accuracy: 0.7177 - val_loss: 0.9534 - val_accuracy: 0.5339\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.5056 - accuracy: 0.7323 - val_loss: 0.8805 - val_accuracy: 0.5158\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.5241 - accuracy: 0.7283 - val_loss: 1.0091 - val_accuracy: 0.5023\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.5201 - accuracy: 0.7253 - val_loss: 0.9885 - val_accuracy: 0.4932\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.5069 - accuracy: 0.7409 - val_loss: 1.0342 - val_accuracy: 0.5068\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.4936 - accuracy: 0.7480 - val_loss: 0.9918 - val_accuracy: 0.5158\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.4855 - accuracy: 0.7414 - val_loss: 0.9764 - val_accuracy: 0.5294\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.4971 - accuracy: 0.7364 - val_loss: 0.9619 - val_accuracy: 0.5204\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.4839 - accuracy: 0.7601 - val_loss: 1.1171 - val_accuracy: 0.5068\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.4771 - accuracy: 0.7551 - val_loss: 1.1282 - val_accuracy: 0.5520\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.4744 - accuracy: 0.7626 - val_loss: 1.0966 - val_accuracy: 0.5294\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.4630 - accuracy: 0.7606 - val_loss: 1.1558 - val_accuracy: 0.5068\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.4633 - accuracy: 0.7641 - val_loss: 1.0788 - val_accuracy: 0.5385\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.4819 - accuracy: 0.7566 - val_loss: 1.0725 - val_accuracy: 0.5339\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.4670 - accuracy: 0.7581 - val_loss: 1.1519 - val_accuracy: 0.5385\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.4502 - accuracy: 0.7732 - val_loss: 1.1732 - val_accuracy: 0.4932\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.4582 - accuracy: 0.7551 - val_loss: 1.1928 - val_accuracy: 0.4887\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.4659 - accuracy: 0.7641 - val_loss: 1.2115 - val_accuracy: 0.5385\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.4473 - accuracy: 0.7818 - val_loss: 1.2027 - val_accuracy: 0.4932\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.4305 - accuracy: 0.7833 - val_loss: 1.2374 - val_accuracy: 0.5294\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.4258 - accuracy: 0.7818 - val_loss: 1.3364 - val_accuracy: 0.5068\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.4269 - accuracy: 0.7838 - val_loss: 1.2318 - val_accuracy: 0.5294\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.4067 - accuracy: 0.7944 - val_loss: 1.3818 - val_accuracy: 0.4977\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.3847 - accuracy: 0.8066 - val_loss: 1.3333 - val_accuracy: 0.5520\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.3786 - accuracy: 0.8091 - val_loss: 1.4752 - val_accuracy: 0.5113\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.3869 - accuracy: 0.8040 - val_loss: 1.3600 - val_accuracy: 0.4977\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.3918 - accuracy: 0.8000 - val_loss: 1.5129 - val_accuracy: 0.5339\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.3864 - accuracy: 0.8061 - val_loss: 1.4342 - val_accuracy: 0.5475\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.3615 - accuracy: 0.8222 - val_loss: 1.5128 - val_accuracy: 0.4977\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.3756 - accuracy: 0.8141 - val_loss: 1.4514 - val_accuracy: 0.5339\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.3888 - accuracy: 0.7995 - val_loss: 1.4158 - val_accuracy: 0.5249\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.3591 - accuracy: 0.8247 - val_loss: 1.5545 - val_accuracy: 0.5430\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.3362 - accuracy: 0.8359 - val_loss: 1.5190 - val_accuracy: 0.5294\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3394 - accuracy: 0.8359 - val_loss: 1.5563 - val_accuracy: 0.4887\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.3563 - accuracy: 0.8258 - val_loss: 1.4738 - val_accuracy: 0.5204\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.3397 - accuracy: 0.8369 - val_loss: 1.6621 - val_accuracy: 0.5249\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.3123 - accuracy: 0.8485 - val_loss: 1.6972 - val_accuracy: 0.5068\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.3127 - accuracy: 0.8490 - val_loss: 1.6821 - val_accuracy: 0.5204\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.3245 - accuracy: 0.8439 - val_loss: 1.6441 - val_accuracy: 0.5294\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.3296 - accuracy: 0.8354 - val_loss: 1.6980 - val_accuracy: 0.4977\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.3125 - accuracy: 0.8465 - val_loss: 1.7211 - val_accuracy: 0.5204\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.2911 - accuracy: 0.8601 - val_loss: 1.8065 - val_accuracy: 0.5294\n",
      "Epoch 204/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.2935 - accuracy: 0.8586 - val_loss: 1.9126 - val_accuracy: 0.5204\n",
      "Epoch 205/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.3127 - accuracy: 0.8515 - val_loss: 1.8263 - val_accuracy: 0.5249\n",
      "Epoch 206/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.3037 - accuracy: 0.8581 - val_loss: 1.8042 - val_accuracy: 0.5023\n",
      "Epoch 207/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.2738 - accuracy: 0.8697 - val_loss: 1.9345 - val_accuracy: 0.4796\n",
      "Epoch 208/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.2749 - accuracy: 0.8657 - val_loss: 1.8673 - val_accuracy: 0.4977\n",
      "Epoch 209/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.2553 - accuracy: 0.8742 - val_loss: 1.9912 - val_accuracy: 0.5249\n",
      "Epoch 210/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.2422 - accuracy: 0.8833 - val_loss: 2.0095 - val_accuracy: 0.5339\n",
      "Epoch 211/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.2557 - accuracy: 0.8798 - val_loss: 1.9129 - val_accuracy: 0.5385\n",
      "Epoch 212/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.2534 - accuracy: 0.8833 - val_loss: 2.0013 - val_accuracy: 0.5339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.2354 - accuracy: 0.8914 - val_loss: 2.0587 - val_accuracy: 0.4977\n",
      "Epoch 214/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.2389 - accuracy: 0.8848 - val_loss: 2.0053 - val_accuracy: 0.5204\n",
      "Epoch 215/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.2644 - accuracy: 0.8717 - val_loss: 2.0772 - val_accuracy: 0.5068\n",
      "Epoch 216/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.2962 - accuracy: 0.8611 - val_loss: 2.0445 - val_accuracy: 0.5249\n",
      "Epoch 217/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.2709 - accuracy: 0.8737 - val_loss: 2.0396 - val_accuracy: 0.5294\n",
      "Epoch 218/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.2589 - accuracy: 0.8758 - val_loss: 2.0587 - val_accuracy: 0.5158\n",
      "Epoch 219/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.2486 - accuracy: 0.8838 - val_loss: 2.1186 - val_accuracy: 0.5068\n",
      "Epoch 220/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.2069 - accuracy: 0.8970 - val_loss: 2.2319 - val_accuracy: 0.5068\n",
      "Epoch 221/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.1866 - accuracy: 0.9152 - val_loss: 2.1985 - val_accuracy: 0.5249\n",
      "Epoch 222/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.1998 - accuracy: 0.9076 - val_loss: 2.1599 - val_accuracy: 0.4977\n",
      "Epoch 223/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 0.1989 - accuracy: 0.9146 - val_loss: 2.2302 - val_accuracy: 0.5158\n",
      "Epoch 224/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.2020 - accuracy: 0.9045 - val_loss: 2.2720 - val_accuracy: 0.5023\n",
      "Epoch 225/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.1922 - accuracy: 0.9111 - val_loss: 2.2978 - val_accuracy: 0.5113\n",
      "Epoch 226/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.1719 - accuracy: 0.9222 - val_loss: 2.2723 - val_accuracy: 0.5158\n",
      "Epoch 227/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.1543 - accuracy: 0.9359 - val_loss: 2.4087 - val_accuracy: 0.5249\n",
      "Epoch 228/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.1423 - accuracy: 0.9364 - val_loss: 2.5071 - val_accuracy: 0.4932\n",
      "Epoch 229/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.1365 - accuracy: 0.9439 - val_loss: 2.6410 - val_accuracy: 0.5204\n",
      "Epoch 230/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.1345 - accuracy: 0.9394 - val_loss: 2.5859 - val_accuracy: 0.5204\n",
      "Epoch 231/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.1301 - accuracy: 0.9460 - val_loss: 2.6546 - val_accuracy: 0.5068\n",
      "Epoch 232/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 0.1267 - accuracy: 0.9444 - val_loss: 2.6756 - val_accuracy: 0.5158\n",
      "Epoch 233/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.1413 - accuracy: 0.9399 - val_loss: 2.6298 - val_accuracy: 0.5158\n",
      "Epoch 234/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.1507 - accuracy: 0.9298 - val_loss: 2.6892 - val_accuracy: 0.5430\n",
      "Epoch 235/1000\n",
      "1980/1980 [==============================] - 1s 450us/step - loss: 0.1485 - accuracy: 0.9389 - val_loss: 2.7717 - val_accuracy: 0.5385\n",
      "Epoch 236/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 0.1745 - accuracy: 0.9288 - val_loss: 2.6269 - val_accuracy: 0.5204\n",
      "Epoch 237/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.1945 - accuracy: 0.9136 - val_loss: 2.6327 - val_accuracy: 0.4842\n",
      "Epoch 238/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.4646 - accuracy: 0.8192 - val_loss: 2.0030 - val_accuracy: 0.5294\n",
      "Epoch 239/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.3557 - accuracy: 0.8419 - val_loss: 1.8578 - val_accuracy: 0.4977\n",
      "Epoch 240/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.2592 - accuracy: 0.8848 - val_loss: 2.0640 - val_accuracy: 0.5023\n",
      "Epoch 241/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.2010 - accuracy: 0.9167 - val_loss: 2.2767 - val_accuracy: 0.4661\n",
      "Epoch 242/1000\n",
      "1980/1980 [==============================] - 1s 455us/step - loss: 0.1976 - accuracy: 0.9192 - val_loss: 2.2955 - val_accuracy: 0.5068\n",
      "Epoch 243/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.1854 - accuracy: 0.9202 - val_loss: 2.3144 - val_accuracy: 0.5068\n",
      "Epoch 244/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.1628 - accuracy: 0.9313 - val_loss: 2.2824 - val_accuracy: 0.5249\n",
      "Epoch 245/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.1253 - accuracy: 0.9449 - val_loss: 2.4906 - val_accuracy: 0.5385\n",
      "Epoch 246/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.1261 - accuracy: 0.9495 - val_loss: 2.4151 - val_accuracy: 0.5113\n",
      "Epoch 247/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.1129 - accuracy: 0.9525 - val_loss: 2.4748 - val_accuracy: 0.5113\n",
      "Epoch 248/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.1039 - accuracy: 0.9535 - val_loss: 2.6152 - val_accuracy: 0.5158\n",
      "Epoch 249/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0902 - accuracy: 0.9631 - val_loss: 2.7056 - val_accuracy: 0.4706\n",
      "Epoch 250/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0975 - accuracy: 0.9566 - val_loss: 2.7876 - val_accuracy: 0.4932\n",
      "Epoch 251/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0947 - accuracy: 0.9581 - val_loss: 2.8144 - val_accuracy: 0.4932\n",
      "Epoch 252/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0902 - accuracy: 0.9652 - val_loss: 2.8037 - val_accuracy: 0.4842\n",
      "Epoch 253/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0881 - accuracy: 0.9682 - val_loss: 2.9407 - val_accuracy: 0.5068\n",
      "Epoch 254/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0826 - accuracy: 0.9662 - val_loss: 2.9173 - val_accuracy: 0.4796\n",
      "Epoch 255/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0958 - accuracy: 0.9606 - val_loss: 2.8381 - val_accuracy: 0.4932\n",
      "Epoch 256/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.1176 - accuracy: 0.9561 - val_loss: 2.6710 - val_accuracy: 0.5023\n",
      "Epoch 257/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.1065 - accuracy: 0.9581 - val_loss: 2.8828 - val_accuracy: 0.5023\n",
      "Epoch 258/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.1069 - accuracy: 0.9561 - val_loss: 2.9378 - val_accuracy: 0.4932\n",
      "Epoch 259/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0966 - accuracy: 0.9586 - val_loss: 3.0305 - val_accuracy: 0.4932\n",
      "Epoch 260/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.1140 - accuracy: 0.9485 - val_loss: 3.0161 - val_accuracy: 0.5023\n",
      "Epoch 261/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.1144 - accuracy: 0.9556 - val_loss: 2.9876 - val_accuracy: 0.5113\n",
      "Epoch 262/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.1295 - accuracy: 0.9485 - val_loss: 2.8745 - val_accuracy: 0.5204\n",
      "Epoch 263/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.1421 - accuracy: 0.9475 - val_loss: 2.8843 - val_accuracy: 0.5068\n",
      "Epoch 264/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.1532 - accuracy: 0.9394 - val_loss: 2.9049 - val_accuracy: 0.4977\n",
      "Epoch 265/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0975 - accuracy: 0.9596 - val_loss: 2.7899 - val_accuracy: 0.5249\n",
      "Epoch 266/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.1000 - accuracy: 0.9611 - val_loss: 2.9149 - val_accuracy: 0.5339\n",
      "Epoch 267/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.1012 - accuracy: 0.9576 - val_loss: 2.9209 - val_accuracy: 0.5023\n",
      "Epoch 268/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0989 - accuracy: 0.9652 - val_loss: 3.0198 - val_accuracy: 0.5023\n",
      "Epoch 269/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.0812 - accuracy: 0.9692 - val_loss: 3.1038 - val_accuracy: 0.5113\n",
      "Epoch 270/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.0600 - accuracy: 0.9773 - val_loss: 3.0849 - val_accuracy: 0.5068\n",
      "Epoch 271/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.0517 - accuracy: 0.9803 - val_loss: 3.1899 - val_accuracy: 0.4977\n",
      "Epoch 272/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.0577 - accuracy: 0.9803 - val_loss: 3.0984 - val_accuracy: 0.5068\n",
      "Epoch 273/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0484 - accuracy: 0.9838 - val_loss: 3.2340 - val_accuracy: 0.5068\n",
      "Epoch 274/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0441 - accuracy: 0.9823 - val_loss: 3.1909 - val_accuracy: 0.4887\n",
      "Epoch 275/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0464 - accuracy: 0.9808 - val_loss: 3.2788 - val_accuracy: 0.4842\n",
      "Epoch 276/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0424 - accuracy: 0.9859 - val_loss: 3.1996 - val_accuracy: 0.5204\n",
      "Epoch 277/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0384 - accuracy: 0.9828 - val_loss: 3.3957 - val_accuracy: 0.5113\n",
      "Epoch 278/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0426 - accuracy: 0.9833 - val_loss: 3.3389 - val_accuracy: 0.4932\n",
      "Epoch 279/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0556 - accuracy: 0.9793 - val_loss: 3.2212 - val_accuracy: 0.5204\n",
      "Epoch 280/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0553 - accuracy: 0.9778 - val_loss: 3.2072 - val_accuracy: 0.5158\n",
      "Epoch 281/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0395 - accuracy: 0.9838 - val_loss: 3.2922 - val_accuracy: 0.5158\n",
      "Epoch 282/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0319 - accuracy: 0.9889 - val_loss: 3.3423 - val_accuracy: 0.5023\n",
      "Epoch 283/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0288 - accuracy: 0.9894 - val_loss: 3.3991 - val_accuracy: 0.5068\n",
      "Epoch 284/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0308 - accuracy: 0.9899 - val_loss: 3.5092 - val_accuracy: 0.5158\n",
      "Epoch 285/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0374 - accuracy: 0.9848 - val_loss: 3.4288 - val_accuracy: 0.4977\n",
      "Epoch 286/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0423 - accuracy: 0.9843 - val_loss: 3.5127 - val_accuracy: 0.5023\n",
      "Epoch 287/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0404 - accuracy: 0.9838 - val_loss: 3.4716 - val_accuracy: 0.4977\n",
      "Epoch 288/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0410 - accuracy: 0.9823 - val_loss: 3.4943 - val_accuracy: 0.4932\n",
      "Epoch 289/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0502 - accuracy: 0.9833 - val_loss: 3.5545 - val_accuracy: 0.4796\n",
      "Epoch 290/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0486 - accuracy: 0.9813 - val_loss: 3.5671 - val_accuracy: 0.4796\n",
      "Epoch 291/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0370 - accuracy: 0.9874 - val_loss: 3.4479 - val_accuracy: 0.5068\n",
      "Epoch 292/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0481 - accuracy: 0.9823 - val_loss: 3.6070 - val_accuracy: 0.4751\n",
      "Epoch 293/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0474 - accuracy: 0.9848 - val_loss: 3.5494 - val_accuracy: 0.5204\n",
      "Epoch 294/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.1100 - accuracy: 0.9611 - val_loss: 3.4459 - val_accuracy: 0.5158\n",
      "Epoch 295/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.1496 - accuracy: 0.9444 - val_loss: 3.2868 - val_accuracy: 0.5430\n",
      "Epoch 296/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.2528 - accuracy: 0.9056 - val_loss: 3.1059 - val_accuracy: 0.4977\n",
      "Epoch 297/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.3109 - accuracy: 0.8742 - val_loss: 2.7555 - val_accuracy: 0.5294\n",
      "Epoch 298/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.2498 - accuracy: 0.9061 - val_loss: 2.8320 - val_accuracy: 0.5204\n",
      "Epoch 299/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.1551 - accuracy: 0.9444 - val_loss: 2.6536 - val_accuracy: 0.5204\n",
      "Epoch 300/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.1318 - accuracy: 0.9475 - val_loss: 2.8749 - val_accuracy: 0.5068\n",
      "Epoch 301/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0915 - accuracy: 0.9626 - val_loss: 2.9827 - val_accuracy: 0.5113\n",
      "Epoch 302/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0495 - accuracy: 0.9869 - val_loss: 3.1086 - val_accuracy: 0.5068\n",
      "Epoch 303/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0418 - accuracy: 0.9874 - val_loss: 3.1253 - val_accuracy: 0.4977\n",
      "Epoch 304/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0301 - accuracy: 0.9919 - val_loss: 3.2209 - val_accuracy: 0.4977\n",
      "Epoch 305/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0298 - accuracy: 0.9909 - val_loss: 3.2197 - val_accuracy: 0.5113\n",
      "Epoch 306/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0253 - accuracy: 0.9929 - val_loss: 3.2534 - val_accuracy: 0.5023\n",
      "Epoch 307/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0261 - accuracy: 0.9919 - val_loss: 3.3209 - val_accuracy: 0.5023\n",
      "Epoch 308/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0299 - accuracy: 0.9904 - val_loss: 3.3394 - val_accuracy: 0.5113\n",
      "Epoch 309/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0300 - accuracy: 0.9919 - val_loss: 3.2736 - val_accuracy: 0.5385\n",
      "Epoch 310/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0217 - accuracy: 0.9934 - val_loss: 3.3846 - val_accuracy: 0.5204\n",
      "Epoch 311/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.0199 - accuracy: 0.9924 - val_loss: 3.3626 - val_accuracy: 0.5430\n",
      "Epoch 312/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.0169 - accuracy: 0.9934 - val_loss: 3.4421 - val_accuracy: 0.5204\n",
      "Epoch 313/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.0150 - accuracy: 0.9955 - val_loss: 3.4656 - val_accuracy: 0.5204\n",
      "Epoch 314/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.0143 - accuracy: 0.9970 - val_loss: 3.5155 - val_accuracy: 0.5249\n",
      "Epoch 315/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.0137 - accuracy: 0.9960 - val_loss: 3.5321 - val_accuracy: 0.5249\n",
      "Epoch 316/1000\n",
      "1980/1980 [==============================] - 1s 445us/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 3.5188 - val_accuracy: 0.5294\n",
      "Epoch 317/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0130 - accuracy: 0.9965 - val_loss: 3.5616 - val_accuracy: 0.5294\n",
      "Epoch 318/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 3.5648 - val_accuracy: 0.5204\n",
      "Epoch 319/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.0120 - accuracy: 0.9975 - val_loss: 3.6191 - val_accuracy: 0.5249\n",
      "Epoch 320/1000\n",
      "1980/1980 [==============================] - 1s 442us/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 3.6236 - val_accuracy: 0.5249\n",
      "Epoch 321/1000\n",
      "1980/1980 [==============================] - 1s 434us/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 3.6576 - val_accuracy: 0.5204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0100 - accuracy: 0.9980 - val_loss: 3.6366 - val_accuracy: 0.5204\n",
      "Epoch 323/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 3.6418 - val_accuracy: 0.5204\n",
      "Epoch 324/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0113 - accuracy: 0.9980 - val_loss: 3.6757 - val_accuracy: 0.5158\n",
      "Epoch 325/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0107 - accuracy: 0.9975 - val_loss: 3.6912 - val_accuracy: 0.5249\n",
      "Epoch 326/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 3.6837 - val_accuracy: 0.5023\n",
      "Epoch 327/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0373 - accuracy: 0.9838 - val_loss: 3.5474 - val_accuracy: 0.5113\n",
      "Epoch 328/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0476 - accuracy: 0.9838 - val_loss: 3.5010 - val_accuracy: 0.5023\n",
      "Epoch 329/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 3.5921 - val_accuracy: 0.4977\n",
      "Epoch 330/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0323 - accuracy: 0.9904 - val_loss: 3.5354 - val_accuracy: 0.5294\n",
      "Epoch 331/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0419 - accuracy: 0.9828 - val_loss: 3.5831 - val_accuracy: 0.5204\n",
      "Epoch 332/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0787 - accuracy: 0.9763 - val_loss: 3.4942 - val_accuracy: 0.5204\n",
      "Epoch 333/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0826 - accuracy: 0.9727 - val_loss: 3.5440 - val_accuracy: 0.4796\n",
      "Epoch 334/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.1319 - accuracy: 0.9500 - val_loss: 3.2290 - val_accuracy: 0.5249\n",
      "Epoch 335/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.2290 - accuracy: 0.9141 - val_loss: 3.0691 - val_accuracy: 0.4887\n",
      "Epoch 336/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.1398 - accuracy: 0.9465 - val_loss: 3.3033 - val_accuracy: 0.4932\n",
      "Epoch 337/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0970 - accuracy: 0.9646 - val_loss: 3.1425 - val_accuracy: 0.5385\n",
      "Epoch 338/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.1072 - accuracy: 0.9596 - val_loss: 2.9493 - val_accuracy: 0.5294\n",
      "Epoch 339/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.0797 - accuracy: 0.9727 - val_loss: 2.9790 - val_accuracy: 0.5294\n",
      "Epoch 340/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.0590 - accuracy: 0.9818 - val_loss: 3.3450 - val_accuracy: 0.5158\n",
      "Epoch 341/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0413 - accuracy: 0.9869 - val_loss: 3.3407 - val_accuracy: 0.5204\n",
      "Epoch 342/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0461 - accuracy: 0.9874 - val_loss: 3.4201 - val_accuracy: 0.5158\n",
      "Epoch 343/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0527 - accuracy: 0.9818 - val_loss: 3.3576 - val_accuracy: 0.5158\n",
      "Epoch 344/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0526 - accuracy: 0.9813 - val_loss: 3.3450 - val_accuracy: 0.5113\n",
      "Epoch 345/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0573 - accuracy: 0.9818 - val_loss: 3.2906 - val_accuracy: 0.4977\n",
      "Epoch 346/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0581 - accuracy: 0.9798 - val_loss: 3.2666 - val_accuracy: 0.5204\n",
      "Epoch 347/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0481 - accuracy: 0.9833 - val_loss: 3.3048 - val_accuracy: 0.5294\n",
      "Epoch 348/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0414 - accuracy: 0.9899 - val_loss: 3.3529 - val_accuracy: 0.5158\n",
      "Epoch 349/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0380 - accuracy: 0.9874 - val_loss: 3.3703 - val_accuracy: 0.5158\n",
      "Epoch 350/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0299 - accuracy: 0.9919 - val_loss: 3.4872 - val_accuracy: 0.4977\n",
      "Epoch 351/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0291 - accuracy: 0.9899 - val_loss: 3.4251 - val_accuracy: 0.4977\n",
      "Epoch 352/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0220 - accuracy: 0.9944 - val_loss: 3.5410 - val_accuracy: 0.4932\n",
      "Epoch 353/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0192 - accuracy: 0.9949 - val_loss: 3.5235 - val_accuracy: 0.5113\n",
      "Epoch 354/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.0160 - accuracy: 0.9955 - val_loss: 3.5822 - val_accuracy: 0.5113\n",
      "Epoch 355/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0142 - accuracy: 0.9970 - val_loss: 3.6152 - val_accuracy: 0.5068\n",
      "Epoch 356/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0108 - accuracy: 0.9975 - val_loss: 3.6528 - val_accuracy: 0.5158\n",
      "Epoch 357/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.0116 - accuracy: 0.9975 - val_loss: 3.6210 - val_accuracy: 0.5113\n",
      "Epoch 358/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 3.6453 - val_accuracy: 0.5158\n",
      "Epoch 359/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0078 - accuracy: 0.9990 - val_loss: 3.6717 - val_accuracy: 0.5113\n",
      "Epoch 360/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 3.6977 - val_accuracy: 0.5158\n",
      "Epoch 361/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0072 - accuracy: 0.9990 - val_loss: 3.7227 - val_accuracy: 0.5113\n",
      "Epoch 362/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 3.7482 - val_accuracy: 0.5113\n",
      "Epoch 363/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0063 - accuracy: 0.9995 - val_loss: 3.7814 - val_accuracy: 0.5158\n",
      "Epoch 364/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 3.7851 - val_accuracy: 0.5158\n",
      "Epoch 365/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0060 - accuracy: 0.9990 - val_loss: 3.8080 - val_accuracy: 0.5113\n",
      "Epoch 366/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 3.8216 - val_accuracy: 0.5158\n",
      "Epoch 367/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0054 - accuracy: 0.9995 - val_loss: 3.8525 - val_accuracy: 0.5113\n",
      "Epoch 368/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0053 - accuracy: 0.9995 - val_loss: 3.8683 - val_accuracy: 0.5113\n",
      "Epoch 369/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 3.8835 - val_accuracy: 0.5158\n",
      "Epoch 370/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 3.8814 - val_accuracy: 0.5113\n",
      "Epoch 371/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.8929 - val_accuracy: 0.5158\n",
      "Epoch 372/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 3.9230 - val_accuracy: 0.5113\n",
      "Epoch 373/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 3.9229 - val_accuracy: 0.5068\n",
      "Epoch 374/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.9409 - val_accuracy: 0.5113\n",
      "Epoch 375/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.9587 - val_accuracy: 0.5113\n",
      "Epoch 376/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.9779 - val_accuracy: 0.5113\n",
      "Epoch 377/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.9913 - val_accuracy: 0.5113\n",
      "Epoch 378/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.9925 - val_accuracy: 0.5113\n",
      "Epoch 379/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.0007 - val_accuracy: 0.5113\n",
      "Epoch 380/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.9997 - val_accuracy: 0.5158\n",
      "Epoch 381/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.0078 - val_accuracy: 0.5113\n",
      "Epoch 382/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.0238 - val_accuracy: 0.5113\n",
      "Epoch 383/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.0352 - val_accuracy: 0.5113\n",
      "Epoch 384/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0071 - accuracy: 0.9990 - val_loss: 3.9719 - val_accuracy: 0.5339\n",
      "Epoch 385/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0071 - accuracy: 0.9990 - val_loss: 3.9995 - val_accuracy: 0.5113\n",
      "Epoch 386/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0133 - accuracy: 0.9975 - val_loss: 4.0244 - val_accuracy: 0.5158\n",
      "Epoch 387/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 3.9228 - val_accuracy: 0.5158\n",
      "Epoch 388/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0161 - accuracy: 0.9955 - val_loss: 3.8478 - val_accuracy: 0.5249\n",
      "Epoch 389/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0447 - accuracy: 0.9848 - val_loss: 4.0461 - val_accuracy: 0.4977\n",
      "Epoch 390/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.1312 - accuracy: 0.9581 - val_loss: 3.6467 - val_accuracy: 0.4842\n",
      "Epoch 391/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.2685 - accuracy: 0.9131 - val_loss: 3.3102 - val_accuracy: 0.4932\n",
      "Epoch 392/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.2924 - accuracy: 0.9066 - val_loss: 3.0629 - val_accuracy: 0.4706\n",
      "Epoch 393/1000\n",
      "1980/1980 [==============================] - 1s 444us/step - loss: 0.2293 - accuracy: 0.9141 - val_loss: 2.8114 - val_accuracy: 0.5023\n",
      "Epoch 394/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.1809 - accuracy: 0.9313 - val_loss: 2.9029 - val_accuracy: 0.5249\n",
      "Epoch 395/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 0.1088 - accuracy: 0.9586 - val_loss: 2.9928 - val_accuracy: 0.5385\n",
      "Epoch 396/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.0694 - accuracy: 0.9768 - val_loss: 2.9254 - val_accuracy: 0.5068\n",
      "Epoch 397/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.0491 - accuracy: 0.9859 - val_loss: 3.0718 - val_accuracy: 0.5249\n",
      "Epoch 398/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0343 - accuracy: 0.9884 - val_loss: 3.2232 - val_accuracy: 0.5113\n",
      "Epoch 399/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0202 - accuracy: 0.9960 - val_loss: 3.2844 - val_accuracy: 0.5294\n",
      "Epoch 400/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0149 - accuracy: 0.9980 - val_loss: 3.3366 - val_accuracy: 0.5249\n",
      "Epoch 401/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0111 - accuracy: 0.9990 - val_loss: 3.3542 - val_accuracy: 0.5294\n",
      "Epoch 402/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.4290 - val_accuracy: 0.5249\n",
      "Epoch 403/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0076 - accuracy: 0.9995 - val_loss: 3.4749 - val_accuracy: 0.5339\n",
      "Epoch 404/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0076 - accuracy: 0.9990 - val_loss: 3.5000 - val_accuracy: 0.5339\n",
      "Epoch 405/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0065 - accuracy: 0.9995 - val_loss: 3.5261 - val_accuracy: 0.5385\n",
      "Epoch 406/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.5443 - val_accuracy: 0.5339\n",
      "Epoch 407/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 3.5778 - val_accuracy: 0.5294\n",
      "Epoch 408/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.5928 - val_accuracy: 0.5339\n",
      "Epoch 409/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 3.6040 - val_accuracy: 0.5294\n",
      "Epoch 410/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0053 - accuracy: 0.9995 - val_loss: 3.6331 - val_accuracy: 0.5204\n",
      "Epoch 411/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.6580 - val_accuracy: 0.5249\n",
      "Epoch 412/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.6741 - val_accuracy: 0.5249\n",
      "Epoch 413/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 3.6890 - val_accuracy: 0.5249\n",
      "Epoch 414/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.7051 - val_accuracy: 0.5158\n",
      "Epoch 415/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.7057 - val_accuracy: 0.5249\n",
      "Epoch 416/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 3.7218 - val_accuracy: 0.5113\n",
      "Epoch 417/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.7439 - val_accuracy: 0.5158\n",
      "Epoch 418/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.7634 - val_accuracy: 0.5113\n",
      "Epoch 419/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.7703 - val_accuracy: 0.5113\n",
      "Epoch 420/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.7809 - val_accuracy: 0.5158\n",
      "Epoch 421/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.7983 - val_accuracy: 0.5158\n",
      "Epoch 422/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.8078 - val_accuracy: 0.5158\n",
      "Epoch 423/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 3.8183 - val_accuracy: 0.5113\n",
      "Epoch 424/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 3.8336 - val_accuracy: 0.5204\n",
      "Epoch 425/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 3.8216 - val_accuracy: 0.5294\n",
      "Epoch 426/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.8353 - val_accuracy: 0.5113\n",
      "Epoch 427/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 3.8961 - val_accuracy: 0.5158\n",
      "Epoch 428/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.9043 - val_accuracy: 0.5158\n",
      "Epoch 429/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.9105 - val_accuracy: 0.5158\n",
      "Epoch 430/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.8997 - val_accuracy: 0.5249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 431/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 3.8491 - val_accuracy: 0.5158\n",
      "Epoch 432/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.0236 - accuracy: 0.9934 - val_loss: 4.0066 - val_accuracy: 0.5068\n",
      "Epoch 433/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0178 - accuracy: 0.9934 - val_loss: 3.9399 - val_accuracy: 0.5068\n",
      "Epoch 434/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0234 - accuracy: 0.9919 - val_loss: 3.9600 - val_accuracy: 0.5158\n",
      "Epoch 435/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0320 - accuracy: 0.9894 - val_loss: 3.7841 - val_accuracy: 0.4842\n",
      "Epoch 436/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.1076 - accuracy: 0.9662 - val_loss: 3.6086 - val_accuracy: 0.4842\n",
      "Epoch 437/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.1749 - accuracy: 0.9460 - val_loss: 3.2644 - val_accuracy: 0.5249\n",
      "Epoch 438/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.1700 - accuracy: 0.9394 - val_loss: 3.2758 - val_accuracy: 0.4977\n",
      "Epoch 439/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.1161 - accuracy: 0.9556 - val_loss: 3.2292 - val_accuracy: 0.5294\n",
      "Epoch 440/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0915 - accuracy: 0.9667 - val_loss: 3.2472 - val_accuracy: 0.4887\n",
      "Epoch 441/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0673 - accuracy: 0.9778 - val_loss: 3.1858 - val_accuracy: 0.5204\n",
      "Epoch 442/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0793 - accuracy: 0.9753 - val_loss: 3.2931 - val_accuracy: 0.4977\n",
      "Epoch 443/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0651 - accuracy: 0.9763 - val_loss: 3.3064 - val_accuracy: 0.5475\n",
      "Epoch 444/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0723 - accuracy: 0.9753 - val_loss: 3.2604 - val_accuracy: 0.5339\n",
      "Epoch 445/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0720 - accuracy: 0.9763 - val_loss: 3.1978 - val_accuracy: 0.5158\n",
      "Epoch 446/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.0399 - accuracy: 0.9854 - val_loss: 3.4925 - val_accuracy: 0.5249\n",
      "Epoch 447/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0342 - accuracy: 0.9884 - val_loss: 3.4570 - val_accuracy: 0.5339\n",
      "Epoch 448/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0277 - accuracy: 0.9919 - val_loss: 3.5054 - val_accuracy: 0.5249\n",
      "Epoch 449/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0171 - accuracy: 0.9970 - val_loss: 3.5537 - val_accuracy: 0.5113\n",
      "Epoch 450/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 3.6272 - val_accuracy: 0.5023\n",
      "Epoch 451/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.0087 - accuracy: 0.9995 - val_loss: 3.6359 - val_accuracy: 0.5158\n",
      "Epoch 452/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0064 - accuracy: 0.9995 - val_loss: 3.7101 - val_accuracy: 0.5113\n",
      "Epoch 453/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 3.7403 - val_accuracy: 0.5068\n",
      "Epoch 454/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.7574 - val_accuracy: 0.5068\n",
      "Epoch 455/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 3.7694 - val_accuracy: 0.5113\n",
      "Epoch 456/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.7883 - val_accuracy: 0.5158\n",
      "Epoch 457/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 3.7997 - val_accuracy: 0.5158\n",
      "Epoch 458/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.8077 - val_accuracy: 0.5204\n",
      "Epoch 459/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.8269 - val_accuracy: 0.5204\n",
      "Epoch 460/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.8416 - val_accuracy: 0.5113\n",
      "Epoch 461/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.8568 - val_accuracy: 0.5158\n",
      "Epoch 462/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.8636 - val_accuracy: 0.5158\n",
      "Epoch 463/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.8711 - val_accuracy: 0.5158\n",
      "Epoch 464/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.8791 - val_accuracy: 0.5158\n",
      "Epoch 465/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.8892 - val_accuracy: 0.5113\n",
      "Epoch 466/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.8941 - val_accuracy: 0.5113\n",
      "Epoch 467/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9066 - val_accuracy: 0.5113\n",
      "Epoch 468/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.9145 - val_accuracy: 0.5113\n",
      "Epoch 469/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9248 - val_accuracy: 0.5113\n",
      "Epoch 470/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9363 - val_accuracy: 0.5204\n",
      "Epoch 471/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.9399 - val_accuracy: 0.5204\n",
      "Epoch 472/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.9476 - val_accuracy: 0.5158\n",
      "Epoch 473/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.9559 - val_accuracy: 0.5204\n",
      "Epoch 474/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9653 - val_accuracy: 0.5158\n",
      "Epoch 475/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.9788 - val_accuracy: 0.5158\n",
      "Epoch 476/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.9844 - val_accuracy: 0.5158\n",
      "Epoch 477/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9931 - val_accuracy: 0.5204\n",
      "Epoch 478/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.9990 - val_accuracy: 0.5204\n",
      "Epoch 479/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0018 - val_accuracy: 0.5204\n",
      "Epoch 480/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0087 - val_accuracy: 0.5158\n",
      "Epoch 481/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0178 - val_accuracy: 0.5158\n",
      "Epoch 482/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0161 - val_accuracy: 0.5204\n",
      "Epoch 483/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0231 - val_accuracy: 0.5249\n",
      "Epoch 484/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.0322 - val_accuracy: 0.5249\n",
      "Epoch 485/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0412 - val_accuracy: 0.5249\n",
      "Epoch 486/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0512 - val_accuracy: 0.5249\n",
      "Epoch 487/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0556 - val_accuracy: 0.5249\n",
      "Epoch 488/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.0699 - val_accuracy: 0.5249\n",
      "Epoch 489/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0761 - val_accuracy: 0.5204\n",
      "Epoch 490/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0836 - val_accuracy: 0.5294\n",
      "Epoch 491/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.0920 - val_accuracy: 0.5249\n",
      "Epoch 492/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.1023 - val_accuracy: 0.5204\n",
      "Epoch 493/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.1087 - val_accuracy: 0.5204\n",
      "Epoch 494/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1177 - val_accuracy: 0.5158\n",
      "Epoch 495/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 9.7856e-04 - accuracy: 1.0000 - val_loss: 4.1244 - val_accuracy: 0.5249\n",
      "Epoch 496/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1328 - val_accuracy: 0.5249\n",
      "Epoch 497/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 9.1380e-04 - accuracy: 1.0000 - val_loss: 4.1379 - val_accuracy: 0.5294\n",
      "Epoch 498/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 8.5368e-04 - accuracy: 1.0000 - val_loss: 4.1424 - val_accuracy: 0.5204\n",
      "Epoch 499/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 8.1501e-04 - accuracy: 1.0000 - val_loss: 4.1485 - val_accuracy: 0.5204\n",
      "Epoch 500/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 7.9253e-04 - accuracy: 1.0000 - val_loss: 4.1545 - val_accuracy: 0.5249\n",
      "Epoch 501/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 7.8596e-04 - accuracy: 1.0000 - val_loss: 4.1617 - val_accuracy: 0.5249\n",
      "Epoch 502/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 8.1835e-04 - accuracy: 1.0000 - val_loss: 4.1719 - val_accuracy: 0.5249\n",
      "Epoch 503/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 8.5035e-04 - accuracy: 1.0000 - val_loss: 4.1769 - val_accuracy: 0.5204\n",
      "Epoch 504/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 7.6960e-04 - accuracy: 1.0000 - val_loss: 4.1827 - val_accuracy: 0.5249\n",
      "Epoch 505/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 7.8759e-04 - accuracy: 1.0000 - val_loss: 4.1883 - val_accuracy: 0.5249\n",
      "Epoch 506/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 8.5047e-04 - accuracy: 1.0000 - val_loss: 4.1931 - val_accuracy: 0.5249\n",
      "Epoch 507/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 7.9088e-04 - accuracy: 1.0000 - val_loss: 4.1988 - val_accuracy: 0.5249\n",
      "Epoch 508/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 8.4043e-04 - accuracy: 1.0000 - val_loss: 4.2058 - val_accuracy: 0.5249\n",
      "Epoch 509/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 8.0218e-04 - accuracy: 1.0000 - val_loss: 4.2170 - val_accuracy: 0.5249\n",
      "Epoch 510/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 7.8775e-04 - accuracy: 1.0000 - val_loss: 4.2246 - val_accuracy: 0.5249\n",
      "Epoch 511/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.8437e-04 - accuracy: 1.0000 - val_loss: 4.2304 - val_accuracy: 0.5294\n",
      "Epoch 512/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.8536e-04 - accuracy: 1.0000 - val_loss: 4.2363 - val_accuracy: 0.5294\n",
      "Epoch 513/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 7.3302e-04 - accuracy: 1.0000 - val_loss: 4.2446 - val_accuracy: 0.5249\n",
      "Epoch 514/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 8.8574e-04 - accuracy: 1.0000 - val_loss: 4.2479 - val_accuracy: 0.5249\n",
      "Epoch 515/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 7.5674e-04 - accuracy: 1.0000 - val_loss: 4.2535 - val_accuracy: 0.5249\n",
      "Epoch 516/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 6.7465e-04 - accuracy: 1.0000 - val_loss: 4.2607 - val_accuracy: 0.5249\n",
      "Epoch 517/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 7.0747e-04 - accuracy: 1.0000 - val_loss: 4.2686 - val_accuracy: 0.5249\n",
      "Epoch 518/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 6.6312e-04 - accuracy: 1.0000 - val_loss: 4.2686 - val_accuracy: 0.5294\n",
      "Epoch 519/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.9239e-04 - accuracy: 1.0000 - val_loss: 4.2727 - val_accuracy: 0.5249\n",
      "Epoch 520/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 6.4421e-04 - accuracy: 1.0000 - val_loss: 4.2785 - val_accuracy: 0.5294\n",
      "Epoch 521/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 6.5251e-04 - accuracy: 1.0000 - val_loss: 4.2847 - val_accuracy: 0.5294\n",
      "Epoch 522/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 5.7913e-04 - accuracy: 1.0000 - val_loss: 4.2940 - val_accuracy: 0.5294\n",
      "Epoch 523/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 5.8530e-04 - accuracy: 1.0000 - val_loss: 4.2993 - val_accuracy: 0.5294\n",
      "Epoch 524/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 5.4532e-04 - accuracy: 1.0000 - val_loss: 4.3045 - val_accuracy: 0.5294\n",
      "Epoch 525/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 6.1229e-04 - accuracy: 1.0000 - val_loss: 4.3105 - val_accuracy: 0.5294\n",
      "Epoch 526/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 5.6428e-04 - accuracy: 1.0000 - val_loss: 4.3148 - val_accuracy: 0.5294\n",
      "Epoch 527/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 5.5125e-04 - accuracy: 1.0000 - val_loss: 4.3207 - val_accuracy: 0.5294\n",
      "Epoch 528/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 7.1370e-04 - accuracy: 1.0000 - val_loss: 4.3256 - val_accuracy: 0.5204\n",
      "Epoch 529/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.3463e-04 - accuracy: 1.0000 - val_loss: 4.3326 - val_accuracy: 0.5294\n",
      "Epoch 530/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 4.9634e-04 - accuracy: 1.0000 - val_loss: 4.3402 - val_accuracy: 0.5249\n",
      "Epoch 531/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.1673e-04 - accuracy: 1.0000 - val_loss: 4.3457 - val_accuracy: 0.5204\n",
      "Epoch 532/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 5.3808e-04 - accuracy: 1.0000 - val_loss: 4.3518 - val_accuracy: 0.5249\n",
      "Epoch 533/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 5.1902e-04 - accuracy: 1.0000 - val_loss: 4.3571 - val_accuracy: 0.5294\n",
      "Epoch 534/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 5.9164e-04 - accuracy: 1.0000 - val_loss: 4.3611 - val_accuracy: 0.5294\n",
      "Epoch 535/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 4.7213e-04 - accuracy: 1.0000 - val_loss: 4.3696 - val_accuracy: 0.5249\n",
      "Epoch 536/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 5.3595e-04 - accuracy: 1.0000 - val_loss: 4.3717 - val_accuracy: 0.5249\n",
      "Epoch 537/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 5.2043e-04 - accuracy: 1.0000 - val_loss: 4.3745 - val_accuracy: 0.5294\n",
      "Epoch 538/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 382us/step - loss: 4.9537e-04 - accuracy: 1.0000 - val_loss: 4.3790 - val_accuracy: 0.5294\n",
      "Epoch 539/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 4.9277e-04 - accuracy: 1.0000 - val_loss: 4.3864 - val_accuracy: 0.5294\n",
      "Epoch 540/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 5.1057e-04 - accuracy: 1.0000 - val_loss: 4.3894 - val_accuracy: 0.5294\n",
      "Epoch 541/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 4.2059e-04 - accuracy: 1.0000 - val_loss: 4.3945 - val_accuracy: 0.5294\n",
      "Epoch 542/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 4.2908e-04 - accuracy: 1.0000 - val_loss: 4.4004 - val_accuracy: 0.5294\n",
      "Epoch 543/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 4.0866e-04 - accuracy: 1.0000 - val_loss: 4.4057 - val_accuracy: 0.5294\n",
      "Epoch 544/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 4.4751e-04 - accuracy: 1.0000 - val_loss: 4.4099 - val_accuracy: 0.5294\n",
      "Epoch 545/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 4.1373e-04 - accuracy: 1.0000 - val_loss: 4.4163 - val_accuracy: 0.5249\n",
      "Epoch 546/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 4.6780e-04 - accuracy: 1.0000 - val_loss: 4.4225 - val_accuracy: 0.5294\n",
      "Epoch 547/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 3.6128e-04 - accuracy: 1.0000 - val_loss: 4.4287 - val_accuracy: 0.5294\n",
      "Epoch 548/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 4.9714e-04 - accuracy: 1.0000 - val_loss: 4.4368 - val_accuracy: 0.5249\n",
      "Epoch 549/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 4.4498e-04 - accuracy: 1.0000 - val_loss: 4.4416 - val_accuracy: 0.5294\n",
      "Epoch 550/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 3.7650e-04 - accuracy: 1.0000 - val_loss: 4.4429 - val_accuracy: 0.5249\n",
      "Epoch 551/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.7915e-04 - accuracy: 1.0000 - val_loss: 4.4453 - val_accuracy: 0.5294\n",
      "Epoch 552/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 4.1719e-04 - accuracy: 1.0000 - val_loss: 4.4512 - val_accuracy: 0.5249\n",
      "Epoch 553/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 3.5240e-04 - accuracy: 1.0000 - val_loss: 4.4581 - val_accuracy: 0.5294\n",
      "Epoch 554/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 4.4657e-04 - accuracy: 1.0000 - val_loss: 4.4618 - val_accuracy: 0.5249\n",
      "Epoch 555/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 4.0647e-04 - accuracy: 1.0000 - val_loss: 4.4696 - val_accuracy: 0.5249\n",
      "Epoch 556/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 4.2570e-04 - accuracy: 1.0000 - val_loss: 4.4768 - val_accuracy: 0.5204\n",
      "Epoch 557/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.4575e-04 - accuracy: 1.0000 - val_loss: 4.4793 - val_accuracy: 0.5249\n",
      "Epoch 558/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 3.2348e-04 - accuracy: 1.0000 - val_loss: 4.4855 - val_accuracy: 0.5249\n",
      "Epoch 559/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 4.3150e-04 - accuracy: 1.0000 - val_loss: 4.4882 - val_accuracy: 0.5249\n",
      "Epoch 560/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 3.6815e-04 - accuracy: 1.0000 - val_loss: 4.4931 - val_accuracy: 0.5249\n",
      "Epoch 561/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 4.0862e-04 - accuracy: 1.0000 - val_loss: 4.4973 - val_accuracy: 0.5294\n",
      "Epoch 562/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 3.5234e-04 - accuracy: 1.0000 - val_loss: 4.5043 - val_accuracy: 0.5294\n",
      "Epoch 563/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 3.0946e-04 - accuracy: 1.0000 - val_loss: 4.5106 - val_accuracy: 0.5249\n",
      "Epoch 564/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 3.4500e-04 - accuracy: 1.0000 - val_loss: 4.5125 - val_accuracy: 0.5294\n",
      "Epoch 565/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.7617e-04 - accuracy: 1.0000 - val_loss: 4.5169 - val_accuracy: 0.5294\n",
      "Epoch 566/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.1327e-04 - accuracy: 1.0000 - val_loss: 4.5214 - val_accuracy: 0.5294\n",
      "Epoch 567/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 2.9279e-04 - accuracy: 1.0000 - val_loss: 4.5255 - val_accuracy: 0.5294\n",
      "Epoch 568/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 3.4059e-04 - accuracy: 1.0000 - val_loss: 4.5324 - val_accuracy: 0.5249\n",
      "Epoch 569/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.4555e-04 - accuracy: 1.0000 - val_loss: 4.5392 - val_accuracy: 0.5249\n",
      "Epoch 570/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 3.3834e-04 - accuracy: 1.0000 - val_loss: 4.5470 - val_accuracy: 0.5249\n",
      "Epoch 571/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 3.2121e-04 - accuracy: 1.0000 - val_loss: 4.5521 - val_accuracy: 0.5249\n",
      "Epoch 572/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 2.6805e-04 - accuracy: 1.0000 - val_loss: 4.5525 - val_accuracy: 0.5249\n",
      "Epoch 573/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 3.7275e-04 - accuracy: 1.0000 - val_loss: 4.5551 - val_accuracy: 0.5249\n",
      "Epoch 574/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.9869e-04 - accuracy: 1.0000 - val_loss: 4.5606 - val_accuracy: 0.5249\n",
      "Epoch 575/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.1135e-04 - accuracy: 1.0000 - val_loss: 4.5676 - val_accuracy: 0.5249\n",
      "Epoch 576/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.8021e-04 - accuracy: 1.0000 - val_loss: 4.5707 - val_accuracy: 0.5294\n",
      "Epoch 577/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 2.7722e-04 - accuracy: 1.0000 - val_loss: 4.5743 - val_accuracy: 0.5294\n",
      "Epoch 578/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 2.8763e-04 - accuracy: 1.0000 - val_loss: 4.5785 - val_accuracy: 0.5249\n",
      "Epoch 579/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 2.8847e-04 - accuracy: 1.0000 - val_loss: 4.5826 - val_accuracy: 0.5249\n",
      "Epoch 580/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 2.5314e-04 - accuracy: 1.0000 - val_loss: 4.5865 - val_accuracy: 0.5294\n",
      "Epoch 581/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 2.6544e-04 - accuracy: 1.0000 - val_loss: 4.5878 - val_accuracy: 0.5249\n",
      "Epoch 582/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 2.2670e-04 - accuracy: 1.0000 - val_loss: 4.5910 - val_accuracy: 0.5204\n",
      "Epoch 583/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 2.9470e-04 - accuracy: 1.0000 - val_loss: 4.5967 - val_accuracy: 0.5294\n",
      "Epoch 584/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.6079e-04 - accuracy: 1.0000 - val_loss: 4.6018 - val_accuracy: 0.5294\n",
      "Epoch 585/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.7198e-04 - accuracy: 1.0000 - val_loss: 4.6044 - val_accuracy: 0.5294\n",
      "Epoch 586/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 2.1981e-04 - accuracy: 1.0000 - val_loss: 4.6086 - val_accuracy: 0.5249\n",
      "Epoch 587/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 3.2904e-04 - accuracy: 1.0000 - val_loss: 4.6092 - val_accuracy: 0.5249\n",
      "Epoch 588/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 2.4370e-04 - accuracy: 1.0000 - val_loss: 4.6173 - val_accuracy: 0.5249\n",
      "Epoch 589/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.3240e-04 - accuracy: 1.0000 - val_loss: 4.6255 - val_accuracy: 0.5249\n",
      "Epoch 590/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 2.2359e-04 - accuracy: 1.0000 - val_loss: 4.6269 - val_accuracy: 0.5294\n",
      "Epoch 591/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.3260e-04 - accuracy: 1.0000 - val_loss: 4.6300 - val_accuracy: 0.5249\n",
      "Epoch 592/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.0962e-04 - accuracy: 1.0000 - val_loss: 4.6370 - val_accuracy: 0.5294\n",
      "Epoch 593/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.7363e-04 - accuracy: 1.0000 - val_loss: 4.6449 - val_accuracy: 0.5294\n",
      "Epoch 594/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 2.4521e-04 - accuracy: 1.0000 - val_loss: 4.6518 - val_accuracy: 0.5294\n",
      "Epoch 595/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.4533e-04 - accuracy: 1.0000 - val_loss: 4.6557 - val_accuracy: 0.5294\n",
      "Epoch 596/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.0824e-04 - accuracy: 1.0000 - val_loss: 4.6602 - val_accuracy: 0.5294\n",
      "Epoch 597/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 1.8699e-04 - accuracy: 1.0000 - val_loss: 4.6671 - val_accuracy: 0.5249\n",
      "Epoch 598/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.8665e-04 - accuracy: 1.0000 - val_loss: 4.6678 - val_accuracy: 0.5249\n",
      "Epoch 599/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.0056e-04 - accuracy: 1.0000 - val_loss: 4.6710 - val_accuracy: 0.5249\n",
      "Epoch 600/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 2.3457e-04 - accuracy: 1.0000 - val_loss: 4.6782 - val_accuracy: 0.5249\n",
      "Epoch 601/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.2169e-04 - accuracy: 1.0000 - val_loss: 4.6821 - val_accuracy: 0.5249\n",
      "Epoch 602/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 1.9907e-04 - accuracy: 1.0000 - val_loss: 4.6844 - val_accuracy: 0.5249\n",
      "Epoch 603/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 1.8273e-04 - accuracy: 1.0000 - val_loss: 4.6892 - val_accuracy: 0.5249\n",
      "Epoch 604/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 2.0810e-04 - accuracy: 1.0000 - val_loss: 4.6924 - val_accuracy: 0.5249\n",
      "Epoch 605/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 1.8801e-04 - accuracy: 1.0000 - val_loss: 4.6984 - val_accuracy: 0.5294\n",
      "Epoch 606/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 2.0889e-04 - accuracy: 1.0000 - val_loss: 4.7006 - val_accuracy: 0.5294\n",
      "Epoch 607/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 1.9712e-04 - accuracy: 1.0000 - val_loss: 4.6995 - val_accuracy: 0.5249\n",
      "Epoch 608/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 1.9796e-04 - accuracy: 1.0000 - val_loss: 4.7074 - val_accuracy: 0.5249\n",
      "Epoch 609/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 1.9393e-04 - accuracy: 1.0000 - val_loss: 4.7142 - val_accuracy: 0.5249\n",
      "Epoch 610/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 1.7060e-04 - accuracy: 1.0000 - val_loss: 4.7194 - val_accuracy: 0.5249\n",
      "Epoch 611/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 1.8711e-04 - accuracy: 1.0000 - val_loss: 4.7216 - val_accuracy: 0.5249\n",
      "Epoch 612/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 1.5741e-04 - accuracy: 1.0000 - val_loss: 4.7312 - val_accuracy: 0.5249\n",
      "Epoch 613/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 1.9109e-04 - accuracy: 1.0000 - val_loss: 4.7348 - val_accuracy: 0.5249\n",
      "Epoch 614/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 2.5086e-04 - accuracy: 1.0000 - val_loss: 4.7372 - val_accuracy: 0.5249\n",
      "Epoch 615/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 2.0625e-04 - accuracy: 1.0000 - val_loss: 4.7410 - val_accuracy: 0.5249\n",
      "Epoch 616/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.8887e-04 - accuracy: 1.0000 - val_loss: 4.7427 - val_accuracy: 0.5249\n",
      "Epoch 617/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 1.7449e-04 - accuracy: 1.0000 - val_loss: 4.7456 - val_accuracy: 0.5249\n",
      "Epoch 618/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.7342e-04 - accuracy: 1.0000 - val_loss: 4.7513 - val_accuracy: 0.5249\n",
      "Epoch 619/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.6101e-04 - accuracy: 1.0000 - val_loss: 4.7615 - val_accuracy: 0.5249\n",
      "Epoch 620/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 1.8198e-04 - accuracy: 1.0000 - val_loss: 4.7693 - val_accuracy: 0.5249\n",
      "Epoch 621/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 2.1261e-04 - accuracy: 1.0000 - val_loss: 4.7730 - val_accuracy: 0.5294\n",
      "Epoch 622/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 1.8666e-04 - accuracy: 1.0000 - val_loss: 4.7741 - val_accuracy: 0.5294\n",
      "Epoch 623/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 2.3180e-04 - accuracy: 1.0000 - val_loss: 4.7740 - val_accuracy: 0.5294\n",
      "Epoch 624/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 1.9071e-04 - accuracy: 1.0000 - val_loss: 4.7854 - val_accuracy: 0.5294\n",
      "Epoch 625/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.5290e-04 - accuracy: 1.0000 - val_loss: 4.7926 - val_accuracy: 0.5294\n",
      "Epoch 626/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.2741e-04 - accuracy: 1.0000 - val_loss: 4.7970 - val_accuracy: 0.5294\n",
      "Epoch 627/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 1.4088e-04 - accuracy: 1.0000 - val_loss: 4.8043 - val_accuracy: 0.5294\n",
      "Epoch 628/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 1.6968e-04 - accuracy: 1.0000 - val_loss: 4.8052 - val_accuracy: 0.5339\n",
      "Epoch 629/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 1.5308e-04 - accuracy: 1.0000 - val_loss: 4.8141 - val_accuracy: 0.5339\n",
      "Epoch 630/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.5780e-04 - accuracy: 1.0000 - val_loss: 4.8182 - val_accuracy: 0.5294\n",
      "Epoch 631/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.5174e-04 - accuracy: 1.0000 - val_loss: 4.8205 - val_accuracy: 0.5294\n",
      "Epoch 632/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.3202e-04 - accuracy: 1.0000 - val_loss: 4.8225 - val_accuracy: 0.5294\n",
      "Epoch 633/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.5907e-04 - accuracy: 1.0000 - val_loss: 4.8208 - val_accuracy: 0.5294\n",
      "Epoch 634/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.4313e-04 - accuracy: 1.0000 - val_loss: 4.8274 - val_accuracy: 0.5294\n",
      "Epoch 635/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 1.2470e-04 - accuracy: 1.0000 - val_loss: 4.8322 - val_accuracy: 0.5294\n",
      "Epoch 636/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 1.3947e-04 - accuracy: 1.0000 - val_loss: 4.8392 - val_accuracy: 0.5249\n",
      "Epoch 637/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.6716e-04 - accuracy: 1.0000 - val_loss: 4.8517 - val_accuracy: 0.5294\n",
      "Epoch 638/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.2438e-04 - accuracy: 1.0000 - val_loss: 4.8563 - val_accuracy: 0.5294\n",
      "Epoch 639/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.7434e-04 - accuracy: 1.0000 - val_loss: 4.8578 - val_accuracy: 0.5294\n",
      "Epoch 640/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.2489e-04 - accuracy: 1.0000 - val_loss: 4.8551 - val_accuracy: 0.5294\n",
      "Epoch 641/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.1600e-04 - accuracy: 1.0000 - val_loss: 4.8649 - val_accuracy: 0.5294\n",
      "Epoch 642/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 1.2189e-04 - accuracy: 1.0000 - val_loss: 4.8661 - val_accuracy: 0.5294\n",
      "Epoch 643/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.1740e-04 - accuracy: 1.0000 - val_loss: 4.8694 - val_accuracy: 0.5294\n",
      "Epoch 644/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 382us/step - loss: 1.1513e-04 - accuracy: 1.0000 - val_loss: 4.8765 - val_accuracy: 0.5294\n",
      "Epoch 645/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.1094e-04 - accuracy: 1.0000 - val_loss: 4.8807 - val_accuracy: 0.5249\n",
      "Epoch 646/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 1.1768e-04 - accuracy: 1.0000 - val_loss: 4.8826 - val_accuracy: 0.5249\n",
      "Epoch 647/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 1.0854e-04 - accuracy: 1.0000 - val_loss: 4.8861 - val_accuracy: 0.5249\n",
      "Epoch 648/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 1.3173e-04 - accuracy: 1.0000 - val_loss: 4.8904 - val_accuracy: 0.5294\n",
      "Epoch 649/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 1.3463e-04 - accuracy: 1.0000 - val_loss: 4.8870 - val_accuracy: 0.5294\n",
      "Epoch 650/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 1.2148e-04 - accuracy: 1.0000 - val_loss: 4.8889 - val_accuracy: 0.5294\n",
      "Epoch 651/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.1243e-04 - accuracy: 1.0000 - val_loss: 4.8960 - val_accuracy: 0.5294\n",
      "Epoch 652/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.4580e-04 - accuracy: 1.0000 - val_loss: 4.9035 - val_accuracy: 0.5294\n",
      "Epoch 653/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 1.1799e-04 - accuracy: 1.0000 - val_loss: 4.9046 - val_accuracy: 0.5294\n",
      "Epoch 654/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 9.5992e-05 - accuracy: 1.0000 - val_loss: 4.9103 - val_accuracy: 0.5294\n",
      "Epoch 655/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 1.1512e-04 - accuracy: 1.0000 - val_loss: 4.9172 - val_accuracy: 0.5294\n",
      "Epoch 656/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 1.1705e-04 - accuracy: 1.0000 - val_loss: 4.9142 - val_accuracy: 0.5294\n",
      "Epoch 657/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 8.5023e-05 - accuracy: 1.0000 - val_loss: 4.9147 - val_accuracy: 0.5294\n",
      "Epoch 658/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 1.1203e-04 - accuracy: 1.0000 - val_loss: 4.9213 - val_accuracy: 0.5294\n",
      "Epoch 659/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 1.0081e-04 - accuracy: 1.0000 - val_loss: 4.9257 - val_accuracy: 0.5294\n",
      "Epoch 660/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 9.1350e-05 - accuracy: 1.0000 - val_loss: 4.9309 - val_accuracy: 0.5339\n",
      "Epoch 661/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 1.0686e-04 - accuracy: 1.0000 - val_loss: 4.9315 - val_accuracy: 0.5294\n",
      "Epoch 662/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 1.1264e-04 - accuracy: 1.0000 - val_loss: 4.9265 - val_accuracy: 0.5294\n",
      "Epoch 663/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 9.6257e-05 - accuracy: 1.0000 - val_loss: 4.9299 - val_accuracy: 0.5294\n",
      "Epoch 664/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.1278e-04 - accuracy: 1.0000 - val_loss: 4.9291 - val_accuracy: 0.5294\n",
      "Epoch 665/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.3079e-05 - accuracy: 1.0000 - val_loss: 4.9403 - val_accuracy: 0.5294\n",
      "Epoch 666/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 9.8621e-05 - accuracy: 1.0000 - val_loss: 4.9495 - val_accuracy: 0.5249\n",
      "Epoch 667/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 9.7245e-05 - accuracy: 1.0000 - val_loss: 4.9538 - val_accuracy: 0.5294\n",
      "Epoch 668/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 7.9553e-05 - accuracy: 1.0000 - val_loss: 4.9572 - val_accuracy: 0.5294\n",
      "Epoch 669/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.2096e-04 - accuracy: 1.0000 - val_loss: 4.9620 - val_accuracy: 0.5294\n",
      "Epoch 670/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 8.5868e-05 - accuracy: 1.0000 - val_loss: 4.9666 - val_accuracy: 0.5249\n",
      "Epoch 671/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.8569e-05 - accuracy: 1.0000 - val_loss: 4.9753 - val_accuracy: 0.5249\n",
      "Epoch 672/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 7.5832e-05 - accuracy: 1.0000 - val_loss: 4.9760 - val_accuracy: 0.5249\n",
      "Epoch 673/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.6583e-05 - accuracy: 1.0000 - val_loss: 4.9813 - val_accuracy: 0.5339\n",
      "Epoch 674/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.1263e-04 - accuracy: 1.0000 - val_loss: 4.9840 - val_accuracy: 0.5294\n",
      "Epoch 675/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 1.2336e-04 - accuracy: 1.0000 - val_loss: 4.9850 - val_accuracy: 0.5339\n",
      "Epoch 676/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 9.8741e-05 - accuracy: 1.0000 - val_loss: 4.9907 - val_accuracy: 0.5339\n",
      "Epoch 677/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 8.2069e-05 - accuracy: 1.0000 - val_loss: 4.9952 - val_accuracy: 0.5339\n",
      "Epoch 678/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 7.1230e-05 - accuracy: 1.0000 - val_loss: 5.0009 - val_accuracy: 0.5339\n",
      "Epoch 679/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 1.0039e-04 - accuracy: 1.0000 - val_loss: 4.9997 - val_accuracy: 0.5339\n",
      "Epoch 680/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 7.7208e-05 - accuracy: 1.0000 - val_loss: 5.0009 - val_accuracy: 0.5339\n",
      "Epoch 681/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 9.3711e-05 - accuracy: 1.0000 - val_loss: 5.0027 - val_accuracy: 0.5339\n",
      "Epoch 682/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 7.0141e-05 - accuracy: 1.0000 - val_loss: 5.0019 - val_accuracy: 0.5339\n",
      "Epoch 683/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 8.5511e-05 - accuracy: 1.0000 - val_loss: 5.0058 - val_accuracy: 0.5294\n",
      "Epoch 684/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 7.4058e-05 - accuracy: 1.0000 - val_loss: 5.0231 - val_accuracy: 0.5339\n",
      "Epoch 685/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.1390e-05 - accuracy: 1.0000 - val_loss: 5.0295 - val_accuracy: 0.5339\n",
      "Epoch 686/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 7.8584e-05 - accuracy: 1.0000 - val_loss: 5.0399 - val_accuracy: 0.5294\n",
      "Epoch 687/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.9908e-05 - accuracy: 1.0000 - val_loss: 5.0526 - val_accuracy: 0.5249\n",
      "Epoch 688/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 9.5287e-05 - accuracy: 1.0000 - val_loss: 5.0561 - val_accuracy: 0.5249\n",
      "Epoch 689/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.3022e-04 - accuracy: 1.0000 - val_loss: 5.0511 - val_accuracy: 0.5339\n",
      "Epoch 690/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 9.0337e-05 - accuracy: 1.0000 - val_loss: 5.0516 - val_accuracy: 0.5339\n",
      "Epoch 691/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.3094e-05 - accuracy: 1.0000 - val_loss: 5.0534 - val_accuracy: 0.5294\n",
      "Epoch 692/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 7.4294e-05 - accuracy: 1.0000 - val_loss: 5.0671 - val_accuracy: 0.5294\n",
      "Epoch 693/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 8.2684e-05 - accuracy: 1.0000 - val_loss: 5.0642 - val_accuracy: 0.5294\n",
      "Epoch 694/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 6.4612e-05 - accuracy: 1.0000 - val_loss: 5.0604 - val_accuracy: 0.5339\n",
      "Epoch 695/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 5.8934e-05 - accuracy: 1.0000 - val_loss: 5.0669 - val_accuracy: 0.5339\n",
      "Epoch 696/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 7.8584e-05 - accuracy: 1.0000 - val_loss: 5.0661 - val_accuracy: 0.5339\n",
      "Epoch 697/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 6.6313e-05 - accuracy: 1.0000 - val_loss: 5.0725 - val_accuracy: 0.5339\n",
      "Epoch 698/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 5.8266e-05 - accuracy: 1.0000 - val_loss: 5.0768 - val_accuracy: 0.5294\n",
      "Epoch 699/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 7.0609e-05 - accuracy: 1.0000 - val_loss: 5.0861 - val_accuracy: 0.5294\n",
      "Epoch 700/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 7.9973e-05 - accuracy: 1.0000 - val_loss: 5.0889 - val_accuracy: 0.5294\n",
      "Epoch 701/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 7.7056e-05 - accuracy: 1.0000 - val_loss: 5.0856 - val_accuracy: 0.5294\n",
      "Epoch 702/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 4.8358e-05 - accuracy: 1.0000 - val_loss: 5.0942 - val_accuracy: 0.5294\n",
      "Epoch 703/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 8.0698e-05 - accuracy: 1.0000 - val_loss: 5.0979 - val_accuracy: 0.5249\n",
      "Epoch 704/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 5.8626e-05 - accuracy: 1.0000 - val_loss: 5.1010 - val_accuracy: 0.5294\n",
      "Epoch 705/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 5.9776e-05 - accuracy: 1.0000 - val_loss: 5.1121 - val_accuracy: 0.5294\n",
      "Epoch 706/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.9416e-05 - accuracy: 1.0000 - val_loss: 5.1255 - val_accuracy: 0.5339\n",
      "Epoch 707/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 5.2410e-05 - accuracy: 1.0000 - val_loss: 5.1272 - val_accuracy: 0.5339\n",
      "Epoch 708/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.1834e-05 - accuracy: 1.0000 - val_loss: 5.1292 - val_accuracy: 0.5339\n",
      "Epoch 709/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.5632e-05 - accuracy: 1.0000 - val_loss: 5.1329 - val_accuracy: 0.5294\n",
      "Epoch 710/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 6.3164e-05 - accuracy: 1.0000 - val_loss: 5.1366 - val_accuracy: 0.5339\n",
      "Epoch 711/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 6.1092e-05 - accuracy: 1.0000 - val_loss: 5.1335 - val_accuracy: 0.5249\n",
      "Epoch 712/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 6.5845e-05 - accuracy: 1.0000 - val_loss: 5.1370 - val_accuracy: 0.5249\n",
      "Epoch 713/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.4447e-05 - accuracy: 1.0000 - val_loss: 5.1416 - val_accuracy: 0.5249\n",
      "Epoch 714/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 6.2067e-05 - accuracy: 1.0000 - val_loss: 5.1491 - val_accuracy: 0.5249\n",
      "Epoch 715/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 5.4280e-05 - accuracy: 1.0000 - val_loss: 5.1516 - val_accuracy: 0.5249\n",
      "Epoch 716/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 4.8240e-05 - accuracy: 1.0000 - val_loss: 5.1567 - val_accuracy: 0.5249\n",
      "Epoch 717/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 4.7097e-05 - accuracy: 1.0000 - val_loss: 5.1556 - val_accuracy: 0.5249\n",
      "Epoch 718/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 5.3694e-05 - accuracy: 1.0000 - val_loss: 5.1681 - val_accuracy: 0.5249\n",
      "Epoch 719/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.9792e-05 - accuracy: 1.0000 - val_loss: 5.1725 - val_accuracy: 0.5249\n",
      "Epoch 720/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 6.9634e-05 - accuracy: 1.0000 - val_loss: 5.1790 - val_accuracy: 0.5294\n",
      "Epoch 721/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 5.0030e-05 - accuracy: 1.0000 - val_loss: 5.1783 - val_accuracy: 0.5294\n",
      "Epoch 722/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 6.1076e-05 - accuracy: 1.0000 - val_loss: 5.1781 - val_accuracy: 0.5249\n",
      "Epoch 723/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 5.2324e-05 - accuracy: 1.0000 - val_loss: 5.1795 - val_accuracy: 0.5249\n",
      "Epoch 724/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 4.1741e-05 - accuracy: 1.0000 - val_loss: 5.1869 - val_accuracy: 0.5294\n",
      "Epoch 725/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 4.4267e-05 - accuracy: 1.0000 - val_loss: 5.1957 - val_accuracy: 0.5249\n",
      "Epoch 726/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 4.4623e-05 - accuracy: 1.0000 - val_loss: 5.2001 - val_accuracy: 0.5294\n",
      "Epoch 727/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 4.0527e-05 - accuracy: 1.0000 - val_loss: 5.2139 - val_accuracy: 0.5249\n",
      "Epoch 728/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 4.6746e-05 - accuracy: 1.0000 - val_loss: 5.2202 - val_accuracy: 0.5294\n",
      "Epoch 729/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 4.2073e-05 - accuracy: 1.0000 - val_loss: 5.2182 - val_accuracy: 0.5339\n",
      "Epoch 730/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 4.1612e-05 - accuracy: 1.0000 - val_loss: 5.2223 - val_accuracy: 0.5339\n",
      "Epoch 731/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 6.1752e-05 - accuracy: 1.0000 - val_loss: 5.2184 - val_accuracy: 0.5385\n",
      "Epoch 732/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 4.4769e-05 - accuracy: 1.0000 - val_loss: 5.2381 - val_accuracy: 0.5385\n",
      "Epoch 733/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 4.3580e-05 - accuracy: 1.0000 - val_loss: 5.2502 - val_accuracy: 0.5294\n",
      "Epoch 734/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 5.2678e-05 - accuracy: 1.0000 - val_loss: 5.2376 - val_accuracy: 0.5385\n",
      "Epoch 735/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 5.0813e-05 - accuracy: 1.0000 - val_loss: 5.2383 - val_accuracy: 0.5339\n",
      "Epoch 736/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 6.2761e-05 - accuracy: 1.0000 - val_loss: 5.2436 - val_accuracy: 0.5385\n",
      "Epoch 737/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 3.9047e-05 - accuracy: 1.0000 - val_loss: 5.2472 - val_accuracy: 0.5385\n",
      "Epoch 738/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 1.1878e-04 - accuracy: 1.0000 - val_loss: 5.2666 - val_accuracy: 0.5204\n",
      "Epoch 739/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.5838 - accuracy: 0.8753 - val_loss: 2.8707 - val_accuracy: 0.5249\n",
      "Epoch 740/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.9385 - accuracy: 0.5586 - val_loss: 0.7067 - val_accuracy: 0.4615\n",
      "Epoch 741/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.7114 - accuracy: 0.5273 - val_loss: 0.6910 - val_accuracy: 0.5204\n",
      "Epoch 742/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.6924 - accuracy: 0.5328 - val_loss: 0.6908 - val_accuracy: 0.5566\n",
      "Epoch 743/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.6845 - accuracy: 0.5636 - val_loss: 0.6926 - val_accuracy: 0.5566\n",
      "Epoch 744/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.6812 - accuracy: 0.5687 - val_loss: 0.7002 - val_accuracy: 0.5204\n",
      "Epoch 745/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6722 - accuracy: 0.5742 - val_loss: 0.7103 - val_accuracy: 0.5973\n",
      "Epoch 746/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.6582 - accuracy: 0.5924 - val_loss: 0.7162 - val_accuracy: 0.5249\n",
      "Epoch 747/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6480 - accuracy: 0.6121 - val_loss: 0.7684 - val_accuracy: 0.5294\n",
      "Epoch 748/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.6603 - accuracy: 0.6015 - val_loss: 0.7072 - val_accuracy: 0.5430\n",
      "Epoch 749/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.6446 - accuracy: 0.6192 - val_loss: 0.7745 - val_accuracy: 0.5113\n",
      "Epoch 750/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.6245 - accuracy: 0.6490 - val_loss: 0.7751 - val_accuracy: 0.5475\n",
      "Epoch 751/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6056 - accuracy: 0.6636 - val_loss: 0.7951 - val_accuracy: 0.5249\n",
      "Epoch 752/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.5976 - accuracy: 0.6611 - val_loss: 0.8116 - val_accuracy: 0.4977\n",
      "Epoch 753/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.5863 - accuracy: 0.6773 - val_loss: 0.7872 - val_accuracy: 0.4887\n",
      "Epoch 754/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.5641 - accuracy: 0.7000 - val_loss: 0.8477 - val_accuracy: 0.5249\n",
      "Epoch 755/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.5517 - accuracy: 0.7040 - val_loss: 0.8729 - val_accuracy: 0.5113\n",
      "Epoch 756/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.5538 - accuracy: 0.7020 - val_loss: 0.8459 - val_accuracy: 0.5249\n",
      "Epoch 757/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 0.5303 - accuracy: 0.7308 - val_loss: 0.8958 - val_accuracy: 0.4932\n",
      "Epoch 758/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.5173 - accuracy: 0.7394 - val_loss: 1.0009 - val_accuracy: 0.4977\n",
      "Epoch 759/1000\n",
      "1980/1980 [==============================] - 1s 434us/step - loss: 0.5083 - accuracy: 0.7424 - val_loss: 0.9662 - val_accuracy: 0.4977\n",
      "Epoch 760/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.5090 - accuracy: 0.7359 - val_loss: 0.9485 - val_accuracy: 0.4932\n",
      "Epoch 761/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.4873 - accuracy: 0.7465 - val_loss: 0.9645 - val_accuracy: 0.5023\n",
      "Epoch 762/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.4799 - accuracy: 0.7636 - val_loss: 1.1291 - val_accuracy: 0.5158\n",
      "Epoch 763/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.4606 - accuracy: 0.7778 - val_loss: 1.0844 - val_accuracy: 0.5204\n",
      "Epoch 764/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.4411 - accuracy: 0.7798 - val_loss: 1.1157 - val_accuracy: 0.5113\n",
      "Epoch 765/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.4206 - accuracy: 0.8000 - val_loss: 1.2123 - val_accuracy: 0.4977\n",
      "Epoch 766/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.4176 - accuracy: 0.7919 - val_loss: 1.1523 - val_accuracy: 0.4932\n",
      "Epoch 767/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.4143 - accuracy: 0.7919 - val_loss: 1.1933 - val_accuracy: 0.5385\n",
      "Epoch 768/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.4071 - accuracy: 0.7965 - val_loss: 1.2158 - val_accuracy: 0.4887\n",
      "Epoch 769/1000\n",
      "1980/1980 [==============================] - 1s 432us/step - loss: 0.4185 - accuracy: 0.7939 - val_loss: 1.2044 - val_accuracy: 0.4977\n",
      "Epoch 770/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.4236 - accuracy: 0.7934 - val_loss: 1.1487 - val_accuracy: 0.4751\n",
      "Epoch 771/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.3724 - accuracy: 0.8258 - val_loss: 1.2650 - val_accuracy: 0.5068\n",
      "Epoch 772/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.3620 - accuracy: 0.8242 - val_loss: 1.2873 - val_accuracy: 0.4887\n",
      "Epoch 773/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.3433 - accuracy: 0.8399 - val_loss: 1.3871 - val_accuracy: 0.5113\n",
      "Epoch 774/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.3184 - accuracy: 0.8606 - val_loss: 1.4744 - val_accuracy: 0.5113\n",
      "Epoch 775/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.2914 - accuracy: 0.8662 - val_loss: 1.5276 - val_accuracy: 0.5113\n",
      "Epoch 776/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.3121 - accuracy: 0.8556 - val_loss: 1.6229 - val_accuracy: 0.5023\n",
      "Epoch 777/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.3088 - accuracy: 0.8561 - val_loss: 1.6021 - val_accuracy: 0.4887\n",
      "Epoch 778/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.2861 - accuracy: 0.8641 - val_loss: 1.5994 - val_accuracy: 0.4887\n",
      "Epoch 779/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.2672 - accuracy: 0.8823 - val_loss: 1.7068 - val_accuracy: 0.4842\n",
      "Epoch 780/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.2837 - accuracy: 0.8646 - val_loss: 1.7650 - val_accuracy: 0.4570\n",
      "Epoch 781/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.2726 - accuracy: 0.8808 - val_loss: 1.8715 - val_accuracy: 0.4977\n",
      "Epoch 782/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.2483 - accuracy: 0.8929 - val_loss: 1.7285 - val_accuracy: 0.5113\n",
      "Epoch 783/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.2018 - accuracy: 0.9182 - val_loss: 1.9913 - val_accuracy: 0.4887\n",
      "Epoch 784/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.1721 - accuracy: 0.9258 - val_loss: 2.0978 - val_accuracy: 0.4751\n",
      "Epoch 785/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.1611 - accuracy: 0.9288 - val_loss: 2.1272 - val_accuracy: 0.5158\n",
      "Epoch 786/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.1817 - accuracy: 0.9242 - val_loss: 2.1473 - val_accuracy: 0.4932\n",
      "Epoch 787/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.1571 - accuracy: 0.9384 - val_loss: 2.2784 - val_accuracy: 0.5023\n",
      "Epoch 788/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.1872 - accuracy: 0.9258 - val_loss: 2.2344 - val_accuracy: 0.4887\n",
      "Epoch 789/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.2126 - accuracy: 0.9106 - val_loss: 2.0478 - val_accuracy: 0.4842\n",
      "Epoch 790/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.1739 - accuracy: 0.9253 - val_loss: 2.0860 - val_accuracy: 0.5113\n",
      "Epoch 791/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.1610 - accuracy: 0.9354 - val_loss: 2.0336 - val_accuracy: 0.5249\n",
      "Epoch 792/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.1479 - accuracy: 0.9389 - val_loss: 2.1412 - val_accuracy: 0.4932\n",
      "Epoch 793/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.1118 - accuracy: 0.9606 - val_loss: 2.2683 - val_accuracy: 0.5113\n",
      "Epoch 794/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.1247 - accuracy: 0.9561 - val_loss: 2.2795 - val_accuracy: 0.4796\n",
      "Epoch 795/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.1384 - accuracy: 0.9424 - val_loss: 2.4768 - val_accuracy: 0.4842\n",
      "Epoch 796/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.1247 - accuracy: 0.9515 - val_loss: 2.3566 - val_accuracy: 0.5113\n",
      "Epoch 797/1000\n",
      "1980/1980 [==============================] - 1s 442us/step - loss: 0.1077 - accuracy: 0.9566 - val_loss: 2.5211 - val_accuracy: 0.5294\n",
      "Epoch 798/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.1132 - accuracy: 0.9561 - val_loss: 2.3895 - val_accuracy: 0.5113\n",
      "Epoch 799/1000\n",
      "1980/1980 [==============================] - 1s 437us/step - loss: 0.1534 - accuracy: 0.9424 - val_loss: 2.4924 - val_accuracy: 0.5158\n",
      "Epoch 800/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 0.1688 - accuracy: 0.9333 - val_loss: 2.4820 - val_accuracy: 0.5068\n",
      "Epoch 801/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.1240 - accuracy: 0.9525 - val_loss: 2.4416 - val_accuracy: 0.5113\n",
      "Epoch 802/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.1285 - accuracy: 0.9530 - val_loss: 2.3516 - val_accuracy: 0.5430\n",
      "Epoch 803/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.1229 - accuracy: 0.9510 - val_loss: 2.3826 - val_accuracy: 0.5294\n",
      "Epoch 804/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.1062 - accuracy: 0.9586 - val_loss: 2.5245 - val_accuracy: 0.5475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 805/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0843 - accuracy: 0.9727 - val_loss: 2.5873 - val_accuracy: 0.5430\n",
      "Epoch 806/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0608 - accuracy: 0.9798 - val_loss: 2.6669 - val_accuracy: 0.5339\n",
      "Epoch 807/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0788 - accuracy: 0.9727 - val_loss: 2.6578 - val_accuracy: 0.5430\n",
      "Epoch 808/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0636 - accuracy: 0.9798 - val_loss: 2.6772 - val_accuracy: 0.4932\n",
      "Epoch 809/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0538 - accuracy: 0.9813 - val_loss: 2.6693 - val_accuracy: 0.5385\n",
      "Epoch 810/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0516 - accuracy: 0.9838 - val_loss: 2.9242 - val_accuracy: 0.4796\n",
      "Epoch 811/1000\n",
      "1980/1980 [==============================] - 1s 434us/step - loss: 0.0470 - accuracy: 0.9864 - val_loss: 2.8499 - val_accuracy: 0.5158\n",
      "Epoch 812/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.0341 - accuracy: 0.9919 - val_loss: 2.9541 - val_accuracy: 0.5204\n",
      "Epoch 813/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0339 - accuracy: 0.9889 - val_loss: 3.0187 - val_accuracy: 0.5204\n",
      "Epoch 814/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0315 - accuracy: 0.9904 - val_loss: 3.0593 - val_accuracy: 0.5294\n",
      "Epoch 815/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0273 - accuracy: 0.9914 - val_loss: 3.0402 - val_accuracy: 0.5294\n",
      "Epoch 816/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0214 - accuracy: 0.9934 - val_loss: 3.0616 - val_accuracy: 0.5113\n",
      "Epoch 817/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0219 - accuracy: 0.9924 - val_loss: 3.0955 - val_accuracy: 0.5294\n",
      "Epoch 818/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0189 - accuracy: 0.9955 - val_loss: 3.1491 - val_accuracy: 0.5204\n",
      "Epoch 819/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0296 - accuracy: 0.9889 - val_loss: 3.2800 - val_accuracy: 0.5113\n",
      "Epoch 820/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0221 - accuracy: 0.9944 - val_loss: 3.1100 - val_accuracy: 0.5249\n",
      "Epoch 821/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0213 - accuracy: 0.9919 - val_loss: 3.2137 - val_accuracy: 0.5249\n",
      "Epoch 822/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0177 - accuracy: 0.9934 - val_loss: 3.3040 - val_accuracy: 0.5204\n",
      "Epoch 823/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0148 - accuracy: 0.9955 - val_loss: 3.2616 - val_accuracy: 0.5339\n",
      "Epoch 824/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0150 - accuracy: 0.9944 - val_loss: 3.2975 - val_accuracy: 0.5294\n",
      "Epoch 825/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 3.3609 - val_accuracy: 0.5158\n",
      "Epoch 826/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0097 - accuracy: 0.9975 - val_loss: 3.3466 - val_accuracy: 0.5294\n",
      "Epoch 827/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 3.3862 - val_accuracy: 0.5339\n",
      "Epoch 828/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0063 - accuracy: 0.9995 - val_loss: 3.4201 - val_accuracy: 0.5249\n",
      "Epoch 829/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 3.4488 - val_accuracy: 0.5249\n",
      "Epoch 830/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 3.4680 - val_accuracy: 0.5249\n",
      "Epoch 831/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 3.5070 - val_accuracy: 0.5158\n",
      "Epoch 832/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 3.4884 - val_accuracy: 0.5249\n",
      "Epoch 833/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0120 - accuracy: 0.9955 - val_loss: 3.3887 - val_accuracy: 0.5249\n",
      "Epoch 834/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.0364 - accuracy: 0.9909 - val_loss: 3.3392 - val_accuracy: 0.5294\n",
      "Epoch 835/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.1670 - accuracy: 0.9455 - val_loss: 2.5809 - val_accuracy: 0.5385\n",
      "Epoch 836/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.3241 - accuracy: 0.8899 - val_loss: 2.4325 - val_accuracy: 0.5339\n",
      "Epoch 837/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.3197 - accuracy: 0.8687 - val_loss: 1.9190 - val_accuracy: 0.5158\n",
      "Epoch 838/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.2262 - accuracy: 0.9096 - val_loss: 2.1099 - val_accuracy: 0.4751\n",
      "Epoch 839/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.1506 - accuracy: 0.9394 - val_loss: 2.2013 - val_accuracy: 0.5204\n",
      "Epoch 840/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0748 - accuracy: 0.9753 - val_loss: 2.6121 - val_accuracy: 0.5113\n",
      "Epoch 841/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0543 - accuracy: 0.9823 - val_loss: 2.6046 - val_accuracy: 0.5158\n",
      "Epoch 842/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0418 - accuracy: 0.9919 - val_loss: 2.7542 - val_accuracy: 0.5113\n",
      "Epoch 843/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0308 - accuracy: 0.9919 - val_loss: 2.8150 - val_accuracy: 0.5204\n",
      "Epoch 844/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0225 - accuracy: 0.9960 - val_loss: 2.8894 - val_accuracy: 0.5113\n",
      "Epoch 845/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0148 - accuracy: 0.9970 - val_loss: 2.8413 - val_accuracy: 0.5158\n",
      "Epoch 846/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0113 - accuracy: 0.9975 - val_loss: 2.9110 - val_accuracy: 0.5249\n",
      "Epoch 847/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0082 - accuracy: 0.9995 - val_loss: 2.9515 - val_accuracy: 0.5339\n",
      "Epoch 848/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 3.0216 - val_accuracy: 0.5339\n",
      "Epoch 849/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0062 - accuracy: 0.9995 - val_loss: 3.0391 - val_accuracy: 0.5339\n",
      "Epoch 850/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 3.0464 - val_accuracy: 0.5294\n",
      "Epoch 851/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0055 - accuracy: 0.9990 - val_loss: 3.0798 - val_accuracy: 0.5339\n",
      "Epoch 852/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 3.1403 - val_accuracy: 0.5294\n",
      "Epoch 853/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 3.1685 - val_accuracy: 0.5294\n",
      "Epoch 854/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 3.1873 - val_accuracy: 0.5249\n",
      "Epoch 855/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 3.2165 - val_accuracy: 0.5113\n",
      "Epoch 856/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.0048 - accuracy: 0.9995 - val_loss: 3.1938 - val_accuracy: 0.5339\n",
      "Epoch 857/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 3.2665 - val_accuracy: 0.5068\n",
      "Epoch 858/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 3.2338 - val_accuracy: 0.5294\n",
      "Epoch 859/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 3.2500 - val_accuracy: 0.5294\n",
      "Epoch 860/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 3.2628 - val_accuracy: 0.5339\n",
      "Epoch 861/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 3.2880 - val_accuracy: 0.5294\n",
      "Epoch 862/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 3.3313 - val_accuracy: 0.5204\n",
      "Epoch 863/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 3.3454 - val_accuracy: 0.5204\n",
      "Epoch 864/1000\n",
      "1980/1980 [==============================] - 1s 437us/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 3.3602 - val_accuracy: 0.5204\n",
      "Epoch 865/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 3.3915 - val_accuracy: 0.5204\n",
      "Epoch 866/1000\n",
      "1980/1980 [==============================] - 1s 506us/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 3.3964 - val_accuracy: 0.5249\n",
      "Epoch 867/1000\n",
      "1980/1980 [==============================] - 1s 441us/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 3.4052 - val_accuracy: 0.5249\n",
      "Epoch 868/1000\n",
      "1980/1980 [==============================] - 1s 482us/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 3.4120 - val_accuracy: 0.5249\n",
      "Epoch 869/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 3.4456 - val_accuracy: 0.5249\n",
      "Epoch 870/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 3.4389 - val_accuracy: 0.5249\n",
      "Epoch 871/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 3.4790 - val_accuracy: 0.4977\n",
      "Epoch 872/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 3.4873 - val_accuracy: 0.5113\n",
      "Epoch 873/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 3.4684 - val_accuracy: 0.5249\n",
      "Epoch 874/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 3.4626 - val_accuracy: 0.5113\n",
      "Epoch 875/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 3.4950 - val_accuracy: 0.5158\n",
      "Epoch 876/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 3.5079 - val_accuracy: 0.5204\n",
      "Epoch 877/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 3.4973 - val_accuracy: 0.5294\n",
      "Epoch 878/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 3.4814 - val_accuracy: 0.5158\n",
      "Epoch 879/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 3.4927 - val_accuracy: 0.5249\n",
      "Epoch 880/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 3.4503 - val_accuracy: 0.5385\n",
      "Epoch 881/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 3.4560 - val_accuracy: 0.5204\n",
      "Epoch 882/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 3.5073 - val_accuracy: 0.5294\n",
      "Epoch 883/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.0113 - accuracy: 0.9980 - val_loss: 3.5724 - val_accuracy: 0.5249\n",
      "Epoch 884/1000\n",
      "1980/1980 [==============================] - 1s 511us/step - loss: 0.0095 - accuracy: 0.9980 - val_loss: 3.4679 - val_accuracy: 0.5385\n",
      "Epoch 885/1000\n",
      "1980/1980 [==============================] - 1s 494us/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 3.4221 - val_accuracy: 0.5339\n",
      "Epoch 886/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 3.5351 - val_accuracy: 0.5158\n",
      "Epoch 887/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 3.6278 - val_accuracy: 0.5204\n",
      "Epoch 888/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 3.6348 - val_accuracy: 0.5294\n",
      "Epoch 889/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 3.6366 - val_accuracy: 0.5249\n",
      "Epoch 890/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.6401 - val_accuracy: 0.5113\n",
      "Epoch 891/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 3.6529 - val_accuracy: 0.5158\n",
      "Epoch 892/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.6688 - val_accuracy: 0.5204\n",
      "Epoch 893/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 3.6578 - val_accuracy: 0.5158\n",
      "Epoch 894/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.6826 - val_accuracy: 0.5113\n",
      "Epoch 895/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.6889 - val_accuracy: 0.5068\n",
      "Epoch 896/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.7195 - val_accuracy: 0.5294\n",
      "Epoch 897/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.6798 - val_accuracy: 0.5113\n",
      "Epoch 898/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 3.6962 - val_accuracy: 0.5204\n",
      "Epoch 899/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 3.6890 - val_accuracy: 0.5204\n",
      "Epoch 900/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.7463 - val_accuracy: 0.5113\n",
      "Epoch 901/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 3.7442 - val_accuracy: 0.5113\n",
      "Epoch 902/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.7747 - val_accuracy: 0.5068\n",
      "Epoch 903/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.7830 - val_accuracy: 0.5068\n",
      "Epoch 904/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.7631 - val_accuracy: 0.5113\n",
      "Epoch 905/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 3.8107 - val_accuracy: 0.5158\n",
      "Epoch 906/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 3.8108 - val_accuracy: 0.5113\n",
      "Epoch 907/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 8.4579e-04 - accuracy: 1.0000 - val_loss: 3.8063 - val_accuracy: 0.5113\n",
      "Epoch 908/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 9.4917e-04 - accuracy: 1.0000 - val_loss: 3.7980 - val_accuracy: 0.5158\n",
      "Epoch 909/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.8378 - val_accuracy: 0.5023\n",
      "Epoch 910/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 3.8879 - val_accuracy: 0.4932\n",
      "Epoch 911/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0348 - accuracy: 0.9879 - val_loss: 3.6188 - val_accuracy: 0.5204\n",
      "Epoch 912/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.2631 - accuracy: 0.9217 - val_loss: 2.8251 - val_accuracy: 0.5204\n",
      "Epoch 913/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5170 - accuracy: 0.8096 - val_loss: 1.6536 - val_accuracy: 0.5294\n",
      "Epoch 914/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.4131 - accuracy: 0.8056 - val_loss: 1.4678 - val_accuracy: 0.5158\n",
      "Epoch 915/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.2522 - accuracy: 0.8955 - val_loss: 1.7906 - val_accuracy: 0.5294\n",
      "Epoch 916/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.1432 - accuracy: 0.9470 - val_loss: 2.2057 - val_accuracy: 0.5023\n",
      "Epoch 917/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.0847 - accuracy: 0.9712 - val_loss: 2.4741 - val_accuracy: 0.4977\n",
      "Epoch 918/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.0705 - accuracy: 0.9793 - val_loss: 2.7232 - val_accuracy: 0.4751\n",
      "Epoch 919/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0628 - accuracy: 0.9818 - val_loss: 2.7354 - val_accuracy: 0.5068\n",
      "Epoch 920/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0494 - accuracy: 0.9828 - val_loss: 2.9072 - val_accuracy: 0.4842\n",
      "Epoch 921/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0509 - accuracy: 0.9848 - val_loss: 2.9838 - val_accuracy: 0.4796\n",
      "Epoch 922/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0407 - accuracy: 0.9859 - val_loss: 3.0142 - val_accuracy: 0.5068\n",
      "Epoch 923/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.0408 - accuracy: 0.9919 - val_loss: 2.9985 - val_accuracy: 0.5204\n",
      "Epoch 924/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0473 - accuracy: 0.9854 - val_loss: 3.2176 - val_accuracy: 0.5068\n",
      "Epoch 925/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0359 - accuracy: 0.9894 - val_loss: 3.1681 - val_accuracy: 0.5385\n",
      "Epoch 926/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0313 - accuracy: 0.9919 - val_loss: 2.9945 - val_accuracy: 0.5204\n",
      "Epoch 927/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0452 - accuracy: 0.9874 - val_loss: 3.0250 - val_accuracy: 0.5068\n",
      "Epoch 928/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0598 - accuracy: 0.9823 - val_loss: 3.0680 - val_accuracy: 0.5339\n",
      "Epoch 929/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0532 - accuracy: 0.9808 - val_loss: 2.9217 - val_accuracy: 0.5113\n",
      "Epoch 930/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0237 - accuracy: 0.9944 - val_loss: 3.1024 - val_accuracy: 0.5023\n",
      "Epoch 931/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0122 - accuracy: 0.9965 - val_loss: 3.1319 - val_accuracy: 0.5023\n",
      "Epoch 932/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 3.2356 - val_accuracy: 0.4977\n",
      "Epoch 933/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 3.3001 - val_accuracy: 0.4887\n",
      "Epoch 934/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 3.3491 - val_accuracy: 0.4977\n",
      "Epoch 935/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.3826 - val_accuracy: 0.5023\n",
      "Epoch 936/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 3.4033 - val_accuracy: 0.5023\n",
      "Epoch 937/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.4515 - val_accuracy: 0.4977\n",
      "Epoch 938/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 3.4824 - val_accuracy: 0.4932\n",
      "Epoch 939/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.4908 - val_accuracy: 0.4932\n",
      "Epoch 940/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.5221 - val_accuracy: 0.4977\n",
      "Epoch 941/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 3.5311 - val_accuracy: 0.4977\n",
      "Epoch 942/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.5386 - val_accuracy: 0.4932\n",
      "Epoch 943/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.5601 - val_accuracy: 0.4932\n",
      "Epoch 944/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.6059 - val_accuracy: 0.4932\n",
      "Epoch 945/1000\n",
      "1980/1980 [==============================] - 1s 486us/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 3.5769 - val_accuracy: 0.5068\n",
      "Epoch 946/1000\n",
      "1980/1980 [==============================] - 1s 515us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.6172 - val_accuracy: 0.4977\n",
      "Epoch 947/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.6460 - val_accuracy: 0.4977\n",
      "Epoch 948/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 3.6415 - val_accuracy: 0.4932\n",
      "Epoch 949/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 3.6722 - val_accuracy: 0.4932\n",
      "Epoch 950/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 3.5391 - val_accuracy: 0.4977\n",
      "Epoch 951/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 3.5561 - val_accuracy: 0.5023\n",
      "Epoch 952/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.5603 - val_accuracy: 0.4977\n",
      "Epoch 953/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.6243 - val_accuracy: 0.4932\n",
      "Epoch 954/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 3.6492 - val_accuracy: 0.4977\n",
      "Epoch 955/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.6762 - val_accuracy: 0.4932\n",
      "Epoch 956/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.6912 - val_accuracy: 0.4932\n",
      "Epoch 957/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 3.7032 - val_accuracy: 0.4932\n",
      "Epoch 958/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.7251 - val_accuracy: 0.4932\n",
      "Epoch 959/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.7443 - val_accuracy: 0.4932\n",
      "Epoch 960/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 8.6859e-04 - accuracy: 1.0000 - val_loss: 3.7584 - val_accuracy: 0.4932\n",
      "Epoch 961/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.7673 - val_accuracy: 0.4932\n",
      "Epoch 962/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.7953 - val_accuracy: 0.4977\n",
      "Epoch 963/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.7750 - val_accuracy: 0.4932\n",
      "Epoch 964/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 3.7891 - val_accuracy: 0.4887\n",
      "Epoch 965/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 3.8153 - val_accuracy: 0.5023\n",
      "Epoch 966/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 3.8139 - val_accuracy: 0.4977\n",
      "Epoch 967/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8074 - val_accuracy: 0.4977\n",
      "Epoch 968/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8325 - val_accuracy: 0.4977\n",
      "Epoch 969/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 6.9661e-04 - accuracy: 1.0000 - val_loss: 3.8506 - val_accuracy: 0.5023\n",
      "Epoch 970/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 7.3905e-04 - accuracy: 1.0000 - val_loss: 3.8590 - val_accuracy: 0.4977\n",
      "Epoch 971/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 7.1609e-04 - accuracy: 1.0000 - val_loss: 3.8687 - val_accuracy: 0.4977\n",
      "Epoch 972/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 7.0511e-04 - accuracy: 1.0000 - val_loss: 3.8748 - val_accuracy: 0.4977\n",
      "Epoch 973/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 6.2173e-04 - accuracy: 1.0000 - val_loss: 3.8933 - val_accuracy: 0.4977\n",
      "Epoch 974/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 6.8076e-04 - accuracy: 1.0000 - val_loss: 3.9034 - val_accuracy: 0.4977\n",
      "Epoch 975/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 7.1596e-04 - accuracy: 1.0000 - val_loss: 3.9192 - val_accuracy: 0.4977\n",
      "Epoch 976/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 9.7363e-04 - accuracy: 0.9995 - val_loss: 3.9133 - val_accuracy: 0.4977\n",
      "Epoch 977/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 3.9422 - val_accuracy: 0.4796\n",
      "Epoch 978/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.0776 - accuracy: 0.9808 - val_loss: 3.3180 - val_accuracy: 0.5158\n",
      "Epoch 979/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.1929 - accuracy: 0.9359 - val_loss: 3.1067 - val_accuracy: 0.5158\n",
      "Epoch 980/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.2566 - accuracy: 0.9086 - val_loss: 2.4783 - val_accuracy: 0.5249\n",
      "Epoch 981/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.1758 - accuracy: 0.9348 - val_loss: 2.4859 - val_accuracy: 0.5158\n",
      "Epoch 982/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1202 - accuracy: 0.9540 - val_loss: 2.6665 - val_accuracy: 0.4887\n",
      "Epoch 983/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.1188 - accuracy: 0.9561 - val_loss: 2.8790 - val_accuracy: 0.4887\n",
      "Epoch 984/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0652 - accuracy: 0.9773 - val_loss: 2.8873 - val_accuracy: 0.5023\n",
      "Epoch 985/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0539 - accuracy: 0.9828 - val_loss: 2.8268 - val_accuracy: 0.4932\n",
      "Epoch 986/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0396 - accuracy: 0.9889 - val_loss: 3.0282 - val_accuracy: 0.5294\n",
      "Epoch 987/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.0268 - accuracy: 0.9914 - val_loss: 3.1946 - val_accuracy: 0.4932\n",
      "Epoch 988/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0198 - accuracy: 0.9949 - val_loss: 3.0869 - val_accuracy: 0.4977\n",
      "Epoch 989/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0146 - accuracy: 0.9965 - val_loss: 3.2986 - val_accuracy: 0.5068\n",
      "Epoch 990/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 3.2902 - val_accuracy: 0.5113\n",
      "Epoch 991/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0054 - accuracy: 0.9995 - val_loss: 3.3589 - val_accuracy: 0.5023\n",
      "Epoch 992/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 3.4088 - val_accuracy: 0.5068\n",
      "Epoch 993/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.4398 - val_accuracy: 0.5023\n",
      "Epoch 994/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.4599 - val_accuracy: 0.5068\n",
      "Epoch 995/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.4806 - val_accuracy: 0.5113\n",
      "Epoch 996/1000\n",
      "1980/1980 [==============================] - 1s 358us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.5101 - val_accuracy: 0.5068\n",
      "Epoch 997/1000\n",
      "1980/1980 [==============================] - 1s 363us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.5426 - val_accuracy: 0.5023\n",
      "Epoch 998/1000\n",
      "1980/1980 [==============================] - 1s 359us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.5575 - val_accuracy: 0.5113\n",
      "Epoch 999/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.5886 - val_accuracy: 0.5023\n",
      "Epoch 1000/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.6081 - val_accuracy: 0.5068\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 127us/step\n",
      "test loss, test acc: [3.4057431941129725, 0.5020408034324646]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.6852301146750568\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=500, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(15, 64), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_70 (LSTM)               (None, 15, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_72 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 160,673\n",
      "Trainable params: 160,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 2s 911us/step - loss: 0.6968 - accuracy: 0.5066 - val_loss: 0.6900 - val_accuracy: 0.5701\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6920 - accuracy: 0.5308 - val_loss: 0.6933 - val_accuracy: 0.4932\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6926 - accuracy: 0.5232 - val_loss: 0.6889 - val_accuracy: 0.5566\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6918 - accuracy: 0.5197 - val_loss: 0.6856 - val_accuracy: 0.5701\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6950 - accuracy: 0.4985 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6918 - accuracy: 0.5237 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6933 - accuracy: 0.5187 - val_loss: 0.6871 - val_accuracy: 0.5701\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6936 - accuracy: 0.5217 - val_loss: 0.6914 - val_accuracy: 0.5475\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6921 - accuracy: 0.5268 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.6934 - accuracy: 0.5258 - val_loss: 0.6894 - val_accuracy: 0.5701\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6917 - accuracy: 0.5232 - val_loss: 0.6878 - val_accuracy: 0.5701\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 361us/step - loss: 0.6922 - accuracy: 0.5187 - val_loss: 0.6889 - val_accuracy: 0.5701\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.6920 - accuracy: 0.5146 - val_loss: 0.6887 - val_accuracy: 0.5656\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6913 - accuracy: 0.5242 - val_loss: 0.6876 - val_accuracy: 0.5566\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6905 - accuracy: 0.5207 - val_loss: 0.6957 - val_accuracy: 0.4525\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6911 - accuracy: 0.5283 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6921 - accuracy: 0.5202 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6919 - accuracy: 0.5222 - val_loss: 0.6910 - val_accuracy: 0.5294\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6922 - accuracy: 0.5091 - val_loss: 0.6893 - val_accuracy: 0.5385\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6933 - accuracy: 0.5232 - val_loss: 0.6891 - val_accuracy: 0.5701\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6911 - accuracy: 0.5258 - val_loss: 0.6887 - val_accuracy: 0.5656\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6912 - accuracy: 0.5278 - val_loss: 0.6890 - val_accuracy: 0.5475\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6913 - accuracy: 0.5172 - val_loss: 0.7009 - val_accuracy: 0.4389\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6939 - accuracy: 0.5091 - val_loss: 0.6862 - val_accuracy: 0.5701\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6919 - accuracy: 0.5232 - val_loss: 0.6884 - val_accuracy: 0.5701\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6926 - accuracy: 0.5197 - val_loss: 0.6902 - val_accuracy: 0.5701\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6912 - accuracy: 0.5253 - val_loss: 0.6882 - val_accuracy: 0.5701\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6916 - accuracy: 0.5232 - val_loss: 0.6887 - val_accuracy: 0.5656\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.6919 - accuracy: 0.5288 - val_loss: 0.6905 - val_accuracy: 0.5656\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6923 - accuracy: 0.5192 - val_loss: 0.6877 - val_accuracy: 0.5701\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6911 - accuracy: 0.5273 - val_loss: 0.6897 - val_accuracy: 0.5430\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6915 - accuracy: 0.5253 - val_loss: 0.6915 - val_accuracy: 0.5204\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6909 - accuracy: 0.5303 - val_loss: 0.6892 - val_accuracy: 0.5566\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6938 - accuracy: 0.5172 - val_loss: 0.6895 - val_accuracy: 0.5701\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6916 - accuracy: 0.5253 - val_loss: 0.6877 - val_accuracy: 0.5701\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6917 - accuracy: 0.5197 - val_loss: 0.6902 - val_accuracy: 0.5475\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6915 - accuracy: 0.5237 - val_loss: 0.6892 - val_accuracy: 0.5656\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6911 - accuracy: 0.5247 - val_loss: 0.6910 - val_accuracy: 0.5339\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6918 - accuracy: 0.5232 - val_loss: 0.6910 - val_accuracy: 0.5339\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6910 - accuracy: 0.5374 - val_loss: 0.6908 - val_accuracy: 0.5249\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6917 - accuracy: 0.5313 - val_loss: 0.6888 - val_accuracy: 0.5475\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6908 - accuracy: 0.5278 - val_loss: 0.6889 - val_accuracy: 0.5747\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6905 - accuracy: 0.5247 - val_loss: 0.6903 - val_accuracy: 0.5249\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6898 - accuracy: 0.5374 - val_loss: 0.6921 - val_accuracy: 0.4932\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6910 - accuracy: 0.5141 - val_loss: 0.6877 - val_accuracy: 0.5701\n",
      "Epoch 48/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6923 - accuracy: 0.5232 - val_loss: 0.6952 - val_accuracy: 0.4253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6925 - accuracy: 0.5187 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.6911 - accuracy: 0.5227 - val_loss: 0.6902 - val_accuracy: 0.5520\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6910 - accuracy: 0.5247 - val_loss: 0.6899 - val_accuracy: 0.5475\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6905 - accuracy: 0.5222 - val_loss: 0.6917 - val_accuracy: 0.5339\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6914 - accuracy: 0.5227 - val_loss: 0.6932 - val_accuracy: 0.5113\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6896 - accuracy: 0.5278 - val_loss: 0.6906 - val_accuracy: 0.5656\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6912 - accuracy: 0.5212 - val_loss: 0.6918 - val_accuracy: 0.5158\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6922 - accuracy: 0.5217 - val_loss: 0.6887 - val_accuracy: 0.5701\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 0.6913 - accuracy: 0.5237 - val_loss: 0.6888 - val_accuracy: 0.5701\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.6906 - accuracy: 0.5338 - val_loss: 0.6897 - val_accuracy: 0.5430\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6901 - accuracy: 0.5328 - val_loss: 0.6916 - val_accuracy: 0.5249\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6902 - accuracy: 0.5333 - val_loss: 0.6936 - val_accuracy: 0.4977\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6915 - accuracy: 0.5343 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6916 - accuracy: 0.5212 - val_loss: 0.6893 - val_accuracy: 0.5701\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6904 - accuracy: 0.5288 - val_loss: 0.6909 - val_accuracy: 0.5475\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6909 - accuracy: 0.5197 - val_loss: 0.6935 - val_accuracy: 0.5068\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6892 - accuracy: 0.5424 - val_loss: 0.6919 - val_accuracy: 0.5339\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6910 - accuracy: 0.5258 - val_loss: 0.6934 - val_accuracy: 0.5339\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.6899 - accuracy: 0.5364 - val_loss: 0.6915 - val_accuracy: 0.5520\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.6903 - accuracy: 0.5364 - val_loss: 0.6897 - val_accuracy: 0.5611\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.6901 - accuracy: 0.5338 - val_loss: 0.6932 - val_accuracy: 0.5430\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6889 - accuracy: 0.5475 - val_loss: 0.6920 - val_accuracy: 0.5339\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6888 - accuracy: 0.5434 - val_loss: 0.6916 - val_accuracy: 0.5294\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 0.6878 - accuracy: 0.5313 - val_loss: 0.6994 - val_accuracy: 0.4480\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6885 - accuracy: 0.5384 - val_loss: 0.6917 - val_accuracy: 0.5204\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6881 - accuracy: 0.5434 - val_loss: 0.6914 - val_accuracy: 0.5294\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.6875 - accuracy: 0.5419 - val_loss: 0.6927 - val_accuracy: 0.5339\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 487us/step - loss: 0.6885 - accuracy: 0.5414 - val_loss: 0.6920 - val_accuracy: 0.5430\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.6876 - accuracy: 0.5490 - val_loss: 0.7049 - val_accuracy: 0.4570\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6897 - accuracy: 0.5273 - val_loss: 0.6916 - val_accuracy: 0.5294\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6897 - accuracy: 0.5263 - val_loss: 0.6932 - val_accuracy: 0.5113\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6868 - accuracy: 0.5419 - val_loss: 0.6906 - val_accuracy: 0.5656\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6877 - accuracy: 0.5359 - val_loss: 0.6953 - val_accuracy: 0.5294\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6853 - accuracy: 0.5606 - val_loss: 0.6942 - val_accuracy: 0.5566\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6864 - accuracy: 0.5490 - val_loss: 0.6938 - val_accuracy: 0.5475\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6855 - accuracy: 0.5556 - val_loss: 0.7017 - val_accuracy: 0.4796\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6839 - accuracy: 0.5535 - val_loss: 0.6888 - val_accuracy: 0.5701\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6836 - accuracy: 0.5540 - val_loss: 0.6951 - val_accuracy: 0.5475\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6844 - accuracy: 0.5535 - val_loss: 0.6944 - val_accuracy: 0.5520\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6837 - accuracy: 0.5490 - val_loss: 0.6963 - val_accuracy: 0.5520\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6845 - accuracy: 0.5520 - val_loss: 0.6902 - val_accuracy: 0.5747\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6811 - accuracy: 0.5616 - val_loss: 0.6939 - val_accuracy: 0.5113\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6808 - accuracy: 0.5606 - val_loss: 0.6905 - val_accuracy: 0.5339\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6869 - accuracy: 0.5333 - val_loss: 0.6864 - val_accuracy: 0.5430\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6828 - accuracy: 0.5540 - val_loss: 0.6906 - val_accuracy: 0.5385\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6814 - accuracy: 0.5556 - val_loss: 0.6952 - val_accuracy: 0.5294\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6835 - accuracy: 0.5535 - val_loss: 0.6956 - val_accuracy: 0.5611\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6861 - accuracy: 0.5414 - val_loss: 0.6877 - val_accuracy: 0.5204\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6846 - accuracy: 0.5505 - val_loss: 0.6905 - val_accuracy: 0.5204\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6819 - accuracy: 0.5480 - val_loss: 0.6950 - val_accuracy: 0.5204\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6806 - accuracy: 0.5641 - val_loss: 0.7019 - val_accuracy: 0.5113\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6782 - accuracy: 0.5601 - val_loss: 0.6961 - val_accuracy: 0.5566\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6772 - accuracy: 0.5636 - val_loss: 0.7001 - val_accuracy: 0.5385\n",
      "Epoch 102/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6751 - accuracy: 0.5697 - val_loss: 0.7030 - val_accuracy: 0.5611\n",
      "Epoch 103/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6766 - accuracy: 0.5641 - val_loss: 0.6973 - val_accuracy: 0.5566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6829 - accuracy: 0.5697 - val_loss: 0.6901 - val_accuracy: 0.5430\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6803 - accuracy: 0.5697 - val_loss: 0.6989 - val_accuracy: 0.5068\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.6747 - accuracy: 0.5697 - val_loss: 0.7087 - val_accuracy: 0.5249\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.6784 - accuracy: 0.5707 - val_loss: 0.7000 - val_accuracy: 0.5701\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6745 - accuracy: 0.5707 - val_loss: 0.6912 - val_accuracy: 0.5339\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6730 - accuracy: 0.5707 - val_loss: 0.6972 - val_accuracy: 0.5611\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6713 - accuracy: 0.5747 - val_loss: 0.7008 - val_accuracy: 0.5339\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6714 - accuracy: 0.5753 - val_loss: 0.6975 - val_accuracy: 0.5113\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6696 - accuracy: 0.5753 - val_loss: 0.6997 - val_accuracy: 0.5113\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6652 - accuracy: 0.5843 - val_loss: 0.7100 - val_accuracy: 0.5792\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6623 - accuracy: 0.5874 - val_loss: 0.6944 - val_accuracy: 0.5747\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6636 - accuracy: 0.5955 - val_loss: 0.7023 - val_accuracy: 0.5204\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6646 - accuracy: 0.5949 - val_loss: 0.7151 - val_accuracy: 0.5566\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6689 - accuracy: 0.5864 - val_loss: 0.6943 - val_accuracy: 0.5158\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6658 - accuracy: 0.5879 - val_loss: 0.6997 - val_accuracy: 0.5204\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6647 - accuracy: 0.5747 - val_loss: 0.7044 - val_accuracy: 0.5792\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6597 - accuracy: 0.5899 - val_loss: 0.6925 - val_accuracy: 0.5520\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6617 - accuracy: 0.5833 - val_loss: 0.6988 - val_accuracy: 0.5611\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6608 - accuracy: 0.5929 - val_loss: 0.7032 - val_accuracy: 0.5113\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6606 - accuracy: 0.6010 - val_loss: 0.7042 - val_accuracy: 0.5430\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.6536 - accuracy: 0.6005 - val_loss: 0.7290 - val_accuracy: 0.5339\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.6600 - accuracy: 0.6015 - val_loss: 0.7031 - val_accuracy: 0.5339\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.6527 - accuracy: 0.6000 - val_loss: 0.7252 - val_accuracy: 0.5294\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.6546 - accuracy: 0.5919 - val_loss: 0.7007 - val_accuracy: 0.5339\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.6553 - accuracy: 0.6131 - val_loss: 0.6986 - val_accuracy: 0.5792\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.6514 - accuracy: 0.6035 - val_loss: 0.7444 - val_accuracy: 0.5023\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.6559 - accuracy: 0.6015 - val_loss: 0.7094 - val_accuracy: 0.5068\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.6494 - accuracy: 0.6101 - val_loss: 0.7114 - val_accuracy: 0.5204\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.6452 - accuracy: 0.6101 - val_loss: 0.7213 - val_accuracy: 0.4887\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.6531 - accuracy: 0.5884 - val_loss: 0.7101 - val_accuracy: 0.5475\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.6497 - accuracy: 0.6056 - val_loss: 0.7057 - val_accuracy: 0.5837\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6428 - accuracy: 0.6253 - val_loss: 0.7019 - val_accuracy: 0.5566\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6402 - accuracy: 0.6141 - val_loss: 0.7225 - val_accuracy: 0.5475\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6383 - accuracy: 0.6268 - val_loss: 0.7468 - val_accuracy: 0.5068\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6307 - accuracy: 0.6222 - val_loss: 0.7168 - val_accuracy: 0.5656\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6397 - accuracy: 0.6172 - val_loss: 0.7314 - val_accuracy: 0.5611\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6365 - accuracy: 0.6222 - val_loss: 0.7364 - val_accuracy: 0.5204\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6294 - accuracy: 0.6177 - val_loss: 0.7351 - val_accuracy: 0.5113\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6375 - accuracy: 0.6136 - val_loss: 0.7300 - val_accuracy: 0.5023\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6371 - accuracy: 0.6172 - val_loss: 0.7266 - val_accuracy: 0.5430\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.6303 - accuracy: 0.6207 - val_loss: 0.7358 - val_accuracy: 0.5068\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6235 - accuracy: 0.6237 - val_loss: 0.7519 - val_accuracy: 0.5475\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6227 - accuracy: 0.6283 - val_loss: 0.7518 - val_accuracy: 0.5656\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.6316 - accuracy: 0.6283 - val_loss: 0.7428 - val_accuracy: 0.5068\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.6205 - accuracy: 0.6247 - val_loss: 0.7522 - val_accuracy: 0.5294\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6153 - accuracy: 0.6429 - val_loss: 0.7454 - val_accuracy: 0.4842\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.6319 - accuracy: 0.6192 - val_loss: 0.7132 - val_accuracy: 0.5023\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.6192 - accuracy: 0.6293 - val_loss: 0.7429 - val_accuracy: 0.5611\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6131 - accuracy: 0.6566 - val_loss: 0.8148 - val_accuracy: 0.5385\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6153 - accuracy: 0.6409 - val_loss: 0.7694 - val_accuracy: 0.5158\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6084 - accuracy: 0.6480 - val_loss: 0.7851 - val_accuracy: 0.5520\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.6123 - accuracy: 0.6313 - val_loss: 0.7332 - val_accuracy: 0.5339\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.6095 - accuracy: 0.6444 - val_loss: 0.7992 - val_accuracy: 0.5023\n",
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6083 - accuracy: 0.6475 - val_loss: 0.7727 - val_accuracy: 0.5475\n",
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.6096 - accuracy: 0.6414 - val_loss: 0.8262 - val_accuracy: 0.5023\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.6040 - accuracy: 0.6475 - val_loss: 0.7697 - val_accuracy: 0.5294\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.5912 - accuracy: 0.6621 - val_loss: 0.8126 - val_accuracy: 0.5249\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.5980 - accuracy: 0.6556 - val_loss: 0.8180 - val_accuracy: 0.5385\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5977 - accuracy: 0.6520 - val_loss: 0.7706 - val_accuracy: 0.5249\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.5955 - accuracy: 0.6480 - val_loss: 0.8514 - val_accuracy: 0.5113\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.5941 - accuracy: 0.6591 - val_loss: 0.8725 - val_accuracy: 0.5023\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6106 - accuracy: 0.6667 - val_loss: 0.7658 - val_accuracy: 0.5385\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.5960 - accuracy: 0.6591 - val_loss: 0.7953 - val_accuracy: 0.5385\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.5952 - accuracy: 0.6535 - val_loss: 0.7724 - val_accuracy: 0.5249\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.5870 - accuracy: 0.6601 - val_loss: 0.7932 - val_accuracy: 0.5339\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.5875 - accuracy: 0.6500 - val_loss: 0.7700 - val_accuracy: 0.5475\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6072 - accuracy: 0.6434 - val_loss: 0.8134 - val_accuracy: 0.5204\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.5873 - accuracy: 0.6571 - val_loss: 0.7751 - val_accuracy: 0.5385\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.5850 - accuracy: 0.6697 - val_loss: 0.8215 - val_accuracy: 0.5475\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.5819 - accuracy: 0.6646 - val_loss: 0.7416 - val_accuracy: 0.5294\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.5922 - accuracy: 0.6596 - val_loss: 0.9158 - val_accuracy: 0.5339\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.5718 - accuracy: 0.6707 - val_loss: 0.8248 - val_accuracy: 0.5294\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.5688 - accuracy: 0.6737 - val_loss: 0.7794 - val_accuracy: 0.5339\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.5715 - accuracy: 0.6697 - val_loss: 0.8778 - val_accuracy: 0.5113\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.5668 - accuracy: 0.6783 - val_loss: 0.8035 - val_accuracy: 0.5339\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.5507 - accuracy: 0.6854 - val_loss: 0.9086 - val_accuracy: 0.5158\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.5485 - accuracy: 0.6944 - val_loss: 0.8688 - val_accuracy: 0.5068\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.5475 - accuracy: 0.6955 - val_loss: 0.9437 - val_accuracy: 0.5339\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.5377 - accuracy: 0.6970 - val_loss: 0.8518 - val_accuracy: 0.5204\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.5305 - accuracy: 0.7040 - val_loss: 0.8561 - val_accuracy: 0.5068\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.5493 - accuracy: 0.6949 - val_loss: 0.9243 - val_accuracy: 0.5204\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.5276 - accuracy: 0.7071 - val_loss: 1.0260 - val_accuracy: 0.5158\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.5509 - accuracy: 0.6949 - val_loss: 0.9562 - val_accuracy: 0.5023\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.5399 - accuracy: 0.6980 - val_loss: 0.9098 - val_accuracy: 0.5113\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.5340 - accuracy: 0.7045 - val_loss: 0.8061 - val_accuracy: 0.5430\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.5670 - accuracy: 0.6818 - val_loss: 0.9257 - val_accuracy: 0.5204\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.5372 - accuracy: 0.7035 - val_loss: 0.9879 - val_accuracy: 0.5204\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.5062 - accuracy: 0.7222 - val_loss: 0.9348 - val_accuracy: 0.4706\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.5295 - accuracy: 0.7106 - val_loss: 0.9393 - val_accuracy: 0.5023\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.5162 - accuracy: 0.7136 - val_loss: 0.9685 - val_accuracy: 0.4842\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.5035 - accuracy: 0.7303 - val_loss: 1.0864 - val_accuracy: 0.4932\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.5050 - accuracy: 0.7247 - val_loss: 1.1414 - val_accuracy: 0.5068\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.5037 - accuracy: 0.7237 - val_loss: 1.0139 - val_accuracy: 0.5475\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.4914 - accuracy: 0.7323 - val_loss: 1.0695 - val_accuracy: 0.5294\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.4928 - accuracy: 0.7338 - val_loss: 1.0568 - val_accuracy: 0.4796\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.4862 - accuracy: 0.7465 - val_loss: 1.0269 - val_accuracy: 0.5430\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.4758 - accuracy: 0.7359 - val_loss: 1.0553 - val_accuracy: 0.5023\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.4785 - accuracy: 0.7419 - val_loss: 1.1401 - val_accuracy: 0.4977\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.4680 - accuracy: 0.7525 - val_loss: 1.0798 - val_accuracy: 0.5158\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.4598 - accuracy: 0.7616 - val_loss: 1.1044 - val_accuracy: 0.5068\n",
      "Epoch 204/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.4680 - accuracy: 0.7500 - val_loss: 1.1465 - val_accuracy: 0.4706\n",
      "Epoch 205/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.4619 - accuracy: 0.7520 - val_loss: 1.1311 - val_accuracy: 0.4932\n",
      "Epoch 206/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.4412 - accuracy: 0.7662 - val_loss: 1.2479 - val_accuracy: 0.4887\n",
      "Epoch 207/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.4518 - accuracy: 0.7626 - val_loss: 1.2333 - val_accuracy: 0.4706\n",
      "Epoch 208/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.4357 - accuracy: 0.7818 - val_loss: 1.1794 - val_accuracy: 0.4661\n",
      "Epoch 209/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.4220 - accuracy: 0.7712 - val_loss: 1.1630 - val_accuracy: 0.4887\n",
      "Epoch 210/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.4139 - accuracy: 0.7843 - val_loss: 1.3479 - val_accuracy: 0.5023\n",
      "Epoch 211/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.4127 - accuracy: 0.7889 - val_loss: 1.3554 - val_accuracy: 0.5023\n",
      "Epoch 212/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.4334 - accuracy: 0.7692 - val_loss: 1.1807 - val_accuracy: 0.5068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.4349 - accuracy: 0.7753 - val_loss: 1.1791 - val_accuracy: 0.5068\n",
      "Epoch 214/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.4307 - accuracy: 0.7798 - val_loss: 1.1653 - val_accuracy: 0.4661\n",
      "Epoch 215/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.3978 - accuracy: 0.7949 - val_loss: 1.4560 - val_accuracy: 0.4796\n",
      "Epoch 216/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.3941 - accuracy: 0.8051 - val_loss: 1.1842 - val_accuracy: 0.4932\n",
      "Epoch 217/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.4289 - accuracy: 0.7833 - val_loss: 1.3587 - val_accuracy: 0.4842\n",
      "Epoch 218/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 0.4018 - accuracy: 0.7965 - val_loss: 1.4714 - val_accuracy: 0.5068\n",
      "Epoch 219/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.3932 - accuracy: 0.8056 - val_loss: 1.3058 - val_accuracy: 0.4706\n",
      "Epoch 220/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.3725 - accuracy: 0.8081 - val_loss: 1.3536 - val_accuracy: 0.5023\n",
      "Epoch 221/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.3879 - accuracy: 0.8051 - val_loss: 1.4079 - val_accuracy: 0.4932\n",
      "Epoch 222/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.3650 - accuracy: 0.8237 - val_loss: 1.4431 - val_accuracy: 0.5023\n",
      "Epoch 223/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.3684 - accuracy: 0.8172 - val_loss: 1.4240 - val_accuracy: 0.5385\n",
      "Epoch 224/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.3484 - accuracy: 0.8288 - val_loss: 1.4990 - val_accuracy: 0.5068\n",
      "Epoch 225/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3207 - accuracy: 0.8404 - val_loss: 1.6055 - val_accuracy: 0.4842\n",
      "Epoch 226/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3215 - accuracy: 0.8424 - val_loss: 1.5997 - val_accuracy: 0.4661\n",
      "Epoch 227/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.3012 - accuracy: 0.8591 - val_loss: 1.6757 - val_accuracy: 0.4932\n",
      "Epoch 228/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3293 - accuracy: 0.8429 - val_loss: 1.5415 - val_accuracy: 0.5113\n",
      "Epoch 229/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.3235 - accuracy: 0.8424 - val_loss: 1.5605 - val_accuracy: 0.5158\n",
      "Epoch 230/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.3039 - accuracy: 0.8551 - val_loss: 1.5661 - val_accuracy: 0.4661\n",
      "Epoch 231/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3098 - accuracy: 0.8535 - val_loss: 1.6132 - val_accuracy: 0.5204\n",
      "Epoch 232/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.2802 - accuracy: 0.8742 - val_loss: 1.5962 - val_accuracy: 0.4977\n",
      "Epoch 233/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.2844 - accuracy: 0.8652 - val_loss: 1.6154 - val_accuracy: 0.4706\n",
      "Epoch 234/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.2628 - accuracy: 0.8788 - val_loss: 1.7689 - val_accuracy: 0.4842\n",
      "Epoch 235/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.2563 - accuracy: 0.8788 - val_loss: 1.7241 - val_accuracy: 0.5158\n",
      "Epoch 236/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.2877 - accuracy: 0.8646 - val_loss: 1.7852 - val_accuracy: 0.4842\n",
      "Epoch 237/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.2766 - accuracy: 0.8717 - val_loss: 1.9207 - val_accuracy: 0.4796\n",
      "Epoch 238/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.3201 - accuracy: 0.8485 - val_loss: 1.8651 - val_accuracy: 0.5249\n",
      "Epoch 239/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.2820 - accuracy: 0.8682 - val_loss: 1.8250 - val_accuracy: 0.4796\n",
      "Epoch 240/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.2558 - accuracy: 0.8818 - val_loss: 1.7709 - val_accuracy: 0.4570\n",
      "Epoch 241/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.2442 - accuracy: 0.8843 - val_loss: 1.8446 - val_accuracy: 0.5068\n",
      "Epoch 242/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.2497 - accuracy: 0.8889 - val_loss: 1.9171 - val_accuracy: 0.4570\n",
      "Epoch 243/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.2426 - accuracy: 0.8813 - val_loss: 1.8209 - val_accuracy: 0.4661\n",
      "Epoch 244/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.2414 - accuracy: 0.8874 - val_loss: 2.0298 - val_accuracy: 0.4932\n",
      "Epoch 245/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.2255 - accuracy: 0.9005 - val_loss: 2.0256 - val_accuracy: 0.4977\n",
      "Epoch 246/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.2060 - accuracy: 0.9101 - val_loss: 2.0060 - val_accuracy: 0.4706\n",
      "Epoch 247/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.2068 - accuracy: 0.9172 - val_loss: 2.0722 - val_accuracy: 0.4977\n",
      "Epoch 248/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.1832 - accuracy: 0.9232 - val_loss: 2.2849 - val_accuracy: 0.4842\n",
      "Epoch 249/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.1785 - accuracy: 0.9273 - val_loss: 2.2792 - val_accuracy: 0.5068\n",
      "Epoch 250/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1897 - accuracy: 0.9222 - val_loss: 2.1838 - val_accuracy: 0.4932\n",
      "Epoch 251/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.1736 - accuracy: 0.9328 - val_loss: 2.3409 - val_accuracy: 0.4480\n",
      "Epoch 252/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.1745 - accuracy: 0.9247 - val_loss: 2.3756 - val_accuracy: 0.4796\n",
      "Epoch 253/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.1626 - accuracy: 0.9323 - val_loss: 2.3362 - val_accuracy: 0.4842\n",
      "Epoch 254/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.1443 - accuracy: 0.9424 - val_loss: 2.4075 - val_accuracy: 0.4661\n",
      "Epoch 255/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.1455 - accuracy: 0.9515 - val_loss: 2.4165 - val_accuracy: 0.4570\n",
      "Epoch 256/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.1675 - accuracy: 0.9434 - val_loss: 2.4231 - val_accuracy: 0.4842\n",
      "Epoch 257/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.1517 - accuracy: 0.9465 - val_loss: 2.3081 - val_accuracy: 0.4977\n",
      "Epoch 258/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.1530 - accuracy: 0.9379 - val_loss: 2.4295 - val_accuracy: 0.4570\n",
      "Epoch 259/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1675 - accuracy: 0.9348 - val_loss: 2.2008 - val_accuracy: 0.4887\n",
      "Epoch 260/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.1765 - accuracy: 0.9308 - val_loss: 2.4498 - val_accuracy: 0.4661\n",
      "Epoch 261/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1268 - accuracy: 0.9515 - val_loss: 2.4247 - val_accuracy: 0.4887\n",
      "Epoch 262/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1098 - accuracy: 0.9586 - val_loss: 2.5812 - val_accuracy: 0.4751\n",
      "Epoch 263/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.1442 - accuracy: 0.9424 - val_loss: 2.5306 - val_accuracy: 0.4706\n",
      "Epoch 264/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.1371 - accuracy: 0.9455 - val_loss: 2.4742 - val_accuracy: 0.4887\n",
      "Epoch 265/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.1316 - accuracy: 0.9455 - val_loss: 2.6357 - val_accuracy: 0.4661\n",
      "Epoch 266/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.1078 - accuracy: 0.9530 - val_loss: 2.5635 - val_accuracy: 0.4751\n",
      "Epoch 267/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.1011 - accuracy: 0.9601 - val_loss: 2.7682 - val_accuracy: 0.4570\n",
      "Epoch 268/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0932 - accuracy: 0.9641 - val_loss: 2.7290 - val_accuracy: 0.4796\n",
      "Epoch 269/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0913 - accuracy: 0.9646 - val_loss: 2.6737 - val_accuracy: 0.4661\n",
      "Epoch 270/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0968 - accuracy: 0.9626 - val_loss: 2.7448 - val_accuracy: 0.4661\n",
      "Epoch 271/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0856 - accuracy: 0.9697 - val_loss: 2.7091 - val_accuracy: 0.4842\n",
      "Epoch 272/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0701 - accuracy: 0.9753 - val_loss: 2.6799 - val_accuracy: 0.4842\n",
      "Epoch 273/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0656 - accuracy: 0.9798 - val_loss: 2.8483 - val_accuracy: 0.4570\n",
      "Epoch 274/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0817 - accuracy: 0.9717 - val_loss: 2.8721 - val_accuracy: 0.4751\n",
      "Epoch 275/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0682 - accuracy: 0.9773 - val_loss: 2.7903 - val_accuracy: 0.5204\n",
      "Epoch 276/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0702 - accuracy: 0.9747 - val_loss: 2.8927 - val_accuracy: 0.4932\n",
      "Epoch 277/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0664 - accuracy: 0.9768 - val_loss: 2.9145 - val_accuracy: 0.4932\n",
      "Epoch 278/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1018 - accuracy: 0.9616 - val_loss: 2.6536 - val_accuracy: 0.4706\n",
      "Epoch 279/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0957 - accuracy: 0.9692 - val_loss: 2.7512 - val_accuracy: 0.4751\n",
      "Epoch 280/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0951 - accuracy: 0.9606 - val_loss: 3.0834 - val_accuracy: 0.4525\n",
      "Epoch 281/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0982 - accuracy: 0.9652 - val_loss: 2.8369 - val_accuracy: 0.4706\n",
      "Epoch 282/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0717 - accuracy: 0.9773 - val_loss: 2.7504 - val_accuracy: 0.4751\n",
      "Epoch 283/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0569 - accuracy: 0.9803 - val_loss: 2.9530 - val_accuracy: 0.4751\n",
      "Epoch 284/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.0471 - accuracy: 0.9879 - val_loss: 3.0728 - val_accuracy: 0.4751\n",
      "Epoch 285/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0291 - accuracy: 0.9944 - val_loss: 3.1230 - val_accuracy: 0.4887\n",
      "Epoch 286/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0269 - accuracy: 0.9960 - val_loss: 3.2263 - val_accuracy: 0.4661\n",
      "Epoch 287/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0259 - accuracy: 0.9939 - val_loss: 3.2484 - val_accuracy: 0.4615\n",
      "Epoch 288/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0203 - accuracy: 0.9970 - val_loss: 3.2468 - val_accuracy: 0.4525\n",
      "Epoch 289/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0162 - accuracy: 0.9990 - val_loss: 3.3308 - val_accuracy: 0.4661\n",
      "Epoch 290/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 3.3433 - val_accuracy: 0.4706\n",
      "Epoch 291/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 3.3626 - val_accuracy: 0.4615\n",
      "Epoch 292/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0102 - accuracy: 0.9995 - val_loss: 3.3623 - val_accuracy: 0.4706\n",
      "Epoch 293/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 3.4123 - val_accuracy: 0.4570\n",
      "Epoch 294/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 3.4411 - val_accuracy: 0.4570\n",
      "Epoch 295/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 3.4239 - val_accuracy: 0.4570\n",
      "Epoch 296/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.4839 - val_accuracy: 0.4661\n",
      "Epoch 297/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.4709 - val_accuracy: 0.4661\n",
      "Epoch 298/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.4985 - val_accuracy: 0.4615\n",
      "Epoch 299/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.5216 - val_accuracy: 0.4796\n",
      "Epoch 300/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.5300 - val_accuracy: 0.4842\n",
      "Epoch 301/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.5706 - val_accuracy: 0.4615\n",
      "Epoch 302/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 3.5814 - val_accuracy: 0.4706\n",
      "Epoch 303/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.6196 - val_accuracy: 0.4615\n",
      "Epoch 304/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.6715 - val_accuracy: 0.4796\n",
      "Epoch 305/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.6693 - val_accuracy: 0.4706\n",
      "Epoch 306/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.7114 - val_accuracy: 0.4751\n",
      "Epoch 307/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.6978 - val_accuracy: 0.4796\n",
      "Epoch 308/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.7153 - val_accuracy: 0.4887\n",
      "Epoch 309/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 3.7284 - val_accuracy: 0.4796\n",
      "Epoch 310/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.7397 - val_accuracy: 0.4842\n",
      "Epoch 311/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.7557 - val_accuracy: 0.4706\n",
      "Epoch 312/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.7595 - val_accuracy: 0.4842\n",
      "Epoch 313/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.7680 - val_accuracy: 0.4706\n",
      "Epoch 314/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.7822 - val_accuracy: 0.4796\n",
      "Epoch 315/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.7927 - val_accuracy: 0.4796\n",
      "Epoch 316/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.8102 - val_accuracy: 0.4887\n",
      "Epoch 317/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.8249 - val_accuracy: 0.4842\n",
      "Epoch 318/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.8351 - val_accuracy: 0.4842\n",
      "Epoch 319/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.8525 - val_accuracy: 0.4796\n",
      "Epoch 320/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.8735 - val_accuracy: 0.4796\n",
      "Epoch 321/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.8647 - val_accuracy: 0.4887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.8708 - val_accuracy: 0.4796\n",
      "Epoch 323/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.9007 - val_accuracy: 0.4887\n",
      "Epoch 324/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9037 - val_accuracy: 0.4751\n",
      "Epoch 325/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.9148 - val_accuracy: 0.4796\n",
      "Epoch 326/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.9239 - val_accuracy: 0.4751\n",
      "Epoch 327/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9300 - val_accuracy: 0.4796\n",
      "Epoch 328/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.9555 - val_accuracy: 0.4932\n",
      "Epoch 329/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.9763 - val_accuracy: 0.4887\n",
      "Epoch 330/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9782 - val_accuracy: 0.4842\n",
      "Epoch 331/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9832 - val_accuracy: 0.4887\n",
      "Epoch 332/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.9883 - val_accuracy: 0.4887\n",
      "Epoch 333/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9980 - val_accuracy: 0.4932\n",
      "Epoch 334/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.0158 - val_accuracy: 0.4842\n",
      "Epoch 335/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.0416 - val_accuracy: 0.4796\n",
      "Epoch 336/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.0374 - val_accuracy: 0.4796\n",
      "Epoch 337/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0412 - val_accuracy: 0.4887\n",
      "Epoch 338/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0558 - val_accuracy: 0.4887\n",
      "Epoch 339/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0592 - val_accuracy: 0.4842\n",
      "Epoch 340/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0775 - val_accuracy: 0.4932\n",
      "Epoch 341/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0770 - val_accuracy: 0.4796\n",
      "Epoch 342/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0863 - val_accuracy: 0.4887\n",
      "Epoch 343/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0934 - val_accuracy: 0.4887\n",
      "Epoch 344/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0962 - val_accuracy: 0.4842\n",
      "Epoch 345/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1058 - val_accuracy: 0.4842\n",
      "Epoch 346/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1062 - val_accuracy: 0.4796\n",
      "Epoch 347/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1196 - val_accuracy: 0.4706\n",
      "Epoch 348/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1104 - val_accuracy: 0.4842\n",
      "Epoch 349/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1377 - val_accuracy: 0.4706\n",
      "Epoch 350/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1439 - val_accuracy: 0.4751\n",
      "Epoch 351/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1412 - val_accuracy: 0.4751\n",
      "Epoch 352/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1557 - val_accuracy: 0.4751\n",
      "Epoch 353/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1626 - val_accuracy: 0.4796\n",
      "Epoch 354/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1880 - val_accuracy: 0.4796\n",
      "Epoch 355/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.1989 - val_accuracy: 0.4842\n",
      "Epoch 356/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.2086 - val_accuracy: 0.4842\n",
      "Epoch 357/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 9.3435e-04 - accuracy: 1.0000 - val_loss: 4.2074 - val_accuracy: 0.4842\n",
      "Epoch 358/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2144 - val_accuracy: 0.4842\n",
      "Epoch 359/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 9.4734e-04 - accuracy: 1.0000 - val_loss: 4.2276 - val_accuracy: 0.4796\n",
      "Epoch 360/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2318 - val_accuracy: 0.4796\n",
      "Epoch 361/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2370 - val_accuracy: 0.4796\n",
      "Epoch 362/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 9.3501e-04 - accuracy: 1.0000 - val_loss: 4.2563 - val_accuracy: 0.4796\n",
      "Epoch 363/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 9.7691e-04 - accuracy: 1.0000 - val_loss: 4.2730 - val_accuracy: 0.4796\n",
      "Epoch 364/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 9.5502e-04 - accuracy: 1.0000 - val_loss: 4.2798 - val_accuracy: 0.4842\n",
      "Epoch 365/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 9.8723e-04 - accuracy: 1.0000 - val_loss: 4.2814 - val_accuracy: 0.4796\n",
      "Epoch 366/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 9.7171e-04 - accuracy: 1.0000 - val_loss: 4.2810 - val_accuracy: 0.4796\n",
      "Epoch 367/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 7.9481e-04 - accuracy: 1.0000 - val_loss: 4.2977 - val_accuracy: 0.4842\n",
      "Epoch 368/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 9.3227e-04 - accuracy: 1.0000 - val_loss: 4.3072 - val_accuracy: 0.4842\n",
      "Epoch 369/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 8.4749e-04 - accuracy: 1.0000 - val_loss: 4.3195 - val_accuracy: 0.4887\n",
      "Epoch 370/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 8.5437e-04 - accuracy: 1.0000 - val_loss: 4.3144 - val_accuracy: 0.4887\n",
      "Epoch 371/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.2968 - val_accuracy: 0.4887\n",
      "Epoch 372/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.2920 - val_accuracy: 0.4751\n",
      "Epoch 373/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.1282 - accuracy: 0.9707 - val_loss: 3.6770 - val_accuracy: 0.4434\n",
      "Epoch 374/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.0400 - accuracy: 0.6051 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 375/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.7078 - accuracy: 0.5035 - val_loss: 0.6859 - val_accuracy: 0.5611\n",
      "Epoch 376/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6976 - accuracy: 0.5263 - val_loss: 0.6856 - val_accuracy: 0.5792\n",
      "Epoch 377/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6927 - accuracy: 0.5384 - val_loss: 0.6898 - val_accuracy: 0.5611\n",
      "Epoch 378/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6923 - accuracy: 0.5247 - val_loss: 0.6860 - val_accuracy: 0.5611\n",
      "Epoch 379/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6936 - accuracy: 0.5283 - val_loss: 0.7080 - val_accuracy: 0.4615\n",
      "Epoch 380/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6870 - accuracy: 0.5515 - val_loss: 0.6925 - val_accuracy: 0.5294\n",
      "Epoch 381/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6854 - accuracy: 0.5389 - val_loss: 0.6911 - val_accuracy: 0.5701\n",
      "Epoch 382/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.6830 - accuracy: 0.5510 - val_loss: 0.7036 - val_accuracy: 0.4751\n",
      "Epoch 383/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6793 - accuracy: 0.5646 - val_loss: 0.7022 - val_accuracy: 0.5339\n",
      "Epoch 384/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6749 - accuracy: 0.5712 - val_loss: 0.7046 - val_accuracy: 0.5249\n",
      "Epoch 385/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.6708 - accuracy: 0.5692 - val_loss: 0.7031 - val_accuracy: 0.5249\n",
      "Epoch 386/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.6619 - accuracy: 0.5914 - val_loss: 0.7359 - val_accuracy: 0.4751\n",
      "Epoch 387/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.6573 - accuracy: 0.6015 - val_loss: 0.7264 - val_accuracy: 0.5023\n",
      "Epoch 388/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.6513 - accuracy: 0.6192 - val_loss: 0.7390 - val_accuracy: 0.5204\n",
      "Epoch 389/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.6382 - accuracy: 0.6313 - val_loss: 0.7701 - val_accuracy: 0.4977\n",
      "Epoch 390/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.6343 - accuracy: 0.6258 - val_loss: 0.7539 - val_accuracy: 0.4977\n",
      "Epoch 391/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6274 - accuracy: 0.6374 - val_loss: 0.8101 - val_accuracy: 0.5113\n",
      "Epoch 392/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.6213 - accuracy: 0.6409 - val_loss: 0.7914 - val_accuracy: 0.5068\n",
      "Epoch 393/1000\n",
      "1980/1980 [==============================] - 1s 441us/step - loss: 0.6016 - accuracy: 0.6530 - val_loss: 0.7806 - val_accuracy: 0.4932\n",
      "Epoch 394/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.5979 - accuracy: 0.6586 - val_loss: 0.8635 - val_accuracy: 0.5068\n",
      "Epoch 395/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6033 - accuracy: 0.6510 - val_loss: 0.7534 - val_accuracy: 0.5566\n",
      "Epoch 396/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.5919 - accuracy: 0.6591 - val_loss: 0.8127 - val_accuracy: 0.5204\n",
      "Epoch 397/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.5755 - accuracy: 0.6758 - val_loss: 0.7930 - val_accuracy: 0.5068\n",
      "Epoch 398/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5629 - accuracy: 0.6763 - val_loss: 0.8778 - val_accuracy: 0.5068\n",
      "Epoch 399/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.5642 - accuracy: 0.6788 - val_loss: 0.7733 - val_accuracy: 0.5068\n",
      "Epoch 400/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.5546 - accuracy: 0.6914 - val_loss: 0.8905 - val_accuracy: 0.4887\n",
      "Epoch 401/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.5516 - accuracy: 0.6904 - val_loss: 0.9124 - val_accuracy: 0.5294\n",
      "Epoch 402/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.5404 - accuracy: 0.7051 - val_loss: 0.8885 - val_accuracy: 0.5520\n",
      "Epoch 403/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.5127 - accuracy: 0.7187 - val_loss: 0.9915 - val_accuracy: 0.5113\n",
      "Epoch 404/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.4927 - accuracy: 0.7303 - val_loss: 1.0675 - val_accuracy: 0.4796\n",
      "Epoch 405/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.4890 - accuracy: 0.7278 - val_loss: 1.0866 - val_accuracy: 0.5249\n",
      "Epoch 406/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.4776 - accuracy: 0.7460 - val_loss: 1.0686 - val_accuracy: 0.4706\n",
      "Epoch 407/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.4711 - accuracy: 0.7374 - val_loss: 1.0387 - val_accuracy: 0.4932\n",
      "Epoch 408/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.4893 - accuracy: 0.7328 - val_loss: 1.0969 - val_accuracy: 0.4977\n",
      "Epoch 409/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.4603 - accuracy: 0.7591 - val_loss: 1.1018 - val_accuracy: 0.4887\n",
      "Epoch 410/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.4319 - accuracy: 0.7712 - val_loss: 1.1496 - val_accuracy: 0.5158\n",
      "Epoch 411/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.4327 - accuracy: 0.7682 - val_loss: 1.1220 - val_accuracy: 0.5204\n",
      "Epoch 412/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.4404 - accuracy: 0.7707 - val_loss: 1.2588 - val_accuracy: 0.5113\n",
      "Epoch 413/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.4106 - accuracy: 0.7854 - val_loss: 1.3428 - val_accuracy: 0.5023\n",
      "Epoch 414/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.3990 - accuracy: 0.7929 - val_loss: 1.3067 - val_accuracy: 0.5249\n",
      "Epoch 415/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.4093 - accuracy: 0.7939 - val_loss: 1.2103 - val_accuracy: 0.5158\n",
      "Epoch 416/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.4263 - accuracy: 0.7904 - val_loss: 1.2416 - val_accuracy: 0.5385\n",
      "Epoch 417/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.3896 - accuracy: 0.7995 - val_loss: 1.3747 - val_accuracy: 0.5294\n",
      "Epoch 418/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.3670 - accuracy: 0.8172 - val_loss: 1.2783 - val_accuracy: 0.4842\n",
      "Epoch 419/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.3655 - accuracy: 0.8177 - val_loss: 1.4161 - val_accuracy: 0.5113\n",
      "Epoch 420/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 0.3459 - accuracy: 0.8338 - val_loss: 1.4525 - val_accuracy: 0.5520\n",
      "Epoch 421/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.3257 - accuracy: 0.8384 - val_loss: 1.4292 - val_accuracy: 0.5068\n",
      "Epoch 422/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.3104 - accuracy: 0.8399 - val_loss: 1.5200 - val_accuracy: 0.5204\n",
      "Epoch 423/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.3057 - accuracy: 0.8576 - val_loss: 1.5869 - val_accuracy: 0.5113\n",
      "Epoch 424/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.3377 - accuracy: 0.8343 - val_loss: 1.4176 - val_accuracy: 0.5158\n",
      "Epoch 425/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.3042 - accuracy: 0.8586 - val_loss: 1.6403 - val_accuracy: 0.5294\n",
      "Epoch 426/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.2674 - accuracy: 0.8798 - val_loss: 1.7270 - val_accuracy: 0.5294\n",
      "Epoch 427/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.2461 - accuracy: 0.8889 - val_loss: 1.6805 - val_accuracy: 0.5294\n",
      "Epoch 428/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.2440 - accuracy: 0.8833 - val_loss: 1.6763 - val_accuracy: 0.5385\n",
      "Epoch 429/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.2584 - accuracy: 0.8884 - val_loss: 1.7708 - val_accuracy: 0.5520\n",
      "Epoch 430/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.2285 - accuracy: 0.8919 - val_loss: 1.7861 - val_accuracy: 0.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 431/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.2262 - accuracy: 0.8965 - val_loss: 1.8393 - val_accuracy: 0.5385\n",
      "Epoch 432/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.2841 - accuracy: 0.8854 - val_loss: 1.7159 - val_accuracy: 0.5294\n",
      "Epoch 433/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.2550 - accuracy: 0.8818 - val_loss: 1.8005 - val_accuracy: 0.5385\n",
      "Epoch 434/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.2113 - accuracy: 0.9111 - val_loss: 1.6604 - val_accuracy: 0.5294\n",
      "Epoch 435/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.2215 - accuracy: 0.9040 - val_loss: 1.8719 - val_accuracy: 0.4887\n",
      "Epoch 436/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.2057 - accuracy: 0.9157 - val_loss: 1.8339 - val_accuracy: 0.5294\n",
      "Epoch 437/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.1659 - accuracy: 0.9268 - val_loss: 2.0721 - val_accuracy: 0.5385\n",
      "Epoch 438/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.1520 - accuracy: 0.9364 - val_loss: 1.9232 - val_accuracy: 0.5747\n",
      "Epoch 439/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.1636 - accuracy: 0.9313 - val_loss: 1.8913 - val_accuracy: 0.5566\n",
      "Epoch 440/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.1740 - accuracy: 0.9268 - val_loss: 2.0808 - val_accuracy: 0.5701\n",
      "Epoch 441/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.1899 - accuracy: 0.9182 - val_loss: 2.0017 - val_accuracy: 0.5339\n",
      "Epoch 442/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.1424 - accuracy: 0.9475 - val_loss: 2.0816 - val_accuracy: 0.5566\n",
      "Epoch 443/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.1330 - accuracy: 0.9465 - val_loss: 2.1603 - val_accuracy: 0.5701\n",
      "Epoch 444/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.1310 - accuracy: 0.9500 - val_loss: 2.1832 - val_accuracy: 0.5339\n",
      "Epoch 445/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.1270 - accuracy: 0.9500 - val_loss: 2.0888 - val_accuracy: 0.5430\n",
      "Epoch 446/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.1359 - accuracy: 0.9490 - val_loss: 2.2531 - val_accuracy: 0.5339\n",
      "Epoch 447/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.1337 - accuracy: 0.9490 - val_loss: 2.2140 - val_accuracy: 0.5520\n",
      "Epoch 448/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.1324 - accuracy: 0.9389 - val_loss: 2.0656 - val_accuracy: 0.5430\n",
      "Epoch 449/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.1458 - accuracy: 0.9409 - val_loss: 2.2332 - val_accuracy: 0.5113\n",
      "Epoch 450/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.1312 - accuracy: 0.9465 - val_loss: 2.1783 - val_accuracy: 0.5475\n",
      "Epoch 451/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.1314 - accuracy: 0.9485 - val_loss: 2.3069 - val_accuracy: 0.5520\n",
      "Epoch 452/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.1026 - accuracy: 0.9621 - val_loss: 2.3196 - val_accuracy: 0.5113\n",
      "Epoch 453/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0754 - accuracy: 0.9758 - val_loss: 2.4212 - val_accuracy: 0.5430\n",
      "Epoch 454/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0550 - accuracy: 0.9864 - val_loss: 2.3959 - val_accuracy: 0.5566\n",
      "Epoch 455/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0554 - accuracy: 0.9808 - val_loss: 2.5558 - val_accuracy: 0.5385\n",
      "Epoch 456/1000\n",
      "1980/1980 [==============================] - 1s 497us/step - loss: 0.0643 - accuracy: 0.9763 - val_loss: 2.4149 - val_accuracy: 0.5430\n",
      "Epoch 457/1000\n",
      "1980/1980 [==============================] - 1s 486us/step - loss: 0.0519 - accuracy: 0.9843 - val_loss: 2.5913 - val_accuracy: 0.5385\n",
      "Epoch 458/1000\n",
      "1980/1980 [==============================] - 1s 498us/step - loss: 0.0396 - accuracy: 0.9899 - val_loss: 2.6958 - val_accuracy: 0.5339\n",
      "Epoch 459/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.0368 - accuracy: 0.9904 - val_loss: 2.6907 - val_accuracy: 0.5430\n",
      "Epoch 460/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0302 - accuracy: 0.9914 - val_loss: 2.8217 - val_accuracy: 0.5520\n",
      "Epoch 461/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 2.5597 - val_accuracy: 0.5520\n",
      "Epoch 462/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.1023 - accuracy: 0.9611 - val_loss: 2.8182 - val_accuracy: 0.5068\n",
      "Epoch 463/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.1416 - accuracy: 0.9495 - val_loss: 2.4926 - val_accuracy: 0.5520\n",
      "Epoch 464/1000\n",
      "1980/1980 [==============================] - 1s 513us/step - loss: 0.1768 - accuracy: 0.9384 - val_loss: 2.6012 - val_accuracy: 0.5204\n",
      "Epoch 465/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.1779 - accuracy: 0.9293 - val_loss: 2.3170 - val_accuracy: 0.5475\n",
      "Epoch 466/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.1334 - accuracy: 0.9520 - val_loss: 2.3373 - val_accuracy: 0.5339\n",
      "Epoch 467/1000\n",
      "1980/1980 [==============================] - 1s 432us/step - loss: 0.1048 - accuracy: 0.9682 - val_loss: 2.3431 - val_accuracy: 0.5792\n",
      "Epoch 468/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0556 - accuracy: 0.9823 - val_loss: 2.2662 - val_accuracy: 0.5747\n",
      "Epoch 469/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.0393 - accuracy: 0.9884 - val_loss: 2.5231 - val_accuracy: 0.5430\n",
      "Epoch 470/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.0210 - accuracy: 0.9975 - val_loss: 2.5927 - val_accuracy: 0.5475\n",
      "Epoch 471/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.0151 - accuracy: 0.9990 - val_loss: 2.6985 - val_accuracy: 0.5611\n",
      "Epoch 472/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0124 - accuracy: 0.9995 - val_loss: 2.7591 - val_accuracy: 0.5611\n",
      "Epoch 473/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.6983 - val_accuracy: 0.5520\n",
      "Epoch 474/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 2.8112 - val_accuracy: 0.5611\n",
      "Epoch 475/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 2.8469 - val_accuracy: 0.5611\n",
      "Epoch 476/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.8797 - val_accuracy: 0.5566\n",
      "Epoch 477/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.8811 - val_accuracy: 0.5566\n",
      "Epoch 478/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.9202 - val_accuracy: 0.5656\n",
      "Epoch 479/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.9195 - val_accuracy: 0.5701\n",
      "Epoch 480/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.9699 - val_accuracy: 0.5566\n",
      "Epoch 481/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.9753 - val_accuracy: 0.5611\n",
      "Epoch 482/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.0209 - val_accuracy: 0.5656\n",
      "Epoch 483/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.0156 - val_accuracy: 0.5656\n",
      "Epoch 484/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.0459 - val_accuracy: 0.5656\n",
      "Epoch 485/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.0523 - val_accuracy: 0.5656\n",
      "Epoch 486/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.0951 - val_accuracy: 0.5566\n",
      "Epoch 487/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.1046 - val_accuracy: 0.5701\n",
      "Epoch 488/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.1105 - val_accuracy: 0.5566\n",
      "Epoch 489/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.1295 - val_accuracy: 0.5520\n",
      "Epoch 490/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.1398 - val_accuracy: 0.5611\n",
      "Epoch 491/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 3.1743 - val_accuracy: 0.5566\n",
      "Epoch 492/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.1725 - val_accuracy: 0.5566\n",
      "Epoch 493/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.2012 - val_accuracy: 0.5701\n",
      "Epoch 494/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.2017 - val_accuracy: 0.5701\n",
      "Epoch 495/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.2063 - val_accuracy: 0.5701\n",
      "Epoch 496/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.2086 - val_accuracy: 0.5701\n",
      "Epoch 497/1000\n",
      "1980/1980 [==============================] - 1s 485us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.2211 - val_accuracy: 0.5701\n",
      "Epoch 498/1000\n",
      "1980/1980 [==============================] - 1s 486us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.2448 - val_accuracy: 0.5656\n",
      "Epoch 499/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.2481 - val_accuracy: 0.5520\n",
      "Epoch 500/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.2721 - val_accuracy: 0.5611\n",
      "Epoch 501/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.3002 - val_accuracy: 0.5566\n",
      "Epoch 502/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.2863 - val_accuracy: 0.5701\n",
      "Epoch 503/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.3273 - val_accuracy: 0.5566\n",
      "Epoch 504/1000\n",
      "1980/1980 [==============================] - 1s 504us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.3226 - val_accuracy: 0.5656\n",
      "Epoch 505/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.3143 - val_accuracy: 0.5701\n",
      "Epoch 506/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.3555 - val_accuracy: 0.5520\n",
      "Epoch 507/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.3675 - val_accuracy: 0.5475\n",
      "Epoch 508/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.3579 - val_accuracy: 0.5701\n",
      "Epoch 509/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.3864 - val_accuracy: 0.5611\n",
      "Epoch 510/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.3934 - val_accuracy: 0.5611\n",
      "Epoch 511/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.4010 - val_accuracy: 0.5656\n",
      "Epoch 512/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.4026 - val_accuracy: 0.5520\n",
      "Epoch 513/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.4162 - val_accuracy: 0.5566\n",
      "Epoch 514/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.4152 - val_accuracy: 0.5656\n",
      "Epoch 515/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.4376 - val_accuracy: 0.5566\n",
      "Epoch 516/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.4433 - val_accuracy: 0.5520\n",
      "Epoch 517/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.4389 - val_accuracy: 0.5566\n",
      "Epoch 518/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.4381 - val_accuracy: 0.5611\n",
      "Epoch 519/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.4617 - val_accuracy: 0.5566\n",
      "Epoch 520/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 9.6899e-04 - accuracy: 1.0000 - val_loss: 3.4638 - val_accuracy: 0.5566\n",
      "Epoch 521/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.4746 - val_accuracy: 0.5566\n",
      "Epoch 522/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 8.9129e-04 - accuracy: 1.0000 - val_loss: 3.4772 - val_accuracy: 0.5747\n",
      "Epoch 523/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 9.0472e-04 - accuracy: 1.0000 - val_loss: 3.4871 - val_accuracy: 0.5656\n",
      "Epoch 524/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 8.8350e-04 - accuracy: 1.0000 - val_loss: 3.5090 - val_accuracy: 0.5566\n",
      "Epoch 525/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 9.7238e-04 - accuracy: 1.0000 - val_loss: 3.5156 - val_accuracy: 0.5611\n",
      "Epoch 526/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 9.2359e-04 - accuracy: 1.0000 - val_loss: 3.5316 - val_accuracy: 0.5656\n",
      "Epoch 527/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.7929e-04 - accuracy: 1.0000 - val_loss: 3.5414 - val_accuracy: 0.5656\n",
      "Epoch 528/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 8.9534e-04 - accuracy: 1.0000 - val_loss: 3.5244 - val_accuracy: 0.5611\n",
      "Epoch 529/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.9900e-04 - accuracy: 1.0000 - val_loss: 3.5477 - val_accuracy: 0.5611\n",
      "Epoch 530/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 8.4353e-04 - accuracy: 1.0000 - val_loss: 3.5476 - val_accuracy: 0.5656\n",
      "Epoch 531/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 9.0561e-04 - accuracy: 1.0000 - val_loss: 3.5350 - val_accuracy: 0.5611\n",
      "Epoch 532/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 8.0316e-04 - accuracy: 1.0000 - val_loss: 3.5641 - val_accuracy: 0.5656\n",
      "Epoch 533/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 7.4747e-04 - accuracy: 1.0000 - val_loss: 3.5830 - val_accuracy: 0.5611\n",
      "Epoch 534/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 7.1561e-04 - accuracy: 1.0000 - val_loss: 3.5875 - val_accuracy: 0.5475\n",
      "Epoch 535/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 7.1273e-04 - accuracy: 1.0000 - val_loss: 3.5944 - val_accuracy: 0.5475\n",
      "Epoch 536/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 7.3225e-04 - accuracy: 1.0000 - val_loss: 3.6027 - val_accuracy: 0.5701\n",
      "Epoch 537/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 8.4897e-04 - accuracy: 1.0000 - val_loss: 3.6305 - val_accuracy: 0.5566\n",
      "Epoch 538/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 7.2579e-04 - accuracy: 1.0000 - val_loss: 3.6507 - val_accuracy: 0.5656\n",
      "Epoch 539/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 372us/step - loss: 6.9389e-04 - accuracy: 1.0000 - val_loss: 3.6063 - val_accuracy: 0.5656\n",
      "Epoch 540/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 7.1348e-04 - accuracy: 1.0000 - val_loss: 3.6280 - val_accuracy: 0.5566\n",
      "Epoch 541/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 7.3702e-04 - accuracy: 1.0000 - val_loss: 3.6243 - val_accuracy: 0.5566\n",
      "Epoch 542/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 6.4612e-04 - accuracy: 1.0000 - val_loss: 3.6424 - val_accuracy: 0.5520\n",
      "Epoch 543/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 7.2025e-04 - accuracy: 1.0000 - val_loss: 3.6424 - val_accuracy: 0.5475\n",
      "Epoch 544/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 6.7337e-04 - accuracy: 1.0000 - val_loss: 3.6334 - val_accuracy: 0.5611\n",
      "Epoch 545/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 6.1391e-04 - accuracy: 1.0000 - val_loss: 3.6397 - val_accuracy: 0.5566\n",
      "Epoch 546/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 6.6662e-04 - accuracy: 1.0000 - val_loss: 3.6498 - val_accuracy: 0.5792\n",
      "Epoch 547/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 6.5030e-04 - accuracy: 1.0000 - val_loss: 3.6431 - val_accuracy: 0.5701\n",
      "Epoch 548/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 5.6724e-04 - accuracy: 1.0000 - val_loss: 3.6644 - val_accuracy: 0.5475\n",
      "Epoch 549/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 5.7349e-04 - accuracy: 1.0000 - val_loss: 3.6714 - val_accuracy: 0.5430\n",
      "Epoch 550/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 6.0620e-04 - accuracy: 1.0000 - val_loss: 3.6925 - val_accuracy: 0.5475\n",
      "Epoch 551/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 5.4876e-04 - accuracy: 1.0000 - val_loss: 3.6837 - val_accuracy: 0.5475\n",
      "Epoch 552/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 4.8904e-04 - accuracy: 1.0000 - val_loss: 3.6864 - val_accuracy: 0.5520\n",
      "Epoch 553/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.3021e-04 - accuracy: 1.0000 - val_loss: 3.6871 - val_accuracy: 0.5520\n",
      "Epoch 554/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 5.1688e-04 - accuracy: 1.0000 - val_loss: 3.6887 - val_accuracy: 0.5520\n",
      "Epoch 555/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 4.9361e-04 - accuracy: 1.0000 - val_loss: 3.7076 - val_accuracy: 0.5520\n",
      "Epoch 556/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 5.1398e-04 - accuracy: 1.0000 - val_loss: 3.7235 - val_accuracy: 0.5611\n",
      "Epoch 557/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 4.5972e-04 - accuracy: 1.0000 - val_loss: 3.7154 - val_accuracy: 0.5656\n",
      "Epoch 558/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 4.2474e-04 - accuracy: 1.0000 - val_loss: 3.7258 - val_accuracy: 0.5566\n",
      "Epoch 559/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 4.9834e-04 - accuracy: 1.0000 - val_loss: 3.7432 - val_accuracy: 0.5475\n",
      "Epoch 560/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 4.4751e-04 - accuracy: 1.0000 - val_loss: 3.7259 - val_accuracy: 0.5475\n",
      "Epoch 561/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 4.4149e-04 - accuracy: 1.0000 - val_loss: 3.7336 - val_accuracy: 0.5520\n",
      "Epoch 562/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 4.8053e-04 - accuracy: 1.0000 - val_loss: 3.7438 - val_accuracy: 0.5520\n",
      "Epoch 563/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 4.2179e-04 - accuracy: 1.0000 - val_loss: 3.7425 - val_accuracy: 0.5475\n",
      "Epoch 564/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 4.4718e-04 - accuracy: 1.0000 - val_loss: 3.7551 - val_accuracy: 0.5566\n",
      "Epoch 565/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 4.4735e-04 - accuracy: 1.0000 - val_loss: 3.7694 - val_accuracy: 0.5566\n",
      "Epoch 566/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 4.4327e-04 - accuracy: 1.0000 - val_loss: 3.7819 - val_accuracy: 0.5430\n",
      "Epoch 567/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 4.4524e-04 - accuracy: 1.0000 - val_loss: 3.7737 - val_accuracy: 0.5475\n",
      "Epoch 568/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 4.1878e-04 - accuracy: 1.0000 - val_loss: 3.7791 - val_accuracy: 0.5520\n",
      "Epoch 569/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 4.0771e-04 - accuracy: 1.0000 - val_loss: 3.7721 - val_accuracy: 0.5475\n",
      "Epoch 570/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 3.3780e-04 - accuracy: 1.0000 - val_loss: 3.7834 - val_accuracy: 0.5566\n",
      "Epoch 571/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 3.5565e-04 - accuracy: 1.0000 - val_loss: 3.7982 - val_accuracy: 0.5430\n",
      "Epoch 572/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 3.6610e-04 - accuracy: 1.0000 - val_loss: 3.8107 - val_accuracy: 0.5339\n",
      "Epoch 573/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 3.3125e-04 - accuracy: 1.0000 - val_loss: 3.8233 - val_accuracy: 0.5339\n",
      "Epoch 574/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 3.5955e-04 - accuracy: 1.0000 - val_loss: 3.8316 - val_accuracy: 0.5430\n",
      "Epoch 575/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 3.1650e-04 - accuracy: 1.0000 - val_loss: 3.8343 - val_accuracy: 0.5385\n",
      "Epoch 576/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 3.7298e-04 - accuracy: 1.0000 - val_loss: 3.8489 - val_accuracy: 0.5339\n",
      "Epoch 577/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.7094e-04 - accuracy: 1.0000 - val_loss: 3.8403 - val_accuracy: 0.5339\n",
      "Epoch 578/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 3.0690e-04 - accuracy: 1.0000 - val_loss: 3.8233 - val_accuracy: 0.5385\n",
      "Epoch 579/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 3.4498e-04 - accuracy: 1.0000 - val_loss: 3.8223 - val_accuracy: 0.5566\n",
      "Epoch 580/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.2493e-04 - accuracy: 1.0000 - val_loss: 3.8614 - val_accuracy: 0.5385\n",
      "Epoch 581/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 3.0436e-04 - accuracy: 1.0000 - val_loss: 3.8721 - val_accuracy: 0.5294\n",
      "Epoch 582/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 2.6511e-04 - accuracy: 1.0000 - val_loss: 3.8592 - val_accuracy: 0.5430\n",
      "Epoch 583/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.8316e-04 - accuracy: 1.0000 - val_loss: 3.8639 - val_accuracy: 0.5430\n",
      "Epoch 584/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 2.7156e-04 - accuracy: 1.0000 - val_loss: 3.8741 - val_accuracy: 0.5385\n",
      "Epoch 585/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 3.0165e-04 - accuracy: 1.0000 - val_loss: 3.8781 - val_accuracy: 0.5385\n",
      "Epoch 586/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 2.7347e-04 - accuracy: 1.0000 - val_loss: 3.8876 - val_accuracy: 0.5430\n",
      "Epoch 587/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.7492e-04 - accuracy: 1.0000 - val_loss: 3.9042 - val_accuracy: 0.5385\n",
      "Epoch 588/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.7199e-04 - accuracy: 1.0000 - val_loss: 3.8988 - val_accuracy: 0.5385\n",
      "Epoch 589/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 2.3994e-04 - accuracy: 1.0000 - val_loss: 3.9023 - val_accuracy: 0.5475\n",
      "Epoch 590/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 2.7056e-04 - accuracy: 1.0000 - val_loss: 3.9100 - val_accuracy: 0.5430\n",
      "Epoch 591/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 3.1918e-04 - accuracy: 1.0000 - val_loss: 3.9103 - val_accuracy: 0.5385\n",
      "Epoch 592/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.9863e-04 - accuracy: 1.0000 - val_loss: 3.9116 - val_accuracy: 0.5430\n",
      "Epoch 593/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.6932e-04 - accuracy: 1.0000 - val_loss: 3.9241 - val_accuracy: 0.5339\n",
      "Epoch 594/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 2.5843e-04 - accuracy: 1.0000 - val_loss: 3.9182 - val_accuracy: 0.5339\n",
      "Epoch 595/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 2.4285e-04 - accuracy: 1.0000 - val_loss: 3.9244 - val_accuracy: 0.5430\n",
      "Epoch 596/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 2.4648e-04 - accuracy: 1.0000 - val_loss: 3.9365 - val_accuracy: 0.5249\n",
      "Epoch 597/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.5195e-04 - accuracy: 1.0000 - val_loss: 3.9557 - val_accuracy: 0.5339\n",
      "Epoch 598/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 2.6328e-04 - accuracy: 1.0000 - val_loss: 3.9223 - val_accuracy: 0.5430\n",
      "Epoch 599/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 2.6293e-04 - accuracy: 1.0000 - val_loss: 3.9850 - val_accuracy: 0.5294\n",
      "Epoch 600/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 2.6509e-04 - accuracy: 1.0000 - val_loss: 3.9585 - val_accuracy: 0.5339\n",
      "Epoch 601/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 2.4676e-04 - accuracy: 1.0000 - val_loss: 3.9580 - val_accuracy: 0.5430\n",
      "Epoch 602/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 2.5132e-04 - accuracy: 1.0000 - val_loss: 3.9963 - val_accuracy: 0.5339\n",
      "Epoch 603/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 1.9731e-04 - accuracy: 1.0000 - val_loss: 3.9870 - val_accuracy: 0.5294\n",
      "Epoch 604/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 2.4159e-04 - accuracy: 1.0000 - val_loss: 4.0143 - val_accuracy: 0.5339\n",
      "Epoch 605/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 2.3397e-04 - accuracy: 1.0000 - val_loss: 3.9990 - val_accuracy: 0.5339\n",
      "Epoch 606/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 2.3418e-04 - accuracy: 1.0000 - val_loss: 4.0075 - val_accuracy: 0.5294\n",
      "Epoch 607/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 2.6057e-04 - accuracy: 1.0000 - val_loss: 4.0009 - val_accuracy: 0.5204\n",
      "Epoch 608/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 1.9832e-04 - accuracy: 1.0000 - val_loss: 4.0115 - val_accuracy: 0.5249\n",
      "Epoch 609/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.0884e-04 - accuracy: 1.0000 - val_loss: 4.0274 - val_accuracy: 0.5294\n",
      "Epoch 610/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.5072e-04 - accuracy: 1.0000 - val_loss: 4.0317 - val_accuracy: 0.5339\n",
      "Epoch 611/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 1.9717e-04 - accuracy: 1.0000 - val_loss: 4.0278 - val_accuracy: 0.5339\n",
      "Epoch 612/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 1.8156e-04 - accuracy: 1.0000 - val_loss: 4.0110 - val_accuracy: 0.5339\n",
      "Epoch 613/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 1.9008e-04 - accuracy: 1.0000 - val_loss: 4.0168 - val_accuracy: 0.5294\n",
      "Epoch 614/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 1.8368e-04 - accuracy: 1.0000 - val_loss: 4.0242 - val_accuracy: 0.5294\n",
      "Epoch 615/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 1.6906e-04 - accuracy: 1.0000 - val_loss: 4.0320 - val_accuracy: 0.5385\n",
      "Epoch 616/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 1.9014e-04 - accuracy: 1.0000 - val_loss: 4.0455 - val_accuracy: 0.5339\n",
      "Epoch 617/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 1.9429e-04 - accuracy: 1.0000 - val_loss: 4.0459 - val_accuracy: 0.5294\n",
      "Epoch 618/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 1.9125e-04 - accuracy: 1.0000 - val_loss: 4.0368 - val_accuracy: 0.5249\n",
      "Epoch 619/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 1.7274e-04 - accuracy: 1.0000 - val_loss: 4.0520 - val_accuracy: 0.5204\n",
      "Epoch 620/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.6781e-04 - accuracy: 1.0000 - val_loss: 4.0574 - val_accuracy: 0.5249\n",
      "Epoch 621/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.6989e-04 - accuracy: 1.0000 - val_loss: 4.0502 - val_accuracy: 0.5294\n",
      "Epoch 622/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 1.7653e-04 - accuracy: 1.0000 - val_loss: 4.0599 - val_accuracy: 0.5339\n",
      "Epoch 623/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 1.6005e-04 - accuracy: 1.0000 - val_loss: 4.0609 - val_accuracy: 0.5294\n",
      "Epoch 624/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 1.6277e-04 - accuracy: 1.0000 - val_loss: 4.0628 - val_accuracy: 0.5294\n",
      "Epoch 625/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 1.7294e-04 - accuracy: 1.0000 - val_loss: 4.0602 - val_accuracy: 0.5339\n",
      "Epoch 626/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 1.5195e-04 - accuracy: 1.0000 - val_loss: 4.0680 - val_accuracy: 0.5294\n",
      "Epoch 627/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 1.7644e-04 - accuracy: 1.0000 - val_loss: 4.0872 - val_accuracy: 0.5249\n",
      "Epoch 628/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 1.9067e-04 - accuracy: 1.0000 - val_loss: 4.0692 - val_accuracy: 0.5249\n",
      "Epoch 629/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 1.7182e-04 - accuracy: 1.0000 - val_loss: 4.0833 - val_accuracy: 0.5294\n",
      "Epoch 630/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 1.6291e-04 - accuracy: 1.0000 - val_loss: 4.0919 - val_accuracy: 0.5249\n",
      "Epoch 631/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 1.4517e-04 - accuracy: 1.0000 - val_loss: 4.0861 - val_accuracy: 0.5294\n",
      "Epoch 632/1000\n",
      "1980/1980 [==============================] - 1s 502us/step - loss: 1.4891e-04 - accuracy: 1.0000 - val_loss: 4.0854 - val_accuracy: 0.5385\n",
      "Epoch 633/1000\n",
      "1980/1980 [==============================] - 1s 437us/step - loss: 1.3633e-04 - accuracy: 1.0000 - val_loss: 4.0943 - val_accuracy: 0.5249\n",
      "Epoch 634/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 1.5431e-04 - accuracy: 1.0000 - val_loss: 4.1109 - val_accuracy: 0.5204\n",
      "Epoch 635/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.4456e-04 - accuracy: 1.0000 - val_loss: 4.0706 - val_accuracy: 0.5339\n",
      "Epoch 636/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.5757e-04 - accuracy: 1.0000 - val_loss: 4.1033 - val_accuracy: 0.5385\n",
      "Epoch 637/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 2.0615e-04 - accuracy: 1.0000 - val_loss: 4.0975 - val_accuracy: 0.5294\n",
      "Epoch 638/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 1.6667e-04 - accuracy: 1.0000 - val_loss: 4.0966 - val_accuracy: 0.5339\n",
      "Epoch 639/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 1.5992e-04 - accuracy: 1.0000 - val_loss: 4.0968 - val_accuracy: 0.5385\n",
      "Epoch 640/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 1.2760e-04 - accuracy: 1.0000 - val_loss: 4.1207 - val_accuracy: 0.5249\n",
      "Epoch 641/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 1.5414e-04 - accuracy: 1.0000 - val_loss: 4.1319 - val_accuracy: 0.5249\n",
      "Epoch 642/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 1.4715e-04 - accuracy: 1.0000 - val_loss: 4.1193 - val_accuracy: 0.5294\n",
      "Epoch 643/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.3302e-04 - accuracy: 1.0000 - val_loss: 4.1418 - val_accuracy: 0.5294\n",
      "Epoch 644/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 1.3838e-04 - accuracy: 1.0000 - val_loss: 4.1531 - val_accuracy: 0.5385\n",
      "Epoch 645/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 387us/step - loss: 1.2032e-04 - accuracy: 1.0000 - val_loss: 4.1516 - val_accuracy: 0.5249\n",
      "Epoch 646/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 1.2733e-04 - accuracy: 1.0000 - val_loss: 4.1632 - val_accuracy: 0.5249\n",
      "Epoch 647/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.1564e-04 - accuracy: 1.0000 - val_loss: 4.1720 - val_accuracy: 0.5204\n",
      "Epoch 648/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.4881e-04 - accuracy: 1.0000 - val_loss: 4.1583 - val_accuracy: 0.5385\n",
      "Epoch 649/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 1.2439e-04 - accuracy: 1.0000 - val_loss: 4.1654 - val_accuracy: 0.5294\n",
      "Epoch 650/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.1717e-04 - accuracy: 1.0000 - val_loss: 4.1900 - val_accuracy: 0.5294\n",
      "Epoch 651/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.1018e-04 - accuracy: 1.0000 - val_loss: 4.1914 - val_accuracy: 0.5249\n",
      "Epoch 652/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 1.1005e-04 - accuracy: 1.0000 - val_loss: 4.1998 - val_accuracy: 0.5339\n",
      "Epoch 653/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 1.4469e-04 - accuracy: 1.0000 - val_loss: 4.2015 - val_accuracy: 0.5249\n",
      "Epoch 654/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.0852e-04 - accuracy: 1.0000 - val_loss: 4.2279 - val_accuracy: 0.5204\n",
      "Epoch 655/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.0377e-04 - accuracy: 1.0000 - val_loss: 4.2317 - val_accuracy: 0.5339\n",
      "Epoch 656/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 1.0840e-04 - accuracy: 1.0000 - val_loss: 4.2212 - val_accuracy: 0.5385\n",
      "Epoch 657/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 1.1063e-04 - accuracy: 1.0000 - val_loss: 4.2150 - val_accuracy: 0.5339\n",
      "Epoch 658/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 1.0636e-04 - accuracy: 1.0000 - val_loss: 4.2352 - val_accuracy: 0.5294\n",
      "Epoch 659/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 1.3063e-04 - accuracy: 1.0000 - val_loss: 4.2515 - val_accuracy: 0.5294\n",
      "Epoch 660/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 1.0913e-04 - accuracy: 1.0000 - val_loss: 4.2540 - val_accuracy: 0.5249\n",
      "Epoch 661/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 1.2527e-04 - accuracy: 1.0000 - val_loss: 4.2582 - val_accuracy: 0.5294\n",
      "Epoch 662/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 1.1115e-04 - accuracy: 1.0000 - val_loss: 4.2375 - val_accuracy: 0.5249\n",
      "Epoch 663/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 1.1080e-04 - accuracy: 1.0000 - val_loss: 4.2377 - val_accuracy: 0.5249\n",
      "Epoch 664/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 1.2929e-04 - accuracy: 1.0000 - val_loss: 4.2648 - val_accuracy: 0.5294\n",
      "Epoch 665/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.0249e-04 - accuracy: 1.0000 - val_loss: 4.2620 - val_accuracy: 0.5294\n",
      "Epoch 666/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 9.9527e-05 - accuracy: 1.0000 - val_loss: 4.2615 - val_accuracy: 0.5249\n",
      "Epoch 667/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 1.0698e-04 - accuracy: 1.0000 - val_loss: 4.2689 - val_accuracy: 0.5294\n",
      "Epoch 668/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.0077e-04 - accuracy: 1.0000 - val_loss: 4.2764 - val_accuracy: 0.5294\n",
      "Epoch 669/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 9.6588e-05 - accuracy: 1.0000 - val_loss: 4.2824 - val_accuracy: 0.5249\n",
      "Epoch 670/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 1.0839e-04 - accuracy: 1.0000 - val_loss: 4.2921 - val_accuracy: 0.5249\n",
      "Epoch 671/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 9.0397e-05 - accuracy: 1.0000 - val_loss: 4.2954 - val_accuracy: 0.5294\n",
      "Epoch 672/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.0118e-04 - accuracy: 1.0000 - val_loss: 4.2884 - val_accuracy: 0.5294\n",
      "Epoch 673/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 9.1746e-05 - accuracy: 1.0000 - val_loss: 4.2991 - val_accuracy: 0.5294\n",
      "Epoch 674/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 8.6918e-05 - accuracy: 1.0000 - val_loss: 4.3022 - val_accuracy: 0.5294\n",
      "Epoch 675/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 1.1351e-04 - accuracy: 1.0000 - val_loss: 4.2843 - val_accuracy: 0.5294\n",
      "Epoch 676/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 9.2011e-05 - accuracy: 1.0000 - val_loss: 4.3115 - val_accuracy: 0.5294\n",
      "Epoch 677/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 9.0940e-05 - accuracy: 1.0000 - val_loss: 4.3425 - val_accuracy: 0.5339\n",
      "Epoch 678/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 7.7424e-05 - accuracy: 1.0000 - val_loss: 4.3146 - val_accuracy: 0.5294\n",
      "Epoch 679/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 8.5612e-05 - accuracy: 1.0000 - val_loss: 4.3201 - val_accuracy: 0.5294\n",
      "Epoch 680/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 7.5361e-05 - accuracy: 1.0000 - val_loss: 4.3226 - val_accuracy: 0.5294\n",
      "Epoch 681/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.0734e-04 - accuracy: 1.0000 - val_loss: 4.2946 - val_accuracy: 0.5385\n",
      "Epoch 682/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 9.5653e-05 - accuracy: 1.0000 - val_loss: 4.3121 - val_accuracy: 0.5339\n",
      "Epoch 683/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.3726e-05 - accuracy: 1.0000 - val_loss: 4.3503 - val_accuracy: 0.5339\n",
      "Epoch 684/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 7.6963e-05 - accuracy: 1.0000 - val_loss: 4.3783 - val_accuracy: 0.5249\n",
      "Epoch 685/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.1594e-04 - accuracy: 1.0000 - val_loss: 4.3305 - val_accuracy: 0.5204\n",
      "Epoch 686/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 7.2935e-05 - accuracy: 1.0000 - val_loss: 4.3228 - val_accuracy: 0.5294\n",
      "Epoch 687/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 7.8988e-05 - accuracy: 1.0000 - val_loss: 4.3648 - val_accuracy: 0.5339\n",
      "Epoch 688/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 8.1214e-05 - accuracy: 1.0000 - val_loss: 4.3599 - val_accuracy: 0.5339\n",
      "Epoch 689/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 8.1798e-05 - accuracy: 1.0000 - val_loss: 4.3575 - val_accuracy: 0.5339\n",
      "Epoch 690/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 7.7444e-05 - accuracy: 1.0000 - val_loss: 4.3613 - val_accuracy: 0.5339\n",
      "Epoch 691/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 7.4935e-05 - accuracy: 1.0000 - val_loss: 4.3601 - val_accuracy: 0.5294\n",
      "Epoch 692/1000\n",
      "1980/1980 [==============================] - 1s 500us/step - loss: 7.8239e-05 - accuracy: 1.0000 - val_loss: 4.3512 - val_accuracy: 0.5339\n",
      "Epoch 693/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 7.0077e-05 - accuracy: 1.0000 - val_loss: 4.3744 - val_accuracy: 0.5339\n",
      "Epoch 694/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 8.0339e-05 - accuracy: 1.0000 - val_loss: 4.3791 - val_accuracy: 0.5339\n",
      "Epoch 695/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 6.6797e-05 - accuracy: 1.0000 - val_loss: 4.3811 - val_accuracy: 0.5294\n",
      "Epoch 696/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 7.2432e-05 - accuracy: 1.0000 - val_loss: 4.3990 - val_accuracy: 0.5339\n",
      "Epoch 697/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 7.7917e-05 - accuracy: 1.0000 - val_loss: 4.4114 - val_accuracy: 0.5294\n",
      "Epoch 698/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 7.2916e-05 - accuracy: 1.0000 - val_loss: 4.4097 - val_accuracy: 0.5294\n",
      "Epoch 699/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 7.9618e-05 - accuracy: 1.0000 - val_loss: 4.4068 - val_accuracy: 0.5339\n",
      "Epoch 700/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 7.2259e-05 - accuracy: 1.0000 - val_loss: 4.4039 - val_accuracy: 0.5385\n",
      "Epoch 701/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 7.3473e-05 - accuracy: 1.0000 - val_loss: 4.3960 - val_accuracy: 0.5294\n",
      "Epoch 702/1000\n",
      "1980/1980 [==============================] - 1s 476us/step - loss: 6.6781e-05 - accuracy: 1.0000 - val_loss: 4.3959 - val_accuracy: 0.5294\n",
      "Epoch 703/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 6.8565e-05 - accuracy: 1.0000 - val_loss: 4.4072 - val_accuracy: 0.5339\n",
      "Epoch 704/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 7.1495e-05 - accuracy: 1.0000 - val_loss: 4.3911 - val_accuracy: 0.5249\n",
      "Epoch 705/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 6.5365e-05 - accuracy: 1.0000 - val_loss: 4.4015 - val_accuracy: 0.5339\n",
      "Epoch 706/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 7.2037e-05 - accuracy: 1.0000 - val_loss: 4.4241 - val_accuracy: 0.5294\n",
      "Epoch 707/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 6.8844e-05 - accuracy: 1.0000 - val_loss: 4.4266 - val_accuracy: 0.5339\n",
      "Epoch 708/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 6.1149e-05 - accuracy: 1.0000 - val_loss: 4.4287 - val_accuracy: 0.5339\n",
      "Epoch 709/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 6.6521e-05 - accuracy: 1.0000 - val_loss: 4.4326 - val_accuracy: 0.5339\n",
      "Epoch 710/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 6.5666e-05 - accuracy: 1.0000 - val_loss: 4.4399 - val_accuracy: 0.5249\n",
      "Epoch 711/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 7.9079e-05 - accuracy: 1.0000 - val_loss: 4.4534 - val_accuracy: 0.5385\n",
      "Epoch 712/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 6.7461e-05 - accuracy: 1.0000 - val_loss: 4.4497 - val_accuracy: 0.5294\n",
      "Epoch 713/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 6.7397e-05 - accuracy: 1.0000 - val_loss: 4.4741 - val_accuracy: 0.5339\n",
      "Epoch 714/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 5.4150e-05 - accuracy: 1.0000 - val_loss: 4.4796 - val_accuracy: 0.5294\n",
      "Epoch 715/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 5.3513e-05 - accuracy: 1.0000 - val_loss: 4.4738 - val_accuracy: 0.5249\n",
      "Epoch 716/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.0679e-05 - accuracy: 1.0000 - val_loss: 4.4794 - val_accuracy: 0.5294\n",
      "Epoch 717/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 5.8437e-05 - accuracy: 1.0000 - val_loss: 4.4902 - val_accuracy: 0.5249\n",
      "Epoch 718/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 6.0094e-05 - accuracy: 1.0000 - val_loss: 4.4809 - val_accuracy: 0.5294\n",
      "Epoch 719/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 7.3260e-05 - accuracy: 1.0000 - val_loss: 4.4885 - val_accuracy: 0.5249\n",
      "Epoch 720/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 8.4745e-05 - accuracy: 1.0000 - val_loss: 4.4860 - val_accuracy: 0.5385\n",
      "Epoch 721/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 8.0320e-05 - accuracy: 1.0000 - val_loss: 4.4992 - val_accuracy: 0.5430\n",
      "Epoch 722/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.0336e-05 - accuracy: 1.0000 - val_loss: 4.4752 - val_accuracy: 0.5339\n",
      "Epoch 723/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 5.9607e-05 - accuracy: 1.0000 - val_loss: 4.4499 - val_accuracy: 0.5339\n",
      "Epoch 724/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 5.7447e-05 - accuracy: 1.0000 - val_loss: 4.4626 - val_accuracy: 0.5294\n",
      "Epoch 725/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 5.6825e-05 - accuracy: 1.0000 - val_loss: 4.4808 - val_accuracy: 0.5385\n",
      "Epoch 726/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 8.2461e-05 - accuracy: 1.0000 - val_loss: 4.4885 - val_accuracy: 0.5294\n",
      "Epoch 727/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.8967e-05 - accuracy: 1.0000 - val_loss: 4.5128 - val_accuracy: 0.5294\n",
      "Epoch 728/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 5.8699e-05 - accuracy: 1.0000 - val_loss: 4.5219 - val_accuracy: 0.5339\n",
      "Epoch 729/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 5.3315e-05 - accuracy: 1.0000 - val_loss: 4.5329 - val_accuracy: 0.5294\n",
      "Epoch 730/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 5.3697e-05 - accuracy: 1.0000 - val_loss: 4.5368 - val_accuracy: 0.5339\n",
      "Epoch 731/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 4.7836e-05 - accuracy: 1.0000 - val_loss: 4.5435 - val_accuracy: 0.5339\n",
      "Epoch 732/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 5.2497e-05 - accuracy: 1.0000 - val_loss: 4.5477 - val_accuracy: 0.5430\n",
      "Epoch 733/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 5.2411e-05 - accuracy: 1.0000 - val_loss: 4.5647 - val_accuracy: 0.5385\n",
      "Epoch 734/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 5.0986e-05 - accuracy: 1.0000 - val_loss: 4.5589 - val_accuracy: 0.5385\n",
      "Epoch 735/1000\n",
      "1980/1980 [==============================] - 1s 499us/step - loss: 5.4484e-05 - accuracy: 1.0000 - val_loss: 4.5588 - val_accuracy: 0.5385\n",
      "Epoch 736/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 5.2269e-05 - accuracy: 1.0000 - val_loss: 4.5540 - val_accuracy: 0.5339\n",
      "Epoch 737/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 6.1017e-05 - accuracy: 1.0000 - val_loss: 4.5246 - val_accuracy: 0.5385\n",
      "Epoch 738/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 5.0578e-05 - accuracy: 1.0000 - val_loss: 4.5690 - val_accuracy: 0.5430\n",
      "Epoch 739/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 5.0395e-05 - accuracy: 1.0000 - val_loss: 4.5753 - val_accuracy: 0.5430\n",
      "Epoch 740/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 4.8655e-05 - accuracy: 1.0000 - val_loss: 4.5708 - val_accuracy: 0.5430\n",
      "Epoch 741/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 6.1743e-05 - accuracy: 1.0000 - val_loss: 4.5599 - val_accuracy: 0.5385\n",
      "Epoch 742/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 4.8795e-05 - accuracy: 1.0000 - val_loss: 4.5596 - val_accuracy: 0.5430\n",
      "Epoch 743/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.5650e-05 - accuracy: 1.0000 - val_loss: 4.5649 - val_accuracy: 0.5430\n",
      "Epoch 744/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 5.1671e-05 - accuracy: 1.0000 - val_loss: 4.5851 - val_accuracy: 0.5475\n",
      "Epoch 745/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 4.1053e-05 - accuracy: 1.0000 - val_loss: 4.5877 - val_accuracy: 0.5430\n",
      "Epoch 746/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 6.4373e-05 - accuracy: 1.0000 - val_loss: 4.5904 - val_accuracy: 0.5294\n",
      "Epoch 747/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 4.2629e-05 - accuracy: 1.0000 - val_loss: 4.5788 - val_accuracy: 0.5294\n",
      "Epoch 748/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 4.6736e-05 - accuracy: 1.0000 - val_loss: 4.6065 - val_accuracy: 0.5339\n",
      "Epoch 749/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 4.5479e-05 - accuracy: 1.0000 - val_loss: 4.6144 - val_accuracy: 0.5385\n",
      "Epoch 750/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 4.3249e-05 - accuracy: 1.0000 - val_loss: 4.6150 - val_accuracy: 0.5385\n",
      "Epoch 751/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 360us/step - loss: 4.4114e-05 - accuracy: 1.0000 - val_loss: 4.6373 - val_accuracy: 0.5385\n",
      "Epoch 752/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 4.3369e-05 - accuracy: 1.0000 - val_loss: 4.6661 - val_accuracy: 0.5339\n",
      "Epoch 753/1000\n",
      "1980/1980 [==============================] - 1s 358us/step - loss: 4.2991e-05 - accuracy: 1.0000 - val_loss: 4.6798 - val_accuracy: 0.5385\n",
      "Epoch 754/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 4.3813e-05 - accuracy: 1.0000 - val_loss: 4.6361 - val_accuracy: 0.5385\n",
      "Epoch 755/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 4.2633e-05 - accuracy: 1.0000 - val_loss: 4.6379 - val_accuracy: 0.5385\n",
      "Epoch 756/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 5.2006e-05 - accuracy: 1.0000 - val_loss: 4.6652 - val_accuracy: 0.5385\n",
      "Epoch 757/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 4.1939e-05 - accuracy: 1.0000 - val_loss: 4.6752 - val_accuracy: 0.5385\n",
      "Epoch 758/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 4.1424e-05 - accuracy: 1.0000 - val_loss: 4.6432 - val_accuracy: 0.5430\n",
      "Epoch 759/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 4.4956e-05 - accuracy: 1.0000 - val_loss: 4.6666 - val_accuracy: 0.5430\n",
      "Epoch 760/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 4.4693e-05 - accuracy: 1.0000 - val_loss: 4.6607 - val_accuracy: 0.5430\n",
      "Epoch 761/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 4.0211e-05 - accuracy: 1.0000 - val_loss: 4.6380 - val_accuracy: 0.5430\n",
      "Epoch 762/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 4.3408e-05 - accuracy: 1.0000 - val_loss: 4.6509 - val_accuracy: 0.5339\n",
      "Epoch 763/1000\n",
      "1980/1980 [==============================] - 1s 554us/step - loss: 3.9613e-05 - accuracy: 1.0000 - val_loss: 4.6737 - val_accuracy: 0.5294\n",
      "Epoch 764/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 4.6457e-05 - accuracy: 1.0000 - val_loss: 4.6872 - val_accuracy: 0.5475\n",
      "Epoch 765/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.4243e-05 - accuracy: 1.0000 - val_loss: 4.6810 - val_accuracy: 0.5475\n",
      "Epoch 766/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 3.9901e-05 - accuracy: 1.0000 - val_loss: 4.6901 - val_accuracy: 0.5475\n",
      "Epoch 767/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 4.3739e-05 - accuracy: 1.0000 - val_loss: 4.6907 - val_accuracy: 0.5385\n",
      "Epoch 768/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 4.1440e-05 - accuracy: 1.0000 - val_loss: 4.6954 - val_accuracy: 0.5430\n",
      "Epoch 769/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 3.1780e-05 - accuracy: 1.0000 - val_loss: 4.6923 - val_accuracy: 0.5430\n",
      "Epoch 770/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 3.7611e-05 - accuracy: 1.0000 - val_loss: 4.7135 - val_accuracy: 0.5430\n",
      "Epoch 771/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 3.5251e-05 - accuracy: 1.0000 - val_loss: 4.7134 - val_accuracy: 0.5475\n",
      "Epoch 772/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 4.5145e-05 - accuracy: 1.0000 - val_loss: 4.7134 - val_accuracy: 0.5385\n",
      "Epoch 773/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 3.4643e-05 - accuracy: 1.0000 - val_loss: 4.7382 - val_accuracy: 0.5339\n",
      "Epoch 774/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 3.5495e-05 - accuracy: 1.0000 - val_loss: 4.7376 - val_accuracy: 0.5339\n",
      "Epoch 775/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 3.2446e-05 - accuracy: 1.0000 - val_loss: 4.7395 - val_accuracy: 0.5430\n",
      "Epoch 776/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 3.2899e-05 - accuracy: 1.0000 - val_loss: 4.7361 - val_accuracy: 0.5385\n",
      "Epoch 777/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 3.9845e-05 - accuracy: 1.0000 - val_loss: 4.7243 - val_accuracy: 0.5475\n",
      "Epoch 778/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 3.3915e-05 - accuracy: 1.0000 - val_loss: 4.7165 - val_accuracy: 0.5475\n",
      "Epoch 779/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 3.4354e-05 - accuracy: 1.0000 - val_loss: 4.7397 - val_accuracy: 0.5520\n",
      "Epoch 780/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 3.6493e-05 - accuracy: 1.0000 - val_loss: 4.7417 - val_accuracy: 0.5430\n",
      "Epoch 781/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 3.7911e-05 - accuracy: 1.0000 - val_loss: 4.7091 - val_accuracy: 0.5475\n",
      "Epoch 782/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 3.3314e-05 - accuracy: 1.0000 - val_loss: 4.7125 - val_accuracy: 0.5475\n",
      "Epoch 783/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 3.2532e-05 - accuracy: 1.0000 - val_loss: 4.7322 - val_accuracy: 0.5475\n",
      "Epoch 784/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 3.0912e-05 - accuracy: 1.0000 - val_loss: 4.7182 - val_accuracy: 0.5475\n",
      "Epoch 785/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 3.1459e-05 - accuracy: 1.0000 - val_loss: 4.7256 - val_accuracy: 0.5566\n",
      "Epoch 786/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 8.3760e-05 - accuracy: 1.0000 - val_loss: 4.6714 - val_accuracy: 0.5566\n",
      "Epoch 787/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.2204 - accuracy: 0.9601 - val_loss: 2.8763 - val_accuracy: 0.5068\n",
      "Epoch 788/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 1.2892 - accuracy: 0.5394 - val_loss: 0.7974 - val_accuracy: 0.4615\n",
      "Epoch 789/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.8148 - accuracy: 0.5232 - val_loss: 0.7125 - val_accuracy: 0.4299\n",
      "Epoch 790/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.7603 - accuracy: 0.4874 - val_loss: 0.7108 - val_accuracy: 0.5520\n",
      "Epoch 791/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.7328 - accuracy: 0.5111 - val_loss: 0.6918 - val_accuracy: 0.5249\n",
      "Epoch 792/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.6990 - accuracy: 0.5359 - val_loss: 0.7014 - val_accuracy: 0.4796\n",
      "Epoch 793/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.6926 - accuracy: 0.5298 - val_loss: 0.7030 - val_accuracy: 0.4751\n",
      "Epoch 794/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.6900 - accuracy: 0.5338 - val_loss: 0.6926 - val_accuracy: 0.5520\n",
      "Epoch 795/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6883 - accuracy: 0.5369 - val_loss: 0.6970 - val_accuracy: 0.5113\n",
      "Epoch 796/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6826 - accuracy: 0.5581 - val_loss: 0.7025 - val_accuracy: 0.5339\n",
      "Epoch 797/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.6765 - accuracy: 0.5677 - val_loss: 0.7533 - val_accuracy: 0.4434\n",
      "Epoch 798/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.6846 - accuracy: 0.5510 - val_loss: 0.7047 - val_accuracy: 0.4932\n",
      "Epoch 799/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.6855 - accuracy: 0.5379 - val_loss: 0.6981 - val_accuracy: 0.5520\n",
      "Epoch 800/1000\n",
      "1980/1980 [==============================] - 1s 510us/step - loss: 0.6831 - accuracy: 0.5535 - val_loss: 0.7184 - val_accuracy: 0.4977\n",
      "Epoch 801/1000\n",
      "1980/1980 [==============================] - 1s 501us/step - loss: 0.6769 - accuracy: 0.5616 - val_loss: 0.7141 - val_accuracy: 0.5023\n",
      "Epoch 802/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.6707 - accuracy: 0.5808 - val_loss: 0.7232 - val_accuracy: 0.4796\n",
      "Epoch 803/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.6685 - accuracy: 0.5854 - val_loss: 0.7151 - val_accuracy: 0.5113\n",
      "Epoch 804/1000\n",
      "1980/1980 [==============================] - 1s 495us/step - loss: 0.6649 - accuracy: 0.5985 - val_loss: 0.7110 - val_accuracy: 0.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 805/1000\n",
      "1980/1980 [==============================] - 1s 488us/step - loss: 0.6579 - accuracy: 0.6061 - val_loss: 0.7247 - val_accuracy: 0.5204\n",
      "Epoch 806/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.6553 - accuracy: 0.6076 - val_loss: 0.7175 - val_accuracy: 0.5294\n",
      "Epoch 807/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.6544 - accuracy: 0.5955 - val_loss: 0.7186 - val_accuracy: 0.5475\n",
      "Epoch 808/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.6434 - accuracy: 0.6106 - val_loss: 0.7471 - val_accuracy: 0.5113\n",
      "Epoch 809/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.6463 - accuracy: 0.6253 - val_loss: 0.7167 - val_accuracy: 0.5249\n",
      "Epoch 810/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.6424 - accuracy: 0.6197 - val_loss: 0.7209 - val_accuracy: 0.5204\n",
      "Epoch 811/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.6367 - accuracy: 0.6338 - val_loss: 0.7096 - val_accuracy: 0.5566\n",
      "Epoch 812/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.6242 - accuracy: 0.6399 - val_loss: 0.7447 - val_accuracy: 0.5611\n",
      "Epoch 813/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.6342 - accuracy: 0.6338 - val_loss: 0.7025 - val_accuracy: 0.5294\n",
      "Epoch 814/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 0.6073 - accuracy: 0.6747 - val_loss: 0.7598 - val_accuracy: 0.5068\n",
      "Epoch 815/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.5911 - accuracy: 0.6737 - val_loss: 0.7705 - val_accuracy: 0.5475\n",
      "Epoch 816/1000\n",
      "1980/1980 [==============================] - 1s 499us/step - loss: 0.5987 - accuracy: 0.6636 - val_loss: 0.7578 - val_accuracy: 0.5113\n",
      "Epoch 817/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.5864 - accuracy: 0.6843 - val_loss: 0.7757 - val_accuracy: 0.5475\n",
      "Epoch 818/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.5682 - accuracy: 0.6985 - val_loss: 0.8052 - val_accuracy: 0.5158\n",
      "Epoch 819/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.5572 - accuracy: 0.7030 - val_loss: 0.8298 - val_accuracy: 0.5023\n",
      "Epoch 820/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.5416 - accuracy: 0.7268 - val_loss: 0.8208 - val_accuracy: 0.5294\n",
      "Epoch 821/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.5178 - accuracy: 0.7348 - val_loss: 0.8981 - val_accuracy: 0.4977\n",
      "Epoch 822/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.5143 - accuracy: 0.7364 - val_loss: 0.9092 - val_accuracy: 0.5023\n",
      "Epoch 823/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.5028 - accuracy: 0.7449 - val_loss: 0.9214 - val_accuracy: 0.4932\n",
      "Epoch 824/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.4855 - accuracy: 0.7702 - val_loss: 0.9315 - val_accuracy: 0.4796\n",
      "Epoch 825/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.4425 - accuracy: 0.7843 - val_loss: 0.9684 - val_accuracy: 0.4977\n",
      "Epoch 826/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.4550 - accuracy: 0.7874 - val_loss: 1.0089 - val_accuracy: 0.4977\n",
      "Epoch 827/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.4326 - accuracy: 0.7970 - val_loss: 0.9925 - val_accuracy: 0.4932\n",
      "Epoch 828/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.4196 - accuracy: 0.8051 - val_loss: 1.0084 - val_accuracy: 0.4796\n",
      "Epoch 829/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.4101 - accuracy: 0.8076 - val_loss: 1.0617 - val_accuracy: 0.4615\n",
      "Epoch 830/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 0.4030 - accuracy: 0.8177 - val_loss: 1.0064 - val_accuracy: 0.4887\n",
      "Epoch 831/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.3819 - accuracy: 0.8268 - val_loss: 1.2005 - val_accuracy: 0.4887\n",
      "Epoch 832/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.3436 - accuracy: 0.8515 - val_loss: 1.1785 - val_accuracy: 0.4932\n",
      "Epoch 833/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.3315 - accuracy: 0.8571 - val_loss: 1.2315 - val_accuracy: 0.4253\n",
      "Epoch 834/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.3157 - accuracy: 0.8677 - val_loss: 1.2202 - val_accuracy: 0.5023\n",
      "Epoch 835/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.3043 - accuracy: 0.8652 - val_loss: 1.3374 - val_accuracy: 0.4615\n",
      "Epoch 836/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.2951 - accuracy: 0.8727 - val_loss: 1.2707 - val_accuracy: 0.4751\n",
      "Epoch 837/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.2584 - accuracy: 0.8894 - val_loss: 1.2863 - val_accuracy: 0.5023\n",
      "Epoch 838/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.2505 - accuracy: 0.8914 - val_loss: 1.4410 - val_accuracy: 0.5068\n",
      "Epoch 839/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.2301 - accuracy: 0.9131 - val_loss: 1.3752 - val_accuracy: 0.5113\n",
      "Epoch 840/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.2341 - accuracy: 0.9010 - val_loss: 1.5505 - val_accuracy: 0.5249\n",
      "Epoch 841/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.2077 - accuracy: 0.9192 - val_loss: 1.5661 - val_accuracy: 0.5113\n",
      "Epoch 842/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.1805 - accuracy: 0.9278 - val_loss: 1.6538 - val_accuracy: 0.4842\n",
      "Epoch 843/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.1650 - accuracy: 0.9374 - val_loss: 1.6247 - val_accuracy: 0.5249\n",
      "Epoch 844/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.1347 - accuracy: 0.9530 - val_loss: 1.8296 - val_accuracy: 0.5158\n",
      "Epoch 845/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.1440 - accuracy: 0.9505 - val_loss: 1.7007 - val_accuracy: 0.4887\n",
      "Epoch 846/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.1318 - accuracy: 0.9520 - val_loss: 1.7276 - val_accuracy: 0.5023\n",
      "Epoch 847/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.1231 - accuracy: 0.9515 - val_loss: 1.8204 - val_accuracy: 0.5520\n",
      "Epoch 848/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.1067 - accuracy: 0.9646 - val_loss: 1.7690 - val_accuracy: 0.5204\n",
      "Epoch 849/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0835 - accuracy: 0.9707 - val_loss: 1.9570 - val_accuracy: 0.4706\n",
      "Epoch 850/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0814 - accuracy: 0.9712 - val_loss: 1.9601 - val_accuracy: 0.5158\n",
      "Epoch 851/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.1192 - accuracy: 0.9545 - val_loss: 1.9170 - val_accuracy: 0.4887\n",
      "Epoch 852/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.1077 - accuracy: 0.9652 - val_loss: 1.9322 - val_accuracy: 0.4842\n",
      "Epoch 853/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0722 - accuracy: 0.9788 - val_loss: 1.9666 - val_accuracy: 0.5023\n",
      "Epoch 854/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0778 - accuracy: 0.9747 - val_loss: 2.1883 - val_accuracy: 0.4932\n",
      "Epoch 855/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0643 - accuracy: 0.9768 - val_loss: 2.0933 - val_accuracy: 0.4887\n",
      "Epoch 856/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0520 - accuracy: 0.9854 - val_loss: 2.1390 - val_accuracy: 0.5113\n",
      "Epoch 857/1000\n",
      "1980/1980 [==============================] - 1s 363us/step - loss: 0.0633 - accuracy: 0.9798 - val_loss: 2.1145 - val_accuracy: 0.5339\n",
      "Epoch 858/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0663 - accuracy: 0.9773 - val_loss: 2.1992 - val_accuracy: 0.5023\n",
      "Epoch 859/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0332 - accuracy: 0.9924 - val_loss: 2.2204 - val_accuracy: 0.4706\n",
      "Epoch 860/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0192 - accuracy: 0.9960 - val_loss: 2.3596 - val_accuracy: 0.4977\n",
      "Epoch 861/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0145 - accuracy: 0.9985 - val_loss: 2.3510 - val_accuracy: 0.4706\n",
      "Epoch 862/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 2.4582 - val_accuracy: 0.4887\n",
      "Epoch 863/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0100 - accuracy: 0.9990 - val_loss: 2.4754 - val_accuracy: 0.4887\n",
      "Epoch 864/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.5341 - val_accuracy: 0.4932\n",
      "Epoch 865/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 2.5813 - val_accuracy: 0.4796\n",
      "Epoch 866/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 2.5691 - val_accuracy: 0.4796\n",
      "Epoch 867/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.5835 - val_accuracy: 0.4932\n",
      "Epoch 868/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.6241 - val_accuracy: 0.4887\n",
      "Epoch 869/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.6429 - val_accuracy: 0.5023\n",
      "Epoch 870/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.6424 - val_accuracy: 0.4977\n",
      "Epoch 871/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.6783 - val_accuracy: 0.4977\n",
      "Epoch 872/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.6989 - val_accuracy: 0.5023\n",
      "Epoch 873/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.7159 - val_accuracy: 0.4842\n",
      "Epoch 874/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.7088 - val_accuracy: 0.4932\n",
      "Epoch 875/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.7207 - val_accuracy: 0.4932\n",
      "Epoch 876/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.7451 - val_accuracy: 0.5023\n",
      "Epoch 877/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.7651 - val_accuracy: 0.5023\n",
      "Epoch 878/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.7736 - val_accuracy: 0.4932\n",
      "Epoch 879/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.7886 - val_accuracy: 0.5113\n",
      "Epoch 880/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.7932 - val_accuracy: 0.5023\n",
      "Epoch 881/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.8016 - val_accuracy: 0.5158\n",
      "Epoch 882/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.7968 - val_accuracy: 0.5113\n",
      "Epoch 883/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.8175 - val_accuracy: 0.4932\n",
      "Epoch 884/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.8283 - val_accuracy: 0.5068\n",
      "Epoch 885/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.8151 - val_accuracy: 0.5113\n",
      "Epoch 886/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.8487 - val_accuracy: 0.4977\n",
      "Epoch 887/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.8612 - val_accuracy: 0.4932\n",
      "Epoch 888/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.8635 - val_accuracy: 0.5158\n",
      "Epoch 889/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.8680 - val_accuracy: 0.5113\n",
      "Epoch 890/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.8817 - val_accuracy: 0.5113\n",
      "Epoch 891/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9016 - val_accuracy: 0.4977\n",
      "Epoch 892/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9190 - val_accuracy: 0.5068\n",
      "Epoch 893/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.9277 - val_accuracy: 0.5158\n",
      "Epoch 894/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9352 - val_accuracy: 0.5068\n",
      "Epoch 895/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9372 - val_accuracy: 0.5113\n",
      "Epoch 896/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.9556 - val_accuracy: 0.5068\n",
      "Epoch 897/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.9662 - val_accuracy: 0.5068\n",
      "Epoch 898/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.9710 - val_accuracy: 0.5113\n",
      "Epoch 899/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.9717 - val_accuracy: 0.5113\n",
      "Epoch 900/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.9972 - val_accuracy: 0.5113\n",
      "Epoch 901/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.0118 - val_accuracy: 0.5068\n",
      "Epoch 902/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 9.0365e-04 - accuracy: 1.0000 - val_loss: 3.0127 - val_accuracy: 0.5023\n",
      "Epoch 903/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 9.3030e-04 - accuracy: 1.0000 - val_loss: 3.0173 - val_accuracy: 0.5023\n",
      "Epoch 904/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.0250 - val_accuracy: 0.5158\n",
      "Epoch 905/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.0477 - val_accuracy: 0.5113\n",
      "Epoch 906/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 8.9298e-04 - accuracy: 1.0000 - val_loss: 3.0553 - val_accuracy: 0.5158\n",
      "Epoch 907/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 8.8010e-04 - accuracy: 1.0000 - val_loss: 3.0592 - val_accuracy: 0.5158\n",
      "Epoch 908/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 8.2235e-04 - accuracy: 1.0000 - val_loss: 3.0656 - val_accuracy: 0.5158\n",
      "Epoch 909/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 9.6920e-04 - accuracy: 1.0000 - val_loss: 3.0708 - val_accuracy: 0.5158\n",
      "Epoch 910/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 7.9616e-04 - accuracy: 1.0000 - val_loss: 3.0682 - val_accuracy: 0.5158\n",
      "Epoch 911/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 7.7685e-04 - accuracy: 1.0000 - val_loss: 3.0724 - val_accuracy: 0.5113\n",
      "Epoch 912/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 7.7789e-04 - accuracy: 1.0000 - val_loss: 3.0658 - val_accuracy: 0.5023\n",
      "Epoch 913/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 388us/step - loss: 7.4558e-04 - accuracy: 1.0000 - val_loss: 3.0855 - val_accuracy: 0.5068\n",
      "Epoch 914/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 8.3515e-04 - accuracy: 1.0000 - val_loss: 3.0939 - val_accuracy: 0.5113\n",
      "Epoch 915/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 7.8892e-04 - accuracy: 1.0000 - val_loss: 3.1026 - val_accuracy: 0.5068\n",
      "Epoch 916/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 8.0538e-04 - accuracy: 1.0000 - val_loss: 3.1081 - val_accuracy: 0.4977\n",
      "Epoch 917/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 7.5532e-04 - accuracy: 1.0000 - val_loss: 3.1131 - val_accuracy: 0.5023\n",
      "Epoch 918/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 7.0132e-04 - accuracy: 1.0000 - val_loss: 3.1254 - val_accuracy: 0.5068\n",
      "Epoch 919/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 7.3722e-04 - accuracy: 1.0000 - val_loss: 3.1375 - val_accuracy: 0.5023\n",
      "Epoch 920/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 7.6054e-04 - accuracy: 1.0000 - val_loss: 3.1356 - val_accuracy: 0.5023\n",
      "Epoch 921/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 7.3447e-04 - accuracy: 1.0000 - val_loss: 3.1364 - val_accuracy: 0.5158\n",
      "Epoch 922/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 6.6807e-04 - accuracy: 1.0000 - val_loss: 3.1635 - val_accuracy: 0.5068\n",
      "Epoch 923/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 7.3043e-04 - accuracy: 1.0000 - val_loss: 3.1642 - val_accuracy: 0.5068\n",
      "Epoch 924/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 6.4784e-04 - accuracy: 1.0000 - val_loss: 3.1694 - val_accuracy: 0.5158\n",
      "Epoch 925/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 6.0644e-04 - accuracy: 1.0000 - val_loss: 3.1826 - val_accuracy: 0.5113\n",
      "Epoch 926/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 5.4791e-04 - accuracy: 1.0000 - val_loss: 3.1990 - val_accuracy: 0.5068\n",
      "Epoch 927/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.3892e-04 - accuracy: 1.0000 - val_loss: 3.2023 - val_accuracy: 0.5068\n",
      "Epoch 928/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 5.7388e-04 - accuracy: 1.0000 - val_loss: 3.2138 - val_accuracy: 0.5113\n",
      "Epoch 929/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 5.1503e-04 - accuracy: 1.0000 - val_loss: 3.2348 - val_accuracy: 0.5068\n",
      "Epoch 930/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.3948e-04 - accuracy: 1.0000 - val_loss: 3.2596 - val_accuracy: 0.5068\n",
      "Epoch 931/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 5.2062e-04 - accuracy: 1.0000 - val_loss: 3.2430 - val_accuracy: 0.5023\n",
      "Epoch 932/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 5.1015e-04 - accuracy: 1.0000 - val_loss: 3.2445 - val_accuracy: 0.4977\n",
      "Epoch 933/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.3148e-04 - accuracy: 1.0000 - val_loss: 3.2589 - val_accuracy: 0.5068\n",
      "Epoch 934/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 5.8920e-04 - accuracy: 1.0000 - val_loss: 3.2637 - val_accuracy: 0.5068\n",
      "Epoch 935/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 4.9202e-04 - accuracy: 1.0000 - val_loss: 3.2609 - val_accuracy: 0.5023\n",
      "Epoch 936/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 4.4524e-04 - accuracy: 1.0000 - val_loss: 3.2588 - val_accuracy: 0.5068\n",
      "Epoch 937/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 4.6905e-04 - accuracy: 1.0000 - val_loss: 3.2750 - val_accuracy: 0.5023\n",
      "Epoch 938/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 4.7236e-04 - accuracy: 1.0000 - val_loss: 3.2835 - val_accuracy: 0.5068\n",
      "Epoch 939/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 4.3136e-04 - accuracy: 1.0000 - val_loss: 3.2881 - val_accuracy: 0.5068\n",
      "Epoch 940/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 4.2255e-04 - accuracy: 1.0000 - val_loss: 3.2875 - val_accuracy: 0.5113\n",
      "Epoch 941/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 4.9840e-04 - accuracy: 1.0000 - val_loss: 3.2941 - val_accuracy: 0.5113\n",
      "Epoch 942/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 4.2436e-04 - accuracy: 1.0000 - val_loss: 3.2957 - val_accuracy: 0.5068\n",
      "Epoch 943/1000\n",
      "1980/1980 [==============================] - 1s 444us/step - loss: 4.2289e-04 - accuracy: 1.0000 - val_loss: 3.3122 - val_accuracy: 0.5023\n",
      "Epoch 944/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 4.2854e-04 - accuracy: 1.0000 - val_loss: 3.3292 - val_accuracy: 0.5068\n",
      "Epoch 945/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 4.2641e-04 - accuracy: 1.0000 - val_loss: 3.3437 - val_accuracy: 0.5113\n",
      "Epoch 946/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 4.0657e-04 - accuracy: 1.0000 - val_loss: 3.3529 - val_accuracy: 0.5113\n",
      "Epoch 947/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 4.5749e-04 - accuracy: 1.0000 - val_loss: 3.3694 - val_accuracy: 0.5113\n",
      "Epoch 948/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 3.7566e-04 - accuracy: 1.0000 - val_loss: 3.3686 - val_accuracy: 0.5113\n",
      "Epoch 949/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 4.1126e-04 - accuracy: 1.0000 - val_loss: 3.3620 - val_accuracy: 0.5158\n",
      "Epoch 950/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 3.8953e-04 - accuracy: 1.0000 - val_loss: 3.3656 - val_accuracy: 0.5068\n",
      "Epoch 951/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 3.7500e-04 - accuracy: 1.0000 - val_loss: 3.3671 - val_accuracy: 0.5113\n",
      "Epoch 952/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 3.8774e-04 - accuracy: 1.0000 - val_loss: 3.3773 - val_accuracy: 0.5068\n",
      "Epoch 953/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 4.2568e-04 - accuracy: 1.0000 - val_loss: 3.3804 - val_accuracy: 0.5113\n",
      "Epoch 954/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 4.0414e-04 - accuracy: 1.0000 - val_loss: 3.4049 - val_accuracy: 0.5023\n",
      "Epoch 955/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 3.4638e-04 - accuracy: 1.0000 - val_loss: 3.4091 - val_accuracy: 0.5158\n",
      "Epoch 956/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 3.2326e-04 - accuracy: 1.0000 - val_loss: 3.4064 - val_accuracy: 0.5068\n",
      "Epoch 957/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.5578e-04 - accuracy: 1.0000 - val_loss: 3.4121 - val_accuracy: 0.5204\n",
      "Epoch 958/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 3.6187e-04 - accuracy: 1.0000 - val_loss: 3.4020 - val_accuracy: 0.5068\n",
      "Epoch 959/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.8485e-04 - accuracy: 1.0000 - val_loss: 3.4091 - val_accuracy: 0.5113\n",
      "Epoch 960/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 3.1397e-04 - accuracy: 1.0000 - val_loss: 3.4203 - val_accuracy: 0.5113\n",
      "Epoch 961/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 3.1910e-04 - accuracy: 1.0000 - val_loss: 3.4330 - val_accuracy: 0.5204\n",
      "Epoch 962/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 3.6544e-04 - accuracy: 1.0000 - val_loss: 3.4394 - val_accuracy: 0.5068\n",
      "Epoch 963/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 3.3775e-04 - accuracy: 1.0000 - val_loss: 3.4409 - val_accuracy: 0.5158\n",
      "Epoch 964/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 2.9551e-04 - accuracy: 1.0000 - val_loss: 3.4468 - val_accuracy: 0.5158\n",
      "Epoch 965/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 2.9903e-04 - accuracy: 1.0000 - val_loss: 3.4490 - val_accuracy: 0.5204\n",
      "Epoch 966/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.0186e-04 - accuracy: 1.0000 - val_loss: 3.4512 - val_accuracy: 0.5068\n",
      "Epoch 967/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 3.2495e-04 - accuracy: 1.0000 - val_loss: 3.4672 - val_accuracy: 0.5294\n",
      "Epoch 968/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 2.9743e-04 - accuracy: 1.0000 - val_loss: 3.4861 - val_accuracy: 0.5249\n",
      "Epoch 969/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 3.0877e-04 - accuracy: 1.0000 - val_loss: 3.4839 - val_accuracy: 0.5249\n",
      "Epoch 970/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 2.9910e-04 - accuracy: 1.0000 - val_loss: 3.4960 - val_accuracy: 0.5249\n",
      "Epoch 971/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 2.8428e-04 - accuracy: 1.0000 - val_loss: 3.5010 - val_accuracy: 0.5294\n",
      "Epoch 972/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 2.4530e-04 - accuracy: 1.0000 - val_loss: 3.5118 - val_accuracy: 0.5249\n",
      "Epoch 973/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 2.7523e-04 - accuracy: 1.0000 - val_loss: 3.5218 - val_accuracy: 0.5204\n",
      "Epoch 974/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 3.2784e-04 - accuracy: 1.0000 - val_loss: 3.5148 - val_accuracy: 0.5113\n",
      "Epoch 975/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 2.9739e-04 - accuracy: 1.0000 - val_loss: 3.5181 - val_accuracy: 0.5068\n",
      "Epoch 976/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 2.5520e-04 - accuracy: 1.0000 - val_loss: 3.5251 - val_accuracy: 0.5158\n",
      "Epoch 977/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.6538e-04 - accuracy: 1.0000 - val_loss: 3.5270 - val_accuracy: 0.5249\n",
      "Epoch 978/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 2.3795e-04 - accuracy: 1.0000 - val_loss: 3.5299 - val_accuracy: 0.5249\n",
      "Epoch 979/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 2.1640e-04 - accuracy: 1.0000 - val_loss: 3.5384 - val_accuracy: 0.5204\n",
      "Epoch 980/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 2.2730e-04 - accuracy: 1.0000 - val_loss: 3.5369 - val_accuracy: 0.5158\n",
      "Epoch 981/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 2.1804e-04 - accuracy: 1.0000 - val_loss: 3.5474 - val_accuracy: 0.5158\n",
      "Epoch 982/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 2.3688e-04 - accuracy: 1.0000 - val_loss: 3.5468 - val_accuracy: 0.5158\n",
      "Epoch 983/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 2.2824e-04 - accuracy: 1.0000 - val_loss: 3.5394 - val_accuracy: 0.5158\n",
      "Epoch 984/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 2.3989e-04 - accuracy: 1.0000 - val_loss: 3.5379 - val_accuracy: 0.5204\n",
      "Epoch 985/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 2.0062e-04 - accuracy: 1.0000 - val_loss: 3.5501 - val_accuracy: 0.5204\n",
      "Epoch 986/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 2.1452e-04 - accuracy: 1.0000 - val_loss: 3.5678 - val_accuracy: 0.5249\n",
      "Epoch 987/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.7527e-04 - accuracy: 1.0000 - val_loss: 3.5479 - val_accuracy: 0.5249\n",
      "Epoch 988/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 2.3466e-04 - accuracy: 1.0000 - val_loss: 3.5511 - val_accuracy: 0.5249\n",
      "Epoch 989/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 2.0440e-04 - accuracy: 1.0000 - val_loss: 3.5677 - val_accuracy: 0.5249\n",
      "Epoch 990/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.8792e-04 - accuracy: 1.0000 - val_loss: 3.5769 - val_accuracy: 0.5113\n",
      "Epoch 991/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 1.9570e-04 - accuracy: 1.0000 - val_loss: 3.5893 - val_accuracy: 0.5204\n",
      "Epoch 992/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 2.0352e-04 - accuracy: 1.0000 - val_loss: 3.5920 - val_accuracy: 0.5249\n",
      "Epoch 993/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 2.0617e-04 - accuracy: 1.0000 - val_loss: 3.6004 - val_accuracy: 0.5204\n",
      "Epoch 994/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 2.0289e-04 - accuracy: 1.0000 - val_loss: 3.6056 - val_accuracy: 0.5249\n",
      "Epoch 995/1000\n",
      "1980/1980 [==============================] - 1s 441us/step - loss: 1.9796e-04 - accuracy: 1.0000 - val_loss: 3.6152 - val_accuracy: 0.5204\n",
      "Epoch 996/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 2.0541e-04 - accuracy: 1.0000 - val_loss: 3.6233 - val_accuracy: 0.5249\n",
      "Epoch 997/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 1.9938e-04 - accuracy: 1.0000 - val_loss: 3.6256 - val_accuracy: 0.5249\n",
      "Epoch 998/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 1.9850e-04 - accuracy: 1.0000 - val_loss: 3.6199 - val_accuracy: 0.5249\n",
      "Epoch 999/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 1.9716e-04 - accuracy: 1.0000 - val_loss: 3.6285 - val_accuracy: 0.5249\n",
      "Epoch 1000/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 1.7939e-04 - accuracy: 1.0000 - val_loss: 3.6374 - val_accuracy: 0.5249\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 140us/step\n",
      "test loss, test acc: [3.57752053202415, 0.5061224699020386]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.6877859121440838\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_without_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=500, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(2, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_73 (LSTM)               (None, 2, 128)            113152    \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 2, 64)             49408     \n",
      "_________________________________________________________________\n",
      "lstm_75 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 175,009\n",
      "Trainable params: 175,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1991 samples, validate on 222 samples\n",
      "Epoch 1/1000\n",
      "1991/1991 [==============================] - 1s 649us/step - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6808 - val_accuracy: 0.6036\n",
      "Epoch 2/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6926 - accuracy: 0.5229 - val_loss: 0.6890 - val_accuracy: 0.6036\n",
      "Epoch 3/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6920 - accuracy: 0.5229 - val_loss: 0.6847 - val_accuracy: 0.6036\n",
      "Epoch 4/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6914 - accuracy: 0.5229 - val_loss: 0.6841 - val_accuracy: 0.6036\n",
      "Epoch 5/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6915 - accuracy: 0.5229 - val_loss: 0.6855 - val_accuracy: 0.6036\n",
      "Epoch 6/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6914 - accuracy: 0.5229 - val_loss: 0.6853 - val_accuracy: 0.5901\n",
      "Epoch 7/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6913 - accuracy: 0.5239 - val_loss: 0.6925 - val_accuracy: 0.5360\n",
      "Epoch 8/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.6913 - accuracy: 0.5299 - val_loss: 0.6867 - val_accuracy: 0.5766\n",
      "Epoch 9/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6917 - accuracy: 0.5158 - val_loss: 0.6876 - val_accuracy: 0.5766\n",
      "Epoch 10/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6904 - accuracy: 0.5304 - val_loss: 0.6882 - val_accuracy: 0.5586\n",
      "Epoch 11/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6908 - accuracy: 0.5208 - val_loss: 0.6899 - val_accuracy: 0.5360\n",
      "Epoch 12/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6905 - accuracy: 0.5279 - val_loss: 0.6981 - val_accuracy: 0.4550\n",
      "Epoch 13/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6911 - accuracy: 0.5349 - val_loss: 0.6875 - val_accuracy: 0.5676\n",
      "Epoch 14/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6899 - accuracy: 0.5349 - val_loss: 0.6885 - val_accuracy: 0.5541\n",
      "Epoch 15/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6898 - accuracy: 0.5289 - val_loss: 0.6915 - val_accuracy: 0.5225\n",
      "Epoch 16/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6898 - accuracy: 0.5369 - val_loss: 0.6888 - val_accuracy: 0.5450\n",
      "Epoch 17/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6894 - accuracy: 0.5374 - val_loss: 0.6949 - val_accuracy: 0.5180\n",
      "Epoch 18/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6905 - accuracy: 0.5419 - val_loss: 0.6841 - val_accuracy: 0.5721\n",
      "Epoch 19/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6897 - accuracy: 0.5450 - val_loss: 0.6900 - val_accuracy: 0.5495\n",
      "Epoch 20/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6889 - accuracy: 0.5409 - val_loss: 0.7005 - val_accuracy: 0.4865\n",
      "Epoch 21/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6873 - accuracy: 0.5455 - val_loss: 0.6920 - val_accuracy: 0.5315\n",
      "Epoch 22/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6888 - accuracy: 0.5334 - val_loss: 0.6893 - val_accuracy: 0.5360\n",
      "Epoch 23/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6867 - accuracy: 0.5455 - val_loss: 0.6932 - val_accuracy: 0.5270\n",
      "Epoch 24/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6870 - accuracy: 0.5445 - val_loss: 0.7109 - val_accuracy: 0.4595\n",
      "Epoch 25/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6865 - accuracy: 0.5309 - val_loss: 0.6821 - val_accuracy: 0.5946\n",
      "Epoch 26/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6863 - accuracy: 0.5374 - val_loss: 0.7085 - val_accuracy: 0.4189\n",
      "Epoch 27/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6904 - accuracy: 0.5324 - val_loss: 0.6972 - val_accuracy: 0.5270\n",
      "Epoch 28/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6874 - accuracy: 0.5565 - val_loss: 0.6898 - val_accuracy: 0.5450\n",
      "Epoch 29/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6843 - accuracy: 0.5510 - val_loss: 0.7095 - val_accuracy: 0.5045\n",
      "Epoch 30/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6867 - accuracy: 0.5460 - val_loss: 0.6985 - val_accuracy: 0.5090\n",
      "Epoch 31/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 0.6849 - accuracy: 0.5505 - val_loss: 0.6996 - val_accuracy: 0.5225\n",
      "Epoch 32/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6836 - accuracy: 0.5655 - val_loss: 0.7032 - val_accuracy: 0.5135\n",
      "Epoch 33/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6870 - accuracy: 0.5460 - val_loss: 0.7033 - val_accuracy: 0.4910\n",
      "Epoch 34/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6859 - accuracy: 0.5450 - val_loss: 0.7012 - val_accuracy: 0.5045\n",
      "Epoch 35/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.6847 - accuracy: 0.5555 - val_loss: 0.6924 - val_accuracy: 0.5450\n",
      "Epoch 36/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6840 - accuracy: 0.5535 - val_loss: 0.6856 - val_accuracy: 0.5721\n",
      "Epoch 37/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.6872 - accuracy: 0.5434 - val_loss: 0.7186 - val_accuracy: 0.4459\n",
      "Epoch 38/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6875 - accuracy: 0.5434 - val_loss: 0.6995 - val_accuracy: 0.4955\n",
      "Epoch 39/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6832 - accuracy: 0.5595 - val_loss: 0.7107 - val_accuracy: 0.4820\n",
      "Epoch 40/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 0.6821 - accuracy: 0.5645 - val_loss: 0.7083 - val_accuracy: 0.5000\n",
      "Epoch 41/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6831 - accuracy: 0.5515 - val_loss: 0.7206 - val_accuracy: 0.4369\n",
      "Epoch 42/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6852 - accuracy: 0.5555 - val_loss: 0.7057 - val_accuracy: 0.4865\n",
      "Epoch 43/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6817 - accuracy: 0.5605 - val_loss: 0.7089 - val_accuracy: 0.5000\n",
      "Epoch 44/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6824 - accuracy: 0.5580 - val_loss: 0.6968 - val_accuracy: 0.5180\n",
      "Epoch 45/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6816 - accuracy: 0.5671 - val_loss: 0.7041 - val_accuracy: 0.5000\n",
      "Epoch 46/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6784 - accuracy: 0.5620 - val_loss: 0.7056 - val_accuracy: 0.5405\n",
      "Epoch 47/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6799 - accuracy: 0.5655 - val_loss: 0.7092 - val_accuracy: 0.5090\n",
      "Epoch 48/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6816 - accuracy: 0.5550 - val_loss: 0.6952 - val_accuracy: 0.5450\n",
      "Epoch 49/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6789 - accuracy: 0.5605 - val_loss: 0.6971 - val_accuracy: 0.5270\n",
      "Epoch 50/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6768 - accuracy: 0.5761 - val_loss: 0.7278 - val_accuracy: 0.5135\n",
      "Epoch 51/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6752 - accuracy: 0.5610 - val_loss: 0.7269 - val_accuracy: 0.4685\n",
      "Epoch 52/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6753 - accuracy: 0.5681 - val_loss: 0.7396 - val_accuracy: 0.4685\n",
      "Epoch 53/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6748 - accuracy: 0.5751 - val_loss: 0.7335 - val_accuracy: 0.4865\n",
      "Epoch 54/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6819 - accuracy: 0.5660 - val_loss: 0.7047 - val_accuracy: 0.4955\n",
      "Epoch 55/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6808 - accuracy: 0.5665 - val_loss: 0.7142 - val_accuracy: 0.4820\n",
      "Epoch 56/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6803 - accuracy: 0.5660 - val_loss: 0.7243 - val_accuracy: 0.4640\n",
      "Epoch 57/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6814 - accuracy: 0.5480 - val_loss: 0.6841 - val_accuracy: 0.5721\n",
      "Epoch 58/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6770 - accuracy: 0.5691 - val_loss: 0.7072 - val_accuracy: 0.5135\n",
      "Epoch 59/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6752 - accuracy: 0.5741 - val_loss: 0.7195 - val_accuracy: 0.4955\n",
      "Epoch 60/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6712 - accuracy: 0.5726 - val_loss: 0.7293 - val_accuracy: 0.5090\n",
      "Epoch 61/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6763 - accuracy: 0.5681 - val_loss: 0.7440 - val_accuracy: 0.4324\n",
      "Epoch 62/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6785 - accuracy: 0.5701 - val_loss: 0.7308 - val_accuracy: 0.4640\n",
      "Epoch 63/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 0.6753 - accuracy: 0.5721 - val_loss: 0.7160 - val_accuracy: 0.4685\n",
      "Epoch 64/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6693 - accuracy: 0.5846 - val_loss: 0.7040 - val_accuracy: 0.5541\n",
      "Epoch 65/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6726 - accuracy: 0.5686 - val_loss: 0.7233 - val_accuracy: 0.4955\n",
      "Epoch 66/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6752 - accuracy: 0.5756 - val_loss: 0.7012 - val_accuracy: 0.5270\n",
      "Epoch 67/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6734 - accuracy: 0.5595 - val_loss: 0.6976 - val_accuracy: 0.5360\n",
      "Epoch 68/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6726 - accuracy: 0.5711 - val_loss: 0.7251 - val_accuracy: 0.5045\n",
      "Epoch 69/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6667 - accuracy: 0.5816 - val_loss: 0.7214 - val_accuracy: 0.4910\n",
      "Epoch 70/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6676 - accuracy: 0.5816 - val_loss: 0.7308 - val_accuracy: 0.4955\n",
      "Epoch 71/1000\n",
      "1991/1991 [==============================] - 0s 84us/step - loss: 0.6635 - accuracy: 0.5922 - val_loss: 0.7066 - val_accuracy: 0.5450\n",
      "Epoch 72/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.6659 - accuracy: 0.5786 - val_loss: 0.7154 - val_accuracy: 0.5225\n",
      "Epoch 73/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.6664 - accuracy: 0.5841 - val_loss: 0.6975 - val_accuracy: 0.5631\n",
      "Epoch 74/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6636 - accuracy: 0.5932 - val_loss: 0.7322 - val_accuracy: 0.4775\n",
      "Epoch 75/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6621 - accuracy: 0.5897 - val_loss: 0.7346 - val_accuracy: 0.5045\n",
      "Epoch 76/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6593 - accuracy: 0.5897 - val_loss: 0.7278 - val_accuracy: 0.4955\n",
      "Epoch 77/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.6563 - accuracy: 0.5922 - val_loss: 0.7472 - val_accuracy: 0.4865\n",
      "Epoch 78/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.6560 - accuracy: 0.6002 - val_loss: 0.7661 - val_accuracy: 0.4595\n",
      "Epoch 79/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6575 - accuracy: 0.6007 - val_loss: 0.7157 - val_accuracy: 0.5000\n",
      "Epoch 80/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6618 - accuracy: 0.6047 - val_loss: 0.7329 - val_accuracy: 0.5090\n",
      "Epoch 81/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6570 - accuracy: 0.5897 - val_loss: 0.7285 - val_accuracy: 0.4685\n",
      "Epoch 82/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6541 - accuracy: 0.6017 - val_loss: 0.7495 - val_accuracy: 0.5270\n",
      "Epoch 83/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6521 - accuracy: 0.5922 - val_loss: 0.7086 - val_accuracy: 0.5541\n",
      "Epoch 84/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6547 - accuracy: 0.5967 - val_loss: 0.7428 - val_accuracy: 0.4640\n",
      "Epoch 85/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6498 - accuracy: 0.6057 - val_loss: 0.7587 - val_accuracy: 0.4955\n",
      "Epoch 86/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.6528 - accuracy: 0.6012 - val_loss: 0.7321 - val_accuracy: 0.5000\n",
      "Epoch 87/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.6498 - accuracy: 0.6067 - val_loss: 0.7503 - val_accuracy: 0.4865\n",
      "Epoch 88/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6457 - accuracy: 0.6077 - val_loss: 0.7262 - val_accuracy: 0.5090\n",
      "Epoch 89/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6469 - accuracy: 0.6148 - val_loss: 0.7658 - val_accuracy: 0.4955\n",
      "Epoch 90/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6435 - accuracy: 0.6092 - val_loss: 0.7181 - val_accuracy: 0.5541\n",
      "Epoch 91/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6493 - accuracy: 0.5967 - val_loss: 0.7360 - val_accuracy: 0.5495\n",
      "Epoch 92/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6498 - accuracy: 0.6022 - val_loss: 0.7389 - val_accuracy: 0.5360\n",
      "Epoch 93/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6512 - accuracy: 0.5987 - val_loss: 0.7211 - val_accuracy: 0.5225\n",
      "Epoch 94/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6486 - accuracy: 0.6087 - val_loss: 0.7355 - val_accuracy: 0.5315\n",
      "Epoch 95/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6411 - accuracy: 0.6072 - val_loss: 0.7743 - val_accuracy: 0.5090\n",
      "Epoch 96/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.6422 - accuracy: 0.6123 - val_loss: 0.7802 - val_accuracy: 0.4910\n",
      "Epoch 97/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6288 - accuracy: 0.6298 - val_loss: 0.8008 - val_accuracy: 0.5180\n",
      "Epoch 98/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6324 - accuracy: 0.6268 - val_loss: 0.7652 - val_accuracy: 0.5000\n",
      "Epoch 99/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6319 - accuracy: 0.6354 - val_loss: 0.7997 - val_accuracy: 0.5090\n",
      "Epoch 100/1000\n",
      "1991/1991 [==============================] - 0s 85us/step - loss: 0.6336 - accuracy: 0.6183 - val_loss: 0.8150 - val_accuracy: 0.5045\n",
      "Epoch 101/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.6343 - accuracy: 0.6288 - val_loss: 0.7392 - val_accuracy: 0.5045\n",
      "Epoch 102/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6290 - accuracy: 0.6228 - val_loss: 0.7704 - val_accuracy: 0.5090\n",
      "Epoch 103/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6292 - accuracy: 0.6258 - val_loss: 0.8272 - val_accuracy: 0.5045\n",
      "Epoch 104/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.6196 - accuracy: 0.6459 - val_loss: 0.8167 - val_accuracy: 0.5360\n",
      "Epoch 105/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6248 - accuracy: 0.6318 - val_loss: 0.7575 - val_accuracy: 0.5270\n",
      "Epoch 106/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6279 - accuracy: 0.6138 - val_loss: 0.7900 - val_accuracy: 0.5135\n",
      "Epoch 107/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6168 - accuracy: 0.6479 - val_loss: 0.7818 - val_accuracy: 0.5225\n",
      "Epoch 108/1000\n",
      "1991/1991 [==============================] - 0s 86us/step - loss: 0.6162 - accuracy: 0.6374 - val_loss: 0.8623 - val_accuracy: 0.5180\n",
      "Epoch 109/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6078 - accuracy: 0.6444 - val_loss: 0.8157 - val_accuracy: 0.5045\n",
      "Epoch 110/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.6079 - accuracy: 0.6404 - val_loss: 0.8866 - val_accuracy: 0.5180\n",
      "Epoch 111/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6068 - accuracy: 0.6434 - val_loss: 0.8790 - val_accuracy: 0.4955\n",
      "Epoch 112/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.6056 - accuracy: 0.6444 - val_loss: 0.8513 - val_accuracy: 0.5045\n",
      "Epoch 113/1000\n",
      "1991/1991 [==============================] - 0s 87us/step - loss: 0.6070 - accuracy: 0.6459 - val_loss: 0.8513 - val_accuracy: 0.5315\n",
      "Epoch 114/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.6046 - accuracy: 0.6539 - val_loss: 0.8784 - val_accuracy: 0.4730\n",
      "Epoch 115/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.6068 - accuracy: 0.6519 - val_loss: 0.8078 - val_accuracy: 0.5315\n",
      "Epoch 116/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5935 - accuracy: 0.6620 - val_loss: 0.8505 - val_accuracy: 0.5135\n",
      "Epoch 117/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.5921 - accuracy: 0.6489 - val_loss: 0.8919 - val_accuracy: 0.5090\n",
      "Epoch 118/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5935 - accuracy: 0.6539 - val_loss: 0.8762 - val_accuracy: 0.5360\n",
      "Epoch 119/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5956 - accuracy: 0.6560 - val_loss: 0.8338 - val_accuracy: 0.4910\n",
      "Epoch 120/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.5825 - accuracy: 0.6700 - val_loss: 0.8724 - val_accuracy: 0.5270\n",
      "Epoch 121/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5730 - accuracy: 0.6765 - val_loss: 0.9802 - val_accuracy: 0.4820\n",
      "Epoch 122/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5862 - accuracy: 0.6544 - val_loss: 0.7746 - val_accuracy: 0.5721\n",
      "Epoch 123/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5890 - accuracy: 0.6655 - val_loss: 0.8821 - val_accuracy: 0.5135\n",
      "Epoch 124/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5806 - accuracy: 0.6690 - val_loss: 0.8962 - val_accuracy: 0.5315\n",
      "Epoch 125/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5712 - accuracy: 0.6715 - val_loss: 0.9990 - val_accuracy: 0.4865\n",
      "Epoch 126/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5672 - accuracy: 0.6781 - val_loss: 0.8713 - val_accuracy: 0.5090\n",
      "Epoch 127/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.5652 - accuracy: 0.6750 - val_loss: 0.9245 - val_accuracy: 0.5045\n",
      "Epoch 128/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.5638 - accuracy: 0.6816 - val_loss: 0.9005 - val_accuracy: 0.4820\n",
      "Epoch 129/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.5572 - accuracy: 0.6891 - val_loss: 1.0167 - val_accuracy: 0.5270\n",
      "Epoch 130/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5578 - accuracy: 0.6851 - val_loss: 0.8367 - val_accuracy: 0.5405\n",
      "Epoch 131/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.5642 - accuracy: 0.6806 - val_loss: 1.1225 - val_accuracy: 0.4640\n",
      "Epoch 132/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5599 - accuracy: 0.6891 - val_loss: 1.0350 - val_accuracy: 0.4775\n",
      "Epoch 133/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5518 - accuracy: 0.6791 - val_loss: 0.9967 - val_accuracy: 0.5000\n",
      "Epoch 134/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5355 - accuracy: 0.7067 - val_loss: 1.0473 - val_accuracy: 0.4685\n",
      "Epoch 135/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.5518 - accuracy: 0.6926 - val_loss: 0.9731 - val_accuracy: 0.4910\n",
      "Epoch 136/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5392 - accuracy: 0.7027 - val_loss: 1.0436 - val_accuracy: 0.4640\n",
      "Epoch 137/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5229 - accuracy: 0.7097 - val_loss: 1.1064 - val_accuracy: 0.4685\n",
      "Epoch 138/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.5244 - accuracy: 0.7182 - val_loss: 1.0478 - val_accuracy: 0.4865\n",
      "Epoch 139/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5375 - accuracy: 0.7067 - val_loss: 0.9570 - val_accuracy: 0.5135\n",
      "Epoch 140/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.5302 - accuracy: 0.7087 - val_loss: 1.0493 - val_accuracy: 0.5180\n",
      "Epoch 141/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.5209 - accuracy: 0.7042 - val_loss: 1.1489 - val_accuracy: 0.4595\n",
      "Epoch 142/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5382 - accuracy: 0.7082 - val_loss: 0.9750 - val_accuracy: 0.4910\n",
      "Epoch 143/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5224 - accuracy: 0.7202 - val_loss: 1.1524 - val_accuracy: 0.5000\n",
      "Epoch 144/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5046 - accuracy: 0.7243 - val_loss: 1.0260 - val_accuracy: 0.5405\n",
      "Epoch 145/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4945 - accuracy: 0.7293 - val_loss: 1.0310 - val_accuracy: 0.5135\n",
      "Epoch 146/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5009 - accuracy: 0.7243 - val_loss: 1.0762 - val_accuracy: 0.5135\n",
      "Epoch 147/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.4922 - accuracy: 0.7353 - val_loss: 1.1486 - val_accuracy: 0.4775\n",
      "Epoch 148/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.4809 - accuracy: 0.7343 - val_loss: 1.1298 - val_accuracy: 0.4910\n",
      "Epoch 149/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.4722 - accuracy: 0.7519 - val_loss: 1.0957 - val_accuracy: 0.5450\n",
      "Epoch 150/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.4901 - accuracy: 0.7363 - val_loss: 1.1952 - val_accuracy: 0.4955\n",
      "Epoch 151/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.4692 - accuracy: 0.7438 - val_loss: 1.1345 - val_accuracy: 0.5541\n",
      "Epoch 152/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.4876 - accuracy: 0.7408 - val_loss: 1.1751 - val_accuracy: 0.5135\n",
      "Epoch 153/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4739 - accuracy: 0.7388 - val_loss: 1.3066 - val_accuracy: 0.4910\n",
      "Epoch 154/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.4507 - accuracy: 0.7509 - val_loss: 1.2575 - val_accuracy: 0.4865\n",
      "Epoch 155/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.4475 - accuracy: 0.7619 - val_loss: 1.1957 - val_accuracy: 0.5045\n",
      "Epoch 156/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.4575 - accuracy: 0.7599 - val_loss: 1.1355 - val_accuracy: 0.5270\n",
      "Epoch 157/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4486 - accuracy: 0.7484 - val_loss: 1.3636 - val_accuracy: 0.4685\n",
      "Epoch 158/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4329 - accuracy: 0.7800 - val_loss: 1.0923 - val_accuracy: 0.5631\n",
      "Epoch 159/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4486 - accuracy: 0.7584 - val_loss: 1.4422 - val_accuracy: 0.4730\n",
      "Epoch 160/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.4467 - accuracy: 0.7644 - val_loss: 1.2449 - val_accuracy: 0.4820\n",
      "Epoch 161/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.4294 - accuracy: 0.7624 - val_loss: 1.2580 - val_accuracy: 0.5090\n",
      "Epoch 162/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.4126 - accuracy: 0.7835 - val_loss: 1.4442 - val_accuracy: 0.5045\n",
      "Epoch 163/1000\n",
      "1991/1991 [==============================] - 0s 91us/step - loss: 0.4182 - accuracy: 0.7740 - val_loss: 1.4445 - val_accuracy: 0.5000\n",
      "Epoch 164/1000\n",
      "1991/1991 [==============================] - 0s 93us/step - loss: 0.4327 - accuracy: 0.7715 - val_loss: 1.3832 - val_accuracy: 0.4955\n",
      "Epoch 165/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.4043 - accuracy: 0.7906 - val_loss: 1.3514 - val_accuracy: 0.5000\n",
      "Epoch 166/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.3968 - accuracy: 0.7951 - val_loss: 1.4267 - val_accuracy: 0.4775\n",
      "Epoch 167/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 0.3872 - accuracy: 0.7996 - val_loss: 1.5166 - val_accuracy: 0.4775\n",
      "Epoch 168/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.3869 - accuracy: 0.8011 - val_loss: 1.4841 - val_accuracy: 0.4775\n",
      "Epoch 169/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.3919 - accuracy: 0.7916 - val_loss: 1.4624 - val_accuracy: 0.5180\n",
      "Epoch 170/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.3927 - accuracy: 0.7891 - val_loss: 1.5455 - val_accuracy: 0.4910\n",
      "Epoch 171/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.3627 - accuracy: 0.8071 - val_loss: 1.5244 - val_accuracy: 0.4865\n",
      "Epoch 172/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.3663 - accuracy: 0.8011 - val_loss: 1.5125 - val_accuracy: 0.5000\n",
      "Epoch 173/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.3625 - accuracy: 0.8086 - val_loss: 1.4251 - val_accuracy: 0.4955\n",
      "Epoch 174/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.3669 - accuracy: 0.8137 - val_loss: 1.5586 - val_accuracy: 0.4775\n",
      "Epoch 175/1000\n",
      "1991/1991 [==============================] - 0s 87us/step - loss: 0.3939 - accuracy: 0.8041 - val_loss: 1.3618 - val_accuracy: 0.4775\n",
      "Epoch 176/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.3892 - accuracy: 0.8011 - val_loss: 1.5594 - val_accuracy: 0.4730\n",
      "Epoch 177/1000\n",
      "1991/1991 [==============================] - 0s 98us/step - loss: 0.3601 - accuracy: 0.8006 - val_loss: 1.5330 - val_accuracy: 0.5000\n",
      "Epoch 178/1000\n",
      "1991/1991 [==============================] - 0s 92us/step - loss: 0.3434 - accuracy: 0.8282 - val_loss: 1.6033 - val_accuracy: 0.5180\n",
      "Epoch 179/1000\n",
      "1991/1991 [==============================] - 0s 84us/step - loss: 0.3447 - accuracy: 0.8162 - val_loss: 1.4773 - val_accuracy: 0.5180\n",
      "Epoch 180/1000\n",
      "1991/1991 [==============================] - 0s 85us/step - loss: 0.3437 - accuracy: 0.8207 - val_loss: 1.7838 - val_accuracy: 0.5000\n",
      "Epoch 181/1000\n",
      "1991/1991 [==============================] - 0s 92us/step - loss: 0.3473 - accuracy: 0.8282 - val_loss: 1.6463 - val_accuracy: 0.5045\n",
      "Epoch 182/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.3397 - accuracy: 0.8363 - val_loss: 1.5133 - val_accuracy: 0.5000\n",
      "Epoch 183/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 0.3230 - accuracy: 0.8378 - val_loss: 1.6295 - val_accuracy: 0.4865\n",
      "Epoch 184/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.3236 - accuracy: 0.8332 - val_loss: 1.9671 - val_accuracy: 0.4775\n",
      "Epoch 185/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.3411 - accuracy: 0.8322 - val_loss: 1.5970 - val_accuracy: 0.5135\n",
      "Epoch 186/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.3611 - accuracy: 0.8167 - val_loss: 1.5864 - val_accuracy: 0.4910\n",
      "Epoch 187/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.3259 - accuracy: 0.8378 - val_loss: 1.8988 - val_accuracy: 0.4505\n",
      "Epoch 188/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.3000 - accuracy: 0.8463 - val_loss: 1.7125 - val_accuracy: 0.4865\n",
      "Epoch 189/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.2915 - accuracy: 0.8564 - val_loss: 1.7882 - val_accuracy: 0.5000\n",
      "Epoch 190/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.2868 - accuracy: 0.8599 - val_loss: 2.1914 - val_accuracy: 0.4910\n",
      "Epoch 191/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.2837 - accuracy: 0.8594 - val_loss: 1.9092 - val_accuracy: 0.5405\n",
      "Epoch 192/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.2894 - accuracy: 0.8559 - val_loss: 1.8336 - val_accuracy: 0.5090\n",
      "Epoch 193/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.2767 - accuracy: 0.8649 - val_loss: 1.9686 - val_accuracy: 0.5000\n",
      "Epoch 194/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.2626 - accuracy: 0.8689 - val_loss: 2.0268 - val_accuracy: 0.4775\n",
      "Epoch 195/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.2712 - accuracy: 0.8699 - val_loss: 1.9646 - val_accuracy: 0.4820\n",
      "Epoch 196/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2557 - accuracy: 0.8774 - val_loss: 2.0046 - val_accuracy: 0.4820\n",
      "Epoch 197/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2480 - accuracy: 0.8815 - val_loss: 1.8606 - val_accuracy: 0.5000\n",
      "Epoch 198/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2468 - accuracy: 0.8719 - val_loss: 1.8491 - val_accuracy: 0.5045\n",
      "Epoch 199/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2479 - accuracy: 0.8830 - val_loss: 2.0877 - val_accuracy: 0.5045\n",
      "Epoch 200/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2279 - accuracy: 0.8935 - val_loss: 2.0168 - val_accuracy: 0.4910\n",
      "Epoch 201/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2154 - accuracy: 0.8995 - val_loss: 2.1563 - val_accuracy: 0.4685\n",
      "Epoch 202/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.2102 - accuracy: 0.9016 - val_loss: 2.3479 - val_accuracy: 0.4955\n",
      "Epoch 203/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2118 - accuracy: 0.8985 - val_loss: 2.2331 - val_accuracy: 0.5135\n",
      "Epoch 204/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2112 - accuracy: 0.9021 - val_loss: 2.0577 - val_accuracy: 0.5000\n",
      "Epoch 205/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2252 - accuracy: 0.8935 - val_loss: 2.3679 - val_accuracy: 0.4820\n",
      "Epoch 206/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2118 - accuracy: 0.8985 - val_loss: 2.2374 - val_accuracy: 0.4685\n",
      "Epoch 207/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.1998 - accuracy: 0.9046 - val_loss: 2.4144 - val_accuracy: 0.4955\n",
      "Epoch 208/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2170 - accuracy: 0.9016 - val_loss: 2.2319 - val_accuracy: 0.4775\n",
      "Epoch 209/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1943 - accuracy: 0.9116 - val_loss: 2.3942 - val_accuracy: 0.4685\n",
      "Epoch 210/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.1988 - accuracy: 0.9051 - val_loss: 2.2616 - val_accuracy: 0.5000\n",
      "Epoch 211/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.1754 - accuracy: 0.9221 - val_loss: 2.4007 - val_accuracy: 0.5045\n",
      "Epoch 212/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1703 - accuracy: 0.9252 - val_loss: 2.3370 - val_accuracy: 0.5090\n",
      "Epoch 213/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1684 - accuracy: 0.9216 - val_loss: 2.3609 - val_accuracy: 0.4865\n",
      "Epoch 214/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1896 - accuracy: 0.9121 - val_loss: 2.3847 - val_accuracy: 0.5180\n",
      "Epoch 215/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1960 - accuracy: 0.9101 - val_loss: 2.7424 - val_accuracy: 0.4820\n",
      "Epoch 216/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1859 - accuracy: 0.9146 - val_loss: 2.3810 - val_accuracy: 0.5000\n",
      "Epoch 217/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.3040 - accuracy: 0.8805 - val_loss: 2.3664 - val_accuracy: 0.4910\n",
      "Epoch 218/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2514 - accuracy: 0.8825 - val_loss: 2.2155 - val_accuracy: 0.4910\n",
      "Epoch 219/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.2714 - accuracy: 0.8724 - val_loss: 2.5505 - val_accuracy: 0.4640\n",
      "Epoch 220/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.2032 - accuracy: 0.9071 - val_loss: 2.4113 - val_accuracy: 0.4550\n",
      "Epoch 221/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1752 - accuracy: 0.9191 - val_loss: 2.3828 - val_accuracy: 0.4685\n",
      "Epoch 222/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1541 - accuracy: 0.9327 - val_loss: 2.3953 - val_accuracy: 0.4730\n",
      "Epoch 223/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1633 - accuracy: 0.9267 - val_loss: 2.4023 - val_accuracy: 0.4820\n",
      "Epoch 224/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1490 - accuracy: 0.9302 - val_loss: 2.3291 - val_accuracy: 0.4550\n",
      "Epoch 225/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1396 - accuracy: 0.9392 - val_loss: 2.6581 - val_accuracy: 0.4775\n",
      "Epoch 226/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1384 - accuracy: 0.9347 - val_loss: 2.6431 - val_accuracy: 0.5180\n",
      "Epoch 227/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.1259 - accuracy: 0.9473 - val_loss: 2.4259 - val_accuracy: 0.5090\n",
      "Epoch 228/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.1254 - accuracy: 0.9432 - val_loss: 2.5703 - val_accuracy: 0.4910\n",
      "Epoch 229/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.1257 - accuracy: 0.9437 - val_loss: 2.5908 - val_accuracy: 0.4865\n",
      "Epoch 230/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1169 - accuracy: 0.9473 - val_loss: 2.5358 - val_accuracy: 0.5000\n",
      "Epoch 231/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1261 - accuracy: 0.9442 - val_loss: 2.5007 - val_accuracy: 0.4595\n",
      "Epoch 232/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.1265 - accuracy: 0.9412 - val_loss: 2.8292 - val_accuracy: 0.4459\n",
      "Epoch 233/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.1218 - accuracy: 0.9448 - val_loss: 2.6218 - val_accuracy: 0.4865\n",
      "Epoch 234/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.1078 - accuracy: 0.9553 - val_loss: 2.9229 - val_accuracy: 0.4505\n",
      "Epoch 235/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.1058 - accuracy: 0.9558 - val_loss: 2.6570 - val_accuracy: 0.4685\n",
      "Epoch 236/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.1007 - accuracy: 0.9623 - val_loss: 2.9096 - val_accuracy: 0.4595\n",
      "Epoch 237/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0909 - accuracy: 0.9628 - val_loss: 3.0406 - val_accuracy: 0.4459\n",
      "Epoch 238/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0991 - accuracy: 0.9613 - val_loss: 2.9507 - val_accuracy: 0.4595\n",
      "Epoch 239/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0963 - accuracy: 0.9618 - val_loss: 2.9097 - val_accuracy: 0.4595\n",
      "Epoch 240/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0960 - accuracy: 0.9613 - val_loss: 2.7658 - val_accuracy: 0.4820\n",
      "Epoch 241/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0925 - accuracy: 0.9618 - val_loss: 2.8592 - val_accuracy: 0.4369\n",
      "Epoch 242/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.1249 - accuracy: 0.9463 - val_loss: 2.8395 - val_accuracy: 0.4685\n",
      "Epoch 243/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1557 - accuracy: 0.9337 - val_loss: 3.0727 - val_accuracy: 0.4414\n",
      "Epoch 244/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1423 - accuracy: 0.9397 - val_loss: 2.9412 - val_accuracy: 0.4414\n",
      "Epoch 245/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1811 - accuracy: 0.9242 - val_loss: 3.1041 - val_accuracy: 0.4640\n",
      "Epoch 246/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1481 - accuracy: 0.9367 - val_loss: 2.9614 - val_accuracy: 0.4820\n",
      "Epoch 247/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1333 - accuracy: 0.9458 - val_loss: 2.7609 - val_accuracy: 0.4640\n",
      "Epoch 248/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1162 - accuracy: 0.9558 - val_loss: 2.9329 - val_accuracy: 0.4955\n",
      "Epoch 249/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.1536 - accuracy: 0.9297 - val_loss: 2.9442 - val_accuracy: 0.4505\n",
      "Epoch 250/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.1014 - accuracy: 0.9598 - val_loss: 2.8709 - val_accuracy: 0.4550\n",
      "Epoch 251/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0802 - accuracy: 0.9714 - val_loss: 2.8916 - val_accuracy: 0.4640\n",
      "Epoch 252/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0717 - accuracy: 0.9734 - val_loss: 2.9432 - val_accuracy: 0.4730\n",
      "Epoch 253/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0671 - accuracy: 0.9729 - val_loss: 3.1571 - val_accuracy: 0.4595\n",
      "Epoch 254/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0686 - accuracy: 0.9729 - val_loss: 2.9323 - val_accuracy: 0.4640\n",
      "Epoch 255/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0658 - accuracy: 0.9779 - val_loss: 3.0132 - val_accuracy: 0.4775\n",
      "Epoch 256/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0682 - accuracy: 0.9729 - val_loss: 3.1058 - val_accuracy: 0.4685\n",
      "Epoch 257/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0589 - accuracy: 0.9789 - val_loss: 3.2389 - val_accuracy: 0.4820\n",
      "Epoch 258/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0688 - accuracy: 0.9709 - val_loss: 3.2666 - val_accuracy: 0.4550\n",
      "Epoch 259/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0634 - accuracy: 0.9759 - val_loss: 2.8962 - val_accuracy: 0.4910\n",
      "Epoch 260/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0643 - accuracy: 0.9774 - val_loss: 3.0496 - val_accuracy: 0.4685\n",
      "Epoch 261/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0656 - accuracy: 0.9764 - val_loss: 3.1212 - val_accuracy: 0.4820\n",
      "Epoch 262/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0607 - accuracy: 0.9789 - val_loss: 3.2682 - val_accuracy: 0.4550\n",
      "Epoch 263/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0578 - accuracy: 0.9789 - val_loss: 3.3061 - val_accuracy: 0.4505\n",
      "Epoch 264/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0515 - accuracy: 0.9839 - val_loss: 3.2566 - val_accuracy: 0.4595\n",
      "Epoch 265/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0454 - accuracy: 0.9834 - val_loss: 3.2222 - val_accuracy: 0.4820\n",
      "Epoch 266/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0456 - accuracy: 0.9844 - val_loss: 3.4036 - val_accuracy: 0.4775\n",
      "Epoch 267/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 3.4670 - val_accuracy: 0.4640\n",
      "Epoch 268/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0463 - accuracy: 0.9839 - val_loss: 3.3354 - val_accuracy: 0.4730\n",
      "Epoch 269/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0521 - accuracy: 0.9819 - val_loss: 3.3372 - val_accuracy: 0.4730\n",
      "Epoch 270/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0465 - accuracy: 0.9844 - val_loss: 3.1580 - val_accuracy: 0.4955\n",
      "Epoch 271/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0572 - accuracy: 0.9829 - val_loss: 3.5745 - val_accuracy: 0.4550\n",
      "Epoch 272/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0407 - accuracy: 0.9884 - val_loss: 3.5753 - val_accuracy: 0.4640\n",
      "Epoch 273/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0421 - accuracy: 0.9874 - val_loss: 3.4361 - val_accuracy: 0.4775\n",
      "Epoch 274/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0773 - accuracy: 0.9744 - val_loss: 3.4450 - val_accuracy: 0.4775\n",
      "Epoch 275/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1818 - accuracy: 0.9277 - val_loss: 3.2149 - val_accuracy: 0.4865\n",
      "Epoch 276/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.2391 - accuracy: 0.9101 - val_loss: 3.3608 - val_accuracy: 0.4369\n",
      "Epoch 277/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.2924 - accuracy: 0.9021 - val_loss: 3.0207 - val_accuracy: 0.5090\n",
      "Epoch 278/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.2022 - accuracy: 0.9216 - val_loss: 2.8092 - val_accuracy: 0.4685\n",
      "Epoch 279/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.1143 - accuracy: 0.9493 - val_loss: 2.7220 - val_accuracy: 0.5180\n",
      "Epoch 280/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0858 - accuracy: 0.9674 - val_loss: 3.0443 - val_accuracy: 0.4595\n",
      "Epoch 281/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0727 - accuracy: 0.9724 - val_loss: 3.3188 - val_accuracy: 0.4459\n",
      "Epoch 282/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0549 - accuracy: 0.9819 - val_loss: 3.0259 - val_accuracy: 0.4595\n",
      "Epoch 283/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0531 - accuracy: 0.9824 - val_loss: 3.2419 - val_accuracy: 0.4369\n",
      "Epoch 284/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.0489 - accuracy: 0.9839 - val_loss: 3.1858 - val_accuracy: 0.4595\n",
      "Epoch 285/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0491 - accuracy: 0.9854 - val_loss: 3.3575 - val_accuracy: 0.4505\n",
      "Epoch 286/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.0409 - accuracy: 0.9884 - val_loss: 3.3464 - val_accuracy: 0.4414\n",
      "Epoch 287/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.0335 - accuracy: 0.9910 - val_loss: 3.2874 - val_accuracy: 0.4685\n",
      "Epoch 288/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0307 - accuracy: 0.9930 - val_loss: 3.4231 - val_accuracy: 0.4730\n",
      "Epoch 289/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0347 - accuracy: 0.9879 - val_loss: 3.5325 - val_accuracy: 0.4910\n",
      "Epoch 290/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0268 - accuracy: 0.9915 - val_loss: 3.6094 - val_accuracy: 0.4640\n",
      "Epoch 291/1000\n",
      "1991/1991 [==============================] - 0s 83us/step - loss: 0.0223 - accuracy: 0.9960 - val_loss: 3.5354 - val_accuracy: 0.4685\n",
      "Epoch 292/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0235 - accuracy: 0.9945 - val_loss: 3.6401 - val_accuracy: 0.4595\n",
      "Epoch 293/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.0251 - accuracy: 0.9930 - val_loss: 3.5318 - val_accuracy: 0.4685\n",
      "Epoch 294/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0200 - accuracy: 0.9950 - val_loss: 3.5660 - val_accuracy: 0.4775\n",
      "Epoch 295/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 3.4728 - val_accuracy: 0.4910\n",
      "Epoch 296/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0195 - accuracy: 0.9945 - val_loss: 3.6987 - val_accuracy: 0.4685\n",
      "Epoch 297/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.0225 - accuracy: 0.9925 - val_loss: 3.7773 - val_accuracy: 0.4685\n",
      "Epoch 298/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0269 - accuracy: 0.9905 - val_loss: 3.7102 - val_accuracy: 0.4459\n",
      "Epoch 299/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0275 - accuracy: 0.9930 - val_loss: 3.7818 - val_accuracy: 0.4730\n",
      "Epoch 300/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0282 - accuracy: 0.9920 - val_loss: 3.5984 - val_accuracy: 0.4730\n",
      "Epoch 301/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0334 - accuracy: 0.9890 - val_loss: 3.6033 - val_accuracy: 0.5000\n",
      "Epoch 302/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0472 - accuracy: 0.9824 - val_loss: 3.9747 - val_accuracy: 0.4414\n",
      "Epoch 303/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0437 - accuracy: 0.9859 - val_loss: 3.5724 - val_accuracy: 0.4820\n",
      "Epoch 304/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0324 - accuracy: 0.9864 - val_loss: 3.7181 - val_accuracy: 0.4685\n",
      "Epoch 305/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0311 - accuracy: 0.9915 - val_loss: 3.8947 - val_accuracy: 0.4730\n",
      "Epoch 306/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0224 - accuracy: 0.9935 - val_loss: 3.9662 - val_accuracy: 0.4775\n",
      "Epoch 307/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0254 - accuracy: 0.9930 - val_loss: 4.0038 - val_accuracy: 0.4685\n",
      "Epoch 308/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0259 - accuracy: 0.9925 - val_loss: 3.9059 - val_accuracy: 0.4775\n",
      "Epoch 309/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 0.0162 - accuracy: 0.9965 - val_loss: 4.1274 - val_accuracy: 0.4369\n",
      "Epoch 310/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0201 - accuracy: 0.9940 - val_loss: 3.8639 - val_accuracy: 0.4865\n",
      "Epoch 311/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0172 - accuracy: 0.9950 - val_loss: 3.9880 - val_accuracy: 0.4730\n",
      "Epoch 312/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0203 - accuracy: 0.9930 - val_loss: 4.2837 - val_accuracy: 0.4550\n",
      "Epoch 313/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0196 - accuracy: 0.9940 - val_loss: 3.8785 - val_accuracy: 0.4595\n",
      "Epoch 314/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0136 - accuracy: 0.9975 - val_loss: 3.8958 - val_accuracy: 0.4640\n",
      "Epoch 315/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0111 - accuracy: 0.9980 - val_loss: 4.2061 - val_accuracy: 0.4595\n",
      "Epoch 316/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0098 - accuracy: 0.9980 - val_loss: 4.1377 - val_accuracy: 0.4595\n",
      "Epoch 317/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0088 - accuracy: 0.9990 - val_loss: 4.1454 - val_accuracy: 0.4730\n",
      "Epoch 318/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 4.2496 - val_accuracy: 0.4775\n",
      "Epoch 319/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0079 - accuracy: 0.9990 - val_loss: 4.1343 - val_accuracy: 0.4685\n",
      "Epoch 320/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0070 - accuracy: 0.9990 - val_loss: 4.1833 - val_accuracy: 0.4685\n",
      "Epoch 321/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 4.2020 - val_accuracy: 0.4595\n",
      "Epoch 322/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 4.2202 - val_accuracy: 0.4730\n",
      "Epoch 323/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 4.2827 - val_accuracy: 0.4640\n",
      "Epoch 324/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 4.2115 - val_accuracy: 0.4595\n",
      "Epoch 325/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 4.2326 - val_accuracy: 0.4820\n",
      "Epoch 326/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 4.3583 - val_accuracy: 0.4685\n",
      "Epoch 327/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 4.3657 - val_accuracy: 0.4730\n",
      "Epoch 328/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 4.2481 - val_accuracy: 0.4550\n",
      "Epoch 329/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 4.3161 - val_accuracy: 0.4730\n",
      "Epoch 330/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 4.3974 - val_accuracy: 0.4640\n",
      "Epoch 331/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 4.3662 - val_accuracy: 0.4775\n",
      "Epoch 332/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 4.2996 - val_accuracy: 0.4685\n",
      "Epoch 333/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 4.3798 - val_accuracy: 0.4685\n",
      "Epoch 334/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 4.2659 - val_accuracy: 0.4505\n",
      "Epoch 335/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 4.3653 - val_accuracy: 0.4640\n",
      "Epoch 336/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 4.4885 - val_accuracy: 0.4595\n",
      "Epoch 337/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 4.2806 - val_accuracy: 0.4550\n",
      "Epoch 338/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 4.4012 - val_accuracy: 0.4550\n",
      "Epoch 339/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 4.2792 - val_accuracy: 0.4550\n",
      "Epoch 340/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0088 - accuracy: 0.9980 - val_loss: 4.2643 - val_accuracy: 0.4595\n",
      "Epoch 341/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 4.1492 - val_accuracy: 0.4550\n",
      "Epoch 342/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0289 - accuracy: 0.9910 - val_loss: 4.4506 - val_accuracy: 0.4820\n",
      "Epoch 343/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0398 - accuracy: 0.9834 - val_loss: 4.3439 - val_accuracy: 0.4505\n",
      "Epoch 344/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.1183 - accuracy: 0.9658 - val_loss: 3.8214 - val_accuracy: 0.4685\n",
      "Epoch 345/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.3076 - accuracy: 0.9121 - val_loss: 3.9643 - val_accuracy: 0.4730\n",
      "Epoch 346/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.3965 - accuracy: 0.8704 - val_loss: 2.6574 - val_accuracy: 0.5135\n",
      "Epoch 347/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2365 - accuracy: 0.9091 - val_loss: 3.5678 - val_accuracy: 0.4414\n",
      "Epoch 348/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1687 - accuracy: 0.9453 - val_loss: 3.4551 - val_accuracy: 0.4279\n",
      "Epoch 349/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0777 - accuracy: 0.9749 - val_loss: 3.4205 - val_accuracy: 0.4414\n",
      "Epoch 350/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0447 - accuracy: 0.9879 - val_loss: 3.6250 - val_accuracy: 0.4369\n",
      "Epoch 351/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0289 - accuracy: 0.9945 - val_loss: 3.7655 - val_accuracy: 0.4550\n",
      "Epoch 352/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0192 - accuracy: 0.9970 - val_loss: 3.7618 - val_accuracy: 0.4369\n",
      "Epoch 353/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 3.7720 - val_accuracy: 0.4369\n",
      "Epoch 354/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0092 - accuracy: 0.9995 - val_loss: 3.8363 - val_accuracy: 0.4414\n",
      "Epoch 355/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0077 - accuracy: 0.9995 - val_loss: 3.9151 - val_accuracy: 0.4640\n",
      "Epoch 356/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 3.9469 - val_accuracy: 0.4505\n",
      "Epoch 357/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 3.9744 - val_accuracy: 0.4595\n",
      "Epoch 358/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 4.0100 - val_accuracy: 0.4640\n",
      "Epoch 359/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0052 - accuracy: 0.9995 - val_loss: 4.0552 - val_accuracy: 0.4640\n",
      "Epoch 360/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 4.0252 - val_accuracy: 0.4595\n",
      "Epoch 361/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 4.0860 - val_accuracy: 0.4640\n",
      "Epoch 362/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 4.0701 - val_accuracy: 0.4685\n",
      "Epoch 363/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 4.0923 - val_accuracy: 0.4685\n",
      "Epoch 364/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 4.1175 - val_accuracy: 0.4685\n",
      "Epoch 365/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 4.1293 - val_accuracy: 0.4640\n",
      "Epoch 366/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.1602 - val_accuracy: 0.4685\n",
      "Epoch 367/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 4.1680 - val_accuracy: 0.4550\n",
      "Epoch 368/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 4.1645 - val_accuracy: 0.4595\n",
      "Epoch 369/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.2192 - val_accuracy: 0.4640\n",
      "Epoch 370/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 4.2507 - val_accuracy: 0.4550\n",
      "Epoch 371/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 4.2567 - val_accuracy: 0.4640\n",
      "Epoch 372/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.2897 - val_accuracy: 0.4685\n",
      "Epoch 373/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.2946 - val_accuracy: 0.4640\n",
      "Epoch 374/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.3033 - val_accuracy: 0.4595\n",
      "Epoch 375/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.3337 - val_accuracy: 0.4685\n",
      "Epoch 376/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.3346 - val_accuracy: 0.4595\n",
      "Epoch 377/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.3328 - val_accuracy: 0.4505\n",
      "Epoch 378/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.3622 - val_accuracy: 0.4730\n",
      "Epoch 379/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.3835 - val_accuracy: 0.4730\n",
      "Epoch 380/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.3704 - val_accuracy: 0.4640\n",
      "Epoch 381/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.3998 - val_accuracy: 0.4640\n",
      "Epoch 382/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.4212 - val_accuracy: 0.4595\n",
      "Epoch 383/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.4328 - val_accuracy: 0.4640\n",
      "Epoch 384/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.3632 - val_accuracy: 0.4640\n",
      "Epoch 385/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.4513 - val_accuracy: 0.4685\n",
      "Epoch 386/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.4530 - val_accuracy: 0.4640\n",
      "Epoch 387/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.4757 - val_accuracy: 0.4595\n",
      "Epoch 388/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.4890 - val_accuracy: 0.4640\n",
      "Epoch 389/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.4723 - val_accuracy: 0.4640\n",
      "Epoch 390/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.5038 - val_accuracy: 0.4595\n",
      "Epoch 391/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.4936 - val_accuracy: 0.4685\n",
      "Epoch 392/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.5162 - val_accuracy: 0.4640\n",
      "Epoch 393/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.5331 - val_accuracy: 0.4685\n",
      "Epoch 394/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.5375 - val_accuracy: 0.4550\n",
      "Epoch 395/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.5443 - val_accuracy: 0.4550\n",
      "Epoch 396/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.5664 - val_accuracy: 0.4550\n",
      "Epoch 397/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.5581 - val_accuracy: 0.4550\n",
      "Epoch 398/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.6116 - val_accuracy: 0.4685\n",
      "Epoch 399/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.5452 - val_accuracy: 0.4505\n",
      "Epoch 400/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.5953 - val_accuracy: 0.4640\n",
      "Epoch 401/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.5889 - val_accuracy: 0.4550\n",
      "Epoch 402/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.5960 - val_accuracy: 0.4550\n",
      "Epoch 403/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.6199 - val_accuracy: 0.4730\n",
      "Epoch 404/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.5922 - val_accuracy: 0.4595\n",
      "Epoch 405/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.6425 - val_accuracy: 0.4685\n",
      "Epoch 406/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.6473 - val_accuracy: 0.4595\n",
      "Epoch 407/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6415 - val_accuracy: 0.4595\n",
      "Epoch 408/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6772 - val_accuracy: 0.4640\n",
      "Epoch 409/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.5304e-04 - accuracy: 1.0000 - val_loss: 4.6666 - val_accuracy: 0.4595\n",
      "Epoch 410/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.9247e-04 - accuracy: 1.0000 - val_loss: 4.6826 - val_accuracy: 0.4640\n",
      "Epoch 411/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.4449e-04 - accuracy: 1.0000 - val_loss: 4.6937 - val_accuracy: 0.4550\n",
      "Epoch 412/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 9.5975e-04 - accuracy: 1.0000 - val_loss: 4.7091 - val_accuracy: 0.4640\n",
      "Epoch 413/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 9.0070e-04 - accuracy: 1.0000 - val_loss: 4.7089 - val_accuracy: 0.4550\n",
      "Epoch 414/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 9.6735e-04 - accuracy: 1.0000 - val_loss: 4.7155 - val_accuracy: 0.4550\n",
      "Epoch 415/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.6182e-04 - accuracy: 1.0000 - val_loss: 4.7320 - val_accuracy: 0.4550\n",
      "Epoch 416/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.4449e-04 - accuracy: 1.0000 - val_loss: 4.7316 - val_accuracy: 0.4595\n",
      "Epoch 417/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.2456e-04 - accuracy: 1.0000 - val_loss: 4.7377 - val_accuracy: 0.4595\n",
      "Epoch 418/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 8.2514e-04 - accuracy: 1.0000 - val_loss: 4.7501 - val_accuracy: 0.4640\n",
      "Epoch 419/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.2305e-04 - accuracy: 1.0000 - val_loss: 4.7584 - val_accuracy: 0.4550\n",
      "Epoch 420/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 7.6424e-04 - accuracy: 1.0000 - val_loss: 4.7695 - val_accuracy: 0.4595\n",
      "Epoch 421/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 8.6323e-04 - accuracy: 1.0000 - val_loss: 4.7710 - val_accuracy: 0.4595\n",
      "Epoch 422/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 7.6377e-04 - accuracy: 1.0000 - val_loss: 4.7818 - val_accuracy: 0.4640\n",
      "Epoch 423/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.7084e-04 - accuracy: 1.0000 - val_loss: 4.8107 - val_accuracy: 0.4640\n",
      "Epoch 424/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.1213e-04 - accuracy: 1.0000 - val_loss: 4.7988 - val_accuracy: 0.4640\n",
      "Epoch 425/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 7.7380e-04 - accuracy: 1.0000 - val_loss: 4.8086 - val_accuracy: 0.4595\n",
      "Epoch 426/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 6.8453e-04 - accuracy: 1.0000 - val_loss: 4.8170 - val_accuracy: 0.4640\n",
      "Epoch 427/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.6371e-04 - accuracy: 1.0000 - val_loss: 4.8219 - val_accuracy: 0.4640\n",
      "Epoch 428/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 6.6048e-04 - accuracy: 1.0000 - val_loss: 4.8273 - val_accuracy: 0.4595\n",
      "Epoch 429/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 7.3277e-04 - accuracy: 1.0000 - val_loss: 4.8353 - val_accuracy: 0.4640\n",
      "Epoch 430/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 7.8540e-04 - accuracy: 1.0000 - val_loss: 4.8260 - val_accuracy: 0.4640\n",
      "Epoch 431/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.5989e-04 - accuracy: 1.0000 - val_loss: 4.8420 - val_accuracy: 0.4730\n",
      "Epoch 432/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 7.5087e-04 - accuracy: 1.0000 - val_loss: 4.8378 - val_accuracy: 0.4640\n",
      "Epoch 433/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.6046e-04 - accuracy: 1.0000 - val_loss: 4.8487 - val_accuracy: 0.4640\n",
      "Epoch 434/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.1101e-04 - accuracy: 1.0000 - val_loss: 4.8574 - val_accuracy: 0.4640\n",
      "Epoch 435/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 5.9923e-04 - accuracy: 1.0000 - val_loss: 4.8703 - val_accuracy: 0.4685\n",
      "Epoch 436/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 6.3060e-04 - accuracy: 1.0000 - val_loss: 4.8816 - val_accuracy: 0.4640\n",
      "Epoch 437/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.8097e-04 - accuracy: 1.0000 - val_loss: 4.8890 - val_accuracy: 0.4640\n",
      "Epoch 438/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.0771e-04 - accuracy: 1.0000 - val_loss: 4.8975 - val_accuracy: 0.4640\n",
      "Epoch 439/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.6760e-04 - accuracy: 1.0000 - val_loss: 4.9106 - val_accuracy: 0.4595\n",
      "Epoch 440/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.0776e-04 - accuracy: 1.0000 - val_loss: 4.9264 - val_accuracy: 0.4595\n",
      "Epoch 441/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.3980e-04 - accuracy: 1.0000 - val_loss: 4.9215 - val_accuracy: 0.4640\n",
      "Epoch 442/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.3859e-04 - accuracy: 1.0000 - val_loss: 4.9168 - val_accuracy: 0.4595\n",
      "Epoch 443/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 5.7444e-04 - accuracy: 1.0000 - val_loss: 4.9330 - val_accuracy: 0.4640\n",
      "Epoch 444/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.0314e-04 - accuracy: 1.0000 - val_loss: 4.9412 - val_accuracy: 0.4640\n",
      "Epoch 445/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.2658e-04 - accuracy: 1.0000 - val_loss: 4.9504 - val_accuracy: 0.4595\n",
      "Epoch 446/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.1838e-04 - accuracy: 1.0000 - val_loss: 4.9248 - val_accuracy: 0.4595\n",
      "Epoch 447/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 5.2462e-04 - accuracy: 1.0000 - val_loss: 4.9434 - val_accuracy: 0.4640\n",
      "Epoch 448/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 5.2809e-04 - accuracy: 1.0000 - val_loss: 4.9455 - val_accuracy: 0.4685\n",
      "Epoch 449/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 4.6537e-04 - accuracy: 1.0000 - val_loss: 4.9605 - val_accuracy: 0.4640\n",
      "Epoch 450/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.6808e-04 - accuracy: 1.0000 - val_loss: 4.9694 - val_accuracy: 0.4640\n",
      "Epoch 451/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 5.6059e-04 - accuracy: 1.0000 - val_loss: 4.9631 - val_accuracy: 0.4640\n",
      "Epoch 452/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 4.6875e-04 - accuracy: 1.0000 - val_loss: 4.9941 - val_accuracy: 0.4640\n",
      "Epoch 453/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 4.8128e-04 - accuracy: 1.0000 - val_loss: 4.9990 - val_accuracy: 0.4685\n",
      "Epoch 454/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 4.6628e-04 - accuracy: 1.0000 - val_loss: 4.9980 - val_accuracy: 0.4595\n",
      "Epoch 455/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 4.4619e-04 - accuracy: 1.0000 - val_loss: 5.0099 - val_accuracy: 0.4640\n",
      "Epoch 456/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 5.2978e-04 - accuracy: 1.0000 - val_loss: 5.0138 - val_accuracy: 0.4685\n",
      "Epoch 457/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 4.3713e-04 - accuracy: 1.0000 - val_loss: 4.9973 - val_accuracy: 0.4640\n",
      "Epoch 458/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 4.2352e-04 - accuracy: 1.0000 - val_loss: 5.0237 - val_accuracy: 0.4640\n",
      "Epoch 459/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.9705e-04 - accuracy: 1.0000 - val_loss: 5.0083 - val_accuracy: 0.4550\n",
      "Epoch 460/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 4.4802e-04 - accuracy: 1.0000 - val_loss: 5.0286 - val_accuracy: 0.4685\n",
      "Epoch 461/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 5.8355e-04 - accuracy: 1.0000 - val_loss: 5.0480 - val_accuracy: 0.4595\n",
      "Epoch 462/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 5.2886e-04 - accuracy: 1.0000 - val_loss: 5.0749 - val_accuracy: 0.4595\n",
      "Epoch 463/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 4.4288e-04 - accuracy: 1.0000 - val_loss: 5.0752 - val_accuracy: 0.4550\n",
      "Epoch 464/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 3.8879e-04 - accuracy: 1.0000 - val_loss: 5.0859 - val_accuracy: 0.4595\n",
      "Epoch 465/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 3.7291e-04 - accuracy: 1.0000 - val_loss: 5.0787 - val_accuracy: 0.4640\n",
      "Epoch 466/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 4.0916e-04 - accuracy: 1.0000 - val_loss: 5.0786 - val_accuracy: 0.4640\n",
      "Epoch 467/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 3.9835e-04 - accuracy: 1.0000 - val_loss: 5.0838 - val_accuracy: 0.4640\n",
      "Epoch 468/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.5797e-04 - accuracy: 1.0000 - val_loss: 5.1039 - val_accuracy: 0.4640\n",
      "Epoch 469/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.6173e-04 - accuracy: 1.0000 - val_loss: 5.1117 - val_accuracy: 0.4640\n",
      "Epoch 470/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.5449e-04 - accuracy: 1.0000 - val_loss: 5.1062 - val_accuracy: 0.4595\n",
      "Epoch 471/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.1887e-04 - accuracy: 1.0000 - val_loss: 5.1134 - val_accuracy: 0.4640\n",
      "Epoch 472/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 3.3644e-04 - accuracy: 1.0000 - val_loss: 5.1068 - val_accuracy: 0.4640\n",
      "Epoch 473/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.5044e-04 - accuracy: 1.0000 - val_loss: 5.1074 - val_accuracy: 0.4685\n",
      "Epoch 474/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.5320e-04 - accuracy: 1.0000 - val_loss: 5.1178 - val_accuracy: 0.4685\n",
      "Epoch 475/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 3.2760e-04 - accuracy: 1.0000 - val_loss: 5.1419 - val_accuracy: 0.4685\n",
      "Epoch 476/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.5672e-04 - accuracy: 1.0000 - val_loss: 5.1350 - val_accuracy: 0.4640\n",
      "Epoch 477/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.3138e-04 - accuracy: 1.0000 - val_loss: 5.1336 - val_accuracy: 0.4595\n",
      "Epoch 478/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.8509e-04 - accuracy: 1.0000 - val_loss: 5.1518 - val_accuracy: 0.4595\n",
      "Epoch 479/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.3334e-04 - accuracy: 1.0000 - val_loss: 5.1793 - val_accuracy: 0.4640\n",
      "Epoch 480/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.6307e-04 - accuracy: 1.0000 - val_loss: 5.1604 - val_accuracy: 0.4550\n",
      "Epoch 481/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.1248e-04 - accuracy: 1.0000 - val_loss: 5.1749 - val_accuracy: 0.4595\n",
      "Epoch 482/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.0102e-04 - accuracy: 1.0000 - val_loss: 5.1852 - val_accuracy: 0.4640\n",
      "Epoch 483/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.7721e-04 - accuracy: 1.0000 - val_loss: 5.1892 - val_accuracy: 0.4640\n",
      "Epoch 484/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.1704e-04 - accuracy: 1.0000 - val_loss: 5.1775 - val_accuracy: 0.4595\n",
      "Epoch 485/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.8765e-04 - accuracy: 1.0000 - val_loss: 5.2047 - val_accuracy: 0.4685\n",
      "Epoch 486/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.0150e-04 - accuracy: 1.0000 - val_loss: 5.1967 - val_accuracy: 0.4595\n",
      "Epoch 487/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 3.8254e-04 - accuracy: 1.0000 - val_loss: 5.1947 - val_accuracy: 0.4595\n",
      "Epoch 488/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 3.1260e-04 - accuracy: 1.0000 - val_loss: 5.2039 - val_accuracy: 0.4640\n",
      "Epoch 489/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.2755e-04 - accuracy: 1.0000 - val_loss: 5.2308 - val_accuracy: 0.4640\n",
      "Epoch 490/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.6527e-04 - accuracy: 1.0000 - val_loss: 5.1938 - val_accuracy: 0.4640\n",
      "Epoch 491/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.8291e-04 - accuracy: 1.0000 - val_loss: 5.2170 - val_accuracy: 0.4640\n",
      "Epoch 492/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.6366e-04 - accuracy: 1.0000 - val_loss: 5.2250 - val_accuracy: 0.4640\n",
      "Epoch 493/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.9011e-04 - accuracy: 1.0000 - val_loss: 5.2419 - val_accuracy: 0.4640\n",
      "Epoch 494/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.6010e-04 - accuracy: 1.0000 - val_loss: 5.2450 - val_accuracy: 0.4640\n",
      "Epoch 495/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.7483e-04 - accuracy: 1.0000 - val_loss: 5.2456 - val_accuracy: 0.4640\n",
      "Epoch 496/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.3032e-04 - accuracy: 1.0000 - val_loss: 5.2565 - val_accuracy: 0.4595\n",
      "Epoch 497/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.7979e-04 - accuracy: 1.0000 - val_loss: 5.2689 - val_accuracy: 0.4640\n",
      "Epoch 498/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.4777e-04 - accuracy: 1.0000 - val_loss: 5.2548 - val_accuracy: 0.4640\n",
      "Epoch 499/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.9618e-04 - accuracy: 1.0000 - val_loss: 5.2650 - val_accuracy: 0.4640\n",
      "Epoch 500/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.5010e-04 - accuracy: 1.0000 - val_loss: 5.2719 - val_accuracy: 0.4595\n",
      "Epoch 501/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.5184e-04 - accuracy: 1.0000 - val_loss: 5.2874 - val_accuracy: 0.4640\n",
      "Epoch 502/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.3311e-04 - accuracy: 1.0000 - val_loss: 5.2942 - val_accuracy: 0.4640\n",
      "Epoch 503/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.3069e-04 - accuracy: 1.0000 - val_loss: 5.3001 - val_accuracy: 0.4640\n",
      "Epoch 504/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.5587e-04 - accuracy: 1.0000 - val_loss: 5.3106 - val_accuracy: 0.4640\n",
      "Epoch 505/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.5361e-04 - accuracy: 1.0000 - val_loss: 5.3100 - val_accuracy: 0.4595\n",
      "Epoch 506/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.2486e-04 - accuracy: 1.0000 - val_loss: 5.3046 - val_accuracy: 0.4595\n",
      "Epoch 507/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.6411e-04 - accuracy: 1.0000 - val_loss: 5.3218 - val_accuracy: 0.4595\n",
      "Epoch 508/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.2315e-04 - accuracy: 1.0000 - val_loss: 5.3241 - val_accuracy: 0.4595\n",
      "Epoch 509/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.0149e-04 - accuracy: 1.0000 - val_loss: 5.3199 - val_accuracy: 0.4640\n",
      "Epoch 510/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.4146e-04 - accuracy: 1.0000 - val_loss: 5.3272 - val_accuracy: 0.4685\n",
      "Epoch 511/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.3700e-04 - accuracy: 1.0000 - val_loss: 5.3422 - val_accuracy: 0.4685\n",
      "Epoch 512/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.0140e-04 - accuracy: 1.0000 - val_loss: 5.3543 - val_accuracy: 0.4640\n",
      "Epoch 513/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.5988e-04 - accuracy: 1.0000 - val_loss: 5.3586 - val_accuracy: 0.4640\n",
      "Epoch 514/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.0362e-04 - accuracy: 1.0000 - val_loss: 5.3600 - val_accuracy: 0.4595\n",
      "Epoch 515/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1154e-04 - accuracy: 1.0000 - val_loss: 5.3545 - val_accuracy: 0.4595\n",
      "Epoch 516/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.0873e-04 - accuracy: 1.0000 - val_loss: 5.3833 - val_accuracy: 0.4595\n",
      "Epoch 517/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 2.1066e-04 - accuracy: 1.0000 - val_loss: 5.3866 - val_accuracy: 0.4595\n",
      "Epoch 518/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8265e-04 - accuracy: 1.0000 - val_loss: 5.4047 - val_accuracy: 0.4640\n",
      "Epoch 519/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.8512e-04 - accuracy: 1.0000 - val_loss: 5.4096 - val_accuracy: 0.4595\n",
      "Epoch 520/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.2100e-04 - accuracy: 1.0000 - val_loss: 5.4033 - val_accuracy: 0.4595\n",
      "Epoch 521/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9821e-04 - accuracy: 1.0000 - val_loss: 5.4091 - val_accuracy: 0.4595\n",
      "Epoch 522/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.8781e-04 - accuracy: 1.0000 - val_loss: 5.4053 - val_accuracy: 0.4595\n",
      "Epoch 523/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 2.3571e-04 - accuracy: 1.0000 - val_loss: 5.3799 - val_accuracy: 0.4640\n",
      "Epoch 524/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 2.1214e-04 - accuracy: 1.0000 - val_loss: 5.3955 - val_accuracy: 0.4595\n",
      "Epoch 525/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8394e-04 - accuracy: 1.0000 - val_loss: 5.4339 - val_accuracy: 0.4640\n",
      "Epoch 526/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.9694e-04 - accuracy: 1.0000 - val_loss: 5.4176 - val_accuracy: 0.4595\n",
      "Epoch 527/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8661e-04 - accuracy: 1.0000 - val_loss: 5.4427 - val_accuracy: 0.4595\n",
      "Epoch 528/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.8598e-04 - accuracy: 1.0000 - val_loss: 5.4390 - val_accuracy: 0.4595\n",
      "Epoch 529/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 2.1331e-04 - accuracy: 1.0000 - val_loss: 5.4189 - val_accuracy: 0.4595\n",
      "Epoch 530/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.9670e-04 - accuracy: 1.0000 - val_loss: 5.4563 - val_accuracy: 0.4640\n",
      "Epoch 531/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8096e-04 - accuracy: 1.0000 - val_loss: 5.4540 - val_accuracy: 0.4595\n",
      "Epoch 532/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.7594e-04 - accuracy: 1.0000 - val_loss: 5.4657 - val_accuracy: 0.4595\n",
      "Epoch 533/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9877e-04 - accuracy: 1.0000 - val_loss: 5.4529 - val_accuracy: 0.4640\n",
      "Epoch 534/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.7239e-04 - accuracy: 1.0000 - val_loss: 5.4633 - val_accuracy: 0.4640\n",
      "Epoch 535/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8017e-04 - accuracy: 1.0000 - val_loss: 5.4650 - val_accuracy: 0.4595\n",
      "Epoch 536/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.4619e-04 - accuracy: 1.0000 - val_loss: 5.4732 - val_accuracy: 0.4640\n",
      "Epoch 537/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.6162e-04 - accuracy: 1.0000 - val_loss: 5.4799 - val_accuracy: 0.4595\n",
      "Epoch 538/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.5350e-04 - accuracy: 1.0000 - val_loss: 5.4849 - val_accuracy: 0.4640\n",
      "Epoch 539/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.6586e-04 - accuracy: 1.0000 - val_loss: 5.5005 - val_accuracy: 0.4640\n",
      "Epoch 540/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 1.8604e-04 - accuracy: 1.0000 - val_loss: 5.5116 - val_accuracy: 0.4595\n",
      "Epoch 541/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.4464e-04 - accuracy: 1.0000 - val_loss: 5.5074 - val_accuracy: 0.4595\n",
      "Epoch 542/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.4462e-04 - accuracy: 1.0000 - val_loss: 5.5000 - val_accuracy: 0.4595\n",
      "Epoch 543/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.6071e-04 - accuracy: 1.0000 - val_loss: 5.4977 - val_accuracy: 0.4595\n",
      "Epoch 544/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.4736e-04 - accuracy: 1.0000 - val_loss: 5.5008 - val_accuracy: 0.4595\n",
      "Epoch 545/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.7103e-04 - accuracy: 1.0000 - val_loss: 5.5092 - val_accuracy: 0.4595\n",
      "Epoch 546/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.3931e-04 - accuracy: 1.0000 - val_loss: 5.5292 - val_accuracy: 0.4595\n",
      "Epoch 547/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.2999e-04 - accuracy: 1.0000 - val_loss: 5.5221 - val_accuracy: 0.4640\n",
      "Epoch 548/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.3618e-04 - accuracy: 1.0000 - val_loss: 5.5341 - val_accuracy: 0.4640\n",
      "Epoch 549/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.4160e-04 - accuracy: 1.0000 - val_loss: 5.5421 - val_accuracy: 0.4640\n",
      "Epoch 550/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.5462e-04 - accuracy: 1.0000 - val_loss: 5.5425 - val_accuracy: 0.4640\n",
      "Epoch 551/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.4345e-04 - accuracy: 1.0000 - val_loss: 5.5516 - val_accuracy: 0.4640\n",
      "Epoch 552/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.3991e-04 - accuracy: 1.0000 - val_loss: 5.5431 - val_accuracy: 0.4595\n",
      "Epoch 553/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.4704e-04 - accuracy: 1.0000 - val_loss: 5.5507 - val_accuracy: 0.4595\n",
      "Epoch 554/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.4741e-04 - accuracy: 1.0000 - val_loss: 5.5574 - val_accuracy: 0.4595\n",
      "Epoch 555/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.2888e-04 - accuracy: 1.0000 - val_loss: 5.5652 - val_accuracy: 0.4595\n",
      "Epoch 556/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2269e-04 - accuracy: 1.0000 - val_loss: 5.5769 - val_accuracy: 0.4595\n",
      "Epoch 557/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.2981e-04 - accuracy: 1.0000 - val_loss: 5.5832 - val_accuracy: 0.4595\n",
      "Epoch 558/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.2415e-04 - accuracy: 1.0000 - val_loss: 5.5901 - val_accuracy: 0.4595\n",
      "Epoch 559/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.2555e-04 - accuracy: 1.0000 - val_loss: 5.5849 - val_accuracy: 0.4640\n",
      "Epoch 560/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.1376e-04 - accuracy: 1.0000 - val_loss: 5.5899 - val_accuracy: 0.4595\n",
      "Epoch 561/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.3075e-04 - accuracy: 1.0000 - val_loss: 5.6031 - val_accuracy: 0.4640\n",
      "Epoch 562/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 1.2768e-04 - accuracy: 1.0000 - val_loss: 5.5942 - val_accuracy: 0.4595\n",
      "Epoch 563/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 1.2178e-04 - accuracy: 1.0000 - val_loss: 5.5997 - val_accuracy: 0.4595\n",
      "Epoch 564/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.1531e-04 - accuracy: 1.0000 - val_loss: 5.6182 - val_accuracy: 0.4595\n",
      "Epoch 565/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 1.0563e-04 - accuracy: 1.0000 - val_loss: 5.6268 - val_accuracy: 0.4595\n",
      "Epoch 566/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.0387e-04 - accuracy: 1.0000 - val_loss: 5.6334 - val_accuracy: 0.4640\n",
      "Epoch 567/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.1760e-04 - accuracy: 1.0000 - val_loss: 5.6409 - val_accuracy: 0.4595\n",
      "Epoch 568/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.1742e-04 - accuracy: 1.0000 - val_loss: 5.6429 - val_accuracy: 0.4640\n",
      "Epoch 569/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0475e-04 - accuracy: 1.0000 - val_loss: 5.6410 - val_accuracy: 0.4640\n",
      "Epoch 570/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2902e-04 - accuracy: 1.0000 - val_loss: 5.6353 - val_accuracy: 0.4595\n",
      "Epoch 571/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0967e-04 - accuracy: 1.0000 - val_loss: 5.6378 - val_accuracy: 0.4595\n",
      "Epoch 572/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.2242e-04 - accuracy: 1.0000 - val_loss: 5.6579 - val_accuracy: 0.4640\n",
      "Epoch 573/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.0534e-04 - accuracy: 1.0000 - val_loss: 5.6598 - val_accuracy: 0.4595\n",
      "Epoch 574/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 1.3814e-04 - accuracy: 1.0000 - val_loss: 5.6434 - val_accuracy: 0.4595\n",
      "Epoch 575/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.6395e-05 - accuracy: 1.0000 - val_loss: 5.6455 - val_accuracy: 0.4640\n",
      "Epoch 576/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.0875e-04 - accuracy: 1.0000 - val_loss: 5.6463 - val_accuracy: 0.4595\n",
      "Epoch 577/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.1367e-04 - accuracy: 1.0000 - val_loss: 5.6702 - val_accuracy: 0.4595\n",
      "Epoch 578/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.1010e-04 - accuracy: 1.0000 - val_loss: 5.6929 - val_accuracy: 0.4640\n",
      "Epoch 579/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.0509e-04 - accuracy: 1.0000 - val_loss: 5.6799 - val_accuracy: 0.4595\n",
      "Epoch 580/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0855e-04 - accuracy: 1.0000 - val_loss: 5.6828 - val_accuracy: 0.4640\n",
      "Epoch 581/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.2380e-04 - accuracy: 1.0000 - val_loss: 5.6876 - val_accuracy: 0.4640\n",
      "Epoch 582/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 9.9839e-05 - accuracy: 1.0000 - val_loss: 5.6851 - val_accuracy: 0.4595\n",
      "Epoch 583/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.0056e-04 - accuracy: 1.0000 - val_loss: 5.7108 - val_accuracy: 0.4595\n",
      "Epoch 584/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.8432e-05 - accuracy: 1.0000 - val_loss: 5.7069 - val_accuracy: 0.4595\n",
      "Epoch 585/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.0718e-05 - accuracy: 1.0000 - val_loss: 5.7134 - val_accuracy: 0.4640\n",
      "Epoch 586/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.9348e-05 - accuracy: 1.0000 - val_loss: 5.7128 - val_accuracy: 0.4595\n",
      "Epoch 587/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.0120e-05 - accuracy: 1.0000 - val_loss: 5.7047 - val_accuracy: 0.4595\n",
      "Epoch 588/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.3358e-05 - accuracy: 1.0000 - val_loss: 5.7297 - val_accuracy: 0.4640\n",
      "Epoch 589/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 7.7643e-05 - accuracy: 1.0000 - val_loss: 5.7387 - val_accuracy: 0.4640\n",
      "Epoch 590/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.6235e-05 - accuracy: 1.0000 - val_loss: 5.7358 - val_accuracy: 0.4640\n",
      "Epoch 591/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.1837e-05 - accuracy: 1.0000 - val_loss: 5.7372 - val_accuracy: 0.4640\n",
      "Epoch 592/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.5988e-05 - accuracy: 1.0000 - val_loss: 5.7595 - val_accuracy: 0.4640\n",
      "Epoch 593/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0706e-04 - accuracy: 1.0000 - val_loss: 5.7637 - val_accuracy: 0.4640\n",
      "Epoch 594/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 9.6225e-05 - accuracy: 1.0000 - val_loss: 5.7814 - val_accuracy: 0.4640\n",
      "Epoch 595/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.0457e-05 - accuracy: 1.0000 - val_loss: 5.7861 - val_accuracy: 0.4640\n",
      "Epoch 596/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.9569e-05 - accuracy: 1.0000 - val_loss: 5.7850 - val_accuracy: 0.4640\n",
      "Epoch 597/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.7900e-05 - accuracy: 1.0000 - val_loss: 5.7979 - val_accuracy: 0.4640\n",
      "Epoch 598/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.2221e-05 - accuracy: 1.0000 - val_loss: 5.7840 - val_accuracy: 0.4640\n",
      "Epoch 599/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.5962e-05 - accuracy: 1.0000 - val_loss: 5.7736 - val_accuracy: 0.4595\n",
      "Epoch 600/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 7.7769e-05 - accuracy: 1.0000 - val_loss: 5.7855 - val_accuracy: 0.4595\n",
      "Epoch 601/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 7.2430e-05 - accuracy: 1.0000 - val_loss: 5.7851 - val_accuracy: 0.4640\n",
      "Epoch 602/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.2458e-05 - accuracy: 1.0000 - val_loss: 5.7868 - val_accuracy: 0.4595\n",
      "Epoch 603/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.5777e-05 - accuracy: 1.0000 - val_loss: 5.8077 - val_accuracy: 0.4640\n",
      "Epoch 604/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 7.6864e-05 - accuracy: 1.0000 - val_loss: 5.8232 - val_accuracy: 0.4640\n",
      "Epoch 605/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.8502e-05 - accuracy: 1.0000 - val_loss: 5.8046 - val_accuracy: 0.4595\n",
      "Epoch 606/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 7.5325e-05 - accuracy: 1.0000 - val_loss: 5.7954 - val_accuracy: 0.4640\n",
      "Epoch 607/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 7.7989e-05 - accuracy: 1.0000 - val_loss: 5.8185 - val_accuracy: 0.4685\n",
      "Epoch 608/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.2597e-05 - accuracy: 1.0000 - val_loss: 5.8129 - val_accuracy: 0.4640\n",
      "Epoch 609/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 6.8389e-05 - accuracy: 1.0000 - val_loss: 5.8307 - val_accuracy: 0.4595\n",
      "Epoch 610/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 7.9735e-05 - accuracy: 1.0000 - val_loss: 5.8430 - val_accuracy: 0.4595\n",
      "Epoch 611/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.2257e-05 - accuracy: 1.0000 - val_loss: 5.8486 - val_accuracy: 0.4595\n",
      "Epoch 612/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 6.4424e-05 - accuracy: 1.0000 - val_loss: 5.8504 - val_accuracy: 0.4640\n",
      "Epoch 613/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 7.5788e-05 - accuracy: 1.0000 - val_loss: 5.8491 - val_accuracy: 0.4640\n",
      "Epoch 614/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 8.6840e-05 - accuracy: 1.0000 - val_loss: 5.8613 - val_accuracy: 0.4640\n",
      "Epoch 615/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 8.0454e-05 - accuracy: 1.0000 - val_loss: 5.8645 - val_accuracy: 0.4595\n",
      "Epoch 616/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 7.9784e-05 - accuracy: 1.0000 - val_loss: 5.8527 - val_accuracy: 0.4595\n",
      "Epoch 617/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 6.5687e-05 - accuracy: 1.0000 - val_loss: 5.8579 - val_accuracy: 0.4595\n",
      "Epoch 618/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 5.3311e-05 - accuracy: 1.0000 - val_loss: 5.8780 - val_accuracy: 0.4640\n",
      "Epoch 619/1000\n",
      "1991/1991 [==============================] - 0s 86us/step - loss: 5.9733e-05 - accuracy: 1.0000 - val_loss: 5.8777 - val_accuracy: 0.4640\n",
      "Epoch 620/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 6.8209e-05 - accuracy: 1.0000 - val_loss: 5.8693 - val_accuracy: 0.4640\n",
      "Epoch 621/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 8.1249e-05 - accuracy: 1.0000 - val_loss: 5.8986 - val_accuracy: 0.4640\n",
      "Epoch 622/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 6.4487e-05 - accuracy: 1.0000 - val_loss: 5.8958 - val_accuracy: 0.4640\n",
      "Epoch 623/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 7.7326e-05 - accuracy: 1.0000 - val_loss: 5.9036 - val_accuracy: 0.4640\n",
      "Epoch 624/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 6.7296e-05 - accuracy: 1.0000 - val_loss: 5.9251 - val_accuracy: 0.4640\n",
      "Epoch 625/1000\n",
      "1991/1991 [==============================] - 0s 85us/step - loss: 6.1149e-05 - accuracy: 1.0000 - val_loss: 5.9270 - val_accuracy: 0.4640\n",
      "Epoch 626/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 6.4890e-05 - accuracy: 1.0000 - val_loss: 5.9289 - val_accuracy: 0.4595\n",
      "Epoch 627/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 6.7871e-05 - accuracy: 1.0000 - val_loss: 5.9449 - val_accuracy: 0.4640\n",
      "Epoch 628/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 6.8152e-05 - accuracy: 1.0000 - val_loss: 5.9368 - val_accuracy: 0.4640\n",
      "Epoch 629/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 5.8929e-05 - accuracy: 1.0000 - val_loss: 5.9330 - val_accuracy: 0.4685\n",
      "Epoch 630/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 5.1491e-05 - accuracy: 1.0000 - val_loss: 5.9436 - val_accuracy: 0.4685\n",
      "Epoch 631/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 6.9927e-05 - accuracy: 1.0000 - val_loss: 5.9276 - val_accuracy: 0.4640\n",
      "Epoch 632/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.1770e-05 - accuracy: 1.0000 - val_loss: 5.9589 - val_accuracy: 0.4640\n",
      "Epoch 633/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.1565e-05 - accuracy: 1.0000 - val_loss: 5.9673 - val_accuracy: 0.4640\n",
      "Epoch 634/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 6.1477e-05 - accuracy: 1.0000 - val_loss: 5.9610 - val_accuracy: 0.4640\n",
      "Epoch 635/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 6.4142e-05 - accuracy: 1.0000 - val_loss: 5.9542 - val_accuracy: 0.4640\n",
      "Epoch 636/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 5.1492e-05 - accuracy: 1.0000 - val_loss: 5.9588 - val_accuracy: 0.4685\n",
      "Epoch 637/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 5.2088e-05 - accuracy: 1.0000 - val_loss: 5.9576 - val_accuracy: 0.4640\n",
      "Epoch 638/1000\n",
      "1991/1991 [==============================] - 0s 114us/step - loss: 5.5906e-05 - accuracy: 1.0000 - val_loss: 5.9728 - val_accuracy: 0.4640\n",
      "Epoch 639/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 5.2988e-05 - accuracy: 1.0000 - val_loss: 5.9813 - val_accuracy: 0.4640\n",
      "Epoch 640/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 4.9930e-05 - accuracy: 1.0000 - val_loss: 5.9782 - val_accuracy: 0.4640\n",
      "Epoch 641/1000\n",
      "1991/1991 [==============================] - 0s 89us/step - loss: 5.6129e-05 - accuracy: 1.0000 - val_loss: 5.9806 - val_accuracy: 0.4595\n",
      "Epoch 642/1000\n",
      "1991/1991 [==============================] - 0s 96us/step - loss: 5.2296e-05 - accuracy: 1.0000 - val_loss: 5.9809 - val_accuracy: 0.4640\n",
      "Epoch 643/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 5.2649e-05 - accuracy: 1.0000 - val_loss: 5.9843 - val_accuracy: 0.4640\n",
      "Epoch 644/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 6.0930e-05 - accuracy: 1.0000 - val_loss: 5.9757 - val_accuracy: 0.4595\n",
      "Epoch 645/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 5.7938e-05 - accuracy: 1.0000 - val_loss: 5.9778 - val_accuracy: 0.4595\n",
      "Epoch 646/1000\n",
      "1991/1991 [==============================] - 0s 87us/step - loss: 4.7190e-05 - accuracy: 1.0000 - val_loss: 5.9966 - val_accuracy: 0.4640\n",
      "Epoch 647/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 4.9600e-05 - accuracy: 1.0000 - val_loss: 6.0276 - val_accuracy: 0.4640\n",
      "Epoch 648/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 4.7981e-05 - accuracy: 1.0000 - val_loss: 6.0337 - val_accuracy: 0.4640\n",
      "Epoch 649/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 4.4904e-05 - accuracy: 1.0000 - val_loss: 6.0156 - val_accuracy: 0.4595\n",
      "Epoch 650/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 4.7873e-05 - accuracy: 1.0000 - val_loss: 6.0422 - val_accuracy: 0.4640\n",
      "Epoch 651/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 4.9780e-05 - accuracy: 1.0000 - val_loss: 6.0384 - val_accuracy: 0.4640\n",
      "Epoch 652/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 5.2261e-05 - accuracy: 1.0000 - val_loss: 6.0372 - val_accuracy: 0.4640\n",
      "Epoch 653/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 4.6594e-05 - accuracy: 1.0000 - val_loss: 6.0505 - val_accuracy: 0.4595\n",
      "Epoch 654/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 4.5635e-05 - accuracy: 1.0000 - val_loss: 6.0453 - val_accuracy: 0.4595\n",
      "Epoch 655/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 4.3778e-05 - accuracy: 1.0000 - val_loss: 6.0385 - val_accuracy: 0.4595\n",
      "Epoch 656/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 4.7551e-05 - accuracy: 1.0000 - val_loss: 6.0465 - val_accuracy: 0.4595\n",
      "Epoch 657/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.9188e-05 - accuracy: 1.0000 - val_loss: 6.0588 - val_accuracy: 0.4640\n",
      "Epoch 658/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 4.3868e-05 - accuracy: 1.0000 - val_loss: 6.0609 - val_accuracy: 0.4640\n",
      "Epoch 659/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 1.2190e-04 - accuracy: 1.0000 - val_loss: 6.0722 - val_accuracy: 0.4595\n",
      "Epoch 660/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.8128 - accuracy: 0.8910 - val_loss: 3.7803 - val_accuracy: 0.4189\n",
      "Epoch 661/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.8077 - accuracy: 0.5610 - val_loss: 0.7183 - val_accuracy: 0.4550\n",
      "Epoch 662/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.6878 - accuracy: 0.5153 - val_loss: 0.6996 - val_accuracy: 0.4685\n",
      "Epoch 663/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.6851 - accuracy: 0.5655 - val_loss: 0.7000 - val_accuracy: 0.4910\n",
      "Epoch 664/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.6798 - accuracy: 0.5706 - val_loss: 0.7040 - val_accuracy: 0.5135\n",
      "Epoch 665/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6742 - accuracy: 0.5876 - val_loss: 0.7104 - val_accuracy: 0.5135\n",
      "Epoch 666/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.6658 - accuracy: 0.5851 - val_loss: 0.7213 - val_accuracy: 0.4955\n",
      "Epoch 667/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.6607 - accuracy: 0.5917 - val_loss: 0.7474 - val_accuracy: 0.4910\n",
      "Epoch 668/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.6492 - accuracy: 0.6042 - val_loss: 0.7837 - val_accuracy: 0.4595\n",
      "Epoch 669/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.6363 - accuracy: 0.6253 - val_loss: 0.8032 - val_accuracy: 0.4820\n",
      "Epoch 670/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.6236 - accuracy: 0.6288 - val_loss: 0.8305 - val_accuracy: 0.5090\n",
      "Epoch 671/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.6074 - accuracy: 0.6434 - val_loss: 0.8460 - val_accuracy: 0.4820\n",
      "Epoch 672/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.5902 - accuracy: 0.6645 - val_loss: 0.9242 - val_accuracy: 0.4820\n",
      "Epoch 673/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.5774 - accuracy: 0.6816 - val_loss: 0.9956 - val_accuracy: 0.4324\n",
      "Epoch 674/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.5753 - accuracy: 0.6700 - val_loss: 0.8118 - val_accuracy: 0.4775\n",
      "Epoch 675/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.5672 - accuracy: 0.6956 - val_loss: 0.9546 - val_accuracy: 0.4910\n",
      "Epoch 676/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 0.5310 - accuracy: 0.7167 - val_loss: 1.1302 - val_accuracy: 0.4865\n",
      "Epoch 677/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.5185 - accuracy: 0.7258 - val_loss: 1.0254 - val_accuracy: 0.5315\n",
      "Epoch 678/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.4933 - accuracy: 0.7343 - val_loss: 1.0871 - val_accuracy: 0.4775\n",
      "Epoch 679/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.4950 - accuracy: 0.7338 - val_loss: 1.1122 - val_accuracy: 0.4730\n",
      "Epoch 680/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.4628 - accuracy: 0.7604 - val_loss: 1.1751 - val_accuracy: 0.5000\n",
      "Epoch 681/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.4546 - accuracy: 0.7680 - val_loss: 1.2639 - val_accuracy: 0.4595\n",
      "Epoch 682/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.4552 - accuracy: 0.7715 - val_loss: 1.5049 - val_accuracy: 0.4459\n",
      "Epoch 683/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.4593 - accuracy: 0.7785 - val_loss: 1.3152 - val_accuracy: 0.4414\n",
      "Epoch 684/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.4417 - accuracy: 0.7830 - val_loss: 1.2197 - val_accuracy: 0.4865\n",
      "Epoch 685/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.3933 - accuracy: 0.8086 - val_loss: 1.4393 - val_accuracy: 0.4865\n",
      "Epoch 686/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.3744 - accuracy: 0.8192 - val_loss: 1.6223 - val_accuracy: 0.4550\n",
      "Epoch 687/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 0.3822 - accuracy: 0.8127 - val_loss: 1.5045 - val_accuracy: 0.4505\n",
      "Epoch 688/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 0.3692 - accuracy: 0.8222 - val_loss: 1.3837 - val_accuracy: 0.4775\n",
      "Epoch 689/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.3644 - accuracy: 0.8272 - val_loss: 1.2891 - val_accuracy: 0.5360\n",
      "Epoch 690/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.3647 - accuracy: 0.8257 - val_loss: 1.3868 - val_accuracy: 0.4640\n",
      "Epoch 691/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.2920 - accuracy: 0.8654 - val_loss: 1.8350 - val_accuracy: 0.4685\n",
      "Epoch 692/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.2606 - accuracy: 0.8835 - val_loss: 1.6944 - val_accuracy: 0.4820\n",
      "Epoch 693/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.2336 - accuracy: 0.8995 - val_loss: 1.9862 - val_accuracy: 0.4910\n",
      "Epoch 694/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.2537 - accuracy: 0.8880 - val_loss: 1.9454 - val_accuracy: 0.4910\n",
      "Epoch 695/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.2131 - accuracy: 0.9076 - val_loss: 1.9120 - val_accuracy: 0.5090\n",
      "Epoch 696/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.2139 - accuracy: 0.9071 - val_loss: 2.1277 - val_accuracy: 0.4595\n",
      "Epoch 697/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.2264 - accuracy: 0.9006 - val_loss: 2.0040 - val_accuracy: 0.5000\n",
      "Epoch 698/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1952 - accuracy: 0.9161 - val_loss: 2.2572 - val_accuracy: 0.5000\n",
      "Epoch 699/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1893 - accuracy: 0.9191 - val_loss: 2.2387 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.1615 - accuracy: 0.9332 - val_loss: 2.3648 - val_accuracy: 0.5135\n",
      "Epoch 701/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.1589 - accuracy: 0.9302 - val_loss: 2.2881 - val_accuracy: 0.4955\n",
      "Epoch 702/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1460 - accuracy: 0.9382 - val_loss: 2.3032 - val_accuracy: 0.5270\n",
      "Epoch 703/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1519 - accuracy: 0.9367 - val_loss: 2.3519 - val_accuracy: 0.5090\n",
      "Epoch 704/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.1912 - accuracy: 0.9272 - val_loss: 2.4324 - val_accuracy: 0.5090\n",
      "Epoch 705/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.1918 - accuracy: 0.9191 - val_loss: 2.2419 - val_accuracy: 0.4955\n",
      "Epoch 706/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.1480 - accuracy: 0.9377 - val_loss: 2.2802 - val_accuracy: 0.5315\n",
      "Epoch 707/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.1394 - accuracy: 0.9468 - val_loss: 2.3225 - val_accuracy: 0.5270\n",
      "Epoch 708/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.1144 - accuracy: 0.9593 - val_loss: 2.3208 - val_accuracy: 0.4955\n",
      "Epoch 709/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0756 - accuracy: 0.9809 - val_loss: 2.4230 - val_accuracy: 0.5225\n",
      "Epoch 710/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0656 - accuracy: 0.9769 - val_loss: 2.6344 - val_accuracy: 0.5360\n",
      "Epoch 711/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0653 - accuracy: 0.9774 - val_loss: 2.5385 - val_accuracy: 0.5225\n",
      "Epoch 712/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0800 - accuracy: 0.9749 - val_loss: 2.8036 - val_accuracy: 0.5135\n",
      "Epoch 713/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0822 - accuracy: 0.9704 - val_loss: 2.7537 - val_accuracy: 0.4955\n",
      "Epoch 714/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0847 - accuracy: 0.9663 - val_loss: 2.7818 - val_accuracy: 0.4910\n",
      "Epoch 715/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.1002 - accuracy: 0.9633 - val_loss: 2.6647 - val_accuracy: 0.5135\n",
      "Epoch 716/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0701 - accuracy: 0.9749 - val_loss: 2.8182 - val_accuracy: 0.5135\n",
      "Epoch 717/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0674 - accuracy: 0.9764 - val_loss: 2.7059 - val_accuracy: 0.5360\n",
      "Epoch 718/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0516 - accuracy: 0.9849 - val_loss: 2.9571 - val_accuracy: 0.5045\n",
      "Epoch 719/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0583 - accuracy: 0.9794 - val_loss: 2.8570 - val_accuracy: 0.5270\n",
      "Epoch 720/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0448 - accuracy: 0.9859 - val_loss: 2.9468 - val_accuracy: 0.5225\n",
      "Epoch 721/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0884 - accuracy: 0.9689 - val_loss: 2.8851 - val_accuracy: 0.5360\n",
      "Epoch 722/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0673 - accuracy: 0.9774 - val_loss: 3.2221 - val_accuracy: 0.5180\n",
      "Epoch 723/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0652 - accuracy: 0.9779 - val_loss: 3.1776 - val_accuracy: 0.5180\n",
      "Epoch 724/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0690 - accuracy: 0.9744 - val_loss: 3.1277 - val_accuracy: 0.4775\n",
      "Epoch 725/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0688 - accuracy: 0.9724 - val_loss: 2.9386 - val_accuracy: 0.5090\n",
      "Epoch 726/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0490 - accuracy: 0.9854 - val_loss: 2.8678 - val_accuracy: 0.5315\n",
      "Epoch 727/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0322 - accuracy: 0.9925 - val_loss: 3.2285 - val_accuracy: 0.5180\n",
      "Epoch 728/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0334 - accuracy: 0.9920 - val_loss: 3.0509 - val_accuracy: 0.5135\n",
      "Epoch 729/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0311 - accuracy: 0.9930 - val_loss: 3.2979 - val_accuracy: 0.5135\n",
      "Epoch 730/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0239 - accuracy: 0.9930 - val_loss: 3.3214 - val_accuracy: 0.4910\n",
      "Epoch 731/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0348 - accuracy: 0.9895 - val_loss: 3.3924 - val_accuracy: 0.5090\n",
      "Epoch 732/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0391 - accuracy: 0.9844 - val_loss: 3.3963 - val_accuracy: 0.5000\n",
      "Epoch 733/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0255 - accuracy: 0.9935 - val_loss: 3.2929 - val_accuracy: 0.5090\n",
      "Epoch 734/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 3.4215 - val_accuracy: 0.5000\n",
      "Epoch 735/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0157 - accuracy: 0.9965 - val_loss: 3.3573 - val_accuracy: 0.5090\n",
      "Epoch 736/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0127 - accuracy: 0.9980 - val_loss: 3.3632 - val_accuracy: 0.5045\n",
      "Epoch 737/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0127 - accuracy: 0.9970 - val_loss: 3.5011 - val_accuracy: 0.4865\n",
      "Epoch 738/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0156 - accuracy: 0.9955 - val_loss: 3.4805 - val_accuracy: 0.5225\n",
      "Epoch 739/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0498 - accuracy: 0.9819 - val_loss: 3.4851 - val_accuracy: 0.4955\n",
      "Epoch 740/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0467 - accuracy: 0.9844 - val_loss: 3.3115 - val_accuracy: 0.5225\n",
      "Epoch 741/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0546 - accuracy: 0.9844 - val_loss: 3.4711 - val_accuracy: 0.5000\n",
      "Epoch 742/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0612 - accuracy: 0.9794 - val_loss: 3.2615 - val_accuracy: 0.5405\n",
      "Epoch 743/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0657 - accuracy: 0.9754 - val_loss: 3.2450 - val_accuracy: 0.5045\n",
      "Epoch 744/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0358 - accuracy: 0.9869 - val_loss: 3.4152 - val_accuracy: 0.4775\n",
      "Epoch 745/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 3.3224 - val_accuracy: 0.5090\n",
      "Epoch 746/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0193 - accuracy: 0.9955 - val_loss: 3.5359 - val_accuracy: 0.4910\n",
      "Epoch 747/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0335 - accuracy: 0.9900 - val_loss: 3.4399 - val_accuracy: 0.5000\n",
      "Epoch 748/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0741 - accuracy: 0.9819 - val_loss: 3.3500 - val_accuracy: 0.5090\n",
      "Epoch 749/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0425 - accuracy: 0.9859 - val_loss: 3.3316 - val_accuracy: 0.5315\n",
      "Epoch 750/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 3.5384 - val_accuracy: 0.5090\n",
      "Epoch 751/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0144 - accuracy: 0.9975 - val_loss: 3.7037 - val_accuracy: 0.4955\n",
      "Epoch 752/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 3.5457 - val_accuracy: 0.5090\n",
      "Epoch 753/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 3.6682 - val_accuracy: 0.5090\n",
      "Epoch 754/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 3.6895 - val_accuracy: 0.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 755/1000\n",
      "1991/1991 [==============================] - 0s 84us/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 3.7188 - val_accuracy: 0.5090\n",
      "Epoch 756/1000\n",
      "1991/1991 [==============================] - 0s 85us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.7011 - val_accuracy: 0.5090\n",
      "Epoch 757/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.7191 - val_accuracy: 0.5045\n",
      "Epoch 758/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 3.7580 - val_accuracy: 0.5045\n",
      "Epoch 759/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.7524 - val_accuracy: 0.4955\n",
      "Epoch 760/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.7954 - val_accuracy: 0.5135\n",
      "Epoch 761/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.8124 - val_accuracy: 0.5090\n",
      "Epoch 762/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.7987 - val_accuracy: 0.5045\n",
      "Epoch 763/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.8066 - val_accuracy: 0.4910\n",
      "Epoch 764/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.8283 - val_accuracy: 0.5000\n",
      "Epoch 765/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.8731 - val_accuracy: 0.5000\n",
      "Epoch 766/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.8455 - val_accuracy: 0.5135\n",
      "Epoch 767/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.8492 - val_accuracy: 0.4955\n",
      "Epoch 768/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.8614 - val_accuracy: 0.4955\n",
      "Epoch 769/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.8923 - val_accuracy: 0.4955\n",
      "Epoch 770/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.9138 - val_accuracy: 0.5045\n",
      "Epoch 771/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9290 - val_accuracy: 0.5000\n",
      "Epoch 772/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.9169 - val_accuracy: 0.5045\n",
      "Epoch 773/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9378 - val_accuracy: 0.5090\n",
      "Epoch 774/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9440 - val_accuracy: 0.5045\n",
      "Epoch 775/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9375 - val_accuracy: 0.4910\n",
      "Epoch 776/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.9783 - val_accuracy: 0.5000\n",
      "Epoch 777/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9630 - val_accuracy: 0.4910\n",
      "Epoch 778/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.9916 - val_accuracy: 0.5000\n",
      "Epoch 779/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.9891 - val_accuracy: 0.5135\n",
      "Epoch 780/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0076 - val_accuracy: 0.4910\n",
      "Epoch 781/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0183 - val_accuracy: 0.4910\n",
      "Epoch 782/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.9968 - val_accuracy: 0.4955\n",
      "Epoch 783/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0123 - val_accuracy: 0.4820\n",
      "Epoch 784/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.0312 - val_accuracy: 0.5000\n",
      "Epoch 785/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0174 - val_accuracy: 0.4955\n",
      "Epoch 786/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.0052 - val_accuracy: 0.5045\n",
      "Epoch 787/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 9.3910e-04 - accuracy: 1.0000 - val_loss: 4.0391 - val_accuracy: 0.4955\n",
      "Epoch 788/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.2605e-04 - accuracy: 1.0000 - val_loss: 4.0517 - val_accuracy: 0.4910\n",
      "Epoch 789/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.7870e-04 - accuracy: 1.0000 - val_loss: 4.0523 - val_accuracy: 0.5000\n",
      "Epoch 790/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.0857 - val_accuracy: 0.4865\n",
      "Epoch 791/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.7797e-04 - accuracy: 1.0000 - val_loss: 4.0656 - val_accuracy: 0.5000\n",
      "Epoch 792/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.3725e-04 - accuracy: 1.0000 - val_loss: 4.0644 - val_accuracy: 0.5000\n",
      "Epoch 793/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.6771e-04 - accuracy: 1.0000 - val_loss: 4.0903 - val_accuracy: 0.4955\n",
      "Epoch 794/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.5558e-04 - accuracy: 1.0000 - val_loss: 4.0948 - val_accuracy: 0.5000\n",
      "Epoch 795/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 7.9153e-04 - accuracy: 1.0000 - val_loss: 4.0962 - val_accuracy: 0.4955\n",
      "Epoch 796/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 7.3749e-04 - accuracy: 1.0000 - val_loss: 4.1236 - val_accuracy: 0.4955\n",
      "Epoch 797/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.7190e-04 - accuracy: 1.0000 - val_loss: 4.1104 - val_accuracy: 0.4865\n",
      "Epoch 798/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.6804e-04 - accuracy: 1.0000 - val_loss: 4.1206 - val_accuracy: 0.4820\n",
      "Epoch 799/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 8.8106e-04 - accuracy: 1.0000 - val_loss: 4.1452 - val_accuracy: 0.4820\n",
      "Epoch 800/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 8.2152e-04 - accuracy: 1.0000 - val_loss: 4.1581 - val_accuracy: 0.4955\n",
      "Epoch 801/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 6.9201e-04 - accuracy: 1.0000 - val_loss: 4.1461 - val_accuracy: 0.5090\n",
      "Epoch 802/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.6533e-04 - accuracy: 1.0000 - val_loss: 4.1687 - val_accuracy: 0.5045\n",
      "Epoch 803/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 6.8528e-04 - accuracy: 1.0000 - val_loss: 4.1701 - val_accuracy: 0.4955\n",
      "Epoch 804/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 7.8518e-04 - accuracy: 1.0000 - val_loss: 4.1805 - val_accuracy: 0.5000\n",
      "Epoch 805/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 7.9469e-04 - accuracy: 1.0000 - val_loss: 4.1719 - val_accuracy: 0.4865\n",
      "Epoch 806/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 8.0644e-04 - accuracy: 1.0000 - val_loss: 4.1821 - val_accuracy: 0.4865\n",
      "Epoch 807/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 5.8201e-04 - accuracy: 1.0000 - val_loss: 4.1971 - val_accuracy: 0.4910\n",
      "Epoch 808/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 5.8613e-04 - accuracy: 1.0000 - val_loss: 4.1826 - val_accuracy: 0.4955\n",
      "Epoch 809/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.3725e-04 - accuracy: 1.0000 - val_loss: 4.2091 - val_accuracy: 0.4955\n",
      "Epoch 810/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.4810e-04 - accuracy: 1.0000 - val_loss: 4.2025 - val_accuracy: 0.4955\n",
      "Epoch 811/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.6738e-04 - accuracy: 1.0000 - val_loss: 4.2187 - val_accuracy: 0.5000\n",
      "Epoch 812/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.6165e-04 - accuracy: 1.0000 - val_loss: 4.2160 - val_accuracy: 0.5090\n",
      "Epoch 813/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.3050e-04 - accuracy: 1.0000 - val_loss: 4.2153 - val_accuracy: 0.5135\n",
      "Epoch 814/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 5.4436e-04 - accuracy: 1.0000 - val_loss: 4.2562 - val_accuracy: 0.5045\n",
      "Epoch 815/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.8442e-04 - accuracy: 1.0000 - val_loss: 4.2478 - val_accuracy: 0.4955\n",
      "Epoch 816/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.7284e-04 - accuracy: 1.0000 - val_loss: 4.2395 - val_accuracy: 0.5000\n",
      "Epoch 817/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.1781e-04 - accuracy: 1.0000 - val_loss: 4.2391 - val_accuracy: 0.4955\n",
      "Epoch 818/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 5.4448e-04 - accuracy: 1.0000 - val_loss: 4.2622 - val_accuracy: 0.5000\n",
      "Epoch 819/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 5.0592e-04 - accuracy: 1.0000 - val_loss: 4.2717 - val_accuracy: 0.4955\n",
      "Epoch 820/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 8.9524e-04 - accuracy: 1.0000 - val_loss: 4.2739 - val_accuracy: 0.4820\n",
      "Epoch 821/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 4.4611 - val_accuracy: 0.4865\n",
      "Epoch 822/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0819 - accuracy: 0.9754 - val_loss: 4.0932 - val_accuracy: 0.5000\n",
      "Epoch 823/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.3750 - accuracy: 0.9086 - val_loss: 4.1601 - val_accuracy: 0.4955\n",
      "Epoch 824/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4047 - accuracy: 0.8639 - val_loss: 2.2572 - val_accuracy: 0.5045\n",
      "Epoch 825/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2643 - accuracy: 0.9006 - val_loss: 2.3128 - val_accuracy: 0.4640\n",
      "Epoch 826/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1161 - accuracy: 0.9618 - val_loss: 2.2803 - val_accuracy: 0.5135\n",
      "Epoch 827/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0470 - accuracy: 0.9869 - val_loss: 2.8330 - val_accuracy: 0.4865\n",
      "Epoch 828/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0241 - accuracy: 0.9950 - val_loss: 3.0670 - val_accuracy: 0.5090\n",
      "Epoch 829/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0122 - accuracy: 0.9980 - val_loss: 3.3145 - val_accuracy: 0.4820\n",
      "Epoch 830/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0091 - accuracy: 0.9990 - val_loss: 3.2749 - val_accuracy: 0.5000\n",
      "Epoch 831/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.3548 - val_accuracy: 0.4775\n",
      "Epoch 832/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.3963 - val_accuracy: 0.4820\n",
      "Epoch 833/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.4361 - val_accuracy: 0.4955\n",
      "Epoch 834/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.4256 - val_accuracy: 0.5000\n",
      "Epoch 835/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.5020 - val_accuracy: 0.4910\n",
      "Epoch 836/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.5292 - val_accuracy: 0.4865\n",
      "Epoch 837/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.5507 - val_accuracy: 0.4820\n",
      "Epoch 838/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.5695 - val_accuracy: 0.4865\n",
      "Epoch 839/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.5992 - val_accuracy: 0.4820\n",
      "Epoch 840/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.6111 - val_accuracy: 0.4865\n",
      "Epoch 841/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.6270 - val_accuracy: 0.4865\n",
      "Epoch 842/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.6965 - val_accuracy: 0.4865\n",
      "Epoch 843/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.7027 - val_accuracy: 0.4865\n",
      "Epoch 844/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.7196 - val_accuracy: 0.4820\n",
      "Epoch 845/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.7056 - val_accuracy: 0.4865\n",
      "Epoch 846/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.7310 - val_accuracy: 0.4865\n",
      "Epoch 847/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.7499 - val_accuracy: 0.4910\n",
      "Epoch 848/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.7669 - val_accuracy: 0.4865\n",
      "Epoch 849/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.7920 - val_accuracy: 0.4820\n",
      "Epoch 850/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.7898 - val_accuracy: 0.4775\n",
      "Epoch 851/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.8216 - val_accuracy: 0.4865\n",
      "Epoch 852/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8217 - val_accuracy: 0.4865\n",
      "Epoch 853/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.8251 - val_accuracy: 0.4865\n",
      "Epoch 854/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.8459 - val_accuracy: 0.4910\n",
      "Epoch 855/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8736 - val_accuracy: 0.4910\n",
      "Epoch 856/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 9.9846e-04 - accuracy: 1.0000 - val_loss: 3.8697 - val_accuracy: 0.4865\n",
      "Epoch 857/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.8747 - val_accuracy: 0.4865\n",
      "Epoch 858/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 9.9728e-04 - accuracy: 1.0000 - val_loss: 3.8823 - val_accuracy: 0.4820\n",
      "Epoch 859/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.9041 - val_accuracy: 0.4820\n",
      "Epoch 860/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 9.4789e-04 - accuracy: 1.0000 - val_loss: 3.9076 - val_accuracy: 0.4865\n",
      "Epoch 861/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 9.1337e-04 - accuracy: 1.0000 - val_loss: 3.9083 - val_accuracy: 0.4865\n",
      "Epoch 862/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 8.4523e-04 - accuracy: 1.0000 - val_loss: 3.9362 - val_accuracy: 0.4820\n",
      "Epoch 863/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 62us/step - loss: 7.8004e-04 - accuracy: 1.0000 - val_loss: 3.9388 - val_accuracy: 0.4775\n",
      "Epoch 864/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 8.4641e-04 - accuracy: 1.0000 - val_loss: 3.9540 - val_accuracy: 0.4820\n",
      "Epoch 865/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 8.2318e-04 - accuracy: 1.0000 - val_loss: 3.9666 - val_accuracy: 0.4865\n",
      "Epoch 866/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 7.1634e-04 - accuracy: 1.0000 - val_loss: 3.9678 - val_accuracy: 0.4820\n",
      "Epoch 867/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 7.5930e-04 - accuracy: 1.0000 - val_loss: 3.9852 - val_accuracy: 0.4865\n",
      "Epoch 868/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 8.0680e-04 - accuracy: 1.0000 - val_loss: 3.9803 - val_accuracy: 0.4865\n",
      "Epoch 869/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 6.9070e-04 - accuracy: 1.0000 - val_loss: 3.9925 - val_accuracy: 0.4865\n",
      "Epoch 870/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.7263e-04 - accuracy: 1.0000 - val_loss: 4.0165 - val_accuracy: 0.4865\n",
      "Epoch 871/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 6.3983e-04 - accuracy: 1.0000 - val_loss: 4.0196 - val_accuracy: 0.4820\n",
      "Epoch 872/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.5808e-04 - accuracy: 1.0000 - val_loss: 4.0255 - val_accuracy: 0.4820\n",
      "Epoch 873/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 7.1819e-04 - accuracy: 1.0000 - val_loss: 4.0340 - val_accuracy: 0.4865\n",
      "Epoch 874/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 6.1366e-04 - accuracy: 1.0000 - val_loss: 4.0473 - val_accuracy: 0.4820\n",
      "Epoch 875/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.8421e-04 - accuracy: 1.0000 - val_loss: 4.0650 - val_accuracy: 0.4775\n",
      "Epoch 876/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.8510e-04 - accuracy: 1.0000 - val_loss: 4.0595 - val_accuracy: 0.4820\n",
      "Epoch 877/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.4006e-04 - accuracy: 1.0000 - val_loss: 4.0688 - val_accuracy: 0.4865\n",
      "Epoch 878/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.1598e-04 - accuracy: 1.0000 - val_loss: 4.0690 - val_accuracy: 0.4865\n",
      "Epoch 879/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 5.9418e-04 - accuracy: 1.0000 - val_loss: 4.0844 - val_accuracy: 0.4820\n",
      "Epoch 880/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 5.9628e-04 - accuracy: 1.0000 - val_loss: 4.0829 - val_accuracy: 0.4910\n",
      "Epoch 881/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.7570e-04 - accuracy: 1.0000 - val_loss: 4.0859 - val_accuracy: 0.4865\n",
      "Epoch 882/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.3291e-04 - accuracy: 1.0000 - val_loss: 4.1000 - val_accuracy: 0.4775\n",
      "Epoch 883/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.5086e-04 - accuracy: 1.0000 - val_loss: 4.0826 - val_accuracy: 0.4910\n",
      "Epoch 884/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.2154e-04 - accuracy: 1.0000 - val_loss: 4.1064 - val_accuracy: 0.4820\n",
      "Epoch 885/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.9557e-04 - accuracy: 1.0000 - val_loss: 4.1071 - val_accuracy: 0.4820\n",
      "Epoch 886/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.2659e-04 - accuracy: 1.0000 - val_loss: 4.1177 - val_accuracy: 0.4910\n",
      "Epoch 887/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.8205e-04 - accuracy: 1.0000 - val_loss: 4.1346 - val_accuracy: 0.4865\n",
      "Epoch 888/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.7355e-04 - accuracy: 1.0000 - val_loss: 4.1584 - val_accuracy: 0.4820\n",
      "Epoch 889/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 5.4174e-04 - accuracy: 1.0000 - val_loss: 4.1656 - val_accuracy: 0.4865\n",
      "Epoch 890/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.8602e-04 - accuracy: 1.0000 - val_loss: 4.1603 - val_accuracy: 0.4865\n",
      "Epoch 891/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 4.3912e-04 - accuracy: 1.0000 - val_loss: 4.1579 - val_accuracy: 0.4865\n",
      "Epoch 892/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.9397e-04 - accuracy: 1.0000 - val_loss: 4.1819 - val_accuracy: 0.4685\n",
      "Epoch 893/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.4089e-04 - accuracy: 1.0000 - val_loss: 4.1768 - val_accuracy: 0.4775\n",
      "Epoch 894/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 3.7723e-04 - accuracy: 1.0000 - val_loss: 4.1877 - val_accuracy: 0.4820\n",
      "Epoch 895/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.6260e-04 - accuracy: 1.0000 - val_loss: 4.1982 - val_accuracy: 0.4775\n",
      "Epoch 896/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.7240e-04 - accuracy: 1.0000 - val_loss: 4.1914 - val_accuracy: 0.4730\n",
      "Epoch 897/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.4840e-04 - accuracy: 1.0000 - val_loss: 4.2049 - val_accuracy: 0.4775\n",
      "Epoch 898/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.0083e-04 - accuracy: 1.0000 - val_loss: 4.2143 - val_accuracy: 0.4820\n",
      "Epoch 899/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.7074e-04 - accuracy: 1.0000 - val_loss: 4.2565 - val_accuracy: 0.4775\n",
      "Epoch 900/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.0499e-04 - accuracy: 1.0000 - val_loss: 4.2411 - val_accuracy: 0.4775\n",
      "Epoch 901/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.2547e-04 - accuracy: 1.0000 - val_loss: 4.2292 - val_accuracy: 0.4865\n",
      "Epoch 902/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.1248e-04 - accuracy: 1.0000 - val_loss: 4.2382 - val_accuracy: 0.4730\n",
      "Epoch 903/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.1751e-04 - accuracy: 1.0000 - val_loss: 4.2454 - val_accuracy: 0.4820\n",
      "Epoch 904/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.0966e-04 - accuracy: 1.0000 - val_loss: 4.2666 - val_accuracy: 0.4820\n",
      "Epoch 905/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.4674e-04 - accuracy: 1.0000 - val_loss: 4.2792 - val_accuracy: 0.4775\n",
      "Epoch 906/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.5241e-04 - accuracy: 1.0000 - val_loss: 4.2918 - val_accuracy: 0.4820\n",
      "Epoch 907/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.6177e-04 - accuracy: 1.0000 - val_loss: 4.2792 - val_accuracy: 0.4730\n",
      "Epoch 908/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.5241e-04 - accuracy: 1.0000 - val_loss: 4.2880 - val_accuracy: 0.4730\n",
      "Epoch 909/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.3591e-04 - accuracy: 1.0000 - val_loss: 4.2895 - val_accuracy: 0.4730\n",
      "Epoch 910/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.5396e-04 - accuracy: 1.0000 - val_loss: 4.2957 - val_accuracy: 0.4730\n",
      "Epoch 911/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.3235e-04 - accuracy: 1.0000 - val_loss: 4.2939 - val_accuracy: 0.4685\n",
      "Epoch 912/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 3.6836e-04 - accuracy: 1.0000 - val_loss: 4.2958 - val_accuracy: 0.4820\n",
      "Epoch 913/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 3.3366e-04 - accuracy: 1.0000 - val_loss: 4.3152 - val_accuracy: 0.4730\n",
      "Epoch 914/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 4.0202e-04 - accuracy: 1.0000 - val_loss: 4.3319 - val_accuracy: 0.4820\n",
      "Epoch 915/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.1873e-04 - accuracy: 1.0000 - val_loss: 4.3287 - val_accuracy: 0.4775\n",
      "Epoch 916/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.4480e-04 - accuracy: 1.0000 - val_loss: 4.3214 - val_accuracy: 0.4820\n",
      "Epoch 917/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.9918e-04 - accuracy: 1.0000 - val_loss: 4.3225 - val_accuracy: 0.4775\n",
      "Epoch 918/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 3.2247e-04 - accuracy: 1.0000 - val_loss: 4.3368 - val_accuracy: 0.4820\n",
      "Epoch 919/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.0701e-04 - accuracy: 1.0000 - val_loss: 4.3521 - val_accuracy: 0.4820\n",
      "Epoch 920/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.0623e-04 - accuracy: 1.0000 - val_loss: 4.3512 - val_accuracy: 0.4820\n",
      "Epoch 921/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.7226e-04 - accuracy: 1.0000 - val_loss: 4.3715 - val_accuracy: 0.4865\n",
      "Epoch 922/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.0750e-04 - accuracy: 1.0000 - val_loss: 4.3660 - val_accuracy: 0.4775\n",
      "Epoch 923/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 3.0266e-04 - accuracy: 1.0000 - val_loss: 4.3604 - val_accuracy: 0.4730\n",
      "Epoch 924/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.8603e-04 - accuracy: 1.0000 - val_loss: 4.3477 - val_accuracy: 0.4775\n",
      "Epoch 925/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.4932e-04 - accuracy: 1.0000 - val_loss: 4.3720 - val_accuracy: 0.4775\n",
      "Epoch 926/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.2085e-04 - accuracy: 1.0000 - val_loss: 4.3733 - val_accuracy: 0.4775\n",
      "Epoch 927/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.9243e-04 - accuracy: 1.0000 - val_loss: 4.3796 - val_accuracy: 0.4685\n",
      "Epoch 928/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.3702e-04 - accuracy: 1.0000 - val_loss: 4.3871 - val_accuracy: 0.4775\n",
      "Epoch 929/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.5553e-04 - accuracy: 1.0000 - val_loss: 4.4125 - val_accuracy: 0.4730\n",
      "Epoch 930/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.7785e-04 - accuracy: 1.0000 - val_loss: 4.4097 - val_accuracy: 0.4730\n",
      "Epoch 931/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.6281e-04 - accuracy: 1.0000 - val_loss: 4.3924 - val_accuracy: 0.4730\n",
      "Epoch 932/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.6601e-04 - accuracy: 1.0000 - val_loss: 4.4031 - val_accuracy: 0.4730\n",
      "Epoch 933/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.2444e-04 - accuracy: 1.0000 - val_loss: 4.4034 - val_accuracy: 0.4730\n",
      "Epoch 934/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.6025e-04 - accuracy: 1.0000 - val_loss: 4.4109 - val_accuracy: 0.4730\n",
      "Epoch 935/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.6038e-04 - accuracy: 1.0000 - val_loss: 4.4199 - val_accuracy: 0.4685\n",
      "Epoch 936/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 2.3865e-04 - accuracy: 1.0000 - val_loss: 4.4378 - val_accuracy: 0.4685\n",
      "Epoch 937/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.6881e-04 - accuracy: 1.0000 - val_loss: 4.4410 - val_accuracy: 0.4685\n",
      "Epoch 938/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1599e-04 - accuracy: 1.0000 - val_loss: 4.4466 - val_accuracy: 0.4685\n",
      "Epoch 939/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.8323e-04 - accuracy: 1.0000 - val_loss: 4.4550 - val_accuracy: 0.4775\n",
      "Epoch 940/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.1251e-04 - accuracy: 1.0000 - val_loss: 4.4613 - val_accuracy: 0.4775\n",
      "Epoch 941/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.3847e-04 - accuracy: 1.0000 - val_loss: 4.4558 - val_accuracy: 0.4730\n",
      "Epoch 942/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.1161e-04 - accuracy: 1.0000 - val_loss: 4.4654 - val_accuracy: 0.4730\n",
      "Epoch 943/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1961e-04 - accuracy: 1.0000 - val_loss: 4.4709 - val_accuracy: 0.4775\n",
      "Epoch 944/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1867e-04 - accuracy: 1.0000 - val_loss: 4.4728 - val_accuracy: 0.4685\n",
      "Epoch 945/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1009e-04 - accuracy: 1.0000 - val_loss: 4.4762 - val_accuracy: 0.4820\n",
      "Epoch 946/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1464e-04 - accuracy: 1.0000 - val_loss: 4.4860 - val_accuracy: 0.4775\n",
      "Epoch 947/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.4494e-04 - accuracy: 1.0000 - val_loss: 4.4858 - val_accuracy: 0.4685\n",
      "Epoch 948/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.0278e-04 - accuracy: 1.0000 - val_loss: 4.4953 - val_accuracy: 0.4820\n",
      "Epoch 949/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.8309e-04 - accuracy: 1.0000 - val_loss: 4.5129 - val_accuracy: 0.4820\n",
      "Epoch 950/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.0258e-04 - accuracy: 1.0000 - val_loss: 4.5192 - val_accuracy: 0.4730\n",
      "Epoch 951/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.2968e-04 - accuracy: 1.0000 - val_loss: 4.5015 - val_accuracy: 0.4865\n",
      "Epoch 952/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.1974e-04 - accuracy: 1.0000 - val_loss: 4.5108 - val_accuracy: 0.4730\n",
      "Epoch 953/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9663e-04 - accuracy: 1.0000 - val_loss: 4.5199 - val_accuracy: 0.4730\n",
      "Epoch 954/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.8730e-04 - accuracy: 1.0000 - val_loss: 4.5230 - val_accuracy: 0.4730\n",
      "Epoch 955/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.9804e-04 - accuracy: 1.0000 - val_loss: 4.5429 - val_accuracy: 0.4730\n",
      "Epoch 956/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.8458e-04 - accuracy: 1.0000 - val_loss: 4.5287 - val_accuracy: 0.4820\n",
      "Epoch 957/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.7715e-04 - accuracy: 1.0000 - val_loss: 4.5399 - val_accuracy: 0.4730\n",
      "Epoch 958/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.7956e-04 - accuracy: 1.0000 - val_loss: 4.5553 - val_accuracy: 0.4730\n",
      "Epoch 959/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.7983e-04 - accuracy: 1.0000 - val_loss: 4.5523 - val_accuracy: 0.4730\n",
      "Epoch 960/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.8151e-04 - accuracy: 1.0000 - val_loss: 4.5740 - val_accuracy: 0.4775\n",
      "Epoch 961/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9655e-04 - accuracy: 1.0000 - val_loss: 4.5663 - val_accuracy: 0.4730\n",
      "Epoch 962/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.6892e-04 - accuracy: 1.0000 - val_loss: 4.5806 - val_accuracy: 0.4730\n",
      "Epoch 963/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.7444e-04 - accuracy: 1.0000 - val_loss: 4.5804 - val_accuracy: 0.4730\n",
      "Epoch 964/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.8399e-04 - accuracy: 1.0000 - val_loss: 4.5780 - val_accuracy: 0.4730\n",
      "Epoch 965/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.2553e-04 - accuracy: 1.0000 - val_loss: 4.5834 - val_accuracy: 0.4730\n",
      "Epoch 966/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9212e-04 - accuracy: 1.0000 - val_loss: 4.5849 - val_accuracy: 0.4730\n",
      "Epoch 967/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.0119e-04 - accuracy: 1.0000 - val_loss: 4.5894 - val_accuracy: 0.4730\n",
      "Epoch 968/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.6128e-04 - accuracy: 1.0000 - val_loss: 4.6000 - val_accuracy: 0.4730\n",
      "Epoch 969/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.6898e-04 - accuracy: 1.0000 - val_loss: 4.6057 - val_accuracy: 0.4730\n",
      "Epoch 970/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.5494e-04 - accuracy: 1.0000 - val_loss: 4.5998 - val_accuracy: 0.4730\n",
      "Epoch 971/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 1.7211e-04 - accuracy: 1.0000 - val_loss: 4.6042 - val_accuracy: 0.4820\n",
      "Epoch 972/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9832e-04 - accuracy: 1.0000 - val_loss: 4.6233 - val_accuracy: 0.4730\n",
      "Epoch 973/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.6200e-04 - accuracy: 1.0000 - val_loss: 4.6487 - val_accuracy: 0.4775\n",
      "Epoch 974/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.4685e-04 - accuracy: 1.0000 - val_loss: 4.6410 - val_accuracy: 0.4730\n",
      "Epoch 975/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.3731e-04 - accuracy: 1.0000 - val_loss: 4.6343 - val_accuracy: 0.4775\n",
      "Epoch 976/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.5053e-04 - accuracy: 1.0000 - val_loss: 4.6356 - val_accuracy: 0.4730\n",
      "Epoch 977/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8311e-04 - accuracy: 1.0000 - val_loss: 4.6430 - val_accuracy: 0.4730\n",
      "Epoch 978/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.7269e-04 - accuracy: 1.0000 - val_loss: 4.6518 - val_accuracy: 0.4730\n",
      "Epoch 979/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.7627e-04 - accuracy: 1.0000 - val_loss: 4.6666 - val_accuracy: 0.4775\n",
      "Epoch 980/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.2929e-04 - accuracy: 1.0000 - val_loss: 4.6587 - val_accuracy: 0.4820\n",
      "Epoch 981/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.5153e-04 - accuracy: 1.0000 - val_loss: 4.6750 - val_accuracy: 0.4775\n",
      "Epoch 982/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2589e-04 - accuracy: 1.0000 - val_loss: 4.6738 - val_accuracy: 0.4685\n",
      "Epoch 983/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.3110e-04 - accuracy: 1.0000 - val_loss: 4.6793 - val_accuracy: 0.4775\n",
      "Epoch 984/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.4028e-04 - accuracy: 1.0000 - val_loss: 4.6826 - val_accuracy: 0.4730\n",
      "Epoch 985/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2172e-04 - accuracy: 1.0000 - val_loss: 4.6902 - val_accuracy: 0.4685\n",
      "Epoch 986/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.3399e-04 - accuracy: 1.0000 - val_loss: 4.6839 - val_accuracy: 0.4820\n",
      "Epoch 987/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2840e-04 - accuracy: 1.0000 - val_loss: 4.6840 - val_accuracy: 0.4775\n",
      "Epoch 988/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.3923e-04 - accuracy: 1.0000 - val_loss: 4.6909 - val_accuracy: 0.4730\n",
      "Epoch 989/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.3954e-04 - accuracy: 1.0000 - val_loss: 4.7048 - val_accuracy: 0.4775\n",
      "Epoch 990/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.3932e-04 - accuracy: 1.0000 - val_loss: 4.7049 - val_accuracy: 0.4730\n",
      "Epoch 991/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 1.3503e-04 - accuracy: 1.0000 - val_loss: 4.6944 - val_accuracy: 0.4730\n",
      "Epoch 992/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.3071e-04 - accuracy: 1.0000 - val_loss: 4.7109 - val_accuracy: 0.4730\n",
      "Epoch 993/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2281e-04 - accuracy: 1.0000 - val_loss: 4.7234 - val_accuracy: 0.4775\n",
      "Epoch 994/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.0739e-04 - accuracy: 1.0000 - val_loss: 4.7288 - val_accuracy: 0.4775\n",
      "Epoch 995/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.4688e-04 - accuracy: 1.0000 - val_loss: 4.7255 - val_accuracy: 0.4685\n",
      "Epoch 996/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.1691e-04 - accuracy: 1.0000 - val_loss: 4.7199 - val_accuracy: 0.4820\n",
      "Epoch 997/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.1754e-04 - accuracy: 1.0000 - val_loss: 4.7145 - val_accuracy: 0.4730\n",
      "Epoch 998/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.3745e-04 - accuracy: 1.0000 - val_loss: 4.7393 - val_accuracy: 0.4685\n",
      "Epoch 999/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.3337e-04 - accuracy: 1.0000 - val_loss: 4.7466 - val_accuracy: 0.4820\n",
      "Epoch 1000/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0747e-04 - accuracy: 1.0000 - val_loss: 4.7363 - val_accuracy: 0.4820\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "246/246 [==============================] - 0s 23us/step\n",
      "test loss, test acc: [4.854146275093885, 0.5487805008888245]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (246, 1)\n",
      "rmse: 0.6615410340599218\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 2\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=500, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(3, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_76 (LSTM)               (None, 3, 128)            113152    \n",
      "_________________________________________________________________\n",
      "lstm_77 (LSTM)               (None, 3, 64)             49408     \n",
      "_________________________________________________________________\n",
      "lstm_78 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 175,009\n",
      "Trainable params: 175,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1990 samples, validate on 222 samples\n",
      "Epoch 1/1000\n",
      "1990/1990 [==============================] - 1s 702us/step - loss: 0.6922 - accuracy: 0.5286 - val_loss: 0.6867 - val_accuracy: 0.5766\n",
      "Epoch 2/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6920 - accuracy: 0.5276 - val_loss: 0.6901 - val_accuracy: 0.5766\n",
      "Epoch 3/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6922 - accuracy: 0.5191 - val_loss: 0.6885 - val_accuracy: 0.5766\n",
      "Epoch 4/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6915 - accuracy: 0.5281 - val_loss: 0.6868 - val_accuracy: 0.5766\n",
      "Epoch 5/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6912 - accuracy: 0.5296 - val_loss: 0.6881 - val_accuracy: 0.5766\n",
      "Epoch 6/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6904 - accuracy: 0.5286 - val_loss: 0.6892 - val_accuracy: 0.5631\n",
      "Epoch 7/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6909 - accuracy: 0.5422 - val_loss: 0.6909 - val_accuracy: 0.5676\n",
      "Epoch 8/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6899 - accuracy: 0.5382 - val_loss: 0.6942 - val_accuracy: 0.5225\n",
      "Epoch 9/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6907 - accuracy: 0.5427 - val_loss: 0.6901 - val_accuracy: 0.5631\n",
      "Epoch 10/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6894 - accuracy: 0.5412 - val_loss: 0.6942 - val_accuracy: 0.5225\n",
      "Epoch 11/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6901 - accuracy: 0.5387 - val_loss: 0.6988 - val_accuracy: 0.5090\n",
      "Epoch 12/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6872 - accuracy: 0.5513 - val_loss: 0.6887 - val_accuracy: 0.5676\n",
      "Epoch 13/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6899 - accuracy: 0.5317 - val_loss: 0.6895 - val_accuracy: 0.5631\n",
      "Epoch 14/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6894 - accuracy: 0.5377 - val_loss: 0.6891 - val_accuracy: 0.5586\n",
      "Epoch 15/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6884 - accuracy: 0.5492 - val_loss: 0.7029 - val_accuracy: 0.5270\n",
      "Epoch 16/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.6901 - accuracy: 0.5397 - val_loss: 0.6835 - val_accuracy: 0.5721\n",
      "Epoch 17/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6901 - accuracy: 0.5302 - val_loss: 0.6890 - val_accuracy: 0.5676\n",
      "Epoch 18/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.6903 - accuracy: 0.5452 - val_loss: 0.6902 - val_accuracy: 0.5676\n",
      "Epoch 19/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6887 - accuracy: 0.5462 - val_loss: 0.6970 - val_accuracy: 0.5315\n",
      "Epoch 20/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6873 - accuracy: 0.5538 - val_loss: 0.7000 - val_accuracy: 0.5360\n",
      "Epoch 21/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6866 - accuracy: 0.5513 - val_loss: 0.7159 - val_accuracy: 0.4685\n",
      "Epoch 22/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6901 - accuracy: 0.5337 - val_loss: 0.6856 - val_accuracy: 0.5631\n",
      "Epoch 23/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6890 - accuracy: 0.5392 - val_loss: 0.6918 - val_accuracy: 0.5586\n",
      "Epoch 24/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6877 - accuracy: 0.5553 - val_loss: 0.6966 - val_accuracy: 0.5405\n",
      "Epoch 25/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6862 - accuracy: 0.5523 - val_loss: 0.7176 - val_accuracy: 0.4955\n",
      "Epoch 26/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6865 - accuracy: 0.5372 - val_loss: 0.6895 - val_accuracy: 0.5586\n",
      "Epoch 27/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6868 - accuracy: 0.5437 - val_loss: 0.7132 - val_accuracy: 0.4685\n",
      "Epoch 28/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6871 - accuracy: 0.5407 - val_loss: 0.6976 - val_accuracy: 0.5405\n",
      "Epoch 29/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6874 - accuracy: 0.5603 - val_loss: 0.6957 - val_accuracy: 0.5495\n",
      "Epoch 30/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6861 - accuracy: 0.5477 - val_loss: 0.7012 - val_accuracy: 0.5541\n",
      "Epoch 31/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 0.6873 - accuracy: 0.5538 - val_loss: 0.6942 - val_accuracy: 0.5495\n",
      "Epoch 32/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6841 - accuracy: 0.5563 - val_loss: 0.7010 - val_accuracy: 0.5541\n",
      "Epoch 33/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6853 - accuracy: 0.5513 - val_loss: 0.7063 - val_accuracy: 0.5270\n",
      "Epoch 34/1000\n",
      "1990/1990 [==============================] - 0s 86us/step - loss: 0.6843 - accuracy: 0.5528 - val_loss: 0.7035 - val_accuracy: 0.5360\n",
      "Epoch 35/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6843 - accuracy: 0.5472 - val_loss: 0.6912 - val_accuracy: 0.5450\n",
      "Epoch 36/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6838 - accuracy: 0.5553 - val_loss: 0.7335 - val_accuracy: 0.4234\n",
      "Epoch 37/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6851 - accuracy: 0.5518 - val_loss: 0.6970 - val_accuracy: 0.5586\n",
      "Epoch 38/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6863 - accuracy: 0.5593 - val_loss: 0.6898 - val_accuracy: 0.5586\n",
      "Epoch 39/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6855 - accuracy: 0.5558 - val_loss: 0.7053 - val_accuracy: 0.5225\n",
      "Epoch 40/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6840 - accuracy: 0.5533 - val_loss: 0.7054 - val_accuracy: 0.5495\n",
      "Epoch 41/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6864 - accuracy: 0.5578 - val_loss: 0.6867 - val_accuracy: 0.5631\n",
      "Epoch 42/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6854 - accuracy: 0.5538 - val_loss: 0.7011 - val_accuracy: 0.5315\n",
      "Epoch 43/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6824 - accuracy: 0.5653 - val_loss: 0.7100 - val_accuracy: 0.5405\n",
      "Epoch 44/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.6828 - accuracy: 0.5618 - val_loss: 0.7007 - val_accuracy: 0.5450\n",
      "Epoch 45/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6816 - accuracy: 0.5693 - val_loss: 0.7209 - val_accuracy: 0.4955\n",
      "Epoch 46/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6844 - accuracy: 0.5568 - val_loss: 0.7142 - val_accuracy: 0.5270\n",
      "Epoch 47/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6828 - accuracy: 0.5678 - val_loss: 0.7023 - val_accuracy: 0.5225\n",
      "Epoch 48/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6830 - accuracy: 0.5573 - val_loss: 0.7038 - val_accuracy: 0.5405\n",
      "Epoch 49/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6821 - accuracy: 0.5613 - val_loss: 0.6990 - val_accuracy: 0.5450\n",
      "Epoch 50/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6811 - accuracy: 0.5593 - val_loss: 0.7011 - val_accuracy: 0.5405\n",
      "Epoch 51/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6826 - accuracy: 0.5553 - val_loss: 0.7068 - val_accuracy: 0.5045\n",
      "Epoch 52/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6824 - accuracy: 0.5613 - val_loss: 0.6973 - val_accuracy: 0.5450\n",
      "Epoch 53/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6790 - accuracy: 0.5643 - val_loss: 0.7116 - val_accuracy: 0.5360\n",
      "Epoch 54/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6833 - accuracy: 0.5583 - val_loss: 0.6963 - val_accuracy: 0.5450\n",
      "Epoch 55/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6815 - accuracy: 0.5683 - val_loss: 0.7033 - val_accuracy: 0.5495\n",
      "Epoch 56/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6782 - accuracy: 0.5719 - val_loss: 0.7143 - val_accuracy: 0.5360\n",
      "Epoch 57/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6791 - accuracy: 0.5653 - val_loss: 0.6977 - val_accuracy: 0.5495\n",
      "Epoch 58/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6853 - accuracy: 0.5513 - val_loss: 0.6942 - val_accuracy: 0.5405\n",
      "Epoch 59/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6810 - accuracy: 0.5683 - val_loss: 0.7104 - val_accuracy: 0.5135\n",
      "Epoch 60/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6799 - accuracy: 0.5709 - val_loss: 0.7033 - val_accuracy: 0.5450\n",
      "Epoch 61/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6761 - accuracy: 0.5764 - val_loss: 0.7379 - val_accuracy: 0.4550\n",
      "Epoch 62/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6832 - accuracy: 0.5548 - val_loss: 0.7194 - val_accuracy: 0.4685\n",
      "Epoch 63/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6830 - accuracy: 0.5704 - val_loss: 0.7022 - val_accuracy: 0.5405\n",
      "Epoch 64/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6822 - accuracy: 0.5603 - val_loss: 0.7085 - val_accuracy: 0.5135\n",
      "Epoch 65/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6793 - accuracy: 0.5673 - val_loss: 0.7151 - val_accuracy: 0.4955\n",
      "Epoch 66/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6782 - accuracy: 0.5714 - val_loss: 0.7145 - val_accuracy: 0.5090\n",
      "Epoch 67/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6767 - accuracy: 0.5739 - val_loss: 0.6934 - val_accuracy: 0.5405\n",
      "Epoch 68/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6750 - accuracy: 0.5744 - val_loss: 0.7068 - val_accuracy: 0.5090\n",
      "Epoch 69/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6811 - accuracy: 0.5668 - val_loss: 0.7346 - val_accuracy: 0.4324\n",
      "Epoch 70/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6810 - accuracy: 0.5588 - val_loss: 0.6914 - val_accuracy: 0.5541\n",
      "Epoch 71/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6771 - accuracy: 0.5698 - val_loss: 0.7176 - val_accuracy: 0.4820\n",
      "Epoch 72/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6758 - accuracy: 0.5784 - val_loss: 0.7176 - val_accuracy: 0.4640\n",
      "Epoch 73/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6768 - accuracy: 0.5784 - val_loss: 0.7197 - val_accuracy: 0.5135\n",
      "Epoch 74/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6758 - accuracy: 0.5653 - val_loss: 0.7209 - val_accuracy: 0.4910\n",
      "Epoch 75/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6759 - accuracy: 0.5698 - val_loss: 0.7131 - val_accuracy: 0.5135\n",
      "Epoch 76/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6748 - accuracy: 0.5769 - val_loss: 0.6959 - val_accuracy: 0.5450\n",
      "Epoch 77/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6865 - accuracy: 0.5518 - val_loss: 0.7191 - val_accuracy: 0.4865\n",
      "Epoch 78/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.6780 - accuracy: 0.5698 - val_loss: 0.7119 - val_accuracy: 0.5180\n",
      "Epoch 79/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6746 - accuracy: 0.5819 - val_loss: 0.7186 - val_accuracy: 0.5045\n",
      "Epoch 80/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6715 - accuracy: 0.5789 - val_loss: 0.7226 - val_accuracy: 0.5180\n",
      "Epoch 81/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6720 - accuracy: 0.5739 - val_loss: 0.7376 - val_accuracy: 0.4775\n",
      "Epoch 82/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6791 - accuracy: 0.5608 - val_loss: 0.7196 - val_accuracy: 0.4910\n",
      "Epoch 83/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.6724 - accuracy: 0.5824 - val_loss: 0.7251 - val_accuracy: 0.5045\n",
      "Epoch 84/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.6705 - accuracy: 0.5809 - val_loss: 0.7300 - val_accuracy: 0.4910\n",
      "Epoch 85/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6733 - accuracy: 0.5719 - val_loss: 0.7196 - val_accuracy: 0.4865\n",
      "Epoch 86/1000\n",
      "1990/1990 [==============================] - 0s 126us/step - loss: 0.6677 - accuracy: 0.5844 - val_loss: 0.7441 - val_accuracy: 0.4955\n",
      "Epoch 87/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 0.6746 - accuracy: 0.5719 - val_loss: 0.7271 - val_accuracy: 0.4550\n",
      "Epoch 88/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 0.6738 - accuracy: 0.5729 - val_loss: 0.7334 - val_accuracy: 0.4910\n",
      "Epoch 89/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6706 - accuracy: 0.5864 - val_loss: 0.7253 - val_accuracy: 0.4775\n",
      "Epoch 90/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6688 - accuracy: 0.5834 - val_loss: 0.7199 - val_accuracy: 0.5135\n",
      "Epoch 91/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6749 - accuracy: 0.5744 - val_loss: 0.6897 - val_accuracy: 0.5450\n",
      "Epoch 92/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6726 - accuracy: 0.5533 - val_loss: 0.7048 - val_accuracy: 0.5270\n",
      "Epoch 93/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6705 - accuracy: 0.5779 - val_loss: 0.7240 - val_accuracy: 0.4865\n",
      "Epoch 94/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6668 - accuracy: 0.5819 - val_loss: 0.7169 - val_accuracy: 0.5090\n",
      "Epoch 95/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6683 - accuracy: 0.5844 - val_loss: 0.7250 - val_accuracy: 0.4550\n",
      "Epoch 96/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6674 - accuracy: 0.5889 - val_loss: 0.7547 - val_accuracy: 0.4865\n",
      "Epoch 97/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6671 - accuracy: 0.5950 - val_loss: 0.7328 - val_accuracy: 0.4595\n",
      "Epoch 98/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.6593 - accuracy: 0.5935 - val_loss: 0.7584 - val_accuracy: 0.5000\n",
      "Epoch 99/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6595 - accuracy: 0.5970 - val_loss: 0.7656 - val_accuracy: 0.4640\n",
      "Epoch 100/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 0.6647 - accuracy: 0.5819 - val_loss: 0.7128 - val_accuracy: 0.5135\n",
      "Epoch 101/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6625 - accuracy: 0.5864 - val_loss: 0.7395 - val_accuracy: 0.4910\n",
      "Epoch 102/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 0.6599 - accuracy: 0.5935 - val_loss: 0.7134 - val_accuracy: 0.4910\n",
      "Epoch 103/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6603 - accuracy: 0.5975 - val_loss: 0.7491 - val_accuracy: 0.4820\n",
      "Epoch 104/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.6660 - accuracy: 0.5960 - val_loss: 0.7401 - val_accuracy: 0.4955\n",
      "Epoch 105/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6643 - accuracy: 0.5925 - val_loss: 0.7333 - val_accuracy: 0.4685\n",
      "Epoch 106/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.6665 - accuracy: 0.5819 - val_loss: 0.7350 - val_accuracy: 0.4910\n",
      "Epoch 107/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 0.6566 - accuracy: 0.5995 - val_loss: 0.7543 - val_accuracy: 0.4459\n",
      "Epoch 108/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6560 - accuracy: 0.6005 - val_loss: 0.7288 - val_accuracy: 0.4595\n",
      "Epoch 109/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.6593 - accuracy: 0.5995 - val_loss: 0.7648 - val_accuracy: 0.4730\n",
      "Epoch 110/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6579 - accuracy: 0.5859 - val_loss: 0.7481 - val_accuracy: 0.4595\n",
      "Epoch 111/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6535 - accuracy: 0.5975 - val_loss: 0.7598 - val_accuracy: 0.4775\n",
      "Epoch 112/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.6469 - accuracy: 0.6040 - val_loss: 0.7899 - val_accuracy: 0.4279\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6512 - accuracy: 0.5985 - val_loss: 0.7459 - val_accuracy: 0.5270\n",
      "Epoch 114/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6482 - accuracy: 0.6020 - val_loss: 0.7523 - val_accuracy: 0.4730\n",
      "Epoch 115/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.6499 - accuracy: 0.6090 - val_loss: 0.7682 - val_accuracy: 0.4550\n",
      "Epoch 116/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6450 - accuracy: 0.6085 - val_loss: 0.7819 - val_accuracy: 0.4595\n",
      "Epoch 117/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6475 - accuracy: 0.6196 - val_loss: 0.7843 - val_accuracy: 0.4459\n",
      "Epoch 118/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6339 - accuracy: 0.6201 - val_loss: 0.7875 - val_accuracy: 0.4685\n",
      "Epoch 119/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6347 - accuracy: 0.6151 - val_loss: 0.8081 - val_accuracy: 0.4324\n",
      "Epoch 120/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6370 - accuracy: 0.6106 - val_loss: 0.7705 - val_accuracy: 0.4595\n",
      "Epoch 121/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6356 - accuracy: 0.6151 - val_loss: 0.8092 - val_accuracy: 0.4595\n",
      "Epoch 122/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6267 - accuracy: 0.6146 - val_loss: 0.8094 - val_accuracy: 0.4550\n",
      "Epoch 123/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6307 - accuracy: 0.6206 - val_loss: 0.8042 - val_accuracy: 0.4865\n",
      "Epoch 124/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6269 - accuracy: 0.6246 - val_loss: 0.8020 - val_accuracy: 0.4730\n",
      "Epoch 125/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6262 - accuracy: 0.6402 - val_loss: 0.7769 - val_accuracy: 0.4730\n",
      "Epoch 126/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6254 - accuracy: 0.6266 - val_loss: 0.8442 - val_accuracy: 0.4730\n",
      "Epoch 127/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6235 - accuracy: 0.6317 - val_loss: 0.8341 - val_accuracy: 0.4595\n",
      "Epoch 128/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6161 - accuracy: 0.6457 - val_loss: 0.8192 - val_accuracy: 0.4550\n",
      "Epoch 129/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6184 - accuracy: 0.6317 - val_loss: 0.8439 - val_accuracy: 0.4414\n",
      "Epoch 130/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6079 - accuracy: 0.6402 - val_loss: 0.8796 - val_accuracy: 0.4414\n",
      "Epoch 131/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6061 - accuracy: 0.6503 - val_loss: 0.8792 - val_accuracy: 0.4685\n",
      "Epoch 132/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.5980 - accuracy: 0.6467 - val_loss: 0.9028 - val_accuracy: 0.4910\n",
      "Epoch 133/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.5978 - accuracy: 0.6603 - val_loss: 0.8628 - val_accuracy: 0.4505\n",
      "Epoch 134/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.5980 - accuracy: 0.6583 - val_loss: 0.8497 - val_accuracy: 0.4640\n",
      "Epoch 135/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.5941 - accuracy: 0.6593 - val_loss: 0.9377 - val_accuracy: 0.4820\n",
      "Epoch 136/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.5798 - accuracy: 0.6663 - val_loss: 0.9004 - val_accuracy: 0.4550\n",
      "Epoch 137/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.5714 - accuracy: 0.6789 - val_loss: 0.9579 - val_accuracy: 0.4955\n",
      "Epoch 138/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.5900 - accuracy: 0.6543 - val_loss: 0.8998 - val_accuracy: 0.4414\n",
      "Epoch 139/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 0.5647 - accuracy: 0.6744 - val_loss: 0.9630 - val_accuracy: 0.4640\n",
      "Epoch 140/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.5718 - accuracy: 0.6729 - val_loss: 0.9894 - val_accuracy: 0.4505\n",
      "Epoch 141/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.5682 - accuracy: 0.6764 - val_loss: 0.8618 - val_accuracy: 0.4865\n",
      "Epoch 142/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.5685 - accuracy: 0.6663 - val_loss: 0.9684 - val_accuracy: 0.4685\n",
      "Epoch 143/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.5645 - accuracy: 0.6749 - val_loss: 1.0286 - val_accuracy: 0.4414\n",
      "Epoch 144/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.5418 - accuracy: 0.6925 - val_loss: 1.0184 - val_accuracy: 0.4414\n",
      "Epoch 145/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.5274 - accuracy: 0.7141 - val_loss: 1.0611 - val_accuracy: 0.4685\n",
      "Epoch 146/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.5395 - accuracy: 0.7030 - val_loss: 0.9474 - val_accuracy: 0.5360\n",
      "Epoch 147/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.5418 - accuracy: 0.6925 - val_loss: 1.1308 - val_accuracy: 0.4730\n",
      "Epoch 148/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.5208 - accuracy: 0.7111 - val_loss: 1.1187 - val_accuracy: 0.4685\n",
      "Epoch 149/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.5235 - accuracy: 0.7065 - val_loss: 1.0654 - val_accuracy: 0.4820\n",
      "Epoch 150/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.5141 - accuracy: 0.7116 - val_loss: 1.0761 - val_accuracy: 0.4459\n",
      "Epoch 151/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.4963 - accuracy: 0.7296 - val_loss: 1.1753 - val_accuracy: 0.4640\n",
      "Epoch 152/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.5002 - accuracy: 0.7337 - val_loss: 1.1576 - val_accuracy: 0.4595\n",
      "Epoch 153/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.4905 - accuracy: 0.7362 - val_loss: 1.2019 - val_accuracy: 0.4910\n",
      "Epoch 154/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4750 - accuracy: 0.7538 - val_loss: 1.2110 - val_accuracy: 0.4865\n",
      "Epoch 155/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.4680 - accuracy: 0.7588 - val_loss: 1.1722 - val_accuracy: 0.4730\n",
      "Epoch 156/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.4720 - accuracy: 0.7457 - val_loss: 1.1327 - val_accuracy: 0.4955\n",
      "Epoch 157/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4694 - accuracy: 0.7533 - val_loss: 1.1826 - val_accuracy: 0.4595\n",
      "Epoch 158/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.4576 - accuracy: 0.7623 - val_loss: 1.4520 - val_accuracy: 0.4279\n",
      "Epoch 159/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.4409 - accuracy: 0.7709 - val_loss: 1.3463 - val_accuracy: 0.4685\n",
      "Epoch 160/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4267 - accuracy: 0.7794 - val_loss: 1.3526 - val_accuracy: 0.4505\n",
      "Epoch 161/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.4267 - accuracy: 0.7814 - val_loss: 1.3637 - val_accuracy: 0.4505\n",
      "Epoch 162/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4167 - accuracy: 0.7920 - val_loss: 1.3198 - val_accuracy: 0.4550\n",
      "Epoch 163/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.4202 - accuracy: 0.7905 - val_loss: 1.4339 - val_accuracy: 0.4955\n",
      "Epoch 164/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.4036 - accuracy: 0.7990 - val_loss: 1.4015 - val_accuracy: 0.4685\n",
      "Epoch 165/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4072 - accuracy: 0.7915 - val_loss: 1.5174 - val_accuracy: 0.4595\n",
      "Epoch 166/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.3904 - accuracy: 0.8020 - val_loss: 1.4252 - val_accuracy: 0.4685\n",
      "Epoch 167/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.3706 - accuracy: 0.8261 - val_loss: 1.4898 - val_accuracy: 0.5000\n",
      "Epoch 168/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.3692 - accuracy: 0.8151 - val_loss: 1.4527 - val_accuracy: 0.4550\n",
      "Epoch 169/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.3728 - accuracy: 0.8286 - val_loss: 1.6456 - val_accuracy: 0.4459\n",
      "Epoch 170/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.3568 - accuracy: 0.8241 - val_loss: 1.6076 - val_accuracy: 0.4595\n",
      "Epoch 171/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.3477 - accuracy: 0.8266 - val_loss: 1.5056 - val_accuracy: 0.4595\n",
      "Epoch 172/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.3475 - accuracy: 0.8357 - val_loss: 1.5863 - val_accuracy: 0.4730\n",
      "Epoch 173/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.3446 - accuracy: 0.8322 - val_loss: 1.6658 - val_accuracy: 0.4550\n",
      "Epoch 174/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.3169 - accuracy: 0.8462 - val_loss: 1.6536 - val_accuracy: 0.4955\n",
      "Epoch 175/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.3014 - accuracy: 0.8563 - val_loss: 1.7772 - val_accuracy: 0.4820\n",
      "Epoch 176/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.2961 - accuracy: 0.8603 - val_loss: 1.6557 - val_accuracy: 0.4820\n",
      "Epoch 177/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.2878 - accuracy: 0.8633 - val_loss: 1.9159 - val_accuracy: 0.4550\n",
      "Epoch 178/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.2840 - accuracy: 0.8769 - val_loss: 1.7792 - val_accuracy: 0.4865\n",
      "Epoch 179/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2947 - accuracy: 0.8638 - val_loss: 1.8041 - val_accuracy: 0.4685\n",
      "Epoch 180/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2722 - accuracy: 0.8739 - val_loss: 1.7839 - val_accuracy: 0.4865\n",
      "Epoch 181/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.2801 - accuracy: 0.8678 - val_loss: 1.8630 - val_accuracy: 0.4820\n",
      "Epoch 182/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2666 - accuracy: 0.8789 - val_loss: 1.9274 - val_accuracy: 0.4550\n",
      "Epoch 183/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.2499 - accuracy: 0.8869 - val_loss: 2.0262 - val_accuracy: 0.4730\n",
      "Epoch 184/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.2442 - accuracy: 0.8829 - val_loss: 2.0631 - val_accuracy: 0.4640\n",
      "Epoch 185/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.2230 - accuracy: 0.8955 - val_loss: 2.0401 - val_accuracy: 0.4775\n",
      "Epoch 186/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.2255 - accuracy: 0.9030 - val_loss: 2.1304 - val_accuracy: 0.4550\n",
      "Epoch 187/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2027 - accuracy: 0.9106 - val_loss: 2.1124 - val_accuracy: 0.4685\n",
      "Epoch 188/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.2069 - accuracy: 0.9101 - val_loss: 2.3011 - val_accuracy: 0.4820\n",
      "Epoch 189/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.1923 - accuracy: 0.9161 - val_loss: 2.3558 - val_accuracy: 0.4595\n",
      "Epoch 190/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.1945 - accuracy: 0.9181 - val_loss: 2.3019 - val_accuracy: 0.4685\n",
      "Epoch 191/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2008 - accuracy: 0.9121 - val_loss: 2.2740 - val_accuracy: 0.4685\n",
      "Epoch 192/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.1879 - accuracy: 0.9186 - val_loss: 2.4165 - val_accuracy: 0.4640\n",
      "Epoch 193/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 0.1788 - accuracy: 0.9261 - val_loss: 2.5076 - val_accuracy: 0.4685\n",
      "Epoch 194/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.1645 - accuracy: 0.9296 - val_loss: 2.4701 - val_accuracy: 0.4865\n",
      "Epoch 195/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.1531 - accuracy: 0.9352 - val_loss: 2.4771 - val_accuracy: 0.5000\n",
      "Epoch 196/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.1492 - accuracy: 0.9372 - val_loss: 2.7046 - val_accuracy: 0.4414\n",
      "Epoch 197/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.1737 - accuracy: 0.9211 - val_loss: 2.6853 - val_accuracy: 0.4685\n",
      "Epoch 198/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.1777 - accuracy: 0.9256 - val_loss: 2.4866 - val_accuracy: 0.4865\n",
      "Epoch 199/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.1754 - accuracy: 0.9236 - val_loss: 2.3268 - val_accuracy: 0.4775\n",
      "Epoch 200/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.2358 - accuracy: 0.8985 - val_loss: 2.3834 - val_accuracy: 0.4730\n",
      "Epoch 201/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.1487 - accuracy: 0.9377 - val_loss: 2.6309 - val_accuracy: 0.4775\n",
      "Epoch 202/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.1231 - accuracy: 0.9553 - val_loss: 2.6867 - val_accuracy: 0.4595\n",
      "Epoch 203/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.1155 - accuracy: 0.9508 - val_loss: 2.6858 - val_accuracy: 0.4955\n",
      "Epoch 204/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.1176 - accuracy: 0.9563 - val_loss: 2.7692 - val_accuracy: 0.4865\n",
      "Epoch 205/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0994 - accuracy: 0.9593 - val_loss: 2.8518 - val_accuracy: 0.4955\n",
      "Epoch 206/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.1171 - accuracy: 0.9508 - val_loss: 2.9593 - val_accuracy: 0.4414\n",
      "Epoch 207/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.1281 - accuracy: 0.9568 - val_loss: 3.0122 - val_accuracy: 0.4414\n",
      "Epoch 208/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.1148 - accuracy: 0.9578 - val_loss: 2.8815 - val_accuracy: 0.4955\n",
      "Epoch 209/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0944 - accuracy: 0.9643 - val_loss: 2.9828 - val_accuracy: 0.4550\n",
      "Epoch 210/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0801 - accuracy: 0.9709 - val_loss: 3.1625 - val_accuracy: 0.4955\n",
      "Epoch 211/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0822 - accuracy: 0.9658 - val_loss: 3.0168 - val_accuracy: 0.4775\n",
      "Epoch 212/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0955 - accuracy: 0.9658 - val_loss: 3.1456 - val_accuracy: 0.4775\n",
      "Epoch 213/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0739 - accuracy: 0.9749 - val_loss: 3.0052 - val_accuracy: 0.4955\n",
      "Epoch 214/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0874 - accuracy: 0.9633 - val_loss: 3.2103 - val_accuracy: 0.4640\n",
      "Epoch 215/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.1007 - accuracy: 0.9598 - val_loss: 3.0499 - val_accuracy: 0.4730\n",
      "Epoch 216/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.1177 - accuracy: 0.9528 - val_loss: 3.1158 - val_accuracy: 0.4820\n",
      "Epoch 217/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0887 - accuracy: 0.9623 - val_loss: 3.2189 - val_accuracy: 0.4550\n",
      "Epoch 218/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.0859 - accuracy: 0.9658 - val_loss: 3.1855 - val_accuracy: 0.4910\n",
      "Epoch 219/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0742 - accuracy: 0.9744 - val_loss: 3.2223 - val_accuracy: 0.4730\n",
      "Epoch 220/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0884 - accuracy: 0.9678 - val_loss: 3.5224 - val_accuracy: 0.4775\n",
      "Epoch 221/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0689 - accuracy: 0.9774 - val_loss: 3.2929 - val_accuracy: 0.4730\n",
      "Epoch 222/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.0698 - accuracy: 0.9714 - val_loss: 3.3984 - val_accuracy: 0.4775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0620 - accuracy: 0.9774 - val_loss: 3.1714 - val_accuracy: 0.5000\n",
      "Epoch 224/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.0528 - accuracy: 0.9814 - val_loss: 3.4007 - val_accuracy: 0.4865\n",
      "Epoch 225/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0446 - accuracy: 0.9859 - val_loss: 3.5156 - val_accuracy: 0.4910\n",
      "Epoch 226/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0379 - accuracy: 0.9910 - val_loss: 3.5253 - val_accuracy: 0.4685\n",
      "Epoch 227/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0312 - accuracy: 0.9915 - val_loss: 3.4614 - val_accuracy: 0.4730\n",
      "Epoch 228/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.0274 - accuracy: 0.9940 - val_loss: 3.3898 - val_accuracy: 0.4865\n",
      "Epoch 229/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.0321 - accuracy: 0.9925 - val_loss: 3.6957 - val_accuracy: 0.4505\n",
      "Epoch 230/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0311 - accuracy: 0.9910 - val_loss: 3.4622 - val_accuracy: 0.4820\n",
      "Epoch 231/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.0222 - accuracy: 0.9945 - val_loss: 3.7273 - val_accuracy: 0.4865\n",
      "Epoch 232/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.0222 - accuracy: 0.9950 - val_loss: 3.5711 - val_accuracy: 0.4910\n",
      "Epoch 233/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.0196 - accuracy: 0.9950 - val_loss: 3.7494 - val_accuracy: 0.4775\n",
      "Epoch 234/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.0232 - accuracy: 0.9945 - val_loss: 3.6694 - val_accuracy: 0.4775\n",
      "Epoch 235/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0346 - accuracy: 0.9859 - val_loss: 3.7181 - val_accuracy: 0.4820\n",
      "Epoch 236/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.0396 - accuracy: 0.9854 - val_loss: 3.8542 - val_accuracy: 0.4685\n",
      "Epoch 237/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0526 - accuracy: 0.9839 - val_loss: 3.7096 - val_accuracy: 0.4550\n",
      "Epoch 238/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.0383 - accuracy: 0.9879 - val_loss: 3.7065 - val_accuracy: 0.4865\n",
      "Epoch 239/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.0319 - accuracy: 0.9899 - val_loss: 3.8491 - val_accuracy: 0.4865\n",
      "Epoch 240/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0349 - accuracy: 0.9899 - val_loss: 3.8125 - val_accuracy: 0.4820\n",
      "Epoch 241/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 0.0395 - accuracy: 0.9834 - val_loss: 3.7933 - val_accuracy: 0.5000\n",
      "Epoch 242/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.0302 - accuracy: 0.9889 - val_loss: 3.9163 - val_accuracy: 0.4955\n",
      "Epoch 243/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0271 - accuracy: 0.9925 - val_loss: 3.7228 - val_accuracy: 0.5000\n",
      "Epoch 244/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.0288 - accuracy: 0.9910 - val_loss: 4.0175 - val_accuracy: 0.4820\n",
      "Epoch 245/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.0245 - accuracy: 0.9935 - val_loss: 3.9452 - val_accuracy: 0.5000\n",
      "Epoch 246/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0338 - accuracy: 0.9935 - val_loss: 3.9187 - val_accuracy: 0.4910\n",
      "Epoch 247/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0278 - accuracy: 0.9905 - val_loss: 3.9500 - val_accuracy: 0.4910\n",
      "Epoch 248/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.0562 - accuracy: 0.9859 - val_loss: 3.9910 - val_accuracy: 0.4955\n",
      "Epoch 249/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 0.1151 - accuracy: 0.9643 - val_loss: 3.6621 - val_accuracy: 0.5225\n",
      "Epoch 250/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 0.2319 - accuracy: 0.9241 - val_loss: 3.7612 - val_accuracy: 0.4955\n",
      "Epoch 251/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.2129 - accuracy: 0.9271 - val_loss: 3.4293 - val_accuracy: 0.4820\n",
      "Epoch 252/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.1512 - accuracy: 0.9382 - val_loss: 3.0004 - val_accuracy: 0.5045\n",
      "Epoch 253/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0739 - accuracy: 0.9719 - val_loss: 2.9218 - val_accuracy: 0.4910\n",
      "Epoch 254/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.0465 - accuracy: 0.9834 - val_loss: 3.1764 - val_accuracy: 0.4775\n",
      "Epoch 255/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0259 - accuracy: 0.9945 - val_loss: 3.4646 - val_accuracy: 0.4730\n",
      "Epoch 256/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 0.0197 - accuracy: 0.9975 - val_loss: 3.3005 - val_accuracy: 0.4685\n",
      "Epoch 257/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 0.0117 - accuracy: 0.9990 - val_loss: 3.3910 - val_accuracy: 0.4820\n",
      "Epoch 258/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 3.4332 - val_accuracy: 0.4820\n",
      "Epoch 259/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.5578 - val_accuracy: 0.4640\n",
      "Epoch 260/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 3.6062 - val_accuracy: 0.4685\n",
      "Epoch 261/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.6639 - val_accuracy: 0.4730\n",
      "Epoch 262/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.6583 - val_accuracy: 0.4730\n",
      "Epoch 263/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 3.7060 - val_accuracy: 0.4730\n",
      "Epoch 264/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0067 - accuracy: 0.9995 - val_loss: 3.6962 - val_accuracy: 0.4865\n",
      "Epoch 265/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 3.7981 - val_accuracy: 0.4775\n",
      "Epoch 266/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.7969 - val_accuracy: 0.4775\n",
      "Epoch 267/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.8325 - val_accuracy: 0.4730\n",
      "Epoch 268/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.8601 - val_accuracy: 0.4775\n",
      "Epoch 269/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.9044 - val_accuracy: 0.4730\n",
      "Epoch 270/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.8870 - val_accuracy: 0.4730\n",
      "Epoch 271/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.9251 - val_accuracy: 0.4730\n",
      "Epoch 272/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9465 - val_accuracy: 0.4775\n",
      "Epoch 273/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.9532 - val_accuracy: 0.4730\n",
      "Epoch 274/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.9651 - val_accuracy: 0.4730\n",
      "Epoch 275/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.9845 - val_accuracy: 0.4730\n",
      "Epoch 276/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.9963 - val_accuracy: 0.4730\n",
      "Epoch 277/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.0065 - val_accuracy: 0.4775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.0368 - val_accuracy: 0.4775\n",
      "Epoch 279/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0409 - val_accuracy: 0.4730\n",
      "Epoch 280/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0467 - val_accuracy: 0.4730\n",
      "Epoch 281/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0764 - val_accuracy: 0.4775\n",
      "Epoch 282/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0924 - val_accuracy: 0.4730\n",
      "Epoch 283/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0990 - val_accuracy: 0.4730\n",
      "Epoch 284/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1083 - val_accuracy: 0.4730\n",
      "Epoch 285/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1213 - val_accuracy: 0.4730\n",
      "Epoch 286/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.1319 - val_accuracy: 0.4730\n",
      "Epoch 287/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1328 - val_accuracy: 0.4730\n",
      "Epoch 288/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1628 - val_accuracy: 0.4775\n",
      "Epoch 289/1000\n",
      "1990/1990 [==============================] - 0s 116us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1656 - val_accuracy: 0.4820\n",
      "Epoch 290/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1676 - val_accuracy: 0.4730\n",
      "Epoch 291/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.1836 - val_accuracy: 0.4730\n",
      "Epoch 292/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1983 - val_accuracy: 0.4730\n",
      "Epoch 293/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2093 - val_accuracy: 0.4730\n",
      "Epoch 294/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2151 - val_accuracy: 0.4730\n",
      "Epoch 295/1000\n",
      "1990/1990 [==============================] - 0s 132us/step - loss: 9.7641e-04 - accuracy: 1.0000 - val_loss: 4.2301 - val_accuracy: 0.4730\n",
      "Epoch 296/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 9.5048e-04 - accuracy: 1.0000 - val_loss: 4.2366 - val_accuracy: 0.4730\n",
      "Epoch 297/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 9.6787e-04 - accuracy: 1.0000 - val_loss: 4.2438 - val_accuracy: 0.4775\n",
      "Epoch 298/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 9.1847e-04 - accuracy: 1.0000 - val_loss: 4.2579 - val_accuracy: 0.4820\n",
      "Epoch 299/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 8.7240e-04 - accuracy: 1.0000 - val_loss: 4.2659 - val_accuracy: 0.4775\n",
      "Epoch 300/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 8.1701e-04 - accuracy: 1.0000 - val_loss: 4.2787 - val_accuracy: 0.4775\n",
      "Epoch 301/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 8.2179e-04 - accuracy: 1.0000 - val_loss: 4.2991 - val_accuracy: 0.4775\n",
      "Epoch 302/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 8.2143e-04 - accuracy: 1.0000 - val_loss: 4.3047 - val_accuracy: 0.4775\n",
      "Epoch 303/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 7.9452e-04 - accuracy: 1.0000 - val_loss: 4.3081 - val_accuracy: 0.4730\n",
      "Epoch 304/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 7.9430e-04 - accuracy: 1.0000 - val_loss: 4.3179 - val_accuracy: 0.4730\n",
      "Epoch 305/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 7.9148e-04 - accuracy: 1.0000 - val_loss: 4.3324 - val_accuracy: 0.4730\n",
      "Epoch 306/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 7.7981e-04 - accuracy: 1.0000 - val_loss: 4.3397 - val_accuracy: 0.4730\n",
      "Epoch 307/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 7.3421e-04 - accuracy: 1.0000 - val_loss: 4.3483 - val_accuracy: 0.4775\n",
      "Epoch 308/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 7.2464e-04 - accuracy: 1.0000 - val_loss: 4.3559 - val_accuracy: 0.4775\n",
      "Epoch 309/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 7.0982e-04 - accuracy: 1.0000 - val_loss: 4.3584 - val_accuracy: 0.4820\n",
      "Epoch 310/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 6.9027e-04 - accuracy: 1.0000 - val_loss: 4.3655 - val_accuracy: 0.4775\n",
      "Epoch 311/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 6.5628e-04 - accuracy: 1.0000 - val_loss: 4.3763 - val_accuracy: 0.4775\n",
      "Epoch 312/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.6808e-04 - accuracy: 1.0000 - val_loss: 4.3840 - val_accuracy: 0.4820\n",
      "Epoch 313/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.5562e-04 - accuracy: 1.0000 - val_loss: 4.3826 - val_accuracy: 0.4865\n",
      "Epoch 314/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.8369e-04 - accuracy: 1.0000 - val_loss: 4.3918 - val_accuracy: 0.4820\n",
      "Epoch 315/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.2654e-04 - accuracy: 1.0000 - val_loss: 4.4005 - val_accuracy: 0.4820\n",
      "Epoch 316/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 5.9235e-04 - accuracy: 1.0000 - val_loss: 4.4149 - val_accuracy: 0.4865\n",
      "Epoch 317/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 5.7421e-04 - accuracy: 1.0000 - val_loss: 4.4283 - val_accuracy: 0.4820\n",
      "Epoch 318/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.5796e-04 - accuracy: 1.0000 - val_loss: 4.4361 - val_accuracy: 0.4820\n",
      "Epoch 319/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.2794e-04 - accuracy: 1.0000 - val_loss: 4.4309 - val_accuracy: 0.4820\n",
      "Epoch 320/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 5.5433e-04 - accuracy: 1.0000 - val_loss: 4.4476 - val_accuracy: 0.4865\n",
      "Epoch 321/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.1772e-04 - accuracy: 1.0000 - val_loss: 4.4566 - val_accuracy: 0.4910\n",
      "Epoch 322/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.5924e-04 - accuracy: 1.0000 - val_loss: 4.4633 - val_accuracy: 0.4865\n",
      "Epoch 323/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.4030e-04 - accuracy: 1.0000 - val_loss: 4.4769 - val_accuracy: 0.4910\n",
      "Epoch 324/1000\n",
      "1990/1990 [==============================] - 0s 87us/step - loss: 5.2165e-04 - accuracy: 1.0000 - val_loss: 4.4630 - val_accuracy: 0.4775\n",
      "Epoch 325/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 5.1659e-04 - accuracy: 1.0000 - val_loss: 4.4651 - val_accuracy: 0.4865\n",
      "Epoch 326/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.0466e-04 - accuracy: 1.0000 - val_loss: 4.4837 - val_accuracy: 0.4865\n",
      "Epoch 327/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.0614e-04 - accuracy: 1.0000 - val_loss: 4.4922 - val_accuracy: 0.4865\n",
      "Epoch 328/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 4.7692e-04 - accuracy: 1.0000 - val_loss: 4.4979 - val_accuracy: 0.4820\n",
      "Epoch 329/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 4.6550e-04 - accuracy: 1.0000 - val_loss: 4.5122 - val_accuracy: 0.4910\n",
      "Epoch 330/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 4.6811e-04 - accuracy: 1.0000 - val_loss: 4.5104 - val_accuracy: 0.4955\n",
      "Epoch 331/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 4.5234e-04 - accuracy: 1.0000 - val_loss: 4.5210 - val_accuracy: 0.4865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.3693e-04 - accuracy: 1.0000 - val_loss: 4.5375 - val_accuracy: 0.4865\n",
      "Epoch 333/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.3417e-04 - accuracy: 1.0000 - val_loss: 4.5434 - val_accuracy: 0.4910\n",
      "Epoch 334/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 4.4036e-04 - accuracy: 1.0000 - val_loss: 4.5483 - val_accuracy: 0.4865\n",
      "Epoch 335/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 4.1160e-04 - accuracy: 1.0000 - val_loss: 4.5515 - val_accuracy: 0.4910\n",
      "Epoch 336/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 4.1954e-04 - accuracy: 1.0000 - val_loss: 4.5668 - val_accuracy: 0.4865\n",
      "Epoch 337/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 4.0923e-04 - accuracy: 1.0000 - val_loss: 4.5784 - val_accuracy: 0.4910\n",
      "Epoch 338/1000\n",
      "1990/1990 [==============================] - 0s 86us/step - loss: 4.0947e-04 - accuracy: 1.0000 - val_loss: 4.5837 - val_accuracy: 0.4865\n",
      "Epoch 339/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 3.8391e-04 - accuracy: 1.0000 - val_loss: 4.5809 - val_accuracy: 0.4865\n",
      "Epoch 340/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 4.0077e-04 - accuracy: 1.0000 - val_loss: 4.5872 - val_accuracy: 0.4910\n",
      "Epoch 341/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 3.9692e-04 - accuracy: 1.0000 - val_loss: 4.5960 - val_accuracy: 0.4910\n",
      "Epoch 342/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 3.8702e-04 - accuracy: 1.0000 - val_loss: 4.5965 - val_accuracy: 0.4910\n",
      "Epoch 343/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 3.5996e-04 - accuracy: 1.0000 - val_loss: 4.6064 - val_accuracy: 0.4865\n",
      "Epoch 344/1000\n",
      "1990/1990 [==============================] - 0s 130us/step - loss: 3.8531e-04 - accuracy: 1.0000 - val_loss: 4.6187 - val_accuracy: 0.4910\n",
      "Epoch 345/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 3.6159e-04 - accuracy: 1.0000 - val_loss: 4.6293 - val_accuracy: 0.4910\n",
      "Epoch 346/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 3.6288e-04 - accuracy: 1.0000 - val_loss: 4.6256 - val_accuracy: 0.4955\n",
      "Epoch 347/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 3.5099e-04 - accuracy: 1.0000 - val_loss: 4.6309 - val_accuracy: 0.4955\n",
      "Epoch 348/1000\n",
      "1990/1990 [==============================] - 0s 156us/step - loss: 3.4839e-04 - accuracy: 1.0000 - val_loss: 4.6444 - val_accuracy: 0.4865\n",
      "Epoch 349/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 3.4492e-04 - accuracy: 1.0000 - val_loss: 4.6473 - val_accuracy: 0.4910\n",
      "Epoch 350/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 3.2922e-04 - accuracy: 1.0000 - val_loss: 4.6501 - val_accuracy: 0.4910\n",
      "Epoch 351/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 3.2485e-04 - accuracy: 1.0000 - val_loss: 4.6556 - val_accuracy: 0.4910\n",
      "Epoch 352/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.2587e-04 - accuracy: 1.0000 - val_loss: 4.6719 - val_accuracy: 0.4865\n",
      "Epoch 353/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 3.0848e-04 - accuracy: 1.0000 - val_loss: 4.6652 - val_accuracy: 0.4910\n",
      "Epoch 354/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 3.1164e-04 - accuracy: 1.0000 - val_loss: 4.6760 - val_accuracy: 0.4910\n",
      "Epoch 355/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 3.1815e-04 - accuracy: 1.0000 - val_loss: 4.6825 - val_accuracy: 0.4910\n",
      "Epoch 356/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 3.0567e-04 - accuracy: 1.0000 - val_loss: 4.6894 - val_accuracy: 0.4910\n",
      "Epoch 357/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 2.7877e-04 - accuracy: 1.0000 - val_loss: 4.6926 - val_accuracy: 0.4910\n",
      "Epoch 358/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 2.8914e-04 - accuracy: 1.0000 - val_loss: 4.6937 - val_accuracy: 0.4910\n",
      "Epoch 359/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 2.8830e-04 - accuracy: 1.0000 - val_loss: 4.6973 - val_accuracy: 0.4955\n",
      "Epoch 360/1000\n",
      "1990/1990 [==============================] - 0s 87us/step - loss: 2.8832e-04 - accuracy: 1.0000 - val_loss: 4.7147 - val_accuracy: 0.4910\n",
      "Epoch 361/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 2.8016e-04 - accuracy: 1.0000 - val_loss: 4.7181 - val_accuracy: 0.4910\n",
      "Epoch 362/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 2.5803e-04 - accuracy: 1.0000 - val_loss: 4.7221 - val_accuracy: 0.4955\n",
      "Epoch 363/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 2.5849e-04 - accuracy: 1.0000 - val_loss: 4.7303 - val_accuracy: 0.4910\n",
      "Epoch 364/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 2.7780e-04 - accuracy: 1.0000 - val_loss: 4.7311 - val_accuracy: 0.4910\n",
      "Epoch 365/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 2.5572e-04 - accuracy: 1.0000 - val_loss: 4.7397 - val_accuracy: 0.4910\n",
      "Epoch 366/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 2.5787e-04 - accuracy: 1.0000 - val_loss: 4.7441 - val_accuracy: 0.4910\n",
      "Epoch 367/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 2.5554e-04 - accuracy: 1.0000 - val_loss: 4.7482 - val_accuracy: 0.4910\n",
      "Epoch 368/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 2.5661e-04 - accuracy: 1.0000 - val_loss: 4.7608 - val_accuracy: 0.4910\n",
      "Epoch 369/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.4560e-04 - accuracy: 1.0000 - val_loss: 4.7672 - val_accuracy: 0.4910\n",
      "Epoch 370/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 2.4376e-04 - accuracy: 1.0000 - val_loss: 4.7654 - val_accuracy: 0.4910\n",
      "Epoch 371/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.4439e-04 - accuracy: 1.0000 - val_loss: 4.7703 - val_accuracy: 0.4910\n",
      "Epoch 372/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 2.2267e-04 - accuracy: 1.0000 - val_loss: 4.7835 - val_accuracy: 0.4865\n",
      "Epoch 373/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 2.1846e-04 - accuracy: 1.0000 - val_loss: 4.7877 - val_accuracy: 0.4865\n",
      "Epoch 374/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.2741e-04 - accuracy: 1.0000 - val_loss: 4.7924 - val_accuracy: 0.4910\n",
      "Epoch 375/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 2.2176e-04 - accuracy: 1.0000 - val_loss: 4.7885 - val_accuracy: 0.4910\n",
      "Epoch 376/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 2.2169e-04 - accuracy: 1.0000 - val_loss: 4.7964 - val_accuracy: 0.4910\n",
      "Epoch 377/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.2109e-04 - accuracy: 1.0000 - val_loss: 4.8080 - val_accuracy: 0.4910\n",
      "Epoch 378/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.0939e-04 - accuracy: 1.0000 - val_loss: 4.8110 - val_accuracy: 0.4910\n",
      "Epoch 379/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.2118e-04 - accuracy: 1.0000 - val_loss: 4.8131 - val_accuracy: 0.4910\n",
      "Epoch 380/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 2.1352e-04 - accuracy: 1.0000 - val_loss: 4.8202 - val_accuracy: 0.4910\n",
      "Epoch 381/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 2.1356e-04 - accuracy: 1.0000 - val_loss: 4.8296 - val_accuracy: 0.4910\n",
      "Epoch 382/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 2.0075e-04 - accuracy: 1.0000 - val_loss: 4.8319 - val_accuracy: 0.4910\n",
      "Epoch 383/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.8989e-04 - accuracy: 1.0000 - val_loss: 4.8311 - val_accuracy: 0.4910\n",
      "Epoch 384/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 1.9882e-04 - accuracy: 1.0000 - val_loss: 4.8415 - val_accuracy: 0.4910\n",
      "Epoch 385/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.9325e-04 - accuracy: 1.0000 - val_loss: 4.8494 - val_accuracy: 0.4910\n",
      "Epoch 386/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 1.8855e-04 - accuracy: 1.0000 - val_loss: 4.8465 - val_accuracy: 0.4910\n",
      "Epoch 387/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 1.8903e-04 - accuracy: 1.0000 - val_loss: 4.8580 - val_accuracy: 0.4910\n",
      "Epoch 388/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 1.8901e-04 - accuracy: 1.0000 - val_loss: 4.8688 - val_accuracy: 0.4910\n",
      "Epoch 389/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 1.8790e-04 - accuracy: 1.0000 - val_loss: 4.8699 - val_accuracy: 0.4910\n",
      "Epoch 390/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.0039e-04 - accuracy: 1.0000 - val_loss: 4.8629 - val_accuracy: 0.4910\n",
      "Epoch 391/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.8903e-04 - accuracy: 1.0000 - val_loss: 4.8656 - val_accuracy: 0.4910\n",
      "Epoch 392/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.8528e-04 - accuracy: 1.0000 - val_loss: 4.8930 - val_accuracy: 0.4910\n",
      "Epoch 393/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 1.7990e-04 - accuracy: 1.0000 - val_loss: 4.8863 - val_accuracy: 0.4910\n",
      "Epoch 394/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 1.8386e-04 - accuracy: 1.0000 - val_loss: 4.8922 - val_accuracy: 0.4865\n",
      "Epoch 395/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.6810e-04 - accuracy: 1.0000 - val_loss: 4.9002 - val_accuracy: 0.4910\n",
      "Epoch 396/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7126e-04 - accuracy: 1.0000 - val_loss: 4.9058 - val_accuracy: 0.4910\n",
      "Epoch 397/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.6036e-04 - accuracy: 1.0000 - val_loss: 4.9074 - val_accuracy: 0.4910\n",
      "Epoch 398/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.7164e-04 - accuracy: 1.0000 - val_loss: 4.9193 - val_accuracy: 0.4910\n",
      "Epoch 399/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 1.6615e-04 - accuracy: 1.0000 - val_loss: 4.9112 - val_accuracy: 0.4910\n",
      "Epoch 400/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 1.6374e-04 - accuracy: 1.0000 - val_loss: 4.9110 - val_accuracy: 0.4910\n",
      "Epoch 401/1000\n",
      "1990/1990 [==============================] - 0s 131us/step - loss: 1.6309e-04 - accuracy: 1.0000 - val_loss: 4.9313 - val_accuracy: 0.4910\n",
      "Epoch 402/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.6410e-04 - accuracy: 1.0000 - val_loss: 4.9326 - val_accuracy: 0.4910\n",
      "Epoch 403/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.4641e-04 - accuracy: 1.0000 - val_loss: 4.9346 - val_accuracy: 0.4910\n",
      "Epoch 404/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 1.6110e-04 - accuracy: 1.0000 - val_loss: 4.9442 - val_accuracy: 0.4910\n",
      "Epoch 405/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 1.6054e-04 - accuracy: 1.0000 - val_loss: 4.9508 - val_accuracy: 0.4910\n",
      "Epoch 406/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 1.5457e-04 - accuracy: 1.0000 - val_loss: 4.9607 - val_accuracy: 0.4910\n",
      "Epoch 407/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 1.4264e-04 - accuracy: 1.0000 - val_loss: 4.9638 - val_accuracy: 0.4910\n",
      "Epoch 408/1000\n",
      "1990/1990 [==============================] - 0s 139us/step - loss: 1.4391e-04 - accuracy: 1.0000 - val_loss: 4.9668 - val_accuracy: 0.4910\n",
      "Epoch 409/1000\n",
      "1990/1990 [==============================] - 0s 157us/step - loss: 1.5436e-04 - accuracy: 1.0000 - val_loss: 4.9665 - val_accuracy: 0.4910\n",
      "Epoch 410/1000\n",
      "1990/1990 [==============================] - 0s 137us/step - loss: 1.4470e-04 - accuracy: 1.0000 - val_loss: 4.9688 - val_accuracy: 0.4910\n",
      "Epoch 411/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 1.4749e-04 - accuracy: 1.0000 - val_loss: 4.9848 - val_accuracy: 0.4910\n",
      "Epoch 412/1000\n",
      "1990/1990 [==============================] - 0s 125us/step - loss: 1.5016e-04 - accuracy: 1.0000 - val_loss: 4.9823 - val_accuracy: 0.4910\n",
      "Epoch 413/1000\n",
      "1990/1990 [==============================] - 0s 136us/step - loss: 1.3885e-04 - accuracy: 1.0000 - val_loss: 4.9864 - val_accuracy: 0.4910\n",
      "Epoch 414/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 1.3748e-04 - accuracy: 1.0000 - val_loss: 4.9960 - val_accuracy: 0.4865\n",
      "Epoch 415/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 1.4117e-04 - accuracy: 1.0000 - val_loss: 4.9934 - val_accuracy: 0.4865\n",
      "Epoch 416/1000\n",
      "1990/1990 [==============================] - 0s 126us/step - loss: 1.3092e-04 - accuracy: 1.0000 - val_loss: 4.9971 - val_accuracy: 0.4910\n",
      "Epoch 417/1000\n",
      "1990/1990 [==============================] - 0s 158us/step - loss: 1.2880e-04 - accuracy: 1.0000 - val_loss: 4.9986 - val_accuracy: 0.4865\n",
      "Epoch 418/1000\n",
      "1990/1990 [==============================] - 0s 127us/step - loss: 1.2617e-04 - accuracy: 1.0000 - val_loss: 5.0014 - val_accuracy: 0.4820\n",
      "Epoch 419/1000\n",
      "1990/1990 [==============================] - 0s 143us/step - loss: 1.2658e-04 - accuracy: 1.0000 - val_loss: 5.0071 - val_accuracy: 0.4865\n",
      "Epoch 420/1000\n",
      "1990/1990 [==============================] - 0s 135us/step - loss: 1.2396e-04 - accuracy: 1.0000 - val_loss: 5.0131 - val_accuracy: 0.4910\n",
      "Epoch 421/1000\n",
      "1990/1990 [==============================] - 0s 153us/step - loss: 1.2414e-04 - accuracy: 1.0000 - val_loss: 5.0269 - val_accuracy: 0.4865\n",
      "Epoch 422/1000\n",
      "1990/1990 [==============================] - 0s 144us/step - loss: 1.2387e-04 - accuracy: 1.0000 - val_loss: 5.0326 - val_accuracy: 0.4865\n",
      "Epoch 423/1000\n",
      "1990/1990 [==============================] - 0s 131us/step - loss: 1.2522e-04 - accuracy: 1.0000 - val_loss: 5.0378 - val_accuracy: 0.4910\n",
      "Epoch 424/1000\n",
      "1990/1990 [==============================] - 0s 138us/step - loss: 1.1654e-04 - accuracy: 1.0000 - val_loss: 5.0413 - val_accuracy: 0.4910\n",
      "Epoch 425/1000\n",
      "1990/1990 [==============================] - 0s 138us/step - loss: 1.1729e-04 - accuracy: 1.0000 - val_loss: 5.0467 - val_accuracy: 0.4910\n",
      "Epoch 426/1000\n",
      "1990/1990 [==============================] - 0s 159us/step - loss: 1.2015e-04 - accuracy: 1.0000 - val_loss: 5.0458 - val_accuracy: 0.4955\n",
      "Epoch 427/1000\n",
      "1990/1990 [==============================] - 0s 143us/step - loss: 1.1001e-04 - accuracy: 1.0000 - val_loss: 5.0451 - val_accuracy: 0.4955\n",
      "Epoch 428/1000\n",
      "1990/1990 [==============================] - 0s 136us/step - loss: 1.1686e-04 - accuracy: 1.0000 - val_loss: 5.0587 - val_accuracy: 0.4955\n",
      "Epoch 429/1000\n",
      "1990/1990 [==============================] - 0s 150us/step - loss: 1.1310e-04 - accuracy: 1.0000 - val_loss: 5.0598 - val_accuracy: 0.4910\n",
      "Epoch 430/1000\n",
      "1990/1990 [==============================] - 0s 159us/step - loss: 1.1206e-04 - accuracy: 1.0000 - val_loss: 5.0673 - val_accuracy: 0.4910\n",
      "Epoch 431/1000\n",
      "1990/1990 [==============================] - 0s 146us/step - loss: 1.1084e-04 - accuracy: 1.0000 - val_loss: 5.0729 - val_accuracy: 0.4865\n",
      "Epoch 432/1000\n",
      "1990/1990 [==============================] - 0s 155us/step - loss: 1.0658e-04 - accuracy: 1.0000 - val_loss: 5.0674 - val_accuracy: 0.4910\n",
      "Epoch 433/1000\n",
      "1990/1990 [==============================] - 0s 141us/step - loss: 1.0518e-04 - accuracy: 1.0000 - val_loss: 5.0724 - val_accuracy: 0.4955\n",
      "Epoch 434/1000\n",
      "1990/1990 [==============================] - 0s 156us/step - loss: 1.0544e-04 - accuracy: 1.0000 - val_loss: 5.0789 - val_accuracy: 0.4910\n",
      "Epoch 435/1000\n",
      "1990/1990 [==============================] - 0s 135us/step - loss: 1.1260e-04 - accuracy: 1.0000 - val_loss: 5.0910 - val_accuracy: 0.4955\n",
      "Epoch 436/1000\n",
      "1990/1990 [==============================] - 0s 161us/step - loss: 1.0209e-04 - accuracy: 1.0000 - val_loss: 5.0936 - val_accuracy: 0.4865\n",
      "Epoch 437/1000\n",
      "1990/1990 [==============================] - 0s 141us/step - loss: 1.0711e-04 - accuracy: 1.0000 - val_loss: 5.0953 - val_accuracy: 0.4910\n",
      "Epoch 438/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 190us/step - loss: 9.9100e-05 - accuracy: 1.0000 - val_loss: 5.0880 - val_accuracy: 0.4910\n",
      "Epoch 439/1000\n",
      "1990/1990 [==============================] - 0s 189us/step - loss: 9.9842e-05 - accuracy: 1.0000 - val_loss: 5.1033 - val_accuracy: 0.4910\n",
      "Epoch 440/1000\n",
      "1990/1990 [==============================] - 0s 158us/step - loss: 1.0230e-04 - accuracy: 1.0000 - val_loss: 5.1076 - val_accuracy: 0.4865\n",
      "Epoch 441/1000\n",
      "1990/1990 [==============================] - 0s 141us/step - loss: 1.0246e-04 - accuracy: 1.0000 - val_loss: 5.1152 - val_accuracy: 0.4910\n",
      "Epoch 442/1000\n",
      "1990/1990 [==============================] - 0s 137us/step - loss: 9.3670e-05 - accuracy: 1.0000 - val_loss: 5.1174 - val_accuracy: 0.4910\n",
      "Epoch 443/1000\n",
      "1990/1990 [==============================] - 0s 130us/step - loss: 1.0104e-04 - accuracy: 1.0000 - val_loss: 5.1112 - val_accuracy: 0.4955\n",
      "Epoch 444/1000\n",
      "1990/1990 [==============================] - 0s 138us/step - loss: 9.4277e-05 - accuracy: 1.0000 - val_loss: 5.1175 - val_accuracy: 0.4955\n",
      "Epoch 445/1000\n",
      "1990/1990 [==============================] - 0s 144us/step - loss: 9.8694e-05 - accuracy: 1.0000 - val_loss: 5.1289 - val_accuracy: 0.4910\n",
      "Epoch 446/1000\n",
      "1990/1990 [==============================] - 0s 132us/step - loss: 9.7013e-05 - accuracy: 1.0000 - val_loss: 5.1330 - val_accuracy: 0.4910\n",
      "Epoch 447/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 8.8923e-05 - accuracy: 1.0000 - val_loss: 5.1405 - val_accuracy: 0.4910\n",
      "Epoch 448/1000\n",
      "1990/1990 [==============================] - 0s 124us/step - loss: 9.2653e-05 - accuracy: 1.0000 - val_loss: 5.1413 - val_accuracy: 0.4865\n",
      "Epoch 449/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 8.7326e-05 - accuracy: 1.0000 - val_loss: 5.1429 - val_accuracy: 0.4865\n",
      "Epoch 450/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 9.3006e-05 - accuracy: 1.0000 - val_loss: 5.1493 - val_accuracy: 0.4910\n",
      "Epoch 451/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 9.1383e-05 - accuracy: 1.0000 - val_loss: 5.1510 - val_accuracy: 0.4865\n",
      "Epoch 452/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 8.7571e-05 - accuracy: 1.0000 - val_loss: 5.1528 - val_accuracy: 0.4910\n",
      "Epoch 453/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 8.8591e-05 - accuracy: 1.0000 - val_loss: 5.1536 - val_accuracy: 0.4910\n",
      "Epoch 454/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 8.3502e-05 - accuracy: 1.0000 - val_loss: 5.1635 - val_accuracy: 0.4910\n",
      "Epoch 455/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 8.0654e-05 - accuracy: 1.0000 - val_loss: 5.1613 - val_accuracy: 0.4910\n",
      "Epoch 456/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 7.9218e-05 - accuracy: 1.0000 - val_loss: 5.1679 - val_accuracy: 0.4955\n",
      "Epoch 457/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 8.1045e-05 - accuracy: 1.0000 - val_loss: 5.1736 - val_accuracy: 0.4955\n",
      "Epoch 458/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 8.4792e-05 - accuracy: 1.0000 - val_loss: 5.1808 - val_accuracy: 0.4865\n",
      "Epoch 459/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 8.3857e-05 - accuracy: 1.0000 - val_loss: 5.1855 - val_accuracy: 0.4910\n",
      "Epoch 460/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 8.6729e-05 - accuracy: 1.0000 - val_loss: 5.1881 - val_accuracy: 0.4865\n",
      "Epoch 461/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 8.2971e-05 - accuracy: 1.0000 - val_loss: 5.1928 - val_accuracy: 0.4865\n",
      "Epoch 462/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 7.8725e-05 - accuracy: 1.0000 - val_loss: 5.1956 - val_accuracy: 0.4910\n",
      "Epoch 463/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 7.8893e-05 - accuracy: 1.0000 - val_loss: 5.2044 - val_accuracy: 0.4910\n",
      "Epoch 464/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 7.7524e-05 - accuracy: 1.0000 - val_loss: 5.2046 - val_accuracy: 0.4910\n",
      "Epoch 465/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 8.0814e-05 - accuracy: 1.0000 - val_loss: 5.2121 - val_accuracy: 0.4820\n",
      "Epoch 466/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 7.6624e-05 - accuracy: 1.0000 - val_loss: 5.2112 - val_accuracy: 0.4865\n",
      "Epoch 467/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 7.1420e-05 - accuracy: 1.0000 - val_loss: 5.2177 - val_accuracy: 0.4910\n",
      "Epoch 468/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 7.4014e-05 - accuracy: 1.0000 - val_loss: 5.2291 - val_accuracy: 0.4955\n",
      "Epoch 469/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 7.1745e-05 - accuracy: 1.0000 - val_loss: 5.2344 - val_accuracy: 0.4955\n",
      "Epoch 470/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 7.9410e-05 - accuracy: 1.0000 - val_loss: 5.2334 - val_accuracy: 0.4955\n",
      "Epoch 471/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 7.3653e-05 - accuracy: 1.0000 - val_loss: 5.2435 - val_accuracy: 0.4865\n",
      "Epoch 472/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 7.8827e-05 - accuracy: 1.0000 - val_loss: 5.2472 - val_accuracy: 0.4865\n",
      "Epoch 473/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.8203e-05 - accuracy: 1.0000 - val_loss: 5.2561 - val_accuracy: 0.4865\n",
      "Epoch 474/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.4740e-05 - accuracy: 1.0000 - val_loss: 5.2460 - val_accuracy: 0.4910\n",
      "Epoch 475/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 7.1892e-05 - accuracy: 1.0000 - val_loss: 5.2510 - val_accuracy: 0.4910\n",
      "Epoch 476/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 6.8199e-05 - accuracy: 1.0000 - val_loss: 5.2624 - val_accuracy: 0.4910\n",
      "Epoch 477/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 6.6015e-05 - accuracy: 1.0000 - val_loss: 5.2724 - val_accuracy: 0.4910\n",
      "Epoch 478/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.9991e-05 - accuracy: 1.0000 - val_loss: 5.2766 - val_accuracy: 0.4910\n",
      "Epoch 479/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 6.2725e-05 - accuracy: 1.0000 - val_loss: 5.2741 - val_accuracy: 0.4910\n",
      "Epoch 480/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 6.5721e-05 - accuracy: 1.0000 - val_loss: 5.2763 - val_accuracy: 0.4910\n",
      "Epoch 481/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.4196e-05 - accuracy: 1.0000 - val_loss: 5.2801 - val_accuracy: 0.4865\n",
      "Epoch 482/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 6.3802e-05 - accuracy: 1.0000 - val_loss: 5.2833 - val_accuracy: 0.4910\n",
      "Epoch 483/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 6.2715e-05 - accuracy: 1.0000 - val_loss: 5.2886 - val_accuracy: 0.4910\n",
      "Epoch 484/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 5.7222e-05 - accuracy: 1.0000 - val_loss: 5.2853 - val_accuracy: 0.4955\n",
      "Epoch 485/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.2669e-05 - accuracy: 1.0000 - val_loss: 5.2925 - val_accuracy: 0.4955\n",
      "Epoch 486/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.0483e-05 - accuracy: 1.0000 - val_loss: 5.2879 - val_accuracy: 0.4955\n",
      "Epoch 487/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 5.7788e-05 - accuracy: 1.0000 - val_loss: 5.2914 - val_accuracy: 0.4910\n",
      "Epoch 488/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 5.6656e-05 - accuracy: 1.0000 - val_loss: 5.2916 - val_accuracy: 0.4955\n",
      "Epoch 489/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 5.6608e-05 - accuracy: 1.0000 - val_loss: 5.2964 - val_accuracy: 0.4955\n",
      "Epoch 490/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 5.9707e-05 - accuracy: 1.0000 - val_loss: 5.3067 - val_accuracy: 0.4910\n",
      "Epoch 491/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 5.4804e-05 - accuracy: 1.0000 - val_loss: 5.3102 - val_accuracy: 0.4955\n",
      "Epoch 492/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 5.6624e-05 - accuracy: 1.0000 - val_loss: 5.3114 - val_accuracy: 0.4910\n",
      "Epoch 493/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 6.1599e-05 - accuracy: 1.0000 - val_loss: 5.3180 - val_accuracy: 0.4865\n",
      "Epoch 494/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 5.6297e-05 - accuracy: 1.0000 - val_loss: 5.3314 - val_accuracy: 0.4910\n",
      "Epoch 495/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 5.5868e-05 - accuracy: 1.0000 - val_loss: 5.3297 - val_accuracy: 0.4955\n",
      "Epoch 496/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 5.6084e-05 - accuracy: 1.0000 - val_loss: 5.3276 - val_accuracy: 0.4955\n",
      "Epoch 497/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 5.4866e-05 - accuracy: 1.0000 - val_loss: 5.3388 - val_accuracy: 0.4910\n",
      "Epoch 498/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 5.0748e-05 - accuracy: 1.0000 - val_loss: 5.3455 - val_accuracy: 0.4955\n",
      "Epoch 499/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 5.1625e-05 - accuracy: 1.0000 - val_loss: 5.3483 - val_accuracy: 0.4955\n",
      "Epoch 500/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 5.5415e-05 - accuracy: 1.0000 - val_loss: 5.3425 - val_accuracy: 0.4955\n",
      "Epoch 501/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 5.1946e-05 - accuracy: 1.0000 - val_loss: 5.3545 - val_accuracy: 0.4910\n",
      "Epoch 502/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.9580e-05 - accuracy: 1.0000 - val_loss: 5.3651 - val_accuracy: 0.4910\n",
      "Epoch 503/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 5.0331e-05 - accuracy: 1.0000 - val_loss: 5.3718 - val_accuracy: 0.4910\n",
      "Epoch 504/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 5.0224e-05 - accuracy: 1.0000 - val_loss: 5.3726 - val_accuracy: 0.4910\n",
      "Epoch 505/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 5.0632e-05 - accuracy: 1.0000 - val_loss: 5.3721 - val_accuracy: 0.4910\n",
      "Epoch 506/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 5.2333e-05 - accuracy: 1.0000 - val_loss: 5.3776 - val_accuracy: 0.4865\n",
      "Epoch 507/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 4.7939e-05 - accuracy: 1.0000 - val_loss: 5.3913 - val_accuracy: 0.4865\n",
      "Epoch 508/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 4.5973e-05 - accuracy: 1.0000 - val_loss: 5.3963 - val_accuracy: 0.4865\n",
      "Epoch 509/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 4.9205e-05 - accuracy: 1.0000 - val_loss: 5.3994 - val_accuracy: 0.4865\n",
      "Epoch 510/1000\n",
      "1990/1990 [==============================] - 0s 135us/step - loss: 4.8500e-05 - accuracy: 1.0000 - val_loss: 5.4065 - val_accuracy: 0.4910\n",
      "Epoch 511/1000\n",
      "1990/1990 [==============================] - 0s 129us/step - loss: 4.6144e-05 - accuracy: 1.0000 - val_loss: 5.4098 - val_accuracy: 0.4910\n",
      "Epoch 512/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 4.1815e-05 - accuracy: 1.0000 - val_loss: 5.4055 - val_accuracy: 0.4910\n",
      "Epoch 513/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 4.5065e-05 - accuracy: 1.0000 - val_loss: 5.4096 - val_accuracy: 0.4955\n",
      "Epoch 514/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 5.0618e-05 - accuracy: 1.0000 - val_loss: 5.4078 - val_accuracy: 0.4865\n",
      "Epoch 515/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 4.3961e-05 - accuracy: 1.0000 - val_loss: 5.4090 - val_accuracy: 0.4910\n",
      "Epoch 516/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 4.8089e-05 - accuracy: 1.0000 - val_loss: 5.4079 - val_accuracy: 0.4865\n",
      "Epoch 517/1000\n",
      "1990/1990 [==============================] - 0s 125us/step - loss: 4.4348e-05 - accuracy: 1.0000 - val_loss: 5.4079 - val_accuracy: 0.4865\n",
      "Epoch 518/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 4.4316e-05 - accuracy: 1.0000 - val_loss: 5.4108 - val_accuracy: 0.4865\n",
      "Epoch 519/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 4.2566e-05 - accuracy: 1.0000 - val_loss: 5.4165 - val_accuracy: 0.4865\n",
      "Epoch 520/1000\n",
      "1990/1990 [==============================] - 0s 134us/step - loss: 4.3012e-05 - accuracy: 1.0000 - val_loss: 5.4305 - val_accuracy: 0.4910\n",
      "Epoch 521/1000\n",
      "1990/1990 [==============================] - 0s 127us/step - loss: 4.3534e-05 - accuracy: 1.0000 - val_loss: 5.4331 - val_accuracy: 0.4910\n",
      "Epoch 522/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 4.0443e-05 - accuracy: 1.0000 - val_loss: 5.4456 - val_accuracy: 0.4820\n",
      "Epoch 523/1000\n",
      "1990/1990 [==============================] - 0s 150us/step - loss: 4.3158e-05 - accuracy: 1.0000 - val_loss: 5.4479 - val_accuracy: 0.4820\n",
      "Epoch 524/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 3.8592e-05 - accuracy: 1.0000 - val_loss: 5.4525 - val_accuracy: 0.4865\n",
      "Epoch 525/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 4.2812e-05 - accuracy: 1.0000 - val_loss: 5.4589 - val_accuracy: 0.4865\n",
      "Epoch 526/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 4.0070e-05 - accuracy: 1.0000 - val_loss: 5.4645 - val_accuracy: 0.4820\n",
      "Epoch 527/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 4.0503e-05 - accuracy: 1.0000 - val_loss: 5.4632 - val_accuracy: 0.4910\n",
      "Epoch 528/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 3.9998e-05 - accuracy: 1.0000 - val_loss: 5.4726 - val_accuracy: 0.4865\n",
      "Epoch 529/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 4.2449e-05 - accuracy: 1.0000 - val_loss: 5.4770 - val_accuracy: 0.4820\n",
      "Epoch 530/1000\n",
      "1990/1990 [==============================] - 0s 123us/step - loss: 4.1026e-05 - accuracy: 1.0000 - val_loss: 5.4804 - val_accuracy: 0.4820\n",
      "Epoch 531/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 4.1960e-05 - accuracy: 1.0000 - val_loss: 5.4736 - val_accuracy: 0.4820\n",
      "Epoch 532/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 3.8356e-05 - accuracy: 1.0000 - val_loss: 5.4927 - val_accuracy: 0.4820\n",
      "Epoch 533/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 3.6292e-05 - accuracy: 1.0000 - val_loss: 5.4960 - val_accuracy: 0.4865\n",
      "Epoch 534/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 3.7695e-05 - accuracy: 1.0000 - val_loss: 5.5085 - val_accuracy: 0.4775\n",
      "Epoch 535/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 3.6857e-05 - accuracy: 1.0000 - val_loss: 5.5088 - val_accuracy: 0.4775\n",
      "Epoch 536/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 3.3615e-05 - accuracy: 1.0000 - val_loss: 5.5046 - val_accuracy: 0.4820\n",
      "Epoch 537/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 3.7189e-05 - accuracy: 1.0000 - val_loss: 5.5097 - val_accuracy: 0.4865\n",
      "Epoch 538/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 3.5257e-05 - accuracy: 1.0000 - val_loss: 5.5096 - val_accuracy: 0.4910\n",
      "Epoch 539/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 3.3235e-05 - accuracy: 1.0000 - val_loss: 5.5108 - val_accuracy: 0.4910\n",
      "Epoch 540/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 3.4874e-05 - accuracy: 1.0000 - val_loss: 5.5225 - val_accuracy: 0.4910\n",
      "Epoch 541/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 3.4715e-05 - accuracy: 1.0000 - val_loss: 5.5269 - val_accuracy: 0.4865\n",
      "Epoch 542/1000\n",
      "1990/1990 [==============================] - 0s 130us/step - loss: 3.3523e-05 - accuracy: 1.0000 - val_loss: 5.5302 - val_accuracy: 0.4865\n",
      "Epoch 543/1000\n",
      "1990/1990 [==============================] - 0s 148us/step - loss: 3.2717e-05 - accuracy: 1.0000 - val_loss: 5.5298 - val_accuracy: 0.4910\n",
      "Epoch 544/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 133us/step - loss: 3.5697e-05 - accuracy: 1.0000 - val_loss: 5.5247 - val_accuracy: 0.4865\n",
      "Epoch 545/1000\n",
      "1990/1990 [==============================] - 0s 137us/step - loss: 3.0795e-05 - accuracy: 1.0000 - val_loss: 5.5322 - val_accuracy: 0.4865\n",
      "Epoch 546/1000\n",
      "1990/1990 [==============================] - 0s 137us/step - loss: 3.1295e-05 - accuracy: 1.0000 - val_loss: 5.5400 - val_accuracy: 0.4865\n",
      "Epoch 547/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 3.3507e-05 - accuracy: 1.0000 - val_loss: 5.5441 - val_accuracy: 0.4910\n",
      "Epoch 548/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 3.3142e-05 - accuracy: 1.0000 - val_loss: 5.5429 - val_accuracy: 0.4865\n",
      "Epoch 549/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 3.1489e-05 - accuracy: 1.0000 - val_loss: 5.5560 - val_accuracy: 0.4910\n",
      "Epoch 550/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 3.0863e-05 - accuracy: 1.0000 - val_loss: 5.5577 - val_accuracy: 0.4865\n",
      "Epoch 551/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 3.0221e-05 - accuracy: 1.0000 - val_loss: 5.5621 - val_accuracy: 0.4865\n",
      "Epoch 552/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 3.0231e-05 - accuracy: 1.0000 - val_loss: 5.5585 - val_accuracy: 0.4910\n",
      "Epoch 553/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 3.2377e-05 - accuracy: 1.0000 - val_loss: 5.5658 - val_accuracy: 0.4910\n",
      "Epoch 554/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 3.3059e-05 - accuracy: 1.0000 - val_loss: 5.5705 - val_accuracy: 0.4910\n",
      "Epoch 555/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 3.1243e-05 - accuracy: 1.0000 - val_loss: 5.5840 - val_accuracy: 0.4910\n",
      "Epoch 556/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 2.9768e-05 - accuracy: 1.0000 - val_loss: 5.5768 - val_accuracy: 0.4910\n",
      "Epoch 557/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 3.0971e-05 - accuracy: 1.0000 - val_loss: 5.5695 - val_accuracy: 0.4910\n",
      "Epoch 558/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 2.9186e-05 - accuracy: 1.0000 - val_loss: 5.5733 - val_accuracy: 0.4910\n",
      "Epoch 559/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 3.2789e-05 - accuracy: 1.0000 - val_loss: 5.5768 - val_accuracy: 0.4865\n",
      "Epoch 560/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 2.9810e-05 - accuracy: 1.0000 - val_loss: 5.5800 - val_accuracy: 0.4865\n",
      "Epoch 561/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 3.0663e-05 - accuracy: 1.0000 - val_loss: 5.5987 - val_accuracy: 0.4865\n",
      "Epoch 562/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 2.9048e-05 - accuracy: 1.0000 - val_loss: 5.5998 - val_accuracy: 0.4820\n",
      "Epoch 563/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.6839e-05 - accuracy: 1.0000 - val_loss: 5.6012 - val_accuracy: 0.4910\n",
      "Epoch 564/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 2.7792e-05 - accuracy: 1.0000 - val_loss: 5.6023 - val_accuracy: 0.4865\n",
      "Epoch 565/1000\n",
      "1990/1990 [==============================] - 0s 126us/step - loss: 2.7815e-05 - accuracy: 1.0000 - val_loss: 5.6126 - val_accuracy: 0.4910\n",
      "Epoch 566/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.7417e-05 - accuracy: 1.0000 - val_loss: 5.6235 - val_accuracy: 0.4865\n",
      "Epoch 567/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 2.9106e-05 - accuracy: 1.0000 - val_loss: 5.6211 - val_accuracy: 0.4820\n",
      "Epoch 568/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 2.6823e-05 - accuracy: 1.0000 - val_loss: 5.6266 - val_accuracy: 0.4820\n",
      "Epoch 569/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 2.6074e-05 - accuracy: 1.0000 - val_loss: 5.6250 - val_accuracy: 0.4865\n",
      "Epoch 570/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.5410e-05 - accuracy: 1.0000 - val_loss: 5.6221 - val_accuracy: 0.4820\n",
      "Epoch 571/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 2.5700e-05 - accuracy: 1.0000 - val_loss: 5.6249 - val_accuracy: 0.4910\n",
      "Epoch 572/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.3499e-05 - accuracy: 1.0000 - val_loss: 5.6366 - val_accuracy: 0.4865\n",
      "Epoch 573/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 2.4116e-05 - accuracy: 1.0000 - val_loss: 5.6361 - val_accuracy: 0.4910\n",
      "Epoch 574/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.5541e-05 - accuracy: 1.0000 - val_loss: 5.6435 - val_accuracy: 0.4910\n",
      "Epoch 575/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 2.4708e-05 - accuracy: 1.0000 - val_loss: 5.6397 - val_accuracy: 0.4910\n",
      "Epoch 576/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.4414e-05 - accuracy: 1.0000 - val_loss: 5.6452 - val_accuracy: 0.4910\n",
      "Epoch 577/1000\n",
      "1990/1990 [==============================] - 0s 138us/step - loss: 2.4563e-05 - accuracy: 1.0000 - val_loss: 5.6542 - val_accuracy: 0.4910\n",
      "Epoch 578/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 2.2416e-05 - accuracy: 1.0000 - val_loss: 5.6481 - val_accuracy: 0.4865\n",
      "Epoch 579/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 2.3868e-05 - accuracy: 1.0000 - val_loss: 5.6577 - val_accuracy: 0.4910\n",
      "Epoch 580/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.4547e-05 - accuracy: 1.0000 - val_loss: 5.6606 - val_accuracy: 0.4910\n",
      "Epoch 581/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 2.2774e-05 - accuracy: 1.0000 - val_loss: 5.6731 - val_accuracy: 0.4865\n",
      "Epoch 582/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.2936e-05 - accuracy: 1.0000 - val_loss: 5.6714 - val_accuracy: 0.4865\n",
      "Epoch 583/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.2723e-05 - accuracy: 1.0000 - val_loss: 5.6851 - val_accuracy: 0.4865\n",
      "Epoch 584/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 2.3795e-05 - accuracy: 1.0000 - val_loss: 5.6876 - val_accuracy: 0.4865\n",
      "Epoch 585/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.2885e-05 - accuracy: 1.0000 - val_loss: 5.6867 - val_accuracy: 0.4820\n",
      "Epoch 586/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 2.3382e-05 - accuracy: 1.0000 - val_loss: 5.6812 - val_accuracy: 0.4865\n",
      "Epoch 587/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 2.1232e-05 - accuracy: 1.0000 - val_loss: 5.6919 - val_accuracy: 0.4865\n",
      "Epoch 588/1000\n",
      "1990/1990 [==============================] - 0s 124us/step - loss: 2.2311e-05 - accuracy: 1.0000 - val_loss: 5.6989 - val_accuracy: 0.4910\n",
      "Epoch 589/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 2.3005e-05 - accuracy: 1.0000 - val_loss: 5.7089 - val_accuracy: 0.4865\n",
      "Epoch 590/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.0644e-05 - accuracy: 1.0000 - val_loss: 5.7033 - val_accuracy: 0.4910\n",
      "Epoch 591/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 2.0346e-05 - accuracy: 1.0000 - val_loss: 5.7094 - val_accuracy: 0.4865\n",
      "Epoch 592/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 2.1714e-05 - accuracy: 1.0000 - val_loss: 5.7194 - val_accuracy: 0.4865\n",
      "Epoch 593/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 2.0209e-05 - accuracy: 1.0000 - val_loss: 5.7178 - val_accuracy: 0.4910\n",
      "Epoch 594/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.1807e-05 - accuracy: 1.0000 - val_loss: 5.7196 - val_accuracy: 0.4910\n",
      "Epoch 595/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.9544e-05 - accuracy: 1.0000 - val_loss: 5.7189 - val_accuracy: 0.4820\n",
      "Epoch 596/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.9244e-05 - accuracy: 1.0000 - val_loss: 5.7217 - val_accuracy: 0.4865\n",
      "Epoch 597/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.0558e-05 - accuracy: 1.0000 - val_loss: 5.7370 - val_accuracy: 0.4865\n",
      "Epoch 598/1000\n",
      "1990/1990 [==============================] - 0s 116us/step - loss: 2.1113e-05 - accuracy: 1.0000 - val_loss: 5.7349 - val_accuracy: 0.4820\n",
      "Epoch 599/1000\n",
      "1990/1990 [==============================] - 0s 129us/step - loss: 1.9884e-05 - accuracy: 1.0000 - val_loss: 5.7345 - val_accuracy: 0.4910\n",
      "Epoch 600/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 2.0814e-05 - accuracy: 1.0000 - val_loss: 5.7480 - val_accuracy: 0.4910\n",
      "Epoch 601/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 2.1037e-05 - accuracy: 1.0000 - val_loss: 5.7457 - val_accuracy: 0.4910\n",
      "Epoch 602/1000\n",
      "1990/1990 [==============================] - 0s 126us/step - loss: 1.9733e-05 - accuracy: 1.0000 - val_loss: 5.7353 - val_accuracy: 0.4865\n",
      "Epoch 603/1000\n",
      "1990/1990 [==============================] - 0s 124us/step - loss: 1.9210e-05 - accuracy: 1.0000 - val_loss: 5.7478 - val_accuracy: 0.4865\n",
      "Epoch 604/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.9021e-05 - accuracy: 1.0000 - val_loss: 5.7701 - val_accuracy: 0.4820\n",
      "Epoch 605/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.7884e-05 - accuracy: 1.0000 - val_loss: 5.7761 - val_accuracy: 0.4820\n",
      "Epoch 606/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.9107e-05 - accuracy: 1.0000 - val_loss: 5.7645 - val_accuracy: 0.4865\n",
      "Epoch 607/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.8238e-05 - accuracy: 1.0000 - val_loss: 5.7597 - val_accuracy: 0.4865\n",
      "Epoch 608/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.7318e-05 - accuracy: 1.0000 - val_loss: 5.7641 - val_accuracy: 0.4865\n",
      "Epoch 609/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7877e-05 - accuracy: 1.0000 - val_loss: 5.7683 - val_accuracy: 0.4910\n",
      "Epoch 610/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.7902e-05 - accuracy: 1.0000 - val_loss: 5.7760 - val_accuracy: 0.4910\n",
      "Epoch 611/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.7386e-05 - accuracy: 1.0000 - val_loss: 5.7745 - val_accuracy: 0.4865\n",
      "Epoch 612/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.7846e-05 - accuracy: 1.0000 - val_loss: 5.7680 - val_accuracy: 0.4865\n",
      "Epoch 613/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.6606e-05 - accuracy: 1.0000 - val_loss: 5.7768 - val_accuracy: 0.4865\n",
      "Epoch 614/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.7448e-05 - accuracy: 1.0000 - val_loss: 5.7862 - val_accuracy: 0.4910\n",
      "Epoch 615/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.6224e-05 - accuracy: 1.0000 - val_loss: 5.7917 - val_accuracy: 0.4910\n",
      "Epoch 616/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.5779e-05 - accuracy: 1.0000 - val_loss: 5.7960 - val_accuracy: 0.4865\n",
      "Epoch 617/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.6908e-05 - accuracy: 1.0000 - val_loss: 5.7873 - val_accuracy: 0.4865\n",
      "Epoch 618/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.7078e-05 - accuracy: 1.0000 - val_loss: 5.7945 - val_accuracy: 0.4865\n",
      "Epoch 619/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.6854e-05 - accuracy: 1.0000 - val_loss: 5.8090 - val_accuracy: 0.4820\n",
      "Epoch 620/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.6600e-05 - accuracy: 1.0000 - val_loss: 5.8198 - val_accuracy: 0.4865\n",
      "Epoch 621/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7285e-05 - accuracy: 1.0000 - val_loss: 5.8113 - val_accuracy: 0.4865\n",
      "Epoch 622/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.6471e-05 - accuracy: 1.0000 - val_loss: 5.8179 - val_accuracy: 0.4865\n",
      "Epoch 623/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 1.4990e-05 - accuracy: 1.0000 - val_loss: 5.8164 - val_accuracy: 0.4865\n",
      "Epoch 624/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.4055e-05 - accuracy: 1.0000 - val_loss: 5.8128 - val_accuracy: 0.4865\n",
      "Epoch 625/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.5219e-05 - accuracy: 1.0000 - val_loss: 5.8270 - val_accuracy: 0.4865\n",
      "Epoch 626/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.3891e-05 - accuracy: 1.0000 - val_loss: 5.8327 - val_accuracy: 0.4910\n",
      "Epoch 627/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.4746e-05 - accuracy: 1.0000 - val_loss: 5.8380 - val_accuracy: 0.4910\n",
      "Epoch 628/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.4265e-05 - accuracy: 1.0000 - val_loss: 5.8466 - val_accuracy: 0.4865\n",
      "Epoch 629/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.5257e-05 - accuracy: 1.0000 - val_loss: 5.8519 - val_accuracy: 0.4865\n",
      "Epoch 630/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.4496e-05 - accuracy: 1.0000 - val_loss: 5.8481 - val_accuracy: 0.4865\n",
      "Epoch 631/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.4123e-05 - accuracy: 1.0000 - val_loss: 5.8600 - val_accuracy: 0.4865\n",
      "Epoch 632/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.6364e-05 - accuracy: 1.0000 - val_loss: 5.8656 - val_accuracy: 0.4865\n",
      "Epoch 633/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.4266e-05 - accuracy: 1.0000 - val_loss: 5.8499 - val_accuracy: 0.4865\n",
      "Epoch 634/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.4775e-05 - accuracy: 1.0000 - val_loss: 5.8578 - val_accuracy: 0.4865\n",
      "Epoch 635/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.2859e-05 - accuracy: 1.0000 - val_loss: 5.8758 - val_accuracy: 0.4910\n",
      "Epoch 636/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.3595e-05 - accuracy: 1.0000 - val_loss: 5.8767 - val_accuracy: 0.4865\n",
      "Epoch 637/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.4408e-05 - accuracy: 1.0000 - val_loss: 5.8721 - val_accuracy: 0.4910\n",
      "Epoch 638/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.3250e-05 - accuracy: 1.0000 - val_loss: 5.8745 - val_accuracy: 0.4910\n",
      "Epoch 639/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.3893e-05 - accuracy: 1.0000 - val_loss: 5.8803 - val_accuracy: 0.4865\n",
      "Epoch 640/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.3822e-05 - accuracy: 1.0000 - val_loss: 5.8800 - val_accuracy: 0.4865\n",
      "Epoch 641/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.4108e-05 - accuracy: 1.0000 - val_loss: 5.8924 - val_accuracy: 0.4865\n",
      "Epoch 642/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.2580e-05 - accuracy: 1.0000 - val_loss: 5.8870 - val_accuracy: 0.4865\n",
      "Epoch 643/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.2950e-05 - accuracy: 1.0000 - val_loss: 5.8848 - val_accuracy: 0.4865\n",
      "Epoch 644/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.1889e-05 - accuracy: 1.0000 - val_loss: 5.8885 - val_accuracy: 0.4865\n",
      "Epoch 645/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.2145e-05 - accuracy: 1.0000 - val_loss: 5.8981 - val_accuracy: 0.4865\n",
      "Epoch 646/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.1531e-05 - accuracy: 1.0000 - val_loss: 5.9072 - val_accuracy: 0.4820\n",
      "Epoch 647/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.2332e-05 - accuracy: 1.0000 - val_loss: 5.9231 - val_accuracy: 0.4820\n",
      "Epoch 648/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.2051e-05 - accuracy: 1.0000 - val_loss: 5.9279 - val_accuracy: 0.4820\n",
      "Epoch 649/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.3279e-05 - accuracy: 1.0000 - val_loss: 5.9246 - val_accuracy: 0.4820\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.3427e-05 - accuracy: 1.0000 - val_loss: 5.9160 - val_accuracy: 0.4820\n",
      "Epoch 651/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.2152e-05 - accuracy: 1.0000 - val_loss: 5.9184 - val_accuracy: 0.4865\n",
      "Epoch 652/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.1742e-05 - accuracy: 1.0000 - val_loss: 5.9136 - val_accuracy: 0.4865\n",
      "Epoch 653/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.1889e-05 - accuracy: 1.0000 - val_loss: 5.9207 - val_accuracy: 0.4865\n",
      "Epoch 654/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.1384e-05 - accuracy: 1.0000 - val_loss: 5.9284 - val_accuracy: 0.4865\n",
      "Epoch 655/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.0876e-05 - accuracy: 1.0000 - val_loss: 5.9326 - val_accuracy: 0.4865\n",
      "Epoch 656/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.2021e-05 - accuracy: 1.0000 - val_loss: 5.9318 - val_accuracy: 0.4865\n",
      "Epoch 657/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 1.1909e-05 - accuracy: 1.0000 - val_loss: 5.9410 - val_accuracy: 0.4820\n",
      "Epoch 658/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 1.2820e-05 - accuracy: 1.0000 - val_loss: 5.9388 - val_accuracy: 0.4820\n",
      "Epoch 659/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 1.0977e-05 - accuracy: 1.0000 - val_loss: 5.9488 - val_accuracy: 0.4820\n",
      "Epoch 660/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0698e-05 - accuracy: 1.0000 - val_loss: 5.9499 - val_accuracy: 0.4820\n",
      "Epoch 661/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.1012e-05 - accuracy: 1.0000 - val_loss: 5.9586 - val_accuracy: 0.4820\n",
      "Epoch 662/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.1059e-05 - accuracy: 1.0000 - val_loss: 5.9605 - val_accuracy: 0.4820\n",
      "Epoch 663/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0780e-05 - accuracy: 1.0000 - val_loss: 5.9636 - val_accuracy: 0.4820\n",
      "Epoch 664/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0192e-05 - accuracy: 1.0000 - val_loss: 5.9648 - val_accuracy: 0.4820\n",
      "Epoch 665/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.0783e-05 - accuracy: 1.0000 - val_loss: 5.9774 - val_accuracy: 0.4820\n",
      "Epoch 666/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.0949e-05 - accuracy: 1.0000 - val_loss: 5.9886 - val_accuracy: 0.4820\n",
      "Epoch 667/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.1839e-05 - accuracy: 1.0000 - val_loss: 5.9880 - val_accuracy: 0.4865\n",
      "Epoch 668/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.1128e-05 - accuracy: 1.0000 - val_loss: 5.9814 - val_accuracy: 0.4865\n",
      "Epoch 669/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.1195e-05 - accuracy: 1.0000 - val_loss: 5.9955 - val_accuracy: 0.4820\n",
      "Epoch 670/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.0239e-05 - accuracy: 1.0000 - val_loss: 6.0007 - val_accuracy: 0.4865\n",
      "Epoch 671/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 1.0664e-05 - accuracy: 1.0000 - val_loss: 6.0062 - val_accuracy: 0.4865\n",
      "Epoch 672/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.0333e-05 - accuracy: 1.0000 - val_loss: 6.0104 - val_accuracy: 0.4820\n",
      "Epoch 673/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 9.8581e-06 - accuracy: 1.0000 - val_loss: 6.0080 - val_accuracy: 0.4775\n",
      "Epoch 674/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 9.7721e-06 - accuracy: 1.0000 - val_loss: 6.0184 - val_accuracy: 0.4910\n",
      "Epoch 675/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.0605e-05 - accuracy: 1.0000 - val_loss: 6.0392 - val_accuracy: 0.4865\n",
      "Epoch 676/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 9.6847e-06 - accuracy: 1.0000 - val_loss: 6.0237 - val_accuracy: 0.4820\n",
      "Epoch 677/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 9.3741e-06 - accuracy: 1.0000 - val_loss: 6.0185 - val_accuracy: 0.4865\n",
      "Epoch 678/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 1.0666e-05 - accuracy: 1.0000 - val_loss: 6.0170 - val_accuracy: 0.4865\n",
      "Epoch 679/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 9.4877e-06 - accuracy: 1.0000 - val_loss: 6.0288 - val_accuracy: 0.4910\n",
      "Epoch 680/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 9.0673e-06 - accuracy: 1.0000 - val_loss: 6.0321 - val_accuracy: 0.4865\n",
      "Epoch 681/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.0020e-05 - accuracy: 1.0000 - val_loss: 6.0378 - val_accuracy: 0.4820\n",
      "Epoch 682/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 9.3903e-06 - accuracy: 1.0000 - val_loss: 6.0369 - val_accuracy: 0.4820\n",
      "Epoch 683/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 8.8005e-06 - accuracy: 1.0000 - val_loss: 6.0344 - val_accuracy: 0.4865\n",
      "Epoch 684/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 8.6924e-06 - accuracy: 1.0000 - val_loss: 6.0373 - val_accuracy: 0.4865\n",
      "Epoch 685/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 9.1393e-06 - accuracy: 1.0000 - val_loss: 6.0430 - val_accuracy: 0.4820\n",
      "Epoch 686/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 8.4604e-06 - accuracy: 1.0000 - val_loss: 6.0507 - val_accuracy: 0.4820\n",
      "Epoch 687/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 9.3789e-06 - accuracy: 1.0000 - val_loss: 6.0607 - val_accuracy: 0.4865\n",
      "Epoch 688/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 7.9066e-06 - accuracy: 1.0000 - val_loss: 6.0610 - val_accuracy: 0.4820\n",
      "Epoch 689/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 8.5807e-06 - accuracy: 1.0000 - val_loss: 6.0719 - val_accuracy: 0.4865\n",
      "Epoch 690/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 8.8669e-06 - accuracy: 1.0000 - val_loss: 6.0813 - val_accuracy: 0.4820\n",
      "Epoch 691/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 8.7242e-06 - accuracy: 1.0000 - val_loss: 6.0888 - val_accuracy: 0.4820\n",
      "Epoch 692/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 8.2173e-06 - accuracy: 1.0000 - val_loss: 6.0744 - val_accuracy: 0.4775\n",
      "Epoch 693/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 8.1883e-06 - accuracy: 1.0000 - val_loss: 6.0852 - val_accuracy: 0.4865\n",
      "Epoch 694/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 8.7884e-06 - accuracy: 1.0000 - val_loss: 6.1000 - val_accuracy: 0.4865\n",
      "Epoch 695/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 8.3259e-06 - accuracy: 1.0000 - val_loss: 6.1017 - val_accuracy: 0.4820\n",
      "Epoch 696/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 7.9245e-06 - accuracy: 1.0000 - val_loss: 6.1022 - val_accuracy: 0.4865\n",
      "Epoch 697/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 8.2176e-06 - accuracy: 1.0000 - val_loss: 6.1056 - val_accuracy: 0.4865\n",
      "Epoch 698/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 8.6251e-06 - accuracy: 1.0000 - val_loss: 6.0901 - val_accuracy: 0.4820\n",
      "Epoch 699/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 8.3401e-06 - accuracy: 1.0000 - val_loss: 6.0982 - val_accuracy: 0.4865\n",
      "Epoch 700/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 7.6266e-06 - accuracy: 1.0000 - val_loss: 6.0999 - val_accuracy: 0.4910\n",
      "Epoch 701/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 7.1481e-06 - accuracy: 1.0000 - val_loss: 6.1125 - val_accuracy: 0.4865\n",
      "Epoch 702/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 7.7866e-06 - accuracy: 1.0000 - val_loss: 6.1149 - val_accuracy: 0.4820\n",
      "Epoch 703/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 6.9463e-06 - accuracy: 1.0000 - val_loss: 6.1180 - val_accuracy: 0.4865\n",
      "Epoch 704/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 7.8300e-06 - accuracy: 1.0000 - val_loss: 6.1255 - val_accuracy: 0.4820\n",
      "Epoch 705/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 7.3653e-06 - accuracy: 1.0000 - val_loss: 6.1289 - val_accuracy: 0.4775\n",
      "Epoch 706/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 7.0371e-06 - accuracy: 1.0000 - val_loss: 6.1312 - val_accuracy: 0.4775\n",
      "Epoch 707/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 8.1317e-06 - accuracy: 1.0000 - val_loss: 6.1379 - val_accuracy: 0.4820\n",
      "Epoch 708/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 6.8111e-06 - accuracy: 1.0000 - val_loss: 6.1399 - val_accuracy: 0.4820\n",
      "Epoch 709/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.9976e-06 - accuracy: 1.0000 - val_loss: 6.1256 - val_accuracy: 0.4865\n",
      "Epoch 710/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 7.2902e-06 - accuracy: 1.0000 - val_loss: 6.1335 - val_accuracy: 0.4910\n",
      "Epoch 711/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.3920e-06 - accuracy: 1.0000 - val_loss: 6.1422 - val_accuracy: 0.4910\n",
      "Epoch 712/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 7.2681e-06 - accuracy: 1.0000 - val_loss: 6.1386 - val_accuracy: 0.4910\n",
      "Epoch 713/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 7.3311e-06 - accuracy: 1.0000 - val_loss: 6.1587 - val_accuracy: 0.4865\n",
      "Epoch 714/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 7.2860e-06 - accuracy: 1.0000 - val_loss: 6.1638 - val_accuracy: 0.4865\n",
      "Epoch 715/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 6.3717e-06 - accuracy: 1.0000 - val_loss: 6.1542 - val_accuracy: 0.4820\n",
      "Epoch 716/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 6.4047e-06 - accuracy: 1.0000 - val_loss: 6.1483 - val_accuracy: 0.4865\n",
      "Epoch 717/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 6.4379e-06 - accuracy: 1.0000 - val_loss: 6.1625 - val_accuracy: 0.4865\n",
      "Epoch 718/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 6.2090e-06 - accuracy: 1.0000 - val_loss: 6.1694 - val_accuracy: 0.4820\n",
      "Epoch 719/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 6.0410e-06 - accuracy: 1.0000 - val_loss: 6.1837 - val_accuracy: 0.4865\n",
      "Epoch 720/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 6.2734e-06 - accuracy: 1.0000 - val_loss: 6.1774 - val_accuracy: 0.4910\n",
      "Epoch 721/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 6.3607e-06 - accuracy: 1.0000 - val_loss: 6.1781 - val_accuracy: 0.4865\n",
      "Epoch 722/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 6.0689e-06 - accuracy: 1.0000 - val_loss: 6.1995 - val_accuracy: 0.4865\n",
      "Epoch 723/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 5.8709e-06 - accuracy: 1.0000 - val_loss: 6.1979 - val_accuracy: 0.4820\n",
      "Epoch 724/1000\n",
      "1990/1990 [==============================] - 0s 123us/step - loss: 6.2785e-06 - accuracy: 1.0000 - val_loss: 6.1874 - val_accuracy: 0.4820\n",
      "Epoch 725/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 5.7363e-06 - accuracy: 1.0000 - val_loss: 6.1912 - val_accuracy: 0.4865\n",
      "Epoch 726/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 6.0849e-06 - accuracy: 1.0000 - val_loss: 6.2005 - val_accuracy: 0.4865\n",
      "Epoch 727/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 6.1927e-06 - accuracy: 1.0000 - val_loss: 6.1988 - val_accuracy: 0.4910\n",
      "Epoch 728/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 5.9000e-06 - accuracy: 1.0000 - val_loss: 6.1976 - val_accuracy: 0.4910\n",
      "Epoch 729/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 5.6770e-06 - accuracy: 1.0000 - val_loss: 6.1991 - val_accuracy: 0.4865\n",
      "Epoch 730/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 5.8367e-06 - accuracy: 1.0000 - val_loss: 6.2076 - val_accuracy: 0.4865\n",
      "Epoch 731/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 5.9268e-06 - accuracy: 1.0000 - val_loss: 6.2105 - val_accuracy: 0.4865\n",
      "Epoch 732/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 5.8121e-06 - accuracy: 1.0000 - val_loss: 6.2213 - val_accuracy: 0.4865\n",
      "Epoch 733/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 5.7581e-06 - accuracy: 1.0000 - val_loss: 6.2157 - val_accuracy: 0.4910\n",
      "Epoch 734/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 5.9987e-06 - accuracy: 1.0000 - val_loss: 6.2230 - val_accuracy: 0.4865\n",
      "Epoch 735/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 5.6543e-06 - accuracy: 1.0000 - val_loss: 6.2235 - val_accuracy: 0.4775\n",
      "Epoch 736/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 5.2537e-06 - accuracy: 1.0000 - val_loss: 6.2304 - val_accuracy: 0.4865\n",
      "Epoch 737/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 5.8845e-06 - accuracy: 1.0000 - val_loss: 6.2369 - val_accuracy: 0.4865\n",
      "Epoch 738/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 5.7079e-06 - accuracy: 1.0000 - val_loss: 6.2278 - val_accuracy: 0.4910\n",
      "Epoch 739/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 5.7144e-06 - accuracy: 1.0000 - val_loss: 6.2350 - val_accuracy: 0.4865\n",
      "Epoch 740/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 5.5338e-06 - accuracy: 1.0000 - val_loss: 6.2375 - val_accuracy: 0.4865\n",
      "Epoch 741/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 5.1141e-06 - accuracy: 1.0000 - val_loss: 6.2541 - val_accuracy: 0.4865\n",
      "Epoch 742/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 5.1032e-06 - accuracy: 1.0000 - val_loss: 6.2508 - val_accuracy: 0.4820\n",
      "Epoch 743/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 5.1255e-06 - accuracy: 1.0000 - val_loss: 6.2537 - val_accuracy: 0.4865\n",
      "Epoch 744/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 4.9121e-06 - accuracy: 1.0000 - val_loss: 6.2518 - val_accuracy: 0.4865\n",
      "Epoch 745/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.8606e-06 - accuracy: 1.0000 - val_loss: 6.2451 - val_accuracy: 0.4865\n",
      "Epoch 746/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.9154e-06 - accuracy: 1.0000 - val_loss: 6.2502 - val_accuracy: 0.4820\n",
      "Epoch 747/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 5.2042e-06 - accuracy: 1.0000 - val_loss: 6.2519 - val_accuracy: 0.4865\n",
      "Epoch 748/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 5.4619e-06 - accuracy: 1.0000 - val_loss: 6.2679 - val_accuracy: 0.4865\n",
      "Epoch 749/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 5.3816e-06 - accuracy: 1.0000 - val_loss: 6.2629 - val_accuracy: 0.4820\n",
      "Epoch 750/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 5.0737e-06 - accuracy: 1.0000 - val_loss: 6.2680 - val_accuracy: 0.4910\n",
      "Epoch 751/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 4.8973e-06 - accuracy: 1.0000 - val_loss: 6.2671 - val_accuracy: 0.4910\n",
      "Epoch 752/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 4.8303e-06 - accuracy: 1.0000 - val_loss: 6.2804 - val_accuracy: 0.4865\n",
      "Epoch 753/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 4.7590e-06 - accuracy: 1.0000 - val_loss: 6.2737 - val_accuracy: 0.4865\n",
      "Epoch 754/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 4.9966e-06 - accuracy: 1.0000 - val_loss: 6.2768 - val_accuracy: 0.4865\n",
      "Epoch 755/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 4.5404e-06 - accuracy: 1.0000 - val_loss: 6.2888 - val_accuracy: 0.4865\n",
      "Epoch 756/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 104us/step - loss: 4.8151e-06 - accuracy: 1.0000 - val_loss: 6.2829 - val_accuracy: 0.4865\n",
      "Epoch 757/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 4.9266e-06 - accuracy: 1.0000 - val_loss: 6.2885 - val_accuracy: 0.4865\n",
      "Epoch 758/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 4.8132e-06 - accuracy: 1.0000 - val_loss: 6.2968 - val_accuracy: 0.4865\n",
      "Epoch 759/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 4.3529e-06 - accuracy: 1.0000 - val_loss: 6.3104 - val_accuracy: 0.4820\n",
      "Epoch 760/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.6639e-06 - accuracy: 1.0000 - val_loss: 6.3041 - val_accuracy: 0.4820\n",
      "Epoch 761/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 4.4783e-06 - accuracy: 1.0000 - val_loss: 6.3164 - val_accuracy: 0.4820\n",
      "Epoch 762/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.0819e-06 - accuracy: 1.0000 - val_loss: 6.3265 - val_accuracy: 0.4865\n",
      "Epoch 763/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 4.6383e-06 - accuracy: 1.0000 - val_loss: 6.3083 - val_accuracy: 0.4865\n",
      "Epoch 764/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 4.0433e-06 - accuracy: 1.0000 - val_loss: 6.3190 - val_accuracy: 0.4865\n",
      "Epoch 765/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 4.0805e-06 - accuracy: 1.0000 - val_loss: 6.3258 - val_accuracy: 0.4865\n",
      "Epoch 766/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 4.4045e-06 - accuracy: 1.0000 - val_loss: 6.3377 - val_accuracy: 0.4865\n",
      "Epoch 767/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.6565e-06 - accuracy: 1.0000 - val_loss: 6.3334 - val_accuracy: 0.4865\n",
      "Epoch 768/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 4.8285e-06 - accuracy: 1.0000 - val_loss: 6.3498 - val_accuracy: 0.4865\n",
      "Epoch 769/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 3.9803e-06 - accuracy: 1.0000 - val_loss: 6.3754 - val_accuracy: 0.4865\n",
      "Epoch 770/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 3.9832e-06 - accuracy: 1.0000 - val_loss: 6.3590 - val_accuracy: 0.4865\n",
      "Epoch 771/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 4.4417e-06 - accuracy: 1.0000 - val_loss: 6.3510 - val_accuracy: 0.4820\n",
      "Epoch 772/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 4.1538e-06 - accuracy: 1.0000 - val_loss: 6.3568 - val_accuracy: 0.4820\n",
      "Epoch 773/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.8459e-06 - accuracy: 1.0000 - val_loss: 6.3643 - val_accuracy: 0.4820\n",
      "Epoch 774/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 4.0079e-06 - accuracy: 1.0000 - val_loss: 6.3768 - val_accuracy: 0.4865\n",
      "Epoch 775/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 4.2316e-06 - accuracy: 1.0000 - val_loss: 6.3667 - val_accuracy: 0.4865\n",
      "Epoch 776/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 4.0987e-06 - accuracy: 1.0000 - val_loss: 6.3692 - val_accuracy: 0.4865\n",
      "Epoch 777/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 3.7349e-06 - accuracy: 1.0000 - val_loss: 6.3806 - val_accuracy: 0.4865\n",
      "Epoch 778/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.7513e-06 - accuracy: 1.0000 - val_loss: 6.3849 - val_accuracy: 0.4865\n",
      "Epoch 779/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 3.7873e-06 - accuracy: 1.0000 - val_loss: 6.3695 - val_accuracy: 0.4865\n",
      "Epoch 780/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 3.9928e-06 - accuracy: 1.0000 - val_loss: 6.3769 - val_accuracy: 0.4865\n",
      "Epoch 781/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.6471e-06 - accuracy: 1.0000 - val_loss: 6.3760 - val_accuracy: 0.4865\n",
      "Epoch 782/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 4.0061e-06 - accuracy: 1.0000 - val_loss: 6.3883 - val_accuracy: 0.4865\n",
      "Epoch 783/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 3.8821e-06 - accuracy: 1.0000 - val_loss: 6.3820 - val_accuracy: 0.4865\n",
      "Epoch 784/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.6370e-06 - accuracy: 1.0000 - val_loss: 6.3882 - val_accuracy: 0.4865\n",
      "Epoch 785/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.4656e-06 - accuracy: 1.0000 - val_loss: 6.3891 - val_accuracy: 0.4865\n",
      "Epoch 786/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 3.5318e-06 - accuracy: 1.0000 - val_loss: 6.3943 - val_accuracy: 0.4865\n",
      "Epoch 787/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 3.5120e-06 - accuracy: 1.0000 - val_loss: 6.3960 - val_accuracy: 0.4865\n",
      "Epoch 788/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 3.3815e-06 - accuracy: 1.0000 - val_loss: 6.4007 - val_accuracy: 0.4865\n",
      "Epoch 789/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 3.8715e-06 - accuracy: 1.0000 - val_loss: 6.3996 - val_accuracy: 0.4865\n",
      "Epoch 790/1000\n",
      "1990/1990 [==============================] - 0s 125us/step - loss: 3.5625e-06 - accuracy: 1.0000 - val_loss: 6.4058 - val_accuracy: 0.4865\n",
      "Epoch 791/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 3.7064e-06 - accuracy: 1.0000 - val_loss: 6.4153 - val_accuracy: 0.4865\n",
      "Epoch 792/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 3.6538e-06 - accuracy: 1.0000 - val_loss: 6.4191 - val_accuracy: 0.4865\n",
      "Epoch 793/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.6712e-06 - accuracy: 1.0000 - val_loss: 6.4281 - val_accuracy: 0.4820\n",
      "Epoch 794/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 3.4337e-06 - accuracy: 1.0000 - val_loss: 6.4167 - val_accuracy: 0.4865\n",
      "Epoch 795/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.9046e-06 - accuracy: 1.0000 - val_loss: 6.4223 - val_accuracy: 0.4865\n",
      "Epoch 796/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.2871e-06 - accuracy: 1.0000 - val_loss: 6.4423 - val_accuracy: 0.4820\n",
      "Epoch 797/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.0558e-06 - accuracy: 1.0000 - val_loss: 6.4537 - val_accuracy: 0.4820\n",
      "Epoch 798/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 3.1030e-06 - accuracy: 1.0000 - val_loss: 6.4567 - val_accuracy: 0.4820\n",
      "Epoch 799/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 3.0308e-06 - accuracy: 1.0000 - val_loss: 6.4513 - val_accuracy: 0.4820\n",
      "Epoch 800/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 2.9854e-06 - accuracy: 1.0000 - val_loss: 6.4445 - val_accuracy: 0.4820\n",
      "Epoch 801/1000\n",
      "1990/1990 [==============================] - 0s 87us/step - loss: 3.2290e-06 - accuracy: 1.0000 - val_loss: 6.4651 - val_accuracy: 0.4820\n",
      "Epoch 802/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 3.1235e-06 - accuracy: 1.0000 - val_loss: 6.4501 - val_accuracy: 0.4820\n",
      "Epoch 803/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 3.0622e-06 - accuracy: 1.0000 - val_loss: 6.4696 - val_accuracy: 0.4820\n",
      "Epoch 804/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.0315e-06 - accuracy: 1.0000 - val_loss: 6.4722 - val_accuracy: 0.4775\n",
      "Epoch 805/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 2.9416e-06 - accuracy: 1.0000 - val_loss: 6.4644 - val_accuracy: 0.4820\n",
      "Epoch 806/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 3.1562e-06 - accuracy: 1.0000 - val_loss: 6.4730 - val_accuracy: 0.4820\n",
      "Epoch 807/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 2.9143e-06 - accuracy: 1.0000 - val_loss: 6.4672 - val_accuracy: 0.4865\n",
      "Epoch 808/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 3.1563e-06 - accuracy: 1.0000 - val_loss: 6.4731 - val_accuracy: 0.4865\n",
      "Epoch 809/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 3.1654e-06 - accuracy: 1.0000 - val_loss: 6.4826 - val_accuracy: 0.4865\n",
      "Epoch 810/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 3.0435e-06 - accuracy: 1.0000 - val_loss: 6.4642 - val_accuracy: 0.4820\n",
      "Epoch 811/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 2.7845e-06 - accuracy: 1.0000 - val_loss: 6.4679 - val_accuracy: 0.4910\n",
      "Epoch 812/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 2.6116e-06 - accuracy: 1.0000 - val_loss: 6.4868 - val_accuracy: 0.4865\n",
      "Epoch 813/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 2.7268e-06 - accuracy: 1.0000 - val_loss: 6.4894 - val_accuracy: 0.4865\n",
      "Epoch 814/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 2.8967e-06 - accuracy: 1.0000 - val_loss: 6.4856 - val_accuracy: 0.4865\n",
      "Epoch 815/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 2.8679e-06 - accuracy: 1.0000 - val_loss: 6.4868 - val_accuracy: 0.4820\n",
      "Epoch 816/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 2.8176e-06 - accuracy: 1.0000 - val_loss: 6.4911 - val_accuracy: 0.4910\n",
      "Epoch 817/1000\n",
      "1990/1990 [==============================] - 0s 132us/step - loss: 2.9845e-06 - accuracy: 1.0000 - val_loss: 6.4952 - val_accuracy: 0.4865\n",
      "Epoch 818/1000\n",
      "1990/1990 [==============================] - 0s 116us/step - loss: 2.5936e-06 - accuracy: 1.0000 - val_loss: 6.5117 - val_accuracy: 0.4910\n",
      "Epoch 819/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.4962e-06 - accuracy: 1.0000 - val_loss: 6.5066 - val_accuracy: 0.4910\n",
      "Epoch 820/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 2.8779e-06 - accuracy: 1.0000 - val_loss: 6.5158 - val_accuracy: 0.4865\n",
      "Epoch 821/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 2.5243e-06 - accuracy: 1.0000 - val_loss: 6.5294 - val_accuracy: 0.4865\n",
      "Epoch 822/1000\n",
      "1990/1990 [==============================] - 0s 116us/step - loss: 2.5357e-06 - accuracy: 1.0000 - val_loss: 6.5321 - val_accuracy: 0.4865\n",
      "Epoch 823/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.5409e-06 - accuracy: 1.0000 - val_loss: 6.5252 - val_accuracy: 0.4910\n",
      "Epoch 824/1000\n",
      "1990/1990 [==============================] - 0s 136us/step - loss: 2.5146e-06 - accuracy: 1.0000 - val_loss: 6.5182 - val_accuracy: 0.4910\n",
      "Epoch 825/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 2.4869e-06 - accuracy: 1.0000 - val_loss: 6.5341 - val_accuracy: 0.4910\n",
      "Epoch 826/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.6104e-06 - accuracy: 1.0000 - val_loss: 6.5445 - val_accuracy: 0.4865\n",
      "Epoch 827/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 2.3443e-06 - accuracy: 1.0000 - val_loss: 6.5493 - val_accuracy: 0.4865\n",
      "Epoch 828/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.4700e-06 - accuracy: 1.0000 - val_loss: 6.5526 - val_accuracy: 0.4865\n",
      "Epoch 829/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.4818e-06 - accuracy: 1.0000 - val_loss: 6.5412 - val_accuracy: 0.4910\n",
      "Epoch 830/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 2.5117e-06 - accuracy: 1.0000 - val_loss: 6.5505 - val_accuracy: 0.4910\n",
      "Epoch 831/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.4506e-06 - accuracy: 1.0000 - val_loss: 6.5564 - val_accuracy: 0.4865\n",
      "Epoch 832/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 2.3246e-06 - accuracy: 1.0000 - val_loss: 6.5626 - val_accuracy: 0.4820\n",
      "Epoch 833/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.4592e-06 - accuracy: 1.0000 - val_loss: 6.5618 - val_accuracy: 0.4865\n",
      "Epoch 834/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 2.4349e-06 - accuracy: 1.0000 - val_loss: 6.5642 - val_accuracy: 0.4865\n",
      "Epoch 835/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.3248e-06 - accuracy: 1.0000 - val_loss: 6.5679 - val_accuracy: 0.4865\n",
      "Epoch 836/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.5333e-06 - accuracy: 1.0000 - val_loss: 6.5707 - val_accuracy: 0.4865\n",
      "Epoch 837/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 2.2082e-06 - accuracy: 1.0000 - val_loss: 6.5724 - val_accuracy: 0.4820\n",
      "Epoch 838/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 2.4940e-06 - accuracy: 1.0000 - val_loss: 6.5810 - val_accuracy: 0.4865\n",
      "Epoch 839/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 2.2541e-06 - accuracy: 1.0000 - val_loss: 6.5855 - val_accuracy: 0.4865\n",
      "Epoch 840/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 2.4961e-06 - accuracy: 1.0000 - val_loss: 6.5827 - val_accuracy: 0.4865\n",
      "Epoch 841/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.3010e-06 - accuracy: 1.0000 - val_loss: 6.5877 - val_accuracy: 0.4865\n",
      "Epoch 842/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.1678e-06 - accuracy: 1.0000 - val_loss: 6.5881 - val_accuracy: 0.4865\n",
      "Epoch 843/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.0768e-06 - accuracy: 1.0000 - val_loss: 6.5848 - val_accuracy: 0.4910\n",
      "Epoch 844/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.2600e-06 - accuracy: 1.0000 - val_loss: 6.5862 - val_accuracy: 0.4910\n",
      "Epoch 845/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.1305e-06 - accuracy: 1.0000 - val_loss: 6.6053 - val_accuracy: 0.4865\n",
      "Epoch 846/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.9900e-06 - accuracy: 1.0000 - val_loss: 6.6047 - val_accuracy: 0.4865\n",
      "Epoch 847/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 2.0953e-06 - accuracy: 1.0000 - val_loss: 6.5977 - val_accuracy: 0.4865\n",
      "Epoch 848/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.9157e-06 - accuracy: 1.0000 - val_loss: 6.5994 - val_accuracy: 0.4865\n",
      "Epoch 849/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.9330e-06 - accuracy: 1.0000 - val_loss: 6.5968 - val_accuracy: 0.4865\n",
      "Epoch 850/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.1653e-06 - accuracy: 1.0000 - val_loss: 6.6155 - val_accuracy: 0.4865\n",
      "Epoch 851/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.8776e-06 - accuracy: 1.0000 - val_loss: 6.6305 - val_accuracy: 0.4865\n",
      "Epoch 852/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 2.1557e-06 - accuracy: 1.0000 - val_loss: 6.6346 - val_accuracy: 0.4865\n",
      "Epoch 853/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 2.0357e-06 - accuracy: 1.0000 - val_loss: 6.6240 - val_accuracy: 0.4910\n",
      "Epoch 854/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 1.9727e-06 - accuracy: 1.0000 - val_loss: 6.6263 - val_accuracy: 0.4910\n",
      "Epoch 855/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.8916e-06 - accuracy: 1.0000 - val_loss: 6.6189 - val_accuracy: 0.4910\n",
      "Epoch 856/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.9073e-06 - accuracy: 1.0000 - val_loss: 6.6195 - val_accuracy: 0.4865\n",
      "Epoch 857/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.8667e-06 - accuracy: 1.0000 - val_loss: 6.6290 - val_accuracy: 0.4865\n",
      "Epoch 858/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.8955e-06 - accuracy: 1.0000 - val_loss: 6.6455 - val_accuracy: 0.4865\n",
      "Epoch 859/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.9898e-06 - accuracy: 1.0000 - val_loss: 6.6535 - val_accuracy: 0.4865\n",
      "Epoch 860/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.8483e-06 - accuracy: 1.0000 - val_loss: 6.6508 - val_accuracy: 0.4865\n",
      "Epoch 861/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.9140e-06 - accuracy: 1.0000 - val_loss: 6.6673 - val_accuracy: 0.4865\n",
      "Epoch 862/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.8936e-06 - accuracy: 1.0000 - val_loss: 6.6504 - val_accuracy: 0.4865\n",
      "Epoch 863/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 1.8422e-06 - accuracy: 1.0000 - val_loss: 6.6635 - val_accuracy: 0.4865\n",
      "Epoch 864/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.8043e-06 - accuracy: 1.0000 - val_loss: 6.6651 - val_accuracy: 0.4865\n",
      "Epoch 865/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.6745e-06 - accuracy: 1.0000 - val_loss: 6.6800 - val_accuracy: 0.4865\n",
      "Epoch 866/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.8744e-06 - accuracy: 1.0000 - val_loss: 6.6847 - val_accuracy: 0.4865\n",
      "Epoch 867/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.6095e-06 - accuracy: 1.0000 - val_loss: 6.6855 - val_accuracy: 0.4865\n",
      "Epoch 868/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.7635e-06 - accuracy: 1.0000 - val_loss: 6.6815 - val_accuracy: 0.4865\n",
      "Epoch 869/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 1.8619e-06 - accuracy: 1.0000 - val_loss: 6.6800 - val_accuracy: 0.4865\n",
      "Epoch 870/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 1.7187e-06 - accuracy: 1.0000 - val_loss: 6.6856 - val_accuracy: 0.4865\n",
      "Epoch 871/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 1.7028e-06 - accuracy: 1.0000 - val_loss: 6.6861 - val_accuracy: 0.4865\n",
      "Epoch 872/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 1.5585e-06 - accuracy: 1.0000 - val_loss: 6.7029 - val_accuracy: 0.4865\n",
      "Epoch 873/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7222e-06 - accuracy: 1.0000 - val_loss: 6.7068 - val_accuracy: 0.4865\n",
      "Epoch 874/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 1.9560e-06 - accuracy: 1.0000 - val_loss: 6.6862 - val_accuracy: 0.4865\n",
      "Epoch 875/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 1.7586e-06 - accuracy: 1.0000 - val_loss: 6.7024 - val_accuracy: 0.4865\n",
      "Epoch 876/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.7518e-06 - accuracy: 1.0000 - val_loss: 6.7039 - val_accuracy: 0.4865\n",
      "Epoch 877/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.7748e-06 - accuracy: 1.0000 - val_loss: 6.6980 - val_accuracy: 0.4865\n",
      "Epoch 878/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.6985e-06 - accuracy: 1.0000 - val_loss: 6.7124 - val_accuracy: 0.4820\n",
      "Epoch 879/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.6336e-06 - accuracy: 1.0000 - val_loss: 6.7140 - val_accuracy: 0.4820\n",
      "Epoch 880/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7060e-06 - accuracy: 1.0000 - val_loss: 6.7198 - val_accuracy: 0.4865\n",
      "Epoch 881/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.6546e-06 - accuracy: 1.0000 - val_loss: 6.7139 - val_accuracy: 0.4865\n",
      "Epoch 882/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.6857e-06 - accuracy: 1.0000 - val_loss: 6.7207 - val_accuracy: 0.4865\n",
      "Epoch 883/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.4931e-06 - accuracy: 1.0000 - val_loss: 6.7356 - val_accuracy: 0.4865\n",
      "Epoch 884/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.3397e-06 - accuracy: 1.0000 - val_loss: 6.7384 - val_accuracy: 0.4865\n",
      "Epoch 885/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.5492e-06 - accuracy: 1.0000 - val_loss: 6.7387 - val_accuracy: 0.4865\n",
      "Epoch 886/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 1.5671e-06 - accuracy: 1.0000 - val_loss: 6.7454 - val_accuracy: 0.4865\n",
      "Epoch 887/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.3981e-06 - accuracy: 1.0000 - val_loss: 6.7616 - val_accuracy: 0.4865\n",
      "Epoch 888/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.4601e-06 - accuracy: 1.0000 - val_loss: 6.7630 - val_accuracy: 0.4865\n",
      "Epoch 889/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.4262e-06 - accuracy: 1.0000 - val_loss: 6.7582 - val_accuracy: 0.4865\n",
      "Epoch 890/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.3796e-06 - accuracy: 1.0000 - val_loss: 6.7604 - val_accuracy: 0.4865\n",
      "Epoch 891/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.4299e-06 - accuracy: 1.0000 - val_loss: 6.7630 - val_accuracy: 0.4865\n",
      "Epoch 892/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.3317e-06 - accuracy: 1.0000 - val_loss: 6.7611 - val_accuracy: 0.4865\n",
      "Epoch 893/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.3229e-06 - accuracy: 1.0000 - val_loss: 6.7693 - val_accuracy: 0.4865\n",
      "Epoch 894/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.3315e-06 - accuracy: 1.0000 - val_loss: 6.7865 - val_accuracy: 0.4865\n",
      "Epoch 895/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.4310e-06 - accuracy: 1.0000 - val_loss: 6.7782 - val_accuracy: 0.4820\n",
      "Epoch 896/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.3513e-06 - accuracy: 1.0000 - val_loss: 6.7931 - val_accuracy: 0.4865\n",
      "Epoch 897/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.2905e-06 - accuracy: 1.0000 - val_loss: 6.7893 - val_accuracy: 0.4865\n",
      "Epoch 898/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.3517e-06 - accuracy: 1.0000 - val_loss: 6.8065 - val_accuracy: 0.4865\n",
      "Epoch 899/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.2985e-06 - accuracy: 1.0000 - val_loss: 6.8034 - val_accuracy: 0.4865\n",
      "Epoch 900/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.2775e-06 - accuracy: 1.0000 - val_loss: 6.8235 - val_accuracy: 0.4865\n",
      "Epoch 901/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.4005e-06 - accuracy: 1.0000 - val_loss: 6.8120 - val_accuracy: 0.4865\n",
      "Epoch 902/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.4260e-06 - accuracy: 1.0000 - val_loss: 6.8045 - val_accuracy: 0.4910\n",
      "Epoch 903/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.2903e-06 - accuracy: 1.0000 - val_loss: 6.8128 - val_accuracy: 0.4820\n",
      "Epoch 904/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.2833e-06 - accuracy: 1.0000 - val_loss: 6.8229 - val_accuracy: 0.4820\n",
      "Epoch 905/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.2431e-06 - accuracy: 1.0000 - val_loss: 6.8338 - val_accuracy: 0.4910\n",
      "Epoch 906/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.2312e-06 - accuracy: 1.0000 - val_loss: 6.8386 - val_accuracy: 0.4865\n",
      "Epoch 907/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 1.3347e-06 - accuracy: 1.0000 - val_loss: 6.8396 - val_accuracy: 0.4865\n",
      "Epoch 908/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.2532e-06 - accuracy: 1.0000 - val_loss: 6.8376 - val_accuracy: 0.4820\n",
      "Epoch 909/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 1.3337e-06 - accuracy: 1.0000 - val_loss: 6.8240 - val_accuracy: 0.4775\n",
      "Epoch 910/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 1.2874e-06 - accuracy: 1.0000 - val_loss: 6.8431 - val_accuracy: 0.4820\n",
      "Epoch 911/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.1196e-06 - accuracy: 1.0000 - val_loss: 6.8540 - val_accuracy: 0.4775\n",
      "Epoch 912/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.2362e-06 - accuracy: 1.0000 - val_loss: 6.8475 - val_accuracy: 0.4820\n",
      "Epoch 913/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.2411e-06 - accuracy: 1.0000 - val_loss: 6.8566 - val_accuracy: 0.4865\n",
      "Epoch 914/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.2425e-06 - accuracy: 1.0000 - val_loss: 6.8506 - val_accuracy: 0.4865\n",
      "Epoch 915/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.3014e-06 - accuracy: 1.0000 - val_loss: 6.8691 - val_accuracy: 0.4865\n",
      "Epoch 916/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.1585e-06 - accuracy: 1.0000 - val_loss: 6.8626 - val_accuracy: 0.4865\n",
      "Epoch 917/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0587e-06 - accuracy: 1.0000 - val_loss: 6.8661 - val_accuracy: 0.4865\n",
      "Epoch 918/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.2095e-06 - accuracy: 1.0000 - val_loss: 6.8619 - val_accuracy: 0.4910\n",
      "Epoch 919/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.2281e-06 - accuracy: 1.0000 - val_loss: 6.8626 - val_accuracy: 0.4910\n",
      "Epoch 920/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0349e-06 - accuracy: 1.0000 - val_loss: 6.8665 - val_accuracy: 0.4910\n",
      "Epoch 921/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.0755e-06 - accuracy: 1.0000 - val_loss: 6.8758 - val_accuracy: 0.4910\n",
      "Epoch 922/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.0524e-06 - accuracy: 1.0000 - val_loss: 6.8815 - val_accuracy: 0.4820\n",
      "Epoch 923/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0867e-06 - accuracy: 1.0000 - val_loss: 6.8876 - val_accuracy: 0.4910\n",
      "Epoch 924/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 9.9644e-07 - accuracy: 1.0000 - val_loss: 6.8780 - val_accuracy: 0.4910\n",
      "Epoch 925/1000\n",
      "1990/1990 [==============================] - 0s 142us/step - loss: 1.1605e-06 - accuracy: 1.0000 - val_loss: 6.8769 - val_accuracy: 0.4910\n",
      "Epoch 926/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.0802e-06 - accuracy: 1.0000 - val_loss: 6.8919 - val_accuracy: 0.4865\n",
      "Epoch 927/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.0552e-06 - accuracy: 1.0000 - val_loss: 6.9056 - val_accuracy: 0.4910\n",
      "Epoch 928/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.0065e-06 - accuracy: 1.0000 - val_loss: 6.9043 - val_accuracy: 0.4910\n",
      "Epoch 929/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.1213e-06 - accuracy: 1.0000 - val_loss: 6.9121 - val_accuracy: 0.4910\n",
      "Epoch 930/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.0675e-06 - accuracy: 1.0000 - val_loss: 6.8894 - val_accuracy: 0.4865\n",
      "Epoch 931/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 9.5108e-07 - accuracy: 1.0000 - val_loss: 6.9138 - val_accuracy: 0.4910\n",
      "Epoch 932/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 9.4725e-07 - accuracy: 1.0000 - val_loss: 6.9192 - val_accuracy: 0.4910\n",
      "Epoch 933/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.0410e-06 - accuracy: 1.0000 - val_loss: 6.9149 - val_accuracy: 0.4865\n",
      "Epoch 934/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 9.4936e-07 - accuracy: 1.0000 - val_loss: 6.9225 - val_accuracy: 0.4865\n",
      "Epoch 935/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 9.6804e-07 - accuracy: 1.0000 - val_loss: 6.9158 - val_accuracy: 0.4865\n",
      "Epoch 936/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.0581e-06 - accuracy: 1.0000 - val_loss: 6.9186 - val_accuracy: 0.4865\n",
      "Epoch 937/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 9.0588e-07 - accuracy: 1.0000 - val_loss: 6.9374 - val_accuracy: 0.4865\n",
      "Epoch 938/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.0048e-06 - accuracy: 1.0000 - val_loss: 6.9233 - val_accuracy: 0.4865\n",
      "Epoch 939/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 8.9959e-07 - accuracy: 1.0000 - val_loss: 6.9004 - val_accuracy: 0.4910\n",
      "Epoch 940/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 9.6387e-07 - accuracy: 1.0000 - val_loss: 6.9083 - val_accuracy: 0.4910\n",
      "Epoch 941/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 8.8059e-07 - accuracy: 1.0000 - val_loss: 6.9365 - val_accuracy: 0.4910\n",
      "Epoch 942/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 9.6848e-07 - accuracy: 1.0000 - val_loss: 6.9371 - val_accuracy: 0.4910\n",
      "Epoch 943/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 8.9963e-07 - accuracy: 1.0000 - val_loss: 6.9381 - val_accuracy: 0.4910\n",
      "Epoch 944/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 9.1159e-07 - accuracy: 1.0000 - val_loss: 6.9366 - val_accuracy: 0.4865\n",
      "Epoch 945/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.0069e-06 - accuracy: 1.0000 - val_loss: 6.9349 - val_accuracy: 0.4910\n",
      "Epoch 946/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 9.4204e-07 - accuracy: 1.0000 - val_loss: 6.9467 - val_accuracy: 0.4910\n",
      "Epoch 947/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.0120e-06 - accuracy: 1.0000 - val_loss: 6.9371 - val_accuracy: 0.4865\n",
      "Epoch 948/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 9.2595e-07 - accuracy: 1.0000 - val_loss: 6.9302 - val_accuracy: 0.4910\n",
      "Epoch 949/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 8.3841e-07 - accuracy: 1.0000 - val_loss: 6.9345 - val_accuracy: 0.4910\n",
      "Epoch 950/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 9.8524e-07 - accuracy: 1.0000 - val_loss: 6.9504 - val_accuracy: 0.4820\n",
      "Epoch 951/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.0460e-06 - accuracy: 1.0000 - val_loss: 6.9509 - val_accuracy: 0.4910\n",
      "Epoch 952/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 8.9544e-07 - accuracy: 1.0000 - val_loss: 6.9413 - val_accuracy: 0.4910\n",
      "Epoch 953/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 7.7678e-07 - accuracy: 1.0000 - val_loss: 6.9535 - val_accuracy: 0.4910\n",
      "Epoch 954/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 7.7483e-07 - accuracy: 1.0000 - val_loss: 6.9717 - val_accuracy: 0.4910\n",
      "Epoch 955/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 8.0313e-07 - accuracy: 1.0000 - val_loss: 6.9795 - val_accuracy: 0.4865\n",
      "Epoch 956/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 8.3655e-07 - accuracy: 1.0000 - val_loss: 6.9838 - val_accuracy: 0.4910\n",
      "Epoch 957/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 9.2335e-07 - accuracy: 1.0000 - val_loss: 6.9819 - val_accuracy: 0.4820\n",
      "Epoch 958/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 7.5616e-07 - accuracy: 1.0000 - val_loss: 6.9781 - val_accuracy: 0.4910\n",
      "Epoch 959/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 8.0663e-07 - accuracy: 1.0000 - val_loss: 6.9933 - val_accuracy: 0.4865\n",
      "Epoch 960/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 7.5134e-07 - accuracy: 1.0000 - val_loss: 6.9966 - val_accuracy: 0.4865\n",
      "Epoch 961/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 7.8474e-07 - accuracy: 1.0000 - val_loss: 7.0144 - val_accuracy: 0.4865\n",
      "Epoch 962/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 7.3750e-07 - accuracy: 1.0000 - val_loss: 7.0119 - val_accuracy: 0.4865\n",
      "Epoch 963/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 7.6060e-07 - accuracy: 1.0000 - val_loss: 7.0332 - val_accuracy: 0.4910\n",
      "Epoch 964/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 7.8424e-07 - accuracy: 1.0000 - val_loss: 7.0380 - val_accuracy: 0.4865\n",
      "Epoch 965/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 7.2746e-07 - accuracy: 1.0000 - val_loss: 7.0248 - val_accuracy: 0.4820\n",
      "Epoch 966/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 7.1927e-07 - accuracy: 1.0000 - val_loss: 7.0208 - val_accuracy: 0.4820\n",
      "Epoch 967/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 7.7209e-07 - accuracy: 1.0000 - val_loss: 7.0348 - val_accuracy: 0.4865\n",
      "Epoch 968/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 101us/step - loss: 7.0168e-07 - accuracy: 1.0000 - val_loss: 7.0510 - val_accuracy: 0.4865\n",
      "Epoch 969/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 6.7221e-07 - accuracy: 1.0000 - val_loss: 7.0437 - val_accuracy: 0.4820\n",
      "Epoch 970/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 7.0285e-07 - accuracy: 1.0000 - val_loss: 7.0563 - val_accuracy: 0.4865\n",
      "Epoch 971/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 7.2667e-07 - accuracy: 1.0000 - val_loss: 7.0619 - val_accuracy: 0.4865\n",
      "Epoch 972/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 8.1890e-07 - accuracy: 1.0000 - val_loss: 7.0409 - val_accuracy: 0.4910\n",
      "Epoch 973/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 7.5879e-07 - accuracy: 1.0000 - val_loss: 7.0534 - val_accuracy: 0.4865\n",
      "Epoch 974/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 7.2283e-07 - accuracy: 1.0000 - val_loss: 7.0389 - val_accuracy: 0.4775\n",
      "Epoch 975/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 8.4370e-07 - accuracy: 1.0000 - val_loss: 7.0603 - val_accuracy: 0.4865\n",
      "Epoch 976/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 6.6467e-07 - accuracy: 1.0000 - val_loss: 7.0638 - val_accuracy: 0.4820\n",
      "Epoch 977/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 6.5251e-07 - accuracy: 1.0000 - val_loss: 7.0594 - val_accuracy: 0.4910\n",
      "Epoch 978/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 7.5873e-07 - accuracy: 1.0000 - val_loss: 7.0547 - val_accuracy: 0.4865\n",
      "Epoch 979/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 7.5596e-07 - accuracy: 1.0000 - val_loss: 7.0635 - val_accuracy: 0.4820\n",
      "Epoch 980/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.5534e-07 - accuracy: 1.0000 - val_loss: 7.0763 - val_accuracy: 0.4820\n",
      "Epoch 981/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 6.4759e-07 - accuracy: 1.0000 - val_loss: 7.0857 - val_accuracy: 0.4820\n",
      "Epoch 982/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.7184e-07 - accuracy: 1.0000 - val_loss: 7.0895 - val_accuracy: 0.4865\n",
      "Epoch 983/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 6.5416e-07 - accuracy: 1.0000 - val_loss: 7.0819 - val_accuracy: 0.4730\n",
      "Epoch 984/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 6.9154e-07 - accuracy: 1.0000 - val_loss: 7.0868 - val_accuracy: 0.4820\n",
      "Epoch 985/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 6.8621e-07 - accuracy: 1.0000 - val_loss: 7.0802 - val_accuracy: 0.4730\n",
      "Epoch 986/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.2914e-07 - accuracy: 1.0000 - val_loss: 7.0928 - val_accuracy: 0.4775\n",
      "Epoch 987/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 6.3497e-07 - accuracy: 1.0000 - val_loss: 7.0879 - val_accuracy: 0.4730\n",
      "Epoch 988/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 6.4463e-07 - accuracy: 1.0000 - val_loss: 7.0900 - val_accuracy: 0.4820\n",
      "Epoch 989/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 6.1439e-07 - accuracy: 1.0000 - val_loss: 7.0880 - val_accuracy: 0.4775\n",
      "Epoch 990/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 6.3522e-07 - accuracy: 1.0000 - val_loss: 7.0923 - val_accuracy: 0.4730\n",
      "Epoch 991/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.8665e-07 - accuracy: 1.0000 - val_loss: 7.0987 - val_accuracy: 0.4775\n",
      "Epoch 992/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 6.7404e-07 - accuracy: 1.0000 - val_loss: 7.1202 - val_accuracy: 0.4775\n",
      "Epoch 993/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 6.6030e-07 - accuracy: 1.0000 - val_loss: 7.1015 - val_accuracy: 0.4775\n",
      "Epoch 994/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 6.6993e-07 - accuracy: 1.0000 - val_loss: 7.1299 - val_accuracy: 0.4775\n",
      "Epoch 995/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 6.7462e-07 - accuracy: 1.0000 - val_loss: 7.1205 - val_accuracy: 0.4820\n",
      "Epoch 996/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 5.9327e-07 - accuracy: 1.0000 - val_loss: 7.1329 - val_accuracy: 0.4820\n",
      "Epoch 997/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 5.6023e-07 - accuracy: 1.0000 - val_loss: 7.1341 - val_accuracy: 0.4820\n",
      "Epoch 998/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 5.8307e-07 - accuracy: 1.0000 - val_loss: 7.1514 - val_accuracy: 0.4865\n",
      "Epoch 999/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 6.0042e-07 - accuracy: 1.0000 - val_loss: 7.1580 - val_accuracy: 0.4865\n",
      "Epoch 1000/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 5.4652e-07 - accuracy: 1.0000 - val_loss: 7.1423 - val_accuracy: 0.4820\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "246/246 [==============================] - 0s 39us/step\n",
      "test loss, test acc: [6.191910716576305, 0.5243902206420898]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (246, 1)\n",
      "rmse: 0.6759263456327976\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 3\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_accuracy\", patience=500, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99999881e-01],\n",
       "       [3.36779465e-07],\n",
       "       [7.94687629e-01],\n",
       "       [2.50774921e-08],\n",
       "       [9.89200885e-07],\n",
       "       [9.99990463e-01],\n",
       "       [1.30112655e-02],\n",
       "       [9.74454358e-03],\n",
       "       [1.75809117e-10],\n",
       "       [4.09911166e-07],\n",
       "       [9.15058862e-10],\n",
       "       [1.02268594e-07],\n",
       "       [9.99974370e-01],\n",
       "       [9.99982238e-01],\n",
       "       [1.09167904e-05],\n",
       "       [9.98772681e-01],\n",
       "       [4.71672484e-12],\n",
       "       [2.11470180e-07],\n",
       "       [2.02098349e-03],\n",
       "       [4.77092783e-07],\n",
       "       [3.28242777e-07],\n",
       "       [2.09092743e-08],\n",
       "       [2.28779186e-07],\n",
       "       [1.83533153e-08],\n",
       "       [9.99721348e-01],\n",
       "       [1.47447432e-03],\n",
       "       [1.49149025e-06],\n",
       "       [7.03328021e-11],\n",
       "       [9.99997497e-01],\n",
       "       [7.24537134e-01],\n",
       "       [1.04117580e-02],\n",
       "       [6.79251621e-04],\n",
       "       [9.99718010e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.99595940e-01],\n",
       "       [9.99999523e-01],\n",
       "       [9.99999523e-01],\n",
       "       [9.95771825e-01],\n",
       "       [1.00000000e+00],\n",
       "       [2.35642117e-10],\n",
       "       [9.56349075e-01],\n",
       "       [1.61956921e-01],\n",
       "       [9.99250352e-01],\n",
       "       [1.88362435e-07],\n",
       "       [3.73131499e-08],\n",
       "       [8.43410313e-01],\n",
       "       [2.21974074e-06],\n",
       "       [1.00000000e+00],\n",
       "       [3.42562645e-09],\n",
       "       [1.99752799e-06],\n",
       "       [9.99998093e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.98138666e-01],\n",
       "       [1.45587313e-03],\n",
       "       [6.07999482e-08],\n",
       "       [1.00000000e+00],\n",
       "       [7.25050853e-11],\n",
       "       [9.96990561e-01],\n",
       "       [5.78786603e-05],\n",
       "       [2.35773027e-01],\n",
       "       [2.35697182e-12],\n",
       "       [1.00000000e+00],\n",
       "       [9.99999523e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.97114897e-01],\n",
       "       [9.97755587e-01],\n",
       "       [5.30216028e-07],\n",
       "       [1.00000000e+00],\n",
       "       [2.18733476e-04],\n",
       "       [2.95684976e-03],\n",
       "       [9.99999285e-01],\n",
       "       [9.99999523e-01],\n",
       "       [1.84736162e-01],\n",
       "       [1.23446986e-08],\n",
       "       [9.99999881e-01],\n",
       "       [9.97935176e-01],\n",
       "       [3.69486344e-08],\n",
       "       [1.22529462e-01],\n",
       "       [1.00000000e+00],\n",
       "       [3.85177756e-09],\n",
       "       [2.47656385e-06],\n",
       "       [1.00000000e+00],\n",
       "       [9.48547240e-05],\n",
       "       [9.99733388e-01],\n",
       "       [1.00000000e+00],\n",
       "       [3.18315180e-10],\n",
       "       [9.99900579e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.57430912e-10],\n",
       "       [4.20036940e-06],\n",
       "       [9.99996543e-01],\n",
       "       [9.99883652e-01],\n",
       "       [5.25691985e-06],\n",
       "       [4.36228449e-11],\n",
       "       [9.99997258e-01],\n",
       "       [7.31205273e-06],\n",
       "       [9.99982357e-01],\n",
       "       [6.56363000e-07],\n",
       "       [1.41548276e-06],\n",
       "       [8.68151009e-01],\n",
       "       [4.90426771e-07],\n",
       "       [2.13089556e-01],\n",
       "       [1.00000000e+00],\n",
       "       [1.48206392e-09],\n",
       "       [9.99990702e-01],\n",
       "       [3.48084654e-06],\n",
       "       [2.37109598e-05],\n",
       "       [8.18135619e-01],\n",
       "       [2.16886656e-06],\n",
       "       [4.12362903e-01],\n",
       "       [9.99999881e-01],\n",
       "       [9.99944806e-01],\n",
       "       [1.00000000e+00],\n",
       "       [8.25106446e-03],\n",
       "       [3.90727967e-01],\n",
       "       [7.45881815e-04],\n",
       "       [4.85295750e-04],\n",
       "       [1.56301051e-01],\n",
       "       [2.88253116e-10],\n",
       "       [2.29266710e-11],\n",
       "       [1.00000000e+00],\n",
       "       [9.99962687e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.96474206e-01],\n",
       "       [4.17348572e-10],\n",
       "       [9.99992132e-01],\n",
       "       [1.20534338e-02],\n",
       "       [1.91952521e-09],\n",
       "       [2.78337513e-08],\n",
       "       [1.00000000e+00],\n",
       "       [9.99784172e-01],\n",
       "       [6.98520908e-10],\n",
       "       [8.57730483e-05],\n",
       "       [9.99999762e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.70011950e-01],\n",
       "       [9.99989867e-01],\n",
       "       [2.34028508e-12],\n",
       "       [1.60002146e-05],\n",
       "       [9.77491260e-01],\n",
       "       [9.99999881e-01],\n",
       "       [2.70684774e-04],\n",
       "       [7.00632441e-08],\n",
       "       [3.42429371e-06],\n",
       "       [1.35736977e-09],\n",
       "       [1.60650526e-09],\n",
       "       [3.89836088e-04],\n",
       "       [2.12798223e-01],\n",
       "       [9.67670739e-01],\n",
       "       [4.52289358e-03],\n",
       "       [9.99908209e-01],\n",
       "       [9.99996543e-01],\n",
       "       [2.24049668e-08],\n",
       "       [3.30218002e-02],\n",
       "       [1.00000000e+00],\n",
       "       [9.99999881e-01],\n",
       "       [1.38297798e-06],\n",
       "       [6.78252343e-10],\n",
       "       [1.20860700e-11],\n",
       "       [2.47961424e-07],\n",
       "       [9.94566679e-01],\n",
       "       [9.68289077e-01],\n",
       "       [1.87214762e-12],\n",
       "       [3.33909497e-07],\n",
       "       [9.18827772e-01],\n",
       "       [9.99998450e-01],\n",
       "       [9.87309158e-01],\n",
       "       [1.96871100e-04],\n",
       "       [9.99996066e-01],\n",
       "       [9.06078945e-12],\n",
       "       [1.18881684e-11],\n",
       "       [4.37960116e-05],\n",
       "       [1.00000000e+00],\n",
       "       [3.41856753e-06],\n",
       "       [9.99295831e-01],\n",
       "       [3.98765609e-04],\n",
       "       [9.99999762e-01],\n",
       "       [6.88446977e-04],\n",
       "       [6.06661979e-06],\n",
       "       [2.27309942e-11],\n",
       "       [3.35893291e-09],\n",
       "       [1.90818287e-06],\n",
       "       [5.60323713e-07],\n",
       "       [2.39945939e-06],\n",
       "       [1.87805802e-01],\n",
       "       [2.85501068e-04],\n",
       "       [7.44731587e-05],\n",
       "       [1.79096445e-01],\n",
       "       [1.59466329e-09],\n",
       "       [1.00000000e+00],\n",
       "       [3.55249408e-09],\n",
       "       [1.00000000e+00],\n",
       "       [1.80677995e-09],\n",
       "       [4.78096626e-06],\n",
       "       [1.33553657e-08],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [1.23114505e-08],\n",
       "       [3.34960640e-12],\n",
       "       [2.52382897e-08],\n",
       "       [1.00000000e+00],\n",
       "       [5.51169387e-06],\n",
       "       [1.00000000e+00],\n",
       "       [1.16733611e-01],\n",
       "       [1.00000000e+00],\n",
       "       [5.30466608e-11],\n",
       "       [9.99067366e-01],\n",
       "       [7.16575002e-03],\n",
       "       [9.99981880e-01],\n",
       "       [4.39635173e-09],\n",
       "       [3.54684886e-08],\n",
       "       [1.30886810e-05],\n",
       "       [9.99999881e-01],\n",
       "       [2.53696214e-07],\n",
       "       [9.99994755e-01],\n",
       "       [9.97847080e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.99985695e-01],\n",
       "       [5.50208324e-06],\n",
       "       [5.46276115e-11],\n",
       "       [2.26636554e-09],\n",
       "       [4.14194554e-01],\n",
       "       [7.13889960e-08],\n",
       "       [1.39186467e-08],\n",
       "       [9.10679591e-06],\n",
       "       [5.76785445e-01],\n",
       "       [6.18324518e-01],\n",
       "       [1.25910628e-05],\n",
       "       [1.75236516e-06],\n",
       "       [4.18880445e-05],\n",
       "       [1.00000000e+00],\n",
       "       [1.34778329e-05],\n",
       "       [3.68557274e-08],\n",
       "       [1.77066090e-06],\n",
       "       [9.99249756e-01],\n",
       "       [1.20994227e-03],\n",
       "       [9.74571347e-01],\n",
       "       [9.99419451e-01],\n",
       "       [9.99792278e-01],\n",
       "       [9.98692334e-01],\n",
       "       [9.97542381e-01],\n",
       "       [4.29325625e-02],\n",
       "       [9.41573363e-03],\n",
       "       [1.05733890e-12],\n",
       "       [3.82211208e-01],\n",
       "       [1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('X_train shape: ', X_train.shape)\n",
    "# print('X_valid shape: ', X_valid.shape)\n",
    "# print('y_train shape: ', y_train.shape)\n",
    "# print('y_valid shape: ', y_valid.shape)\n",
    "\n",
    "# print('X_test shape: ', X_test.shape)\n",
    "# print('y_test shape: ', y_test.shape)\n",
    "\n",
    "# print('X_test_batches shape: ', X_train_batches.shape)\n",
    "# print('Y_test_batches shape: ', y_train_batches.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
