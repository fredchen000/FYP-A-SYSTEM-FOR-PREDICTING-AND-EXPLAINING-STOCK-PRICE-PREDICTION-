{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import lxml\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import yfinance as yf\n",
    "import stockstats\n",
    "\n",
    "import unicodedata\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, TimeDistributed, RepeatVector, LSTM\n",
    "# from keras.layers import CuDNNLSTM as LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_without_absolute = pd.read_pickle('./data/stock_without_absolute.pkl')\n",
    "stock_with_absolute = pd.read_pickle('./data/stock_with_absolute.pkl')\n",
    "\n",
    "label_abs_1d = pd.read_pickle('./data/label_abs_1d.pkl')\n",
    "label_abs_7d = pd.read_pickle('./data/label_abs_7d.pkl')\n",
    "label_abs_30d = pd.read_pickle('./data/label_abs_30d.pkl')\n",
    "\n",
    "label_value_1d = pd.read_pickle('./data/label_value_1d.pkl')\n",
    "label_value_7d = pd.read_pickle('./data/label_value_7d.pkl')\n",
    "label_value_30d = pd.read_pickle('./data/label_value_30d.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2010-01-07     30.282858\n",
       "2010-01-08     30.015715\n",
       "2010-01-11     29.674286\n",
       "2010-01-12     30.092857\n",
       "2010-01-13     29.918571\n",
       "                 ...    \n",
       "2019-11-08    262.200012\n",
       "2019-11-11    261.959991\n",
       "2019-11-12    264.470001\n",
       "2019-11-13    262.640015\n",
       "2019-11-14    265.760010\n",
       "Name: close_1_s, Length: 2462, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_value_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data & Build Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_with_abs = MinMaxScaler()\n",
    "stock_with_abs_norm = scaler_with_abs.fit_transform(stock_with_absolute)\n",
    "\n",
    "scaler_without_abs = MinMaxScaler()\n",
    "stock_without_abs_norm = scaler_without_abs.fit_transform(stock_without_absolute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 10\n",
    "def build_batch(train, label, pastDay=30, futureDay=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train[i:i+pastDay]))\n",
    "        Y_train.append(np.array(label[i+pastDay:i+pastDay+futureDay]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "#     print('trim: ', no_of_rows_drop)\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(predicted_test, test_label, predicted_train, train_label, predicted_valid, valid_label,\n",
    "           file_name, decision_function, clf_name=\"LSTM\"):\n",
    "    print(\"Results for \", clf_name, \": \")\n",
    "    acc_train = accuracy_score(train_label, predicted_train)\n",
    "    acc_test = accuracy_score(test_label, predicted_test)\n",
    "    acc_valid = accuracy_score(valid_label, predicted_valid)\n",
    "    print(\"The Train Accuracy  %0.3f\" % (acc_train))\n",
    "    print(\"The Validation Accuracy  %0.3f\" % (acc_valid))\n",
    "    print(\"The Test Accuracy   %0.3f\" % (acc_test ))\n",
    "\n",
    "    print(\"AUC ROC : %0.3f\" %( roc_auc_score(test_label, predicted_test)))\n",
    "    # confusion matrix\n",
    "    print(\"confusion matrix / precision recall scores\")\n",
    "    print ( confusion_matrix(test_label, predicted_test) )\n",
    "    print ( classification_report(test_label, predicted_test))\n",
    "    \n",
    "    f = open(file_name+'.txt','w')\n",
    "    f.write(\"The Train Accuracy %0.3f\\n\" % (acc_train))\n",
    "    f.write(\"The Validation Accuracy %0.3f\\n\" % (acc_valid))\n",
    "    f.write(\"The Test Accuracy %0.3f\\n\" % (acc_test ))\n",
    "    f.write(\"AUC ROC : %0.3f\\n\" %( roc_auc_score(test_label, predicted_test) ))\n",
    "    f.write( str(confusion_matrix(test_label, predicted_test)) + \"\\n\")\n",
    "    f.write( str(classification_report(test_label, predicted_test)) + \"\\n\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Different Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_1stacks(shape, hidden_layer_size, batch_size):\n",
    "    print(shape)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size,  batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy']) # used for Binary classify\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_1stacks_true_value(shape, hidden_layer_size, batch_size):\n",
    "    print(shape)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size,  batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['accuracy']) # used for Binary classify\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_2stacks(shape, hidden_layer_size, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True, init='glorot_uniform'))\n",
    "    model.add(LSTM(hidden_layer_size, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy']) # used for Binary classify\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_3stacks(shape, hidden_layer_size, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True, init='glorot_uniform'))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(hidden_layer_size, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy']) # used for Binary classify\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_3stacks_true_value(shape, hidden_layer_size, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(hidden_layer_size, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='sgd', metrics=['accuracy']) # or adam\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrendModel_4stacks(shape, hidden_layer_size, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, batch_input_shape=(batch_size, shape[1], shape[2]), stateful=True, init='glorot_uniform'))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, stateful=True, init='glorot_uniform'))\n",
    "    model.add(LSTM(hidden_layer_size, stateful=True, init='glorot_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy']) # used for Binary classify\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(build_model_func, PAST_DAYS, stock_data, label, hidden_layer_size, batch_size):\n",
    "    X_train_batches, y_train_batches = build_batch(stock_data, label, PAST_DAYS, 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, shuffle = False, stratify = None)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle = False, stratify = None)\n",
    "    \n",
    "    X_train = trim_dataset(X_train, batch_size)\n",
    "    y_train = trim_dataset(y_train, batch_size)\n",
    "    X_valid = trim_dataset(X_valid, batch_size)\n",
    "    y_valid = trim_dataset(y_valid, batch_size)\n",
    "    X_test = trim_dataset(X_test, batch_size)\n",
    "    y_test = trim_dataset(y_test, batch_size)\n",
    "    \n",
    "    \n",
    "    model = build_model_func(X_train.shape, hidden_layer_size, batch_size)\n",
    "    callback = EarlyStopping(monitor=\"val_accuracy\", patience=200, verbose=1, mode=\"max\")\n",
    "    model.fit(X_train, y_train, epochs=1000, batch_size=batch_size, verbose=1,validation_data=(X_valid, y_valid), callbacks=[callback], shuffle=False)\n",
    "    return model, X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_loss(build_model_func, PAST_DAYS, stock_data, label, hidden_layer_size, batch_size):\n",
    "    X_train_batches, y_train_batches = build_batch(stock_data, label, PAST_DAYS, 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, shuffle = False, stratify = None)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle = False, stratify = None)\n",
    "    \n",
    "    X_train = trim_dataset(X_train, batch_size)\n",
    "    y_train = trim_dataset(y_train, batch_size)\n",
    "    X_valid = trim_dataset(X_valid, batch_size)\n",
    "    y_valid = trim_dataset(y_valid, batch_size)\n",
    "    X_test = trim_dataset(X_test, batch_size)\n",
    "    y_test = trim_dataset(y_test, batch_size)\n",
    "    \n",
    "    \n",
    "    model = build_model_func(X_train.shape, hidden_layer_size, batch_size)\n",
    "    callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "    model.fit(X_train, y_train, epochs=1000, batch_size=batch_size, verbose=1,validation_data=(X_valid, y_valid), callbacks=[callback], shuffle=False)\n",
    "    return model, X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Value Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"glorot_uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"glorot_uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"glorot_uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1981 samples, validate on 217 samples\n",
      "Epoch 1/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 1.9429 - accuracy: 0.0010 - val_loss: 10.1169 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "1981/1981 [==============================] - 2s 890us/step - loss: 1.9317 - accuracy: 0.0010 - val_loss: 10.1184 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "1981/1981 [==============================] - 2s 886us/step - loss: 1.9232 - accuracy: 0.0010 - val_loss: 10.1252 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "1981/1981 [==============================] - 2s 890us/step - loss: 1.9244 - accuracy: 0.0010 - val_loss: 10.1280 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "1981/1981 [==============================] - 2s 917us/step - loss: 1.9285 - accuracy: 0.0010 - val_loss: 10.1138 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "1981/1981 [==============================] - 2s 922us/step - loss: 1.9244 - accuracy: 0.0010 - val_loss: 10.1117 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "1981/1981 [==============================] - 2s 937us/step - loss: 1.9207 - accuracy: 0.0010 - val_loss: 10.1276 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "1981/1981 [==============================] - 2s 919us/step - loss: 1.9244 - accuracy: 0.0010 - val_loss: 10.1136 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.9237 - accuracy: 0.0010 - val_loss: 10.1139 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.9237 - accuracy: 0.0010 - val_loss: 10.1111 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "1981/1981 [==============================] - 2s 879us/step - loss: 1.9199 - accuracy: 0.0010 - val_loss: 10.1159 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "1981/1981 [==============================] - 2s 848us/step - loss: 1.9234 - accuracy: 0.0010 - val_loss: 10.1118 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "1981/1981 [==============================] - 2s 826us/step - loss: 1.9234 - accuracy: 0.0010 - val_loss: 10.1107 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "1981/1981 [==============================] - 2s 823us/step - loss: 1.9220 - accuracy: 0.0010 - val_loss: 10.1106 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 1.9216 - accuracy: 0.0010 - val_loss: 10.1106 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "1981/1981 [==============================] - 2s 839us/step - loss: 1.9207 - accuracy: 0.0010 - val_loss: 10.1122 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "1981/1981 [==============================] - 2s 821us/step - loss: 1.9213 - accuracy: 0.0010 - val_loss: 10.1118 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "1981/1981 [==============================] - 2s 876us/step - loss: 1.9223 - accuracy: 0.0010 - val_loss: 10.1107 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/1000\n",
      "1981/1981 [==============================] - 2s 976us/step - loss: 1.9210 - accuracy: 0.0010 - val_loss: 10.1110 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "1981/1981 [==============================] - 2s 939us/step - loss: 1.9209 - accuracy: 0.0010 - val_loss: 10.1108 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.9210 - accuracy: 0.0010 - val_loss: 10.1109 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "1981/1981 [==============================] - 2s 954us/step - loss: 1.9213 - accuracy: 0.0010 - val_loss: 10.1109 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "1981/1981 [==============================] - 2s 934us/step - loss: 1.9213 - accuracy: 0.0010 - val_loss: 10.1104 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "1981/1981 [==============================] - 2s 944us/step - loss: 1.9205 - accuracy: 0.0010 - val_loss: 10.1104 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/1000\n",
      "1981/1981 [==============================] - 2s 895us/step - loss: 1.9204 - accuracy: 0.0010 - val_loss: 10.1102 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "1981/1981 [==============================] - 2s 875us/step - loss: 1.9204 - accuracy: 0.0010 - val_loss: 10.1101 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/1000\n",
      "1981/1981 [==============================] - 2s 941us/step - loss: 1.9192 - accuracy: 0.0010 - val_loss: 10.1103 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "1981/1981 [==============================] - 2s 875us/step - loss: 1.9192 - accuracy: 0.0010 - val_loss: 10.1099 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "1981/1981 [==============================] - 2s 875us/step - loss: 1.9175 - accuracy: 0.0010 - val_loss: 10.1100 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "1981/1981 [==============================] - 2s 901us/step - loss: 1.9181 - accuracy: 0.0010 - val_loss: 10.1096 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/1000\n",
      "1981/1981 [==============================] - 2s 881us/step - loss: 1.9174 - accuracy: 0.0010 - val_loss: 10.1090 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/1000\n",
      "1981/1981 [==============================] - 2s 876us/step - loss: 1.9209 - accuracy: 0.0010 - val_loss: 10.1095 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "1981/1981 [==============================] - 2s 873us/step - loss: 1.9178 - accuracy: 0.0010 - val_loss: 10.1099 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "1981/1981 [==============================] - 2s 878us/step - loss: 1.9180 - accuracy: 0.0010 - val_loss: 10.1093 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "1981/1981 [==============================] - 2s 881us/step - loss: 1.9175 - accuracy: 0.0010 - val_loss: 10.1094 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/1000\n",
      "1981/1981 [==============================] - 2s 879us/step - loss: 1.9186 - accuracy: 0.0010 - val_loss: 10.1087 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/1000\n",
      "1981/1981 [==============================] - 2s 878us/step - loss: 1.9145 - accuracy: 0.0010 - val_loss: 10.1082 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/1000\n",
      "1981/1981 [==============================] - 2s 878us/step - loss: 1.9197 - accuracy: 0.0010 - val_loss: 10.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "1981/1981 [==============================] - 2s 892us/step - loss: 1.9130 - accuracy: 0.0010 - val_loss: 10.1084 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "1981/1981 [==============================] - 2s 879us/step - loss: 1.9106 - accuracy: 0.0010 - val_loss: 10.1081 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "1981/1981 [==============================] - 2s 881us/step - loss: 1.9120 - accuracy: 0.0010 - val_loss: 10.1090 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "1981/1981 [==============================] - 2s 942us/step - loss: 1.9161 - accuracy: 0.0010 - val_loss: 10.1095 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "1981/1981 [==============================] - 2s 964us/step - loss: 1.9147 - accuracy: 0.0010 - val_loss: 10.1094 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "1981/1981 [==============================] - 2s 912us/step - loss: 1.9114 - accuracy: 0.0010 - val_loss: 10.1095 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "1981/1981 [==============================] - 2s 875us/step - loss: 1.9098 - accuracy: 0.0010 - val_loss: 10.1107 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "1981/1981 [==============================] - 2s 859us/step - loss: 1.9131 - accuracy: 0.0010 - val_loss: 10.1128 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/1000\n",
      "1981/1981 [==============================] - 2s 926us/step - loss: 1.9112 - accuracy: 0.0010 - val_loss: 10.1128 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/1000\n",
      "1981/1981 [==============================] - 2s 954us/step - loss: 1.9112 - accuracy: 0.0010 - val_loss: 10.1118 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "1981/1981 [==============================] - 2s 958us/step - loss: 1.9103 - accuracy: 0.0010 - val_loss: 10.1134 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "1981/1981 [==============================] - 2s 961us/step - loss: 1.9083 - accuracy: 0.0010 - val_loss: 10.1126 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/1000\n",
      "1981/1981 [==============================] - 2s 884us/step - loss: 1.9074 - accuracy: 0.0010 - val_loss: 10.1144 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "1981/1981 [==============================] - 2s 885us/step - loss: 1.9063 - accuracy: 0.0010 - val_loss: 10.1169 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "1981/1981 [==============================] - 2s 884us/step - loss: 1.9110 - accuracy: 0.0010 - val_loss: 10.1190 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 862us/step - loss: 1.9122 - accuracy: 0.0010 - val_loss: 10.1120 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "1981/1981 [==============================] - 2s 944us/step - loss: 1.9063 - accuracy: 0.0010 - val_loss: 10.1208 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.9079 - accuracy: 0.0010 - val_loss: 10.1180 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/1000\n",
      "1981/1981 [==============================] - 2s 886us/step - loss: 1.9050 - accuracy: 0.0010 - val_loss: 10.1251 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/1000\n",
      "1981/1981 [==============================] - 2s 858us/step - loss: 1.9011 - accuracy: 0.0010 - val_loss: 10.1396 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/1000\n",
      "1981/1981 [==============================] - 2s 903us/step - loss: 1.9029 - accuracy: 0.0010 - val_loss: 10.1171 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/1000\n",
      "1981/1981 [==============================] - 2s 995us/step - loss: 1.9002 - accuracy: 0.0010 - val_loss: 10.1254 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/1000\n",
      "1981/1981 [==============================] - 2s 950us/step - loss: 1.9101 - accuracy: 0.0010 - val_loss: 10.1166 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.9106 - accuracy: 0.0010 - val_loss: 10.1122 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "1981/1981 [==============================] - 2s 885us/step - loss: 1.9048 - accuracy: 0.0010 - val_loss: 10.1237 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 1.9014 - accuracy: 0.0010 - val_loss: 10.1133 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "1981/1981 [==============================] - 2s 844us/step - loss: 1.9023 - accuracy: 0.0010 - val_loss: 10.1094 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "1981/1981 [==============================] - 2s 845us/step - loss: 1.9033 - accuracy: 0.0010 - val_loss: 10.1140 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 1.9080 - accuracy: 0.0010 - val_loss: 10.1141 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1.9007 - accuracy: 0.0010 - val_loss: 10.1170 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1.8995 - accuracy: 0.0010 - val_loss: 10.1342 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1.9027 - accuracy: 0.0010 - val_loss: 10.1248 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/1000\n",
      "1981/1981 [==============================] - 2s 843us/step - loss: 1.8974 - accuracy: 0.0010 - val_loss: 10.1164 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 1.9067 - accuracy: 0.0010 - val_loss: 10.1127 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/1000\n",
      "1981/1981 [==============================] - 2s 824us/step - loss: 1.9029 - accuracy: 0.0010 - val_loss: 10.0948 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/1000\n",
      "1981/1981 [==============================] - 2s 840us/step - loss: 1.8930 - accuracy: 0.0010 - val_loss: 10.1160 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 1.8925 - accuracy: 0.0010 - val_loss: 10.1083 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/1000\n",
      "1981/1981 [==============================] - 2s 918us/step - loss: 1.8946 - accuracy: 0.0010 - val_loss: 10.1062 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/1000\n",
      "1981/1981 [==============================] - 2s 947us/step - loss: 1.8921 - accuracy: 0.0010 - val_loss: 10.1150 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/1000\n",
      "1981/1981 [==============================] - 2s 910us/step - loss: 1.8942 - accuracy: 0.0010 - val_loss: 10.1020 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/1000\n",
      "1981/1981 [==============================] - 2s 868us/step - loss: 1.8982 - accuracy: 0.0010 - val_loss: 10.1264 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/1000\n",
      "1981/1981 [==============================] - 2s 932us/step - loss: 1.8858 - accuracy: 0.0010 - val_loss: 10.1113 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/1000\n",
      "1981/1981 [==============================] - 2s 937us/step - loss: 1.8846 - accuracy: 0.0010 - val_loss: 10.1322 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/1000\n",
      "1981/1981 [==============================] - 2s 875us/step - loss: 1.8816 - accuracy: 0.0010 - val_loss: 10.1103 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/1000\n",
      "1981/1981 [==============================] - 2s 849us/step - loss: 1.8886 - accuracy: 0.0010 - val_loss: 10.1042 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1.8899 - accuracy: 0.0010 - val_loss: 10.1123 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/1000\n",
      "1981/1981 [==============================] - 2s 836us/step - loss: 1.8831 - accuracy: 0.0010 - val_loss: 10.1204 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/1000\n",
      "1981/1981 [==============================] - 2s 880us/step - loss: 1.8932 - accuracy: 0.0010 - val_loss: 10.1164 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/1000\n",
      "1981/1981 [==============================] - 2s 839us/step - loss: 1.8870 - accuracy: 0.0010 - val_loss: 10.1229 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1.8781 - accuracy: 0.0010 - val_loss: 10.1186 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/1000\n",
      "1981/1981 [==============================] - 2s 826us/step - loss: 1.8742 - accuracy: 0.0010 - val_loss: 10.1199 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 1.8769 - accuracy: 0.0010 - val_loss: 10.1149 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/1000\n",
      "1981/1981 [==============================] - 2s 824us/step - loss: 1.8886 - accuracy: 0.0010 - val_loss: 10.1369 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/1000\n",
      "1981/1981 [==============================] - 2s 857us/step - loss: 1.8800 - accuracy: 0.0010 - val_loss: 10.1424 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/1000\n",
      "1981/1981 [==============================] - 2s 930us/step - loss: 1.8813 - accuracy: 0.0010 - val_loss: 10.1246 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "1981/1981 [==============================] - 2s 996us/step - loss: 1.8841 - accuracy: 0.0010 - val_loss: 10.1315 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "1981/1981 [==============================] - 2s 946us/step - loss: 1.8803 - accuracy: 0.0010 - val_loss: 10.1374 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/1000\n",
      "1981/1981 [==============================] - 2s 876us/step - loss: 1.8780 - accuracy: 0.0010 - val_loss: 10.1399 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/1000\n",
      "1981/1981 [==============================] - 2s 894us/step - loss: 1.8742 - accuracy: 0.0010 - val_loss: 10.1340 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "1981/1981 [==============================] - 2s 885us/step - loss: 1.8754 - accuracy: 0.0010 - val_loss: 10.1264 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/1000\n",
      "1981/1981 [==============================] - 2s 861us/step - loss: 1.8759 - accuracy: 0.0010 - val_loss: 10.1279 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/1000\n",
      "1981/1981 [==============================] - 2s 918us/step - loss: 1.8704 - accuracy: 0.0010 - val_loss: 10.1441 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/1000\n",
      "1981/1981 [==============================] - 2s 857us/step - loss: 1.8704 - accuracy: 0.0010 - val_loss: 10.1281 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/1000\n",
      "1981/1981 [==============================] - 2s 924us/step - loss: 1.8629 - accuracy: 0.0010 - val_loss: 10.1476 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/1000\n",
      "1981/1981 [==============================] - 2s 981us/step - loss: 1.8793 - accuracy: 0.0010 - val_loss: 10.1288 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/1000\n",
      "1981/1981 [==============================] - 2s 929us/step - loss: 1.8653 - accuracy: 0.0010 - val_loss: 10.1275 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/1000\n",
      "1981/1981 [==============================] - 2s 858us/step - loss: 1.8553 - accuracy: 0.0010 - val_loss: 10.1405 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/1000\n",
      "1981/1981 [==============================] - 2s 878us/step - loss: 1.8702 - accuracy: 0.0010 - val_loss: 10.1995 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 842us/step - loss: 1.8733 - accuracy: 0.0010 - val_loss: 10.1718 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/1000\n",
      "1981/1981 [==============================] - 2s 826us/step - loss: 1.8632 - accuracy: 0.0010 - val_loss: 10.1399 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/1000\n",
      "1981/1981 [==============================] - 2s 865us/step - loss: 1.8579 - accuracy: 0.0010 - val_loss: 10.1373 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1.8545 - accuracy: 0.0010 - val_loss: 10.1967 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/1000\n",
      "1981/1981 [==============================] - 2s 942us/step - loss: 1.8564 - accuracy: 0.0010 - val_loss: 10.1794 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/1000\n",
      "1981/1981 [==============================] - 2s 917us/step - loss: 1.8462 - accuracy: 0.0010 - val_loss: 10.1976 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/1000\n",
      "1981/1981 [==============================] - 2s 901us/step - loss: 1.8474 - accuracy: 0.0010 - val_loss: 10.1811 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/1000\n",
      "1981/1981 [==============================] - 2s 890us/step - loss: 1.8595 - accuracy: 0.0010 - val_loss: 10.1795 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/1000\n",
      "1981/1981 [==============================] - 2s 903us/step - loss: 1.8447 - accuracy: 0.0010 - val_loss: 10.2411 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/1000\n",
      "1981/1981 [==============================] - 2s 876us/step - loss: 1.8384 - accuracy: 0.0010 - val_loss: 10.2400 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/1000\n",
      "1981/1981 [==============================] - 2s 881us/step - loss: 1.8343 - accuracy: 0.0010 - val_loss: 10.1904 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "1981/1981 [==============================] - 2s 887us/step - loss: 1.8395 - accuracy: 0.0010 - val_loss: 10.2767 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/1000\n",
      "1981/1981 [==============================] - 2s 871us/step - loss: 1.8519 - accuracy: 0.0010 - val_loss: 10.2085 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/1000\n",
      "1981/1981 [==============================] - 2s 909us/step - loss: 1.8225 - accuracy: 0.0010 - val_loss: 10.2502 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "1981/1981 [==============================] - 2s 921us/step - loss: 1.8381 - accuracy: 0.0010 - val_loss: 10.2297 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/1000\n",
      "1981/1981 [==============================] - 2s 893us/step - loss: 1.8205 - accuracy: 0.0010 - val_loss: 10.2961 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "1981/1981 [==============================] - 2s 910us/step - loss: 1.8247 - accuracy: 0.0010 - val_loss: 10.2507 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "1981/1981 [==============================] - 2s 851us/step - loss: 1.8060 - accuracy: 0.0010 - val_loss: 10.3840 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1.8272 - accuracy: 0.0010 - val_loss: 10.3075 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "1981/1981 [==============================] - 2s 847us/step - loss: 1.8147 - accuracy: 0.0010 - val_loss: 10.3000 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "1981/1981 [==============================] - 2s 928us/step - loss: 1.7981 - accuracy: 0.0015 - val_loss: 10.3354 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "1981/1981 [==============================] - 2s 915us/step - loss: 1.7890 - accuracy: 0.0010 - val_loss: 10.3766 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "1981/1981 [==============================] - 2s 949us/step - loss: 1.8188 - accuracy: 0.0010 - val_loss: 10.2990 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 1.7913 - accuracy: 5.0480e-04 - val_loss: 10.3781 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "1981/1981 [==============================] - 2s 821us/step - loss: 1.7897 - accuracy: 0.0010 - val_loss: 10.3067 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "1981/1981 [==============================] - 2s 844us/step - loss: 1.7840 - accuracy: 0.0010 - val_loss: 10.3311 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "1981/1981 [==============================] - 2s 816us/step - loss: 1.7970 - accuracy: 0.0010 - val_loss: 10.4024 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 1.7954 - accuracy: 0.0010 - val_loss: 10.3825 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 1.7928 - accuracy: 0.0010 - val_loss: 10.4459 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "1981/1981 [==============================] - 2s 837us/step - loss: 1.7773 - accuracy: 0.0010 - val_loss: 10.4331 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/1000\n",
      "1981/1981 [==============================] - 2s 814us/step - loss: 1.7674 - accuracy: 0.0010 - val_loss: 10.5298 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "1981/1981 [==============================] - 2s 843us/step - loss: 1.7894 - accuracy: 5.0480e-04 - val_loss: 10.4392 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/1000\n",
      "1981/1981 [==============================] - 2s 875us/step - loss: 1.7713 - accuracy: 0.0010 - val_loss: 10.4805 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 1.7810 - accuracy: 0.0010 - val_loss: 10.4950 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/1000\n",
      "1981/1981 [==============================] - 2s 929us/step - loss: 1.7703 - accuracy: 0.0010 - val_loss: 10.4289 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.7623 - accuracy: 0.0010 - val_loss: 10.4349 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "1981/1981 [==============================] - 2s 973us/step - loss: 1.7640 - accuracy: 5.0480e-04 - val_loss: 10.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "1981/1981 [==============================] - 2s 878us/step - loss: 1.7564 - accuracy: 5.0480e-04 - val_loss: 10.4369 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "1981/1981 [==============================] - 2s 807us/step - loss: 1.7491 - accuracy: 5.0480e-04 - val_loss: 10.4218 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 1.7426 - accuracy: 0.0015 - val_loss: 10.4469 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 1.7550 - accuracy: 0.0015 - val_loss: 10.4938 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/1000\n",
      "1981/1981 [==============================] - 2s 826us/step - loss: 1.7283 - accuracy: 0.0010 - val_loss: 10.3322 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 1.7304 - accuracy: 5.0480e-04 - val_loss: 10.4106 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "1981/1981 [==============================] - 2s 827us/step - loss: 1.7478 - accuracy: 0.0015 - val_loss: 10.4411 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1.7478 - accuracy: 5.0480e-04 - val_loss: 10.3004 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "1981/1981 [==============================] - 2s 975us/step - loss: 1.7394 - accuracy: 0.0015 - val_loss: 10.4619 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/1000\n",
      "1981/1981 [==============================] - 2s 979us/step - loss: 1.7358 - accuracy: 0.0015 - val_loss: 10.5256 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/1000\n",
      "1981/1981 [==============================] - 2s 984us/step - loss: 1.7150 - accuracy: 0.0010 - val_loss: 10.7098 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "1981/1981 [==============================] - 2s 953us/step - loss: 1.7031 - accuracy: 0.0020 - val_loss: 10.4926 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "1981/1981 [==============================] - 2s 961us/step - loss: 1.7267 - accuracy: 0.0010 - val_loss: 10.4784 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "1981/1981 [==============================] - 2s 971us/step - loss: 1.6964 - accuracy: 0.0015 - val_loss: 10.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "1981/1981 [==============================] - 2s 960us/step - loss: 1.7155 - accuracy: 0.0010 - val_loss: 10.5492 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "1981/1981 [==============================] - 2s 855us/step - loss: 1.6790 - accuracy: 0.0010 - val_loss: 10.3584 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/1000\n",
      "1981/1981 [==============================] - 2s 872us/step - loss: 1.6827 - accuracy: 0.0010 - val_loss: 10.2698 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "1981/1981 [==============================] - 2s 937us/step - loss: 1.6741 - accuracy: 0.0020 - val_loss: 10.4388 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "1981/1981 [==============================] - 2s 924us/step - loss: 1.6696 - accuracy: 0.0015 - val_loss: 10.5303 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "1981/1981 [==============================] - 2s 883us/step - loss: 1.6579 - accuracy: 0.0010 - val_loss: 10.7241 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "1981/1981 [==============================] - 2s 822us/step - loss: 1.6708 - accuracy: 0.0015 - val_loss: 10.5958 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 1.6686 - accuracy: 5.0480e-04 - val_loss: 10.6359 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 1.6635 - accuracy: 5.0480e-04 - val_loss: 10.6639 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "1981/1981 [==============================] - 2s 847us/step - loss: 1.6456 - accuracy: 5.0480e-04 - val_loss: 10.8255 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "1981/1981 [==============================] - 2s 832us/step - loss: 1.6502 - accuracy: 0.0020 - val_loss: 10.7385 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/1000\n",
      "1981/1981 [==============================] - 2s 826us/step - loss: 1.6349 - accuracy: 0.0015 - val_loss: 10.5892 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "1981/1981 [==============================] - 2s 840us/step - loss: 1.6712 - accuracy: 5.0480e-04 - val_loss: 10.6436 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "1981/1981 [==============================] - 2s 868us/step - loss: 1.6427 - accuracy: 0.0010 - val_loss: 10.3242 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "1981/1981 [==============================] - 2s 984us/step - loss: 1.6389 - accuracy: 0.0010 - val_loss: 10.7214 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/1000\n",
      "1981/1981 [==============================] - 2s 975us/step - loss: 1.6612 - accuracy: 0.0015 - val_loss: 10.3608 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "1981/1981 [==============================] - 2s 842us/step - loss: 1.5854 - accuracy: 0.0020 - val_loss: 10.6153 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 1.6316 - accuracy: 0.0015 - val_loss: 10.2707 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 1.6310 - accuracy: 0.0010 - val_loss: 10.7222 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 1.6238 - accuracy: 0.0010 - val_loss: 10.4438 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1.5954 - accuracy: 0.0020 - val_loss: 10.5671 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "1981/1981 [==============================] - 2s 812us/step - loss: 1.6229 - accuracy: 0.0015 - val_loss: 10.3654 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "1981/1981 [==============================] - 2s 838us/step - loss: 1.6060 - accuracy: 0.0010 - val_loss: 10.6078 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 1.6090 - accuracy: 0.0010 - val_loss: 10.2997 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/1000\n",
      "1981/1981 [==============================] - 2s 818us/step - loss: 1.5655 - accuracy: 0.0010 - val_loss: 10.3820 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/1000\n",
      "1981/1981 [==============================] - 2s 839us/step - loss: 1.5901 - accuracy: 0.0010 - val_loss: 10.5875 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/1000\n",
      "1981/1981 [==============================] - 2s 925us/step - loss: 1.6101 - accuracy: 0.0020 - val_loss: 10.3150 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/1000\n",
      "1981/1981 [==============================] - 2s 927us/step - loss: 1.5790 - accuracy: 0.0015 - val_loss: 10.1317 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/1000\n",
      "1981/1981 [==============================] - 2s 939us/step - loss: 1.5777 - accuracy: 0.0020 - val_loss: 10.3861 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/1000\n",
      "1981/1981 [==============================] - 2s 899us/step - loss: 1.5922 - accuracy: 0.0015 - val_loss: 10.5325 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/1000\n",
      "1981/1981 [==============================] - 2s 913us/step - loss: 1.5692 - accuracy: 0.0015 - val_loss: 10.4378 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.5768 - accuracy: 0.0015 - val_loss: 10.4306 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/1000\n",
      "1981/1981 [==============================] - 2s 986us/step - loss: 1.5691 - accuracy: 5.0480e-04 - val_loss: 10.4535 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/1000\n",
      "1981/1981 [==============================] - 2s 880us/step - loss: 1.5268 - accuracy: 0.0010 - val_loss: 10.3368 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/1000\n",
      "1981/1981 [==============================] - 2s 843us/step - loss: 1.5305 - accuracy: 0.0010 - val_loss: 10.6639 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/1000\n",
      "1981/1981 [==============================] - 2s 849us/step - loss: 1.5386 - accuracy: 0.0020 - val_loss: 10.5317 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/1000\n",
      "1981/1981 [==============================] - 2s 866us/step - loss: 1.5522 - accuracy: 0.0015 - val_loss: 10.5717 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/1000\n",
      "1981/1981 [==============================] - 2s 847us/step - loss: 1.5309 - accuracy: 0.0015 - val_loss: 10.7081 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/1000\n",
      "1981/1981 [==============================] - 2s 853us/step - loss: 1.5037 - accuracy: 0.0015 - val_loss: 10.8310 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/1000\n",
      "1981/1981 [==============================] - 2s 974us/step - loss: 1.5170 - accuracy: 5.0480e-04 - val_loss: 10.5648 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/1000\n",
      "1981/1981 [==============================] - 2s 921us/step - loss: 1.5045 - accuracy: 0.0020 - val_loss: 10.7197 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/1000\n",
      "1981/1981 [==============================] - 2s 883us/step - loss: 1.4946 - accuracy: 0.0010 - val_loss: 10.7190 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/1000\n",
      "1981/1981 [==============================] - 2s 859us/step - loss: 1.5557 - accuracy: 0.0015 - val_loss: 10.4795 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/1000\n",
      "1981/1981 [==============================] - 2s 874us/step - loss: 1.4957 - accuracy: 0.0015 - val_loss: 10.7964 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/1000\n",
      "1981/1981 [==============================] - 2s 929us/step - loss: 1.5119 - accuracy: 0.0015 - val_loss: 10.4004 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/1000\n",
      "1981/1981 [==============================] - 2s 887us/step - loss: 1.5380 - accuracy: 0.0010 - val_loss: 10.2924 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/1000\n",
      "1981/1981 [==============================] - 2s 871us/step - loss: 1.4665 - accuracy: 0.0015 - val_loss: 10.6662 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/1000\n",
      "1981/1981 [==============================] - 2s 909us/step - loss: 1.4662 - accuracy: 0.0010 - val_loss: 10.6411 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.4749 - accuracy: 0.0015 - val_loss: 10.4786 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.5278 - accuracy: 0.0015 - val_loss: 10.0677 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.4469 - accuracy: 0.0010 - val_loss: 10.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/1000\n",
      "1981/1981 [==============================] - 2s 954us/step - loss: 1.4707 - accuracy: 0.0010 - val_loss: 10.8421 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/1000\n",
      "1981/1981 [==============================] - 2s 893us/step - loss: 1.4429 - accuracy: 0.0015 - val_loss: 10.4184 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/1000\n",
      "1981/1981 [==============================] - 2s 945us/step - loss: 1.4596 - accuracy: 0.0010 - val_loss: 10.8507 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/1000\n",
      "1981/1981 [==============================] - 2s 935us/step - loss: 1.4431 - accuracy: 0.0010 - val_loss: 10.8000 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/1000\n",
      "1981/1981 [==============================] - 2s 950us/step - loss: 1.3832 - accuracy: 0.0020 - val_loss: 10.6114 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/1000\n",
      "1981/1981 [==============================] - 2s 895us/step - loss: 1.4286 - accuracy: 5.0480e-04 - val_loss: 10.4144 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/1000\n",
      "1981/1981 [==============================] - 2s 893us/step - loss: 1.5098 - accuracy: 5.0480e-04 - val_loss: 10.4773 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/1000\n",
      "1981/1981 [==============================] - 2s 848us/step - loss: 1.4296 - accuracy: 0.0015 - val_loss: 10.4867 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/1000\n",
      "1981/1981 [==============================] - 2s 869us/step - loss: 1.4337 - accuracy: 0.0020 - val_loss: 10.6534 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/1000\n",
      "1981/1981 [==============================] - 2s 886us/step - loss: 1.3645 - accuracy: 0.0010 - val_loss: 10.7494 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/1000\n",
      "1981/1981 [==============================] - 2s 923us/step - loss: 1.3920 - accuracy: 0.0020 - val_loss: 10.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/1000\n",
      "1981/1981 [==============================] - 2s 924us/step - loss: 1.4237 - accuracy: 0.0020 - val_loss: 10.3430 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/1000\n",
      "1981/1981 [==============================] - 2s 895us/step - loss: 1.4021 - accuracy: 0.0010 - val_loss: 10.6385 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/1000\n",
      "1981/1981 [==============================] - 2s 890us/step - loss: 1.4298 - accuracy: 0.0015 - val_loss: 10.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1.3787 - accuracy: 0.0015 - val_loss: 10.4832 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/1000\n",
      "1981/1981 [==============================] - 2s 841us/step - loss: 1.3782 - accuracy: 0.0015 - val_loss: 10.8074 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/1000\n",
      "1981/1981 [==============================] - 2s 890us/step - loss: 1.3703 - accuracy: 0.0010 - val_loss: 11.1319 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/1000\n",
      "1981/1981 [==============================] - 2s 878us/step - loss: 1.3713 - accuracy: 0.0010 - val_loss: 10.9287 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/1000\n",
      "1981/1981 [==============================] - 2s 870us/step - loss: 1.4151 - accuracy: 0.0015 - val_loss: 11.0483 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/1000\n",
      "1981/1981 [==============================] - 2s 886us/step - loss: 1.4099 - accuracy: 0.0020 - val_loss: 11.0861 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/1000\n",
      "1981/1981 [==============================] - 2s 883us/step - loss: 1.3337 - accuracy: 5.0480e-04 - val_loss: 10.8318 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1.3524 - accuracy: 0.0015 - val_loss: 10.6297 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/1000\n",
      "1981/1981 [==============================] - 2s 910us/step - loss: 1.4002 - accuracy: 0.0010 - val_loss: 10.9197 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/1000\n",
      "1981/1981 [==============================] - 2s 866us/step - loss: 1.3016 - accuracy: 0.0010 - val_loss: 10.8942 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1.3261 - accuracy: 0.0015 - val_loss: 10.9064 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/1000\n",
      "1981/1981 [==============================] - 2s 840us/step - loss: 1.3480 - accuracy: 0.0015 - val_loss: 10.5720 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1.3417 - accuracy: 0.0015 - val_loss: 10.6905 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/1000\n",
      "1981/1981 [==============================] - 2s 841us/step - loss: 1.3846 - accuracy: 0.0010 - val_loss: 10.6969 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/1000\n",
      "1981/1981 [==============================] - 2s 841us/step - loss: 1.3358 - accuracy: 0.0010 - val_loss: 10.7608 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/1000\n",
      "1981/1981 [==============================] - 2s 841us/step - loss: 1.2899 - accuracy: 0.0015 - val_loss: 10.5282 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1.3165 - accuracy: 0.0010 - val_loss: 10.5918 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1.3397 - accuracy: 0.0015 - val_loss: 10.7173 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/1000\n",
      "1981/1981 [==============================] - 2s 837us/step - loss: 1.3473 - accuracy: 0.0015 - val_loss: 10.3831 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1.3298 - accuracy: 0.0010 - val_loss: 10.6109 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/1000\n",
      "1981/1981 [==============================] - 2s 855us/step - loss: 1.3240 - accuracy: 0.0010 - val_loss: 10.7017 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/1000\n",
      "1981/1981 [==============================] - 2s 839us/step - loss: 1.3047 - accuracy: 0.0020 - val_loss: 10.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/1000\n",
      "1981/1981 [==============================] - 2s 842us/step - loss: 1.2485 - accuracy: 0.0020 - val_loss: 10.8552 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/1000\n",
      "1981/1981 [==============================] - 2s 870us/step - loss: 1.2566 - accuracy: 0.0010 - val_loss: 10.7011 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 1.2752 - accuracy: 0.0015 - val_loss: 10.8496 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/1000\n",
      "1981/1981 [==============================] - 2s 813us/step - loss: 1.3140 - accuracy: 0.0015 - val_loss: 10.5585 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 1.2697 - accuracy: 0.0015 - val_loss: 10.6577 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/1000\n",
      "1981/1981 [==============================] - 2s 860us/step - loss: 1.2263 - accuracy: 0.0020 - val_loss: 10.9738 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/1000\n",
      "1981/1981 [==============================] - 2s 821us/step - loss: 1.2180 - accuracy: 0.0010 - val_loss: 10.9314 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/1000\n",
      "1981/1981 [==============================] - 2s 817us/step - loss: 1.2816 - accuracy: 0.0020 - val_loss: 11.0992 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/1000\n",
      "1981/1981 [==============================] - 2s 867us/step - loss: 1.2849 - accuracy: 0.0020 - val_loss: 10.9426 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 1.2580 - accuracy: 0.0010 - val_loss: 10.4968 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/1000\n",
      "1981/1981 [==============================] - 2s 839us/step - loss: 1.2540 - accuracy: 0.0010 - val_loss: 10.7493 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/1000\n",
      "1981/1981 [==============================] - 2s 967us/step - loss: 1.1916 - accuracy: 0.0010 - val_loss: 11.5053 - val_accuracy: 0.0000e+00\n",
      "Epoch 257/1000\n",
      "1981/1981 [==============================] - 2s 843us/step - loss: 1.1997 - accuracy: 0.0010 - val_loss: 10.6990 - val_accuracy: 0.0000e+00\n",
      "Epoch 258/1000\n",
      "1981/1981 [==============================] - 2s 906us/step - loss: 1.2488 - accuracy: 0.0015 - val_loss: 11.1653 - val_accuracy: 0.0000e+00\n",
      "Epoch 259/1000\n",
      "1981/1981 [==============================] - 2s 874us/step - loss: 1.2296 - accuracy: 0.0030 - val_loss: 10.6476 - val_accuracy: 0.0000e+00\n",
      "Epoch 260/1000\n",
      "1981/1981 [==============================] - 2s 827us/step - loss: 1.1833 - accuracy: 0.0015 - val_loss: 10.9541 - val_accuracy: 0.0000e+00\n",
      "Epoch 261/1000\n",
      "1981/1981 [==============================] - 2s 883us/step - loss: 1.1213 - accuracy: 0.0015 - val_loss: 10.9982 - val_accuracy: 0.0000e+00\n",
      "Epoch 262/1000\n",
      "1981/1981 [==============================] - 2s 868us/step - loss: 1.1687 - accuracy: 0.0010 - val_loss: 11.1144 - val_accuracy: 0.0000e+00\n",
      "Epoch 263/1000\n",
      "1981/1981 [==============================] - 2s 924us/step - loss: 1.1710 - accuracy: 0.0015 - val_loss: 10.6836 - val_accuracy: 0.0000e+00\n",
      "Epoch 264/1000\n",
      "1981/1981 [==============================] - 2s 877us/step - loss: 1.1930 - accuracy: 0.0015 - val_loss: 11.0524 - val_accuracy: 0.0000e+00\n",
      "Epoch 265/1000\n",
      "1981/1981 [==============================] - 2s 846us/step - loss: 1.1875 - accuracy: 0.0020 - val_loss: 10.6455 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/1000\n",
      "1981/1981 [==============================] - 2s 826us/step - loss: 1.1419 - accuracy: 0.0025 - val_loss: 11.0586 - val_accuracy: 0.0000e+00\n",
      "Epoch 267/1000\n",
      "1981/1981 [==============================] - 2s 881us/step - loss: 1.1351 - accuracy: 5.0480e-04 - val_loss: 10.8761 - val_accuracy: 0.0000e+00\n",
      "Epoch 268/1000\n",
      "1981/1981 [==============================] - 2s 827us/step - loss: 1.1633 - accuracy: 0.0010 - val_loss: 10.8560 - val_accuracy: 0.0000e+00\n",
      "Epoch 269/1000\n",
      "1981/1981 [==============================] - 2s 832us/step - loss: 1.1791 - accuracy: 0.0010 - val_loss: 10.4471 - val_accuracy: 0.0000e+00\n",
      "Epoch 270/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1.1518 - accuracy: 5.0480e-04 - val_loss: 10.7753 - val_accuracy: 0.0000e+00\n",
      "Epoch 271/1000\n",
      "1981/1981 [==============================] - 2s 811us/step - loss: 1.2370 - accuracy: 0.0015 - val_loss: 11.0563 - val_accuracy: 0.0000e+00\n",
      "Epoch 272/1000\n",
      "1981/1981 [==============================] - 2s 816us/step - loss: 1.1569 - accuracy: 0.0010 - val_loss: 10.6814 - val_accuracy: 0.0000e+00\n",
      "Epoch 273/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1.1491 - accuracy: 0.0015 - val_loss: 10.6530 - val_accuracy: 0.0000e+00\n",
      "Epoch 274/1000\n",
      "1981/1981 [==============================] - 2s 866us/step - loss: 1.1646 - accuracy: 0.0010 - val_loss: 11.1291 - val_accuracy: 0.0000e+00\n",
      "Epoch 275/1000\n",
      "1981/1981 [==============================] - 2s 848us/step - loss: 1.2492 - accuracy: 0.0010 - val_loss: 11.0689 - val_accuracy: 0.0000e+00\n",
      "Epoch 276/1000\n",
      "1981/1981 [==============================] - 2s 836us/step - loss: 1.1048 - accuracy: 0.0010 - val_loss: 11.0868 - val_accuracy: 0.0000e+00\n",
      "Epoch 277/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 1.1300 - accuracy: 0.0015 - val_loss: 10.6927 - val_accuracy: 0.0000e+00\n",
      "Epoch 278/1000\n",
      "1981/1981 [==============================] - 2s 848us/step - loss: 1.1181 - accuracy: 5.0480e-04 - val_loss: 10.8315 - val_accuracy: 0.0000e+00\n",
      "Epoch 279/1000\n",
      "1981/1981 [==============================] - 2s 848us/step - loss: 1.1503 - accuracy: 0.0020 - val_loss: 10.8696 - val_accuracy: 0.0000e+00\n",
      "Epoch 280/1000\n",
      "1981/1981 [==============================] - 2s 851us/step - loss: 1.1402 - accuracy: 0.0000e+00 - val_loss: 11.1722 - val_accuracy: 0.0000e+00\n",
      "Epoch 281/1000\n",
      "1981/1981 [==============================] - 2s 823us/step - loss: 1.1966 - accuracy: 0.0010 - val_loss: 11.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 282/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1.1167 - accuracy: 0.0015 - val_loss: 11.4028 - val_accuracy: 0.0000e+00\n",
      "Epoch 283/1000\n",
      "1981/1981 [==============================] - 2s 838us/step - loss: 1.1164 - accuracy: 0.0020 - val_loss: 11.5447 - val_accuracy: 0.0000e+00\n",
      "Epoch 284/1000\n",
      "1981/1981 [==============================] - 2s 994us/step - loss: 1.1217 - accuracy: 0.0015 - val_loss: 11.0908 - val_accuracy: 0.0000e+00\n",
      "Epoch 285/1000\n",
      "1981/1981 [==============================] - 2s 925us/step - loss: 1.0906 - accuracy: 0.0015 - val_loss: 10.9424 - val_accuracy: 0.0000e+00\n",
      "Epoch 286/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.0914 - accuracy: 0.0015 - val_loss: 10.9583 - val_accuracy: 0.0000e+00\n",
      "Epoch 287/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1.0148 - accuracy: 0.0010 - val_loss: 10.9240 - val_accuracy: 0.0000e+00\n",
      "Epoch 288/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1.0832 - accuracy: 0.0010 - val_loss: 10.9390 - val_accuracy: 0.0000e+00\n",
      "Epoch 289/1000\n",
      "1981/1981 [==============================] - 2s 882us/step - loss: 1.1138 - accuracy: 0.0015 - val_loss: 10.6415 - val_accuracy: 0.0000e+00\n",
      "Epoch 290/1000\n",
      "1981/1981 [==============================] - 2s 852us/step - loss: 1.1394 - accuracy: 0.0010 - val_loss: 11.1784 - val_accuracy: 0.0000e+00\n",
      "Epoch 291/1000\n",
      "1981/1981 [==============================] - 2s 887us/step - loss: 1.0218 - accuracy: 0.0010 - val_loss: 11.2345 - val_accuracy: 0.0000e+00\n",
      "Epoch 292/1000\n",
      "1981/1981 [==============================] - 2s 884us/step - loss: 1.1082 - accuracy: 0.0015 - val_loss: 10.9979 - val_accuracy: 0.0000e+00\n",
      "Epoch 293/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 1.0547 - accuracy: 0.0010 - val_loss: 10.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 294/1000\n",
      "1981/1981 [==============================] - 2s 821us/step - loss: 1.0141 - accuracy: 0.0015 - val_loss: 11.1958 - val_accuracy: 0.0000e+00\n",
      "Epoch 295/1000\n",
      "1981/1981 [==============================] - 2s 818us/step - loss: 1.0478 - accuracy: 0.0020 - val_loss: 10.8524 - val_accuracy: 0.0000e+00\n",
      "Epoch 296/1000\n",
      "1981/1981 [==============================] - 2s 817us/step - loss: 1.0686 - accuracy: 0.0010 - val_loss: 11.0511 - val_accuracy: 0.0000e+00\n",
      "Epoch 297/1000\n",
      "1981/1981 [==============================] - 2s 810us/step - loss: 0.9954 - accuracy: 0.0010 - val_loss: 10.8940 - val_accuracy: 0.0000e+00\n",
      "Epoch 298/1000\n",
      "1981/1981 [==============================] - 2s 825us/step - loss: 1.0800 - accuracy: 0.0015 - val_loss: 10.6933 - val_accuracy: 0.0000e+00\n",
      "Epoch 299/1000\n",
      "1981/1981 [==============================] - 2s 832us/step - loss: 1.0139 - accuracy: 0.0015 - val_loss: 11.1965 - val_accuracy: 0.0000e+00\n",
      "Epoch 300/1000\n",
      "1981/1981 [==============================] - 2s 840us/step - loss: 1.0084 - accuracy: 0.0015 - val_loss: 10.7558 - val_accuracy: 0.0000e+00\n",
      "Epoch 301/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 1.0471 - accuracy: 0.0010 - val_loss: 10.4686 - val_accuracy: 0.0000e+00\n",
      "Epoch 302/1000\n",
      "1981/1981 [==============================] - 2s 859us/step - loss: 1.0669 - accuracy: 0.0015 - val_loss: 11.4431 - val_accuracy: 0.0000e+00\n",
      "Epoch 303/1000\n",
      "1981/1981 [==============================] - 2s 860us/step - loss: 1.0307 - accuracy: 0.0015 - val_loss: 11.1464 - val_accuracy: 0.0000e+00\n",
      "Epoch 304/1000\n",
      "1981/1981 [==============================] - 2s 839us/step - loss: 1.1554 - accuracy: 0.0010 - val_loss: 10.7393 - val_accuracy: 0.0000e+00\n",
      "Epoch 305/1000\n",
      "1981/1981 [==============================] - 2s 814us/step - loss: 1.0959 - accuracy: 0.0010 - val_loss: 10.8614 - val_accuracy: 0.0000e+00\n",
      "Epoch 306/1000\n",
      "1981/1981 [==============================] - 2s 812us/step - loss: 0.9911 - accuracy: 0.0015 - val_loss: 11.2429 - val_accuracy: 0.0000e+00\n",
      "Epoch 307/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1.1211 - accuracy: 5.0480e-04 - val_loss: 11.1685 - val_accuracy: 0.0000e+00\n",
      "Epoch 308/1000\n",
      "1981/1981 [==============================] - 2s 869us/step - loss: 0.9880 - accuracy: 0.0010 - val_loss: 10.9651 - val_accuracy: 0.0000e+00\n",
      "Epoch 309/1000\n",
      "1981/1981 [==============================] - 2s 852us/step - loss: 0.9068 - accuracy: 0.0020 - val_loss: 10.9340 - val_accuracy: 0.0000e+00\n",
      "Epoch 310/1000\n",
      "1981/1981 [==============================] - 2s 842us/step - loss: 0.9763 - accuracy: 0.0015 - val_loss: 11.1672 - val_accuracy: 0.0000e+00\n",
      "Epoch 311/1000\n",
      "1981/1981 [==============================] - 2s 826us/step - loss: 0.9803 - accuracy: 0.0020 - val_loss: 11.0893 - val_accuracy: 0.0000e+00\n",
      "Epoch 312/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 0.9731 - accuracy: 0.0020 - val_loss: 10.9289 - val_accuracy: 0.0000e+00\n",
      "Epoch 313/1000\n",
      "1981/1981 [==============================] - 2s 843us/step - loss: 0.9954 - accuracy: 0.0015 - val_loss: 11.0211 - val_accuracy: 0.0000e+00\n",
      "Epoch 314/1000\n",
      "1981/1981 [==============================] - 2s 812us/step - loss: 0.9724 - accuracy: 0.0020 - val_loss: 10.9170 - val_accuracy: 0.0000e+00\n",
      "Epoch 315/1000\n",
      "1981/1981 [==============================] - 2s 809us/step - loss: 1.0608 - accuracy: 5.0480e-04 - val_loss: 10.6963 - val_accuracy: 0.0000e+00\n",
      "Epoch 316/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 0.9826 - accuracy: 0.0010 - val_loss: 11.0326 - val_accuracy: 0.0000e+00\n",
      "Epoch 317/1000\n",
      "1981/1981 [==============================] - 2s 812us/step - loss: 0.9739 - accuracy: 0.0015 - val_loss: 11.1727 - val_accuracy: 0.0000e+00\n",
      "Epoch 318/1000\n",
      "1981/1981 [==============================] - 2s 813us/step - loss: 0.9332 - accuracy: 0.0025 - val_loss: 10.6397 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 319/1000\n",
      "1981/1981 [==============================] - 2s 810us/step - loss: 0.9675 - accuracy: 0.0010 - val_loss: 11.2271 - val_accuracy: 0.0000e+00\n",
      "Epoch 320/1000\n",
      "1981/1981 [==============================] - 2s 810us/step - loss: 0.8891 - accuracy: 0.0015 - val_loss: 11.0074 - val_accuracy: 0.0000e+00\n",
      "Epoch 321/1000\n",
      "1981/1981 [==============================] - 2s 809us/step - loss: 0.9514 - accuracy: 0.0010 - val_loss: 11.2857 - val_accuracy: 0.0000e+00\n",
      "Epoch 322/1000\n",
      "1981/1981 [==============================] - 2s 811us/step - loss: 0.9637 - accuracy: 0.0010 - val_loss: 10.9791 - val_accuracy: 0.0000e+00\n",
      "Epoch 323/1000\n",
      "1981/1981 [==============================] - 2s 821us/step - loss: 1.0061 - accuracy: 0.0015 - val_loss: 11.0436 - val_accuracy: 0.0000e+00\n",
      "Epoch 324/1000\n",
      "1981/1981 [==============================] - 2s 812us/step - loss: 1.0030 - accuracy: 0.0010 - val_loss: 11.3136 - val_accuracy: 0.0000e+00\n",
      "Epoch 325/1000\n",
      "1981/1981 [==============================] - 2s 813us/step - loss: 0.9965 - accuracy: 0.0015 - val_loss: 10.5543 - val_accuracy: 0.0000e+00\n",
      "Epoch 326/1000\n",
      "1981/1981 [==============================] - 2s 811us/step - loss: 0.9910 - accuracy: 0.0020 - val_loss: 11.1312 - val_accuracy: 0.0000e+00\n",
      "Epoch 327/1000\n",
      "1981/1981 [==============================] - 2s 813us/step - loss: 0.9162 - accuracy: 0.0010 - val_loss: 11.1793 - val_accuracy: 0.0000e+00\n",
      "Epoch 328/1000\n",
      "1981/1981 [==============================] - 2s 813us/step - loss: 0.8687 - accuracy: 0.0015 - val_loss: 10.6906 - val_accuracy: 0.0000e+00\n",
      "Epoch 329/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 0.9523 - accuracy: 0.0010 - val_loss: 10.8752 - val_accuracy: 0.0000e+00\n",
      "Epoch 330/1000\n",
      "1981/1981 [==============================] - 2s 811us/step - loss: 0.9123 - accuracy: 0.0020 - val_loss: 11.3145 - val_accuracy: 0.0000e+00\n",
      "Epoch 331/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 0.8515 - accuracy: 0.0015 - val_loss: 11.3526 - val_accuracy: 0.0000e+00\n",
      "Epoch 332/1000\n",
      "1981/1981 [==============================] - 2s 813us/step - loss: 0.8954 - accuracy: 0.0010 - val_loss: 11.1028 - val_accuracy: 0.0000e+00\n",
      "Epoch 333/1000\n",
      "1981/1981 [==============================] - 2s 809us/step - loss: 0.9414 - accuracy: 0.0015 - val_loss: 10.8909 - val_accuracy: 0.0000e+00\n",
      "Epoch 334/1000\n",
      "1981/1981 [==============================] - 2s 814us/step - loss: 0.9948 - accuracy: 0.0020 - val_loss: 11.2112 - val_accuracy: 0.0000e+00\n",
      "Epoch 335/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 0.9083 - accuracy: 0.0010 - val_loss: 10.8503 - val_accuracy: 0.0000e+00\n",
      "Epoch 336/1000\n",
      "1981/1981 [==============================] - 2s 812us/step - loss: 0.9095 - accuracy: 0.0015 - val_loss: 11.1222 - val_accuracy: 0.0000e+00\n",
      "Epoch 337/1000\n",
      "1981/1981 [==============================] - 2s 847us/step - loss: 0.9751 - accuracy: 0.0010 - val_loss: 10.9948 - val_accuracy: 0.0000e+00\n",
      "Epoch 338/1000\n",
      "1981/1981 [==============================] - 2s 824us/step - loss: 0.8534 - accuracy: 5.0480e-04 - val_loss: 11.6825 - val_accuracy: 0.0000e+00\n",
      "Epoch 339/1000\n",
      "1981/1981 [==============================] - 2s 899us/step - loss: 0.9970 - accuracy: 0.0015 - val_loss: 11.7746 - val_accuracy: 0.0000e+00\n",
      "Epoch 340/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 0.8755 - accuracy: 0.0020 - val_loss: 11.1483 - val_accuracy: 0.0000e+00\n",
      "Epoch 341/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 0.8769 - accuracy: 0.0010 - val_loss: 11.2214 - val_accuracy: 0.0000e+00\n",
      "Epoch 342/1000\n",
      "1981/1981 [==============================] - 2s 934us/step - loss: 0.8500 - accuracy: 0.0010 - val_loss: 10.8125 - val_accuracy: 0.0000e+00\n",
      "Epoch 343/1000\n",
      "1981/1981 [==============================] - 2s 938us/step - loss: 0.8543 - accuracy: 0.0010 - val_loss: 10.8232 - val_accuracy: 0.0000e+00\n",
      "Epoch 344/1000\n",
      "1981/1981 [==============================] - 2s 912us/step - loss: 0.8472 - accuracy: 0.0010 - val_loss: 10.8163 - val_accuracy: 0.0000e+00\n",
      "Epoch 345/1000\n",
      "1981/1981 [==============================] - 2s 855us/step - loss: 0.8254 - accuracy: 0.0010 - val_loss: 10.9335 - val_accuracy: 0.0000e+00\n",
      "Epoch 346/1000\n",
      "1981/1981 [==============================] - 2s 818us/step - loss: 0.8488 - accuracy: 0.0020 - val_loss: 11.4447 - val_accuracy: 0.0000e+00\n",
      "Epoch 347/1000\n",
      "1981/1981 [==============================] - 2s 857us/step - loss: 0.9432 - accuracy: 5.0480e-04 - val_loss: 11.0760 - val_accuracy: 0.0000e+00\n",
      "Epoch 348/1000\n",
      "1981/1981 [==============================] - 2s 822us/step - loss: 0.8434 - accuracy: 0.0015 - val_loss: 10.8948 - val_accuracy: 0.0000e+00\n",
      "Epoch 349/1000\n",
      "1981/1981 [==============================] - 2s 843us/step - loss: 0.7809 - accuracy: 0.0020 - val_loss: 11.2137 - val_accuracy: 0.0000e+00\n",
      "Epoch 350/1000\n",
      "1981/1981 [==============================] - 2s 844us/step - loss: 0.8551 - accuracy: 0.0015 - val_loss: 11.0238 - val_accuracy: 0.0000e+00\n",
      "Epoch 351/1000\n",
      "1981/1981 [==============================] - 2s 861us/step - loss: 0.8477 - accuracy: 0.0015 - val_loss: 11.2491 - val_accuracy: 0.0000e+00\n",
      "Epoch 352/1000\n",
      "1981/1981 [==============================] - 2s 841us/step - loss: 0.8054 - accuracy: 0.0010 - val_loss: 11.0859 - val_accuracy: 0.0000e+00\n",
      "Epoch 353/1000\n",
      "1981/1981 [==============================] - 2s 852us/step - loss: 0.7478 - accuracy: 0.0015 - val_loss: 11.1606 - val_accuracy: 0.0000e+00\n",
      "Epoch 354/1000\n",
      "1981/1981 [==============================] - 2s 853us/step - loss: 0.7672 - accuracy: 0.0010 - val_loss: 10.9354 - val_accuracy: 0.0000e+00\n",
      "Epoch 355/1000\n",
      "1981/1981 [==============================] - 2s 837us/step - loss: 0.7396 - accuracy: 0.0020 - val_loss: 10.9785 - val_accuracy: 0.0000e+00\n",
      "Epoch 356/1000\n",
      "1981/1981 [==============================] - 2s 837us/step - loss: 0.8183 - accuracy: 0.0015 - val_loss: 10.8989 - val_accuracy: 0.0000e+00\n",
      "Epoch 357/1000\n",
      "1981/1981 [==============================] - 2s 856us/step - loss: 0.8263 - accuracy: 0.0025 - val_loss: 10.8581 - val_accuracy: 0.0000e+00\n",
      "Epoch 358/1000\n",
      "1981/1981 [==============================] - 2s 859us/step - loss: 0.7810 - accuracy: 0.0025 - val_loss: 11.0693 - val_accuracy: 0.0000e+00\n",
      "Epoch 359/1000\n",
      "1981/1981 [==============================] - 2s 849us/step - loss: 0.7870 - accuracy: 0.0015 - val_loss: 11.2126 - val_accuracy: 0.0000e+00\n",
      "Epoch 360/1000\n",
      "1981/1981 [==============================] - 2s 825us/step - loss: 0.7890 - accuracy: 0.0010 - val_loss: 10.8601 - val_accuracy: 0.0000e+00\n",
      "Epoch 361/1000\n",
      "1981/1981 [==============================] - 2s 817us/step - loss: 0.7498 - accuracy: 0.0020 - val_loss: 10.8218 - val_accuracy: 0.0000e+00\n",
      "Epoch 362/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 0.7263 - accuracy: 0.0020 - val_loss: 10.9115 - val_accuracy: 0.0000e+00\n",
      "Epoch 363/1000\n",
      "1981/1981 [==============================] - 2s 816us/step - loss: 0.7344 - accuracy: 0.0015 - val_loss: 11.2955 - val_accuracy: 0.0000e+00\n",
      "Epoch 364/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 0.7526 - accuracy: 0.0020 - val_loss: 10.8997 - val_accuracy: 0.0000e+00\n",
      "Epoch 365/1000\n",
      "1981/1981 [==============================] - 2s 852us/step - loss: 0.7438 - accuracy: 0.0010 - val_loss: 10.9917 - val_accuracy: 0.0000e+00\n",
      "Epoch 366/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 0.7699 - accuracy: 0.0010 - val_loss: 11.1545 - val_accuracy: 0.0000e+00\n",
      "Epoch 367/1000\n",
      "1981/1981 [==============================] - 2s 824us/step - loss: 0.7288 - accuracy: 0.0010 - val_loss: 10.7766 - val_accuracy: 0.0000e+00\n",
      "Epoch 368/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 0.9331 - accuracy: 0.0020 - val_loss: 10.9309 - val_accuracy: 0.0000e+00\n",
      "Epoch 369/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 0.8899 - accuracy: 0.0015 - val_loss: 10.6206 - val_accuracy: 0.0000e+00\n",
      "Epoch 370/1000\n",
      "1981/1981 [==============================] - 2s 855us/step - loss: 0.6931 - accuracy: 0.0020 - val_loss: 11.1809 - val_accuracy: 0.0000e+00\n",
      "Epoch 371/1000\n",
      "1981/1981 [==============================] - 2s 846us/step - loss: 0.7349 - accuracy: 0.0025 - val_loss: 11.2385 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 372/1000\n",
      "1981/1981 [==============================] - 2s 823us/step - loss: 0.6991 - accuracy: 0.0015 - val_loss: 10.9320 - val_accuracy: 0.0000e+00\n",
      "Epoch 373/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 0.7328 - accuracy: 0.0025 - val_loss: 10.8487 - val_accuracy: 0.0000e+00\n",
      "Epoch 374/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 0.7255 - accuracy: 0.0020 - val_loss: 10.9282 - val_accuracy: 0.0000e+00\n",
      "Epoch 375/1000\n",
      "1981/1981 [==============================] - 2s 819us/step - loss: 0.7623 - accuracy: 0.0015 - val_loss: 10.8079 - val_accuracy: 0.0000e+00\n",
      "Epoch 376/1000\n",
      "1981/1981 [==============================] - 2s 849us/step - loss: 0.7655 - accuracy: 0.0025 - val_loss: 10.9313 - val_accuracy: 0.0000e+00\n",
      "Epoch 377/1000\n",
      "1981/1981 [==============================] - 2s 878us/step - loss: 0.7092 - accuracy: 0.0010 - val_loss: 10.9935 - val_accuracy: 0.0000e+00\n",
      "Epoch 378/1000\n",
      "1981/1981 [==============================] - 2s 841us/step - loss: 0.6925 - accuracy: 0.0020 - val_loss: 10.9071 - val_accuracy: 0.0000e+00\n",
      "Epoch 379/1000\n",
      "1981/1981 [==============================] - 2s 816us/step - loss: 0.7125 - accuracy: 0.0010 - val_loss: 11.0535 - val_accuracy: 0.0000e+00\n",
      "Epoch 380/1000\n",
      "1981/1981 [==============================] - 2s 821us/step - loss: 0.8272 - accuracy: 0.0015 - val_loss: 11.2086 - val_accuracy: 0.0000e+00\n",
      "Epoch 381/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 0.7618 - accuracy: 0.0010 - val_loss: 11.2055 - val_accuracy: 0.0000e+00\n",
      "Epoch 382/1000\n",
      "1981/1981 [==============================] - 2s 813us/step - loss: 0.6651 - accuracy: 0.0010 - val_loss: 11.0892 - val_accuracy: 0.0000e+00\n",
      "Epoch 383/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 0.6884 - accuracy: 0.0010 - val_loss: 11.1868 - val_accuracy: 0.0000e+00\n",
      "Epoch 384/1000\n",
      "1981/1981 [==============================] - 2s 837us/step - loss: 0.7066 - accuracy: 0.0010 - val_loss: 10.6979 - val_accuracy: 0.0000e+00\n",
      "Epoch 385/1000\n",
      "1981/1981 [==============================] - 2s 848us/step - loss: 0.6853 - accuracy: 0.0010 - val_loss: 10.9660 - val_accuracy: 0.0000e+00\n",
      "Epoch 386/1000\n",
      "1981/1981 [==============================] - 2s 931us/step - loss: 0.6977 - accuracy: 0.0010 - val_loss: 11.0133 - val_accuracy: 0.0000e+00\n",
      "Epoch 387/1000\n",
      "1981/1981 [==============================] - 2s 955us/step - loss: 0.6908 - accuracy: 0.0015 - val_loss: 11.2467 - val_accuracy: 0.0000e+00\n",
      "Epoch 388/1000\n",
      "1981/1981 [==============================] - 2s 920us/step - loss: 0.6963 - accuracy: 0.0015 - val_loss: 11.2285 - val_accuracy: 0.0000e+00\n",
      "Epoch 389/1000\n",
      "1981/1981 [==============================] - 2s 897us/step - loss: 0.6766 - accuracy: 0.0010 - val_loss: 11.1046 - val_accuracy: 0.0000e+00\n",
      "Epoch 390/1000\n",
      "1981/1981 [==============================] - 2s 871us/step - loss: 0.6321 - accuracy: 0.0025 - val_loss: 11.0821 - val_accuracy: 0.0000e+00\n",
      "Epoch 391/1000\n",
      "1981/1981 [==============================] - 2s 844us/step - loss: 0.6209 - accuracy: 0.0010 - val_loss: 10.9554 - val_accuracy: 0.0000e+00\n",
      "Epoch 392/1000\n",
      "1981/1981 [==============================] - 2s 882us/step - loss: 0.6593 - accuracy: 0.0015 - val_loss: 11.0505 - val_accuracy: 0.0000e+00\n",
      "Epoch 393/1000\n",
      "1981/1981 [==============================] - 2s 859us/step - loss: 0.6812 - accuracy: 0.0020 - val_loss: 10.8390 - val_accuracy: 0.0000e+00\n",
      "Epoch 394/1000\n",
      "1981/1981 [==============================] - 2s 856us/step - loss: 0.6379 - accuracy: 0.0015 - val_loss: 11.2080 - val_accuracy: 0.0000e+00\n",
      "Epoch 395/1000\n",
      "1981/1981 [==============================] - 2s 911us/step - loss: 0.6269 - accuracy: 0.0025 - val_loss: 11.1671 - val_accuracy: 0.0000e+00\n",
      "Epoch 396/1000\n",
      "1981/1981 [==============================] - 2s 867us/step - loss: 0.6471 - accuracy: 0.0020 - val_loss: 10.9700 - val_accuracy: 0.0000e+00\n",
      "Epoch 397/1000\n",
      "1981/1981 [==============================] - 2s 943us/step - loss: 0.6158 - accuracy: 0.0020 - val_loss: 10.6368 - val_accuracy: 0.0000e+00\n",
      "Epoch 398/1000\n",
      "1981/1981 [==============================] - 2s 876us/step - loss: 0.6188 - accuracy: 0.0010 - val_loss: 10.8330 - val_accuracy: 0.0000e+00\n",
      "Epoch 399/1000\n",
      "1981/1981 [==============================] - 2s 870us/step - loss: 0.6542 - accuracy: 0.0010 - val_loss: 10.8880 - val_accuracy: 0.0000e+00\n",
      "Epoch 400/1000\n",
      "1981/1981 [==============================] - 2s 845us/step - loss: 0.6171 - accuracy: 0.0025 - val_loss: 11.4464 - val_accuracy: 0.0000e+00\n",
      "Epoch 401/1000\n",
      "1981/1981 [==============================] - 2s 892us/step - loss: 0.7301 - accuracy: 0.0020 - val_loss: 10.9610 - val_accuracy: 0.0000e+00\n",
      "Epoch 402/1000\n",
      "1981/1981 [==============================] - 2s 855us/step - loss: 0.6096 - accuracy: 0.0020 - val_loss: 10.5040 - val_accuracy: 0.0000e+00\n",
      "Epoch 403/1000\n",
      "1981/1981 [==============================] - 2s 899us/step - loss: 0.5889 - accuracy: 0.0015 - val_loss: 10.8455 - val_accuracy: 0.0000e+00\n",
      "Epoch 404/1000\n",
      "1981/1981 [==============================] - 2s 925us/step - loss: 0.6333 - accuracy: 0.0025 - val_loss: 10.7345 - val_accuracy: 0.0000e+00\n",
      "Epoch 405/1000\n",
      "1981/1981 [==============================] - 2s 901us/step - loss: 0.5971 - accuracy: 0.0015 - val_loss: 10.8117 - val_accuracy: 0.0000e+00\n",
      "Epoch 406/1000\n",
      "1981/1981 [==============================] - 2s 866us/step - loss: 0.7102 - accuracy: 0.0020 - val_loss: 10.8316 - val_accuracy: 0.0000e+00\n",
      "Epoch 407/1000\n",
      "1981/1981 [==============================] - 2s 915us/step - loss: 0.6072 - accuracy: 0.0015 - val_loss: 10.9439 - val_accuracy: 0.0000e+00\n",
      "Epoch 00407: early stopping\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train_loss(buildTrendModel_3stacks_true_value, 7, stock_with_absolute, label_value_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "# result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "#     \"./LSTM_RESULT/LSTM_7d_sequence_1stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 287us/step\n",
      "test loss, test acc: [13.217251522200447, 0.0]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 3.635553612840713\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=7)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test, batch_size=7)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>true_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.160936</td>\n",
       "      <td>7.669998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.270536</td>\n",
       "      <td>8.880005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.695413</td>\n",
       "      <td>0.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.167328</td>\n",
       "      <td>4.490005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.604816</td>\n",
       "      <td>0.379990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>-1.739640</td>\n",
       "      <td>-2.190002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-0.484698</td>\n",
       "      <td>-0.710022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>-0.848922</td>\n",
       "      <td>-2.059998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>-1.567517</td>\n",
       "      <td>0.240021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>-0.137836</td>\n",
       "      <td>-2.510010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  true_value\n",
       "0     -0.160936    7.669998\n",
       "1      4.270536    8.880005\n",
       "2      0.695413    0.199997\n",
       "3     -0.167328    4.490005\n",
       "4      0.604816    0.379990\n",
       "..          ...         ...\n",
       "240   -1.739640   -2.190002\n",
       "241   -0.484698   -0.710022\n",
       "242   -0.848922   -2.059998\n",
       "243   -1.567517    0.240021\n",
       "244   -0.137836   -2.510010\n",
       "\n",
       "[245 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictFrame = pd.DataFrame({'prediction': predictions.reshape(X_test.shape[0]), 'true_value': y_test.reshape(X_test.shape[0])})\n",
    "predictFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('1-day Delta ($)', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"glorot_uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"glorot_uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"glorot_uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1981 samples, validate on 217 samples\n",
      "Epoch 1/1000\n",
      "1981/1981 [==============================] - 3s 1ms/step - loss: 535.9247 - accuracy: 5.0480e-04 - val_loss: 2559.0794 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "1981/1981 [==============================] - 2s 867us/step - loss: 1416.3816 - accuracy: 0.0000e+00 - val_loss: 4129.9571 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "1981/1981 [==============================] - 2s 874us/step - loss: 1301.8579 - accuracy: 0.0000e+00 - val_loss: 4297.8914 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "1981/1981 [==============================] - 2s 855us/step - loss: 1292.6494 - accuracy: 0.0000e+00 - val_loss: 4451.4272 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "1981/1981 [==============================] - 2s 909us/step - loss: 1291.2440 - accuracy: 0.0000e+00 - val_loss: 4589.6198 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "1981/1981 [==============================] - 2s 917us/step - loss: 1268.0676 - accuracy: 0.0000e+00 - val_loss: 4558.2216 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "1981/1981 [==============================] - 2s 944us/step - loss: 1277.2700 - accuracy: 0.0000e+00 - val_loss: 4645.3903 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "1981/1981 [==============================] - 2s 982us/step - loss: 1281.4772 - accuracy: 0.0000e+00 - val_loss: 4597.2900 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "1981/1981 [==============================] - 2s 857us/step - loss: 1296.7598 - accuracy: 0.0000e+00 - val_loss: 4635.5650 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "1981/1981 [==============================] - 2s 883us/step - loss: 1289.9564 - accuracy: 5.0480e-04 - val_loss: 4665.9293 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "1981/1981 [==============================] - 2s 819us/step - loss: 1268.5363 - accuracy: 0.0000e+00 - val_loss: 4692.4361 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1286.9024 - accuracy: 0.0000e+00 - val_loss: 4647.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "1981/1981 [==============================] - 2s 836us/step - loss: 1298.1273 - accuracy: 0.0000e+00 - val_loss: 4742.5186 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "1981/1981 [==============================] - 2s 824us/step - loss: 1283.6279 - accuracy: 0.0000e+00 - val_loss: 4792.7990 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "1981/1981 [==============================] - 2s 839us/step - loss: 1290.0904 - accuracy: 0.0000e+00 - val_loss: 4750.7105 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "1981/1981 [==============================] - 2s 818us/step - loss: 1287.9596 - accuracy: 0.0000e+00 - val_loss: 4749.0540 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 1299.0623 - accuracy: 0.0000e+00 - val_loss: 4772.0243 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1312.4443 - accuracy: 0.0000e+00 - val_loss: 4818.2652 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 1301.3096 - accuracy: 0.0000e+00 - val_loss: 4801.1429 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "1981/1981 [==============================] - 2s 813us/step - loss: 1292.6315 - accuracy: 5.0480e-04 - val_loss: 4850.4527 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1297.9486 - accuracy: 0.0000e+00 - val_loss: 4867.5045 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "1981/1981 [==============================] - 2s 878us/step - loss: 1320.4618 - accuracy: 0.0000e+00 - val_loss: 4902.9105 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "1981/1981 [==============================] - 2s 944us/step - loss: 1298.6033 - accuracy: 0.0000e+00 - val_loss: 4921.9914 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "1981/1981 [==============================] - 2s 976us/step - loss: 1298.6170 - accuracy: 0.0000e+00 - val_loss: 4902.0181 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/1000\n",
      "1981/1981 [==============================] - 2s 891us/step - loss: 1317.9870 - accuracy: 5.0480e-04 - val_loss: 4971.1856 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "1981/1981 [==============================] - 2s 808us/step - loss: 1312.0479 - accuracy: 0.0000e+00 - val_loss: 4965.1658 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/1000\n",
      "1981/1981 [==============================] - 2s 849us/step - loss: 1321.5814 - accuracy: 0.0000e+00 - val_loss: 5039.1908 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "1981/1981 [==============================] - 2s 888us/step - loss: 1320.8747 - accuracy: 0.0000e+00 - val_loss: 5075.3751 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "1981/1981 [==============================] - 2s 910us/step - loss: 1320.6881 - accuracy: 0.0000e+00 - val_loss: 5101.7539 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "1981/1981 [==============================] - 2s 964us/step - loss: 1317.3186 - accuracy: 0.0000e+00 - val_loss: 5167.0023 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/1000\n",
      "1981/1981 [==============================] - 2s 873us/step - loss: 1322.9741 - accuracy: 0.0000e+00 - val_loss: 5161.1086 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/1000\n",
      "1981/1981 [==============================] - 2s 876us/step - loss: 1313.1377 - accuracy: 5.0480e-04 - val_loss: 5189.0994 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "1981/1981 [==============================] - 2s 875us/step - loss: 1336.0124 - accuracy: 0.0000e+00 - val_loss: 5335.8961 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "1981/1981 [==============================] - 2s 911us/step - loss: 1330.2122 - accuracy: 0.0000e+00 - val_loss: 5407.2621 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "1981/1981 [==============================] - 2s 908us/step - loss: 1379.2272 - accuracy: 0.0000e+00 - val_loss: 5631.6530 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/1000\n",
      "1981/1981 [==============================] - 2s 853us/step - loss: 1382.8146 - accuracy: 5.0480e-04 - val_loss: 5925.2416 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/1000\n",
      "1981/1981 [==============================] - 2s 932us/step - loss: 1330.4512 - accuracy: 0.0000e+00 - val_loss: 5743.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1359.5634 - accuracy: 5.0480e-04 - val_loss: 5923.5531 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "1981/1981 [==============================] - 2s 988us/step - loss: 1349.3564 - accuracy: 0.0000e+00 - val_loss: 5958.4051 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1361.0130 - accuracy: 0.0000e+00 - val_loss: 5959.5936 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "1981/1981 [==============================] - 2s 931us/step - loss: 1345.7996 - accuracy: 0.0000e+00 - val_loss: 6033.5292 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "1981/1981 [==============================] - 2s 870us/step - loss: 1335.9379 - accuracy: 0.0000e+00 - val_loss: 6027.3775 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "1981/1981 [==============================] - 2s 910us/step - loss: 1346.3911 - accuracy: 0.0000e+00 - val_loss: 6069.3989 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "1981/1981 [==============================] - 2s 906us/step - loss: 1340.0492 - accuracy: 0.0000e+00 - val_loss: 6046.7169 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "1981/1981 [==============================] - 2s 954us/step - loss: 1336.7153 - accuracy: 0.0000e+00 - val_loss: 5990.8876 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "1981/1981 [==============================] - 2s 944us/step - loss: 1356.3344 - accuracy: 5.0480e-04 - val_loss: 6040.6217 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/1000\n",
      "1981/1981 [==============================] - 2s 892us/step - loss: 1366.9144 - accuracy: 0.0000e+00 - val_loss: 6053.7337 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/1000\n",
      "1981/1981 [==============================] - 2s 903us/step - loss: 1349.9556 - accuracy: 5.0480e-04 - val_loss: 6061.8498 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "1981/1981 [==============================] - 2s 978us/step - loss: 1380.8620 - accuracy: 0.0000e+00 - val_loss: 6125.2717 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "1981/1981 [==============================] - 2s 995us/step - loss: 1343.0171 - accuracy: 0.0000e+00 - val_loss: 5623.4965 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 916us/step - loss: 789.9980 - accuracy: 0.0000e+00 - val_loss: 3754.3876 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "1981/1981 [==============================] - 2s 900us/step - loss: 1027.2110 - accuracy: 0.0000e+00 - val_loss: 2861.1110 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "1981/1981 [==============================] - 2s 920us/step - loss: 1537.5587 - accuracy: 0.0000e+00 - val_loss: 4865.7699 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/1000\n",
      "1981/1981 [==============================] - 2s 899us/step - loss: 1447.5385 - accuracy: 0.0000e+00 - val_loss: 5721.1410 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "1981/1981 [==============================] - 2s 923us/step - loss: 1372.7478 - accuracy: 0.0000e+00 - val_loss: 6130.5546 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/1000\n",
      "1981/1981 [==============================] - 2s 937us/step - loss: 1388.0118 - accuracy: 0.0000e+00 - val_loss: 6250.8326 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/1000\n",
      "1981/1981 [==============================] - 2s 891us/step - loss: 1359.5497 - accuracy: 0.0000e+00 - val_loss: 6252.4236 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/1000\n",
      "1981/1981 [==============================] - 2s 882us/step - loss: 1369.3587 - accuracy: 0.0000e+00 - val_loss: 6289.1773 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/1000\n",
      "1981/1981 [==============================] - 2s 896us/step - loss: 1379.3204 - accuracy: 0.0000e+00 - val_loss: 6182.4586 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/1000\n",
      "1981/1981 [==============================] - 2s 881us/step - loss: 1175.7657 - accuracy: 0.0000e+00 - val_loss: 2065.3634 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/1000\n",
      "1981/1981 [==============================] - 2s 885us/step - loss: 1234.1285 - accuracy: 0.0000e+00 - val_loss: 3675.0489 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/1000\n",
      "1981/1981 [==============================] - 2s 943us/step - loss: 1188.9697 - accuracy: 0.0000e+00 - val_loss: 4425.4293 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "1981/1981 [==============================] - 2s 960us/step - loss: 1330.7678 - accuracy: 0.0000e+00 - val_loss: 4619.5375 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "1981/1981 [==============================] - 2s 908us/step - loss: 1388.4787 - accuracy: 0.0000e+00 - val_loss: 4736.6501 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "1981/1981 [==============================] - 2s 817us/step - loss: 1400.6580 - accuracy: 0.0000e+00 - val_loss: 5740.2569 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "1981/1981 [==============================] - 2s 807us/step - loss: 1420.1349 - accuracy: 5.0480e-04 - val_loss: 6243.2880 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/1000\n",
      "1981/1981 [==============================] - 2s 806us/step - loss: 1398.9261 - accuracy: 0.0000e+00 - val_loss: 6484.9958 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "1981/1981 [==============================] - 2s 805us/step - loss: 1377.4471 - accuracy: 0.0000e+00 - val_loss: 6559.4539 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "1981/1981 [==============================] - 2s 826us/step - loss: 1390.3468 - accuracy: 0.0000e+00 - val_loss: 6550.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "1981/1981 [==============================] - 2s 945us/step - loss: 1385.1360 - accuracy: 0.0000e+00 - val_loss: 6513.9790 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/1000\n",
      "1981/1981 [==============================] - 2s 920us/step - loss: 1375.6411 - accuracy: 5.0480e-04 - val_loss: 6509.4093 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/1000\n",
      "1981/1981 [==============================] - 2s 914us/step - loss: 1391.5335 - accuracy: 0.0000e+00 - val_loss: 6577.7435 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/1000\n",
      "1981/1981 [==============================] - 2s 934us/step - loss: 1374.6418 - accuracy: 5.0480e-04 - val_loss: 6611.9007 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/1000\n",
      "1981/1981 [==============================] - 2s 871us/step - loss: 1363.9428 - accuracy: 0.0000e+00 - val_loss: 6594.3526 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/1000\n",
      "1981/1981 [==============================] - 2s 932us/step - loss: 1385.4213 - accuracy: 0.0000e+00 - val_loss: 6671.3846 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/1000\n",
      "1981/1981 [==============================] - 2s 921us/step - loss: 1359.8807 - accuracy: 0.0000e+00 - val_loss: 6552.9717 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/1000\n",
      "1981/1981 [==============================] - 2s 862us/step - loss: 1382.8832 - accuracy: 0.0000e+00 - val_loss: 6640.7081 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 1379.3701 - accuracy: 0.0000e+00 - val_loss: 6630.4741 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/1000\n",
      "1981/1981 [==============================] - 2s 840us/step - loss: 1378.9692 - accuracy: 5.0480e-04 - val_loss: 6678.0988 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/1000\n",
      "1981/1981 [==============================] - 2s 842us/step - loss: 1375.4305 - accuracy: 5.0480e-04 - val_loss: 6695.4743 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/1000\n",
      "1981/1981 [==============================] - 2s 951us/step - loss: 1371.7890 - accuracy: 0.0000e+00 - val_loss: 6681.6149 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/1000\n",
      "1981/1981 [==============================] - 2s 869us/step - loss: 1364.8633 - accuracy: 0.0000e+00 - val_loss: 6721.8284 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/1000\n",
      "1981/1981 [==============================] - 2s 910us/step - loss: 1401.3828 - accuracy: 0.0000e+00 - val_loss: 6641.2530 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/1000\n",
      "1981/1981 [==============================] - 2s 903us/step - loss: 1394.3046 - accuracy: 5.0480e-04 - val_loss: 6755.3463 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/1000\n",
      "1981/1981 [==============================] - 2s 888us/step - loss: 1382.4136 - accuracy: 0.0000e+00 - val_loss: 6634.0809 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/1000\n",
      "1981/1981 [==============================] - 2s 879us/step - loss: 1368.9006 - accuracy: 0.0000e+00 - val_loss: 6769.2214 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/1000\n",
      "1981/1981 [==============================] - 2s 885us/step - loss: 1349.5455 - accuracy: 0.0000e+00 - val_loss: 6831.6916 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/1000\n",
      "1981/1981 [==============================] - 2s 865us/step - loss: 1386.3098 - accuracy: 0.0000e+00 - val_loss: 6759.1539 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/1000\n",
      "1981/1981 [==============================] - 2s 887us/step - loss: 1361.3994 - accuracy: 0.0000e+00 - val_loss: 6756.9250 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/1000\n",
      "1981/1981 [==============================] - 2s 922us/step - loss: 1389.6567 - accuracy: 0.0000e+00 - val_loss: 6813.4909 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/1000\n",
      "1981/1981 [==============================] - 2s 872us/step - loss: 1361.8514 - accuracy: 0.0000e+00 - val_loss: 6350.2418 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/1000\n",
      "1981/1981 [==============================] - 2s 860us/step - loss: 1264.6211 - accuracy: 0.0000e+00 - val_loss: 4124.6782 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/1000\n",
      "1981/1981 [==============================] - 2s 883us/step - loss: 1337.1328 - accuracy: 0.0000e+00 - val_loss: 4027.0615 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "1981/1981 [==============================] - 2s 871us/step - loss: 1457.6736 - accuracy: 5.0480e-04 - val_loss: 5537.3474 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "1981/1981 [==============================] - 2s 858us/step - loss: 1376.6600 - accuracy: 0.0000e+00 - val_loss: 5827.4085 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1363.5684 - accuracy: 0.0000e+00 - val_loss: 5989.3529 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/1000\n",
      "1981/1981 [==============================] - 2s 860us/step - loss: 1380.4453 - accuracy: 0.0000e+00 - val_loss: 6060.3307 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "1981/1981 [==============================] - 2s 877us/step - loss: 1381.1750 - accuracy: 0.0000e+00 - val_loss: 6162.9851 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/1000\n",
      "1981/1981 [==============================] - 2s 851us/step - loss: 1342.6471 - accuracy: 0.0000e+00 - val_loss: 6293.9502 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/1000\n",
      "1981/1981 [==============================] - 2s 824us/step - loss: 1372.4254 - accuracy: 0.0000e+00 - val_loss: 6281.7478 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 857us/step - loss: 1377.4381 - accuracy: 0.0000e+00 - val_loss: 6231.6816 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/1000\n",
      "1981/1981 [==============================] - 2s 877us/step - loss: 1384.2681 - accuracy: 0.0000e+00 - val_loss: 6436.6083 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/1000\n",
      "1981/1981 [==============================] - 2s 909us/step - loss: 1371.5700 - accuracy: 0.0000e+00 - val_loss: 6359.8031 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/1000\n",
      "1981/1981 [==============================] - 2s 1ms/step - loss: 1366.2757 - accuracy: 0.0000e+00 - val_loss: 6457.8114 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/1000\n",
      "1981/1981 [==============================] - 2s 969us/step - loss: 1372.5997 - accuracy: 0.0000e+00 - val_loss: 6468.1946 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/1000\n",
      "1981/1981 [==============================] - 2s 869us/step - loss: 1366.0313 - accuracy: 0.0000e+00 - val_loss: 6461.4301 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/1000\n",
      "1981/1981 [==============================] - 2s 869us/step - loss: 1367.3490 - accuracy: 0.0000e+00 - val_loss: 6358.1004 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/1000\n",
      "1981/1981 [==============================] - 2s 918us/step - loss: 1367.7012 - accuracy: 5.0480e-04 - val_loss: 6528.2602 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/1000\n",
      "1981/1981 [==============================] - 2s 933us/step - loss: 1356.7615 - accuracy: 0.0000e+00 - val_loss: 6481.8758 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/1000\n",
      "1981/1981 [==============================] - 2s 917us/step - loss: 1366.9152 - accuracy: 0.0000e+00 - val_loss: 6556.0207 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/1000\n",
      "1981/1981 [==============================] - 2s 852us/step - loss: 1379.0638 - accuracy: 0.0000e+00 - val_loss: 6630.3985 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/1000\n",
      "1981/1981 [==============================] - 2s 822us/step - loss: 1365.7271 - accuracy: 0.0000e+00 - val_loss: 6573.5167 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/1000\n",
      "1981/1981 [==============================] - 2s 837us/step - loss: 1370.6207 - accuracy: 0.0000e+00 - val_loss: 6540.8698 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1369.6598 - accuracy: 0.0000e+00 - val_loss: 6655.5591 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 1364.6595 - accuracy: 0.0000e+00 - val_loss: 6630.8613 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/1000\n",
      "1981/1981 [==============================] - 2s 845us/step - loss: 1378.7399 - accuracy: 0.0000e+00 - val_loss: 6601.7577 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/1000\n",
      "1981/1981 [==============================] - 2s 875us/step - loss: 1388.0710 - accuracy: 0.0000e+00 - val_loss: 6681.7053 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "1981/1981 [==============================] - 2s 848us/step - loss: 1363.1554 - accuracy: 0.0000e+00 - val_loss: 6597.2820 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/1000\n",
      "1981/1981 [==============================] - 2s 842us/step - loss: 1378.1560 - accuracy: 0.0000e+00 - val_loss: 6631.8284 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/1000\n",
      "1981/1981 [==============================] - 2s 821us/step - loss: 1366.4444 - accuracy: 0.0000e+00 - val_loss: 6657.4666 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 1357.0584 - accuracy: 0.0000e+00 - val_loss: 6598.8416 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/1000\n",
      "1981/1981 [==============================] - 2s 818us/step - loss: 1371.4665 - accuracy: 0.0000e+00 - val_loss: 6663.7073 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "1981/1981 [==============================] - 2s 821us/step - loss: 1364.1455 - accuracy: 0.0000e+00 - val_loss: 6687.4262 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "1981/1981 [==============================] - 2s 812us/step - loss: 1371.2224 - accuracy: 0.0000e+00 - val_loss: 6650.0363 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1376.9419 - accuracy: 0.0000e+00 - val_loss: 6715.2075 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "1981/1981 [==============================] - 2s 836us/step - loss: 1386.5932 - accuracy: 0.0000e+00 - val_loss: 6601.3258 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "1981/1981 [==============================] - 2s 824us/step - loss: 1379.0762 - accuracy: 0.0000e+00 - val_loss: 6750.6115 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "1981/1981 [==============================] - 2s 815us/step - loss: 1373.3594 - accuracy: 0.0000e+00 - val_loss: 6680.1388 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "1981/1981 [==============================] - 2s 825us/step - loss: 1366.5924 - accuracy: 5.0480e-04 - val_loss: 6817.7162 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "1981/1981 [==============================] - 2s 824us/step - loss: 1369.4444 - accuracy: 0.0000e+00 - val_loss: 6814.4329 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "1981/1981 [==============================] - 2s 846us/step - loss: 1355.9875 - accuracy: 0.0000e+00 - val_loss: 6747.2863 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "1981/1981 [==============================] - 2s 850us/step - loss: 1378.1424 - accuracy: 0.0000e+00 - val_loss: 6700.7709 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "1981/1981 [==============================] - 2s 864us/step - loss: 1373.9891 - accuracy: 0.0000e+00 - val_loss: 6718.8553 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "1981/1981 [==============================] - 2s 843us/step - loss: 1382.5743 - accuracy: 0.0000e+00 - val_loss: 6821.2594 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/1000\n",
      "1981/1981 [==============================] - 2s 872us/step - loss: 1368.2970 - accuracy: 0.0000e+00 - val_loss: 6769.2103 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "1981/1981 [==============================] - 2s 856us/step - loss: 1379.8238 - accuracy: 0.0000e+00 - val_loss: 6819.0585 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/1000\n",
      "1981/1981 [==============================] - 2s 822us/step - loss: 1363.0124 - accuracy: 0.0000e+00 - val_loss: 6774.6037 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "1981/1981 [==============================] - 2s 827us/step - loss: 1374.2944 - accuracy: 0.0000e+00 - val_loss: 6841.6494 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/1000\n",
      "1981/1981 [==============================] - 2s 812us/step - loss: 1366.5832 - accuracy: 0.0000e+00 - val_loss: 6881.2777 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "1981/1981 [==============================] - 2s 848us/step - loss: 1376.4231 - accuracy: 5.0480e-04 - val_loss: 6880.8524 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/1000\n",
      "1981/1981 [==============================] - 2s 841us/step - loss: 1380.3805 - accuracy: 0.0000e+00 - val_loss: 6792.6079 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1386.5278 - accuracy: 0.0000e+00 - val_loss: 6943.2228 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "1981/1981 [==============================] - 2s 843us/step - loss: 1354.5863 - accuracy: 0.0000e+00 - val_loss: 6856.3417 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 1379.9731 - accuracy: 0.0000e+00 - val_loss: 6944.8949 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "1981/1981 [==============================] - 2s 839us/step - loss: 1366.5936 - accuracy: 0.0000e+00 - val_loss: 6858.9505 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "1981/1981 [==============================] - 2s 846us/step - loss: 1367.2322 - accuracy: 0.0000e+00 - val_loss: 6936.7024 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/1000\n",
      "1981/1981 [==============================] - 2s 842us/step - loss: 1380.6500 - accuracy: 0.0000e+00 - val_loss: 7025.4905 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1369.1820 - accuracy: 0.0000e+00 - val_loss: 6973.5079 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "1981/1981 [==============================] - 2s 825us/step - loss: 1373.1456 - accuracy: 0.0000e+00 - val_loss: 6980.6597 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "1981/1981 [==============================] - 2s 844us/step - loss: 1392.4853 - accuracy: 0.0000e+00 - val_loss: 7000.7999 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 892us/step - loss: 1369.7658 - accuracy: 0.0000e+00 - val_loss: 7003.4158 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "1981/1981 [==============================] - 2s 898us/step - loss: 1372.0584 - accuracy: 0.0000e+00 - val_loss: 6988.9056 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/1000\n",
      "1981/1981 [==============================] - 2s 872us/step - loss: 1381.2948 - accuracy: 0.0000e+00 - val_loss: 6969.7885 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/1000\n",
      "1981/1981 [==============================] - 2s 818us/step - loss: 1373.0139 - accuracy: 0.0000e+00 - val_loss: 7001.5378 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "1981/1981 [==============================] - 2s 813us/step - loss: 1371.0331 - accuracy: 0.0000e+00 - val_loss: 7092.4168 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "1981/1981 [==============================] - 2s 807us/step - loss: 1384.9837 - accuracy: 0.0000e+00 - val_loss: 6990.6120 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "1981/1981 [==============================] - 2s 809us/step - loss: 1385.3942 - accuracy: 0.0000e+00 - val_loss: 7098.6971 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "1981/1981 [==============================] - 2s 814us/step - loss: 1387.6811 - accuracy: 5.0480e-04 - val_loss: 7142.2523 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "1981/1981 [==============================] - 2s 817us/step - loss: 1373.9828 - accuracy: 0.0000e+00 - val_loss: 7197.7079 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 1364.0855 - accuracy: 0.0000e+00 - val_loss: 7216.9597 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "1981/1981 [==============================] - 2s 866us/step - loss: 1346.6936 - accuracy: 0.0000e+00 - val_loss: 7128.7066 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "1981/1981 [==============================] - 2s 859us/step - loss: 1355.7487 - accuracy: 0.0000e+00 - val_loss: 7151.6700 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "1981/1981 [==============================] - 2s 855us/step - loss: 1375.1190 - accuracy: 5.0480e-04 - val_loss: 7215.6718 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "1981/1981 [==============================] - 2s 860us/step - loss: 1359.9101 - accuracy: 0.0000e+00 - val_loss: 7181.6693 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/1000\n",
      "1981/1981 [==============================] - 2s 859us/step - loss: 1384.4299 - accuracy: 0.0000e+00 - val_loss: 7309.9226 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "1981/1981 [==============================] - 2s 897us/step - loss: 1360.0822 - accuracy: 0.0000e+00 - val_loss: 7198.8682 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "1981/1981 [==============================] - 2s 855us/step - loss: 1364.1374 - accuracy: 0.0000e+00 - val_loss: 7255.2000 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "1981/1981 [==============================] - 2s 838us/step - loss: 1376.5583 - accuracy: 0.0000e+00 - val_loss: 7288.3230 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/1000\n",
      "1981/1981 [==============================] - 2s 837us/step - loss: 1379.0036 - accuracy: 0.0000e+00 - val_loss: 7194.7744 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1355.9779 - accuracy: 0.0000e+00 - val_loss: 6312.7537 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "1981/1981 [==============================] - 2s 838us/step - loss: 1400.5797 - accuracy: 0.0000e+00 - val_loss: 7058.4188 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "1981/1981 [==============================] - 2s 847us/step - loss: 1380.1092 - accuracy: 0.0000e+00 - val_loss: 7259.3667 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/1000\n",
      "1981/1981 [==============================] - 2s 832us/step - loss: 1379.5373 - accuracy: 0.0000e+00 - val_loss: 7186.0099 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1376.5579 - accuracy: 0.0000e+00 - val_loss: 7244.5227 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1375.0139 - accuracy: 5.0480e-04 - val_loss: 7279.7735 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/1000\n",
      "1981/1981 [==============================] - 2s 823us/step - loss: 1391.4403 - accuracy: 0.0000e+00 - val_loss: 7305.9559 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "1981/1981 [==============================] - 2s 838us/step - loss: 1356.9818 - accuracy: 0.0000e+00 - val_loss: 7242.4624 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "1981/1981 [==============================] - 2s 859us/step - loss: 1377.0713 - accuracy: 0.0000e+00 - val_loss: 7223.0816 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1359.1683 - accuracy: 0.0000e+00 - val_loss: 7216.2119 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "1981/1981 [==============================] - 2s 843us/step - loss: 1374.9353 - accuracy: 0.0000e+00 - val_loss: 7171.2393 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1374.4586 - accuracy: 0.0000e+00 - val_loss: 7102.1657 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/1000\n",
      "1981/1981 [==============================] - 2s 827us/step - loss: 1397.0306 - accuracy: 0.0000e+00 - val_loss: 7188.9803 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 1365.1687 - accuracy: 0.0000e+00 - val_loss: 7168.6561 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/1000\n",
      "1981/1981 [==============================] - 2s 874us/step - loss: 1384.4222 - accuracy: 0.0000e+00 - val_loss: 7289.7521 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/1000\n",
      "1981/1981 [==============================] - 2s 855us/step - loss: 1373.7741 - accuracy: 0.0000e+00 - val_loss: 7288.0093 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/1000\n",
      "1981/1981 [==============================] - 2s 853us/step - loss: 1352.2192 - accuracy: 0.0000e+00 - val_loss: 7250.6829 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 1386.7273 - accuracy: 0.0000e+00 - val_loss: 7323.9050 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/1000\n",
      "1981/1981 [==============================] - 2s 822us/step - loss: 1364.7124 - accuracy: 5.0480e-04 - val_loss: 7178.8257 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/1000\n",
      "1981/1981 [==============================] - 2s 827us/step - loss: 1393.2628 - accuracy: 0.0000e+00 - val_loss: 7303.1056 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/1000\n",
      "1981/1981 [==============================] - 2s 861us/step - loss: 1363.6853 - accuracy: 0.0000e+00 - val_loss: 7291.1284 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/1000\n",
      "1981/1981 [==============================] - 2s 867us/step - loss: 1346.1468 - accuracy: 0.0000e+00 - val_loss: 7162.6021 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1367.1678 - accuracy: 0.0000e+00 - val_loss: 7301.0929 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 1375.7366 - accuracy: 5.0480e-04 - val_loss: 7224.3030 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/1000\n",
      "1981/1981 [==============================] - 2s 838us/step - loss: 1357.3877 - accuracy: 0.0000e+00 - val_loss: 7252.4266 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/1000\n",
      "1981/1981 [==============================] - 2s 842us/step - loss: 1386.0026 - accuracy: 0.0000e+00 - val_loss: 7273.0879 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/1000\n",
      "1981/1981 [==============================] - 2s 827us/step - loss: 1358.9876 - accuracy: 0.0000e+00 - val_loss: 7276.3717 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/1000\n",
      "1981/1981 [==============================] - 2s 811us/step - loss: 1366.4330 - accuracy: 0.0000e+00 - val_loss: 7268.2771 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/1000\n",
      "1981/1981 [==============================] - 2s 819us/step - loss: 1376.3279 - accuracy: 0.0000e+00 - val_loss: 7318.8993 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/1000\n",
      "1981/1981 [==============================] - 2s 825us/step - loss: 1363.8292 - accuracy: 0.0000e+00 - val_loss: 7388.3419 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/1000\n",
      "1981/1981 [==============================] - 2s 822us/step - loss: 1348.8043 - accuracy: 0.0000e+00 - val_loss: 7240.8172 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 822us/step - loss: 1372.4325 - accuracy: 0.0000e+00 - val_loss: 7331.8873 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1370.1039 - accuracy: 0.0000e+00 - val_loss: 7302.1337 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/1000\n",
      "1981/1981 [==============================] - 2s 846us/step - loss: 1368.9634 - accuracy: 0.0000e+00 - val_loss: 7172.7716 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1393.5398 - accuracy: 0.0000e+00 - val_loss: 7374.3369 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/1000\n",
      "1981/1981 [==============================] - 2s 852us/step - loss: 1369.1986 - accuracy: 0.0000e+00 - val_loss: 7350.9051 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/1000\n",
      "1981/1981 [==============================] - 2s 838us/step - loss: 1365.7826 - accuracy: 0.0000e+00 - val_loss: 7405.3309 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1400.9707 - accuracy: 0.0000e+00 - val_loss: 7453.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/1000\n",
      "1981/1981 [==============================] - 2s 858us/step - loss: 1362.1934 - accuracy: 0.0000e+00 - val_loss: 7407.8245 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/1000\n",
      "1981/1981 [==============================] - 2s 851us/step - loss: 1355.0295 - accuracy: 0.0000e+00 - val_loss: 7356.7186 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/1000\n",
      "1981/1981 [==============================] - 2s 837us/step - loss: 1395.6174 - accuracy: 0.0000e+00 - val_loss: 7406.7360 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/1000\n",
      "1981/1981 [==============================] - 2s 836us/step - loss: 1358.9717 - accuracy: 0.0000e+00 - val_loss: 7384.8607 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 1377.3882 - accuracy: 0.0000e+00 - val_loss: 7358.7317 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/1000\n",
      "1981/1981 [==============================] - 2s 846us/step - loss: 1363.1929 - accuracy: 0.0000e+00 - val_loss: 7457.9728 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/1000\n",
      "1981/1981 [==============================] - 2s 820us/step - loss: 1358.8054 - accuracy: 0.0000e+00 - val_loss: 7373.2173 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/1000\n",
      "1981/1981 [==============================] - 2s 878us/step - loss: 1367.2156 - accuracy: 0.0000e+00 - val_loss: 7255.3701 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1408.3740 - accuracy: 0.0000e+00 - val_loss: 7389.3625 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/1000\n",
      "1981/1981 [==============================] - 2s 818us/step - loss: 1373.6323 - accuracy: 0.0000e+00 - val_loss: 7345.0636 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/1000\n",
      "1981/1981 [==============================] - 2s 811us/step - loss: 1364.5514 - accuracy: 0.0000e+00 - val_loss: 7444.5044 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/1000\n",
      "1981/1981 [==============================] - 2s 836us/step - loss: 1366.1230 - accuracy: 0.0000e+00 - val_loss: 7459.9786 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/1000\n",
      "1981/1981 [==============================] - 2s 846us/step - loss: 1357.1469 - accuracy: 0.0000e+00 - val_loss: 7370.4168 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/1000\n",
      "1981/1981 [==============================] - 2s 861us/step - loss: 1369.7943 - accuracy: 0.0000e+00 - val_loss: 7258.1837 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/1000\n",
      "1981/1981 [==============================] - 2s 832us/step - loss: 1367.6321 - accuracy: 0.0000e+00 - val_loss: 7261.8478 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/1000\n",
      "1981/1981 [==============================] - 2s 844us/step - loss: 1385.4513 - accuracy: 0.0000e+00 - val_loss: 7451.4257 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1362.1817 - accuracy: 0.0000e+00 - val_loss: 7439.5391 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/1000\n",
      "1981/1981 [==============================] - 2s 861us/step - loss: 1379.0235 - accuracy: 0.0000e+00 - val_loss: 7466.7372 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/1000\n",
      "1981/1981 [==============================] - 2s 848us/step - loss: 1374.4529 - accuracy: 0.0000e+00 - val_loss: 7475.3409 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/1000\n",
      "1981/1981 [==============================] - 2s 863us/step - loss: 1370.4199 - accuracy: 0.0000e+00 - val_loss: 7487.8080 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/1000\n",
      "1981/1981 [==============================] - 2s 835us/step - loss: 1364.0664 - accuracy: 0.0000e+00 - val_loss: 7421.5541 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/1000\n",
      "1981/1981 [==============================] - 2s 846us/step - loss: 1382.1811 - accuracy: 0.0000e+00 - val_loss: 7257.2395 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1381.7839 - accuracy: 0.0000e+00 - val_loss: 7342.1772 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/1000\n",
      "1981/1981 [==============================] - 2s 829us/step - loss: 1380.1912 - accuracy: 0.0000e+00 - val_loss: 7432.6594 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/1000\n",
      "1981/1981 [==============================] - 2s 849us/step - loss: 1385.8895 - accuracy: 0.0000e+00 - val_loss: 7489.5125 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/1000\n",
      "1981/1981 [==============================] - 2s 908us/step - loss: 1383.1132 - accuracy: 5.0480e-04 - val_loss: 7284.9061 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/1000\n",
      "1981/1981 [==============================] - 2s 933us/step - loss: 1381.0727 - accuracy: 0.0000e+00 - val_loss: 7425.1636 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/1000\n",
      "1981/1981 [==============================] - 2s 899us/step - loss: 1385.9826 - accuracy: 0.0000e+00 - val_loss: 7522.4898 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/1000\n",
      "1981/1981 [==============================] - 2s 875us/step - loss: 1357.4741 - accuracy: 0.0000e+00 - val_loss: 7459.8416 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/1000\n",
      "1981/1981 [==============================] - 2s 849us/step - loss: 1380.8829 - accuracy: 0.0000e+00 - val_loss: 7471.2581 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/1000\n",
      "1981/1981 [==============================] - 2s 834us/step - loss: 1354.5108 - accuracy: 0.0000e+00 - val_loss: 7402.5409 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/1000\n",
      "1981/1981 [==============================] - 2s 882us/step - loss: 1398.3430 - accuracy: 0.0000e+00 - val_loss: 7503.8723 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/1000\n",
      "1981/1981 [==============================] - 2s 849us/step - loss: 1372.4023 - accuracy: 0.0000e+00 - val_loss: 7454.9134 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/1000\n",
      "1981/1981 [==============================] - 2s 830us/step - loss: 1387.7974 - accuracy: 0.0000e+00 - val_loss: 7374.8675 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/1000\n",
      "1981/1981 [==============================] - 2s 824us/step - loss: 1376.1196 - accuracy: 0.0000e+00 - val_loss: 7412.1666 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/1000\n",
      "1981/1981 [==============================] - 2s 886us/step - loss: 1362.5910 - accuracy: 5.0480e-04 - val_loss: 7370.4588 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/1000\n",
      "1981/1981 [==============================] - 2s 841us/step - loss: 1378.5976 - accuracy: 0.0000e+00 - val_loss: 7501.0529 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/1000\n",
      "1981/1981 [==============================] - 2s 851us/step - loss: 1379.0237 - accuracy: 0.0000e+00 - val_loss: 7477.1332 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/1000\n",
      "1981/1981 [==============================] - 2s 872us/step - loss: 1361.7312 - accuracy: 0.0000e+00 - val_loss: 7524.4979 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/1000\n",
      "1981/1981 [==============================] - 2s 862us/step - loss: 1363.1902 - accuracy: 0.0000e+00 - val_loss: 7447.3823 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/1000\n",
      "1981/1981 [==============================] - 2s 833us/step - loss: 1366.1255 - accuracy: 0.0000e+00 - val_loss: 7468.3483 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/1000\n",
      "1981/1981 [==============================] - 2s 870us/step - loss: 1379.5539 - accuracy: 0.0000e+00 - val_loss: 7505.8222 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/1000\n",
      "1981/1981 [==============================] - 2s 880us/step - loss: 1370.7093 - accuracy: 0.0000e+00 - val_loss: 7442.2701 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 2s 849us/step - loss: 1373.1179 - accuracy: 0.0000e+00 - val_loss: 7555.0615 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1351.9797 - accuracy: 0.0000e+00 - val_loss: 7520.1609 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/1000\n",
      "1981/1981 [==============================] - 2s 845us/step - loss: 1372.3357 - accuracy: 0.0000e+00 - val_loss: 7467.5422 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/1000\n",
      "1981/1981 [==============================] - 2s 831us/step - loss: 1377.8832 - accuracy: 0.0000e+00 - val_loss: 7417.3853 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/1000\n",
      "1981/1981 [==============================] - 2s 877us/step - loss: 1362.9772 - accuracy: 0.0000e+00 - val_loss: 7408.9844 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/1000\n",
      "1981/1981 [==============================] - 2s 854us/step - loss: 1383.8196 - accuracy: 0.0000e+00 - val_loss: 7516.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 257/1000\n",
      "1981/1981 [==============================] - 2s 859us/step - loss: 1368.4377 - accuracy: 5.0480e-04 - val_loss: 7501.7675 - val_accuracy: 0.0000e+00\n",
      "Epoch 258/1000\n",
      "1981/1981 [==============================] - 2s 828us/step - loss: 1379.4623 - accuracy: 0.0000e+00 - val_loss: 7539.3026 - val_accuracy: 0.0000e+00\n",
      "Epoch 259/1000\n",
      "1981/1981 [==============================] - 2s 814us/step - loss: 1361.8121 - accuracy: 5.0480e-04 - val_loss: 7548.3639 - val_accuracy: 0.0000e+00\n",
      "Epoch 260/1000\n",
      "1981/1981 [==============================] - 2s 827us/step - loss: 1374.2342 - accuracy: 0.0000e+00 - val_loss: 7486.2329 - val_accuracy: 0.0000e+00\n",
      "Epoch 00260: early stopping\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_3stacks_true_value, 7, stock_with_absolute, label_value_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=7)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test, batch_size=7)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)\n",
    "predictFrame = pd.DataFrame({'prediction': predictions.reshape(X_test.shape[0]), 'true_value': y_test.reshape(X_test.shape[0])})\n",
    "predictFrame\n",
    "\n",
    "# result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "#     \"./LSTM_RESULT/LSTM_7d_sequence_1stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('1-day Delta ($)', fontsize=18)\n",
    "plt.plot(predictFrame)\n",
    "plt.legend(['prediction', 'true_value'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1981, 7, 92)\n",
      "Train on 1981 samples, validate on 217 samples\n",
      "Epoch 1/1000\n",
      "1981/1981 [==============================] - 1s 585us/step - loss: 4395.1429 - accuracy: 0.0000e+00 - val_loss: 14981.0613 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "1981/1981 [==============================] - 1s 423us/step - loss: 949.5662 - accuracy: 0.0000e+00 - val_loss: 7557.3006 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "1981/1981 [==============================] - 1s 396us/step - loss: 1230.5073 - accuracy: 0.0000e+00 - val_loss: 6938.8433 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "1981/1981 [==============================] - 1s 447us/step - loss: 1255.1466 - accuracy: 0.0000e+00 - val_loss: 6874.9892 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "1981/1981 [==============================] - 1s 399us/step - loss: 1233.1241 - accuracy: 0.0000e+00 - val_loss: 6829.3381 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "1981/1981 [==============================] - 1s 398us/step - loss: 1211.9579 - accuracy: 0.0000e+00 - val_loss: 6779.2593 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "1981/1981 [==============================] - 1s 395us/step - loss: 1193.3061 - accuracy: 0.0000e+00 - val_loss: 6727.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "1981/1981 [==============================] - 1s 396us/step - loss: 1176.5881 - accuracy: 0.0000e+00 - val_loss: 6676.5140 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "1981/1981 [==============================] - 1s 413us/step - loss: 1161.4010 - accuracy: 0.0000e+00 - val_loss: 6626.6608 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "1981/1981 [==============================] - 1s 392us/step - loss: 1147.4908 - accuracy: 0.0000e+00 - val_loss: 6578.5184 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "1981/1981 [==============================] - 1s 393us/step - loss: 1134.6858 - accuracy: 0.0000e+00 - val_loss: 6532.2676 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "1981/1981 [==============================] - 1s 401us/step - loss: 1122.8614 - accuracy: 0.0000e+00 - val_loss: 6487.9859 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "1981/1981 [==============================] - 1s 406us/step - loss: 1111.9230 - accuracy: 0.0000e+00 - val_loss: 6445.6910 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "1981/1981 [==============================] - 1s 418us/step - loss: 1101.7973 - accuracy: 0.0000e+00 - val_loss: 6405.3752 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "1981/1981 [==============================] - 1s 469us/step - loss: 1092.4252 - accuracy: 0.0000e+00 - val_loss: 6367.0133 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "1981/1981 [==============================] - 1s 411us/step - loss: 1083.7573 - accuracy: 5.0480e-04 - val_loss: 6330.5675 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "1981/1981 [==============================] - 1s 397us/step - loss: 1075.7520 - accuracy: 5.0480e-04 - val_loss: 6295.9985 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "1981/1981 [==============================] - 1s 387us/step - loss: 1068.3729 - accuracy: 0.0000e+00 - val_loss: 6263.2683 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/1000\n",
      "1981/1981 [==============================] - 1s 376us/step - loss: 1061.5856 - accuracy: 0.0000e+00 - val_loss: 6232.3323 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "1981/1981 [==============================] - 1s 421us/step - loss: 1055.3586 - accuracy: 0.0000e+00 - val_loss: 6203.1459 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/1000\n",
      "1981/1981 [==============================] - 1s 433us/step - loss: 1049.6603 - accuracy: 0.0000e+00 - val_loss: 6175.6623 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "1981/1981 [==============================] - 1s 414us/step - loss: 1044.4593 - accuracy: 0.0000e+00 - val_loss: 6149.8316 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "1981/1981 [==============================] - 1s 394us/step - loss: 1039.7236 - accuracy: 0.0000e+00 - val_loss: 6125.6068 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "1981/1981 [==============================] - 1s 460us/step - loss: 1035.4197 - accuracy: 0.0000e+00 - val_loss: 6102.9311 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/1000\n",
      "1981/1981 [==============================] - 1s 463us/step - loss: 1031.5142 - accuracy: 0.0000e+00 - val_loss: 6081.7549 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "1981/1981 [==============================] - 1s 438us/step - loss: 1027.9710 - accuracy: 0.0000e+00 - val_loss: 6062.0098 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/1000\n",
      "1981/1981 [==============================] - 1s 422us/step - loss: 1024.7556 - accuracy: 5.0480e-04 - val_loss: 6043.6339 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "1981/1981 [==============================] - 1s 385us/step - loss: 1021.8325 - accuracy: 5.0480e-04 - val_loss: 6026.5586 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "1981/1981 [==============================] - 1s 373us/step - loss: 1019.1673 - accuracy: 5.0480e-04 - val_loss: 6010.7077 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "1981/1981 [==============================] - 1s 371us/step - loss: 1016.7275 - accuracy: 5.0480e-04 - val_loss: 5996.0040 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/1000\n",
      "1981/1981 [==============================] - 1s 380us/step - loss: 1014.4826 - accuracy: 5.0480e-04 - val_loss: 5982.3634 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/1000\n",
      "1981/1981 [==============================] - 1s 378us/step - loss: 1012.4061 - accuracy: 5.0480e-04 - val_loss: 5969.7049 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "1981/1981 [==============================] - 1s 378us/step - loss: 1010.4739 - accuracy: 5.0480e-04 - val_loss: 5957.9434 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "1981/1981 [==============================] - 1s 379us/step - loss: 1008.6661 - accuracy: 0.0000e+00 - val_loss: 5946.9972 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "1981/1981 [==============================] - 1s 433us/step - loss: 1006.9664 - accuracy: 0.0000e+00 - val_loss: 5936.7890 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/1000\n",
      "1981/1981 [==============================] - 1s 408us/step - loss: 1005.3611 - accuracy: 0.0000e+00 - val_loss: 5927.2406 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/1000\n",
      "1981/1981 [==============================] - 1s 384us/step - loss: 1003.8400 - accuracy: 0.0000e+00 - val_loss: 5918.2868 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/1000\n",
      "1981/1981 [==============================] - 1s 393us/step - loss: 1002.3952 - accuracy: 0.0000e+00 - val_loss: 5909.8678 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "1981/1981 [==============================] - 1s 436us/step - loss: 1001.0197 - accuracy: 0.0000e+00 - val_loss: 5901.9246 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "1981/1981 [==============================] - 1s 415us/step - loss: 999.7093 - accuracy: 0.0000e+00 - val_loss: 5894.4146 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "1981/1981 [==============================] - 1s 406us/step - loss: 998.4591 - accuracy: 0.0000e+00 - val_loss: 5887.2915 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "1981/1981 [==============================] - 1s 395us/step - loss: 997.2665 - accuracy: 0.0000e+00 - val_loss: 5880.5230 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "1981/1981 [==============================] - 1s 418us/step - loss: 996.1283 - accuracy: 5.0480e-04 - val_loss: 5874.0785 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "1981/1981 [==============================] - 1s 466us/step - loss: 995.0419 - accuracy: 5.0480e-04 - val_loss: 5867.9301 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "1981/1981 [==============================] - 1s 481us/step - loss: 994.0050 - accuracy: 5.0480e-04 - val_loss: 5862.0563 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "1981/1981 [==============================] - 1s 562us/step - loss: 993.0157 - accuracy: 5.0480e-04 - val_loss: 5856.4408 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/1000\n",
      "1981/1981 [==============================] - 1s 554us/step - loss: 992.0717 - accuracy: 5.0480e-04 - val_loss: 5851.0676 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/1000\n",
      "1981/1981 [==============================] - 1s 475us/step - loss: 991.1706 - accuracy: 5.0480e-04 - val_loss: 5845.9157 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "1981/1981 [==============================] - 1s 503us/step - loss: 990.3116 - accuracy: 5.0480e-04 - val_loss: 5840.9829 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "1981/1981 [==============================] - 1s 427us/step - loss: 989.4917 - accuracy: 5.0480e-04 - val_loss: 5836.2520 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 1s 476us/step - loss: 988.7097 - accuracy: 5.0480e-04 - val_loss: 5831.7153 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "1981/1981 [==============================] - 1s 413us/step - loss: 987.9639 - accuracy: 5.0480e-04 - val_loss: 5827.3649 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "1981/1981 [==============================] - 1s 428us/step - loss: 987.2523 - accuracy: 0.0000e+00 - val_loss: 5823.1932 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/1000\n",
      "1981/1981 [==============================] - 1s 416us/step - loss: 986.5736 - accuracy: 0.0000e+00 - val_loss: 5819.1921 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "1981/1981 [==============================] - 1s 432us/step - loss: 985.9261 - accuracy: 0.0000e+00 - val_loss: 5815.3530 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/1000\n",
      "1981/1981 [==============================] - 1s 419us/step - loss: 985.3083 - accuracy: 0.0000e+00 - val_loss: 5811.6747 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/1000\n",
      "1981/1981 [==============================] - 1s 412us/step - loss: 984.7187 - accuracy: 0.0000e+00 - val_loss: 5808.1464 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/1000\n",
      "1981/1981 [==============================] - 1s 404us/step - loss: 984.1562 - accuracy: 0.0000e+00 - val_loss: 5804.7654 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/1000\n",
      "1981/1981 [==============================] - 1s 419us/step - loss: 983.6193 - accuracy: 0.0000e+00 - val_loss: 5801.5254 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/1000\n",
      "1981/1981 [==============================] - 1s 458us/step - loss: 983.1066 - accuracy: 0.0000e+00 - val_loss: 5798.4229 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/1000\n",
      "1981/1981 [==============================] - 1s 402us/step - loss: 982.6175 - accuracy: 0.0000e+00 - val_loss: 5795.4519 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/1000\n",
      "1981/1981 [==============================] - 1s 393us/step - loss: 982.1504 - accuracy: 0.0000e+00 - val_loss: 5792.6065 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "1981/1981 [==============================] - 1s 428us/step - loss: 981.7042 - accuracy: 0.0000e+00 - val_loss: 5789.8900 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "1981/1981 [==============================] - 1s 476us/step - loss: 981.2781 - accuracy: 0.0000e+00 - val_loss: 5787.2739 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "1981/1981 [==============================] - 1s 390us/step - loss: 980.8717 - accuracy: 0.0000e+00 - val_loss: 5784.7980 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "1981/1981 [==============================] - 1s 389us/step - loss: 980.4816 - accuracy: 0.0000e+00 - val_loss: 5782.4157 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/1000\n",
      "1981/1981 [==============================] - 1s 381us/step - loss: 980.1099 - accuracy: 0.0000e+00 - val_loss: 5780.1474 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "1981/1981 [==============================] - 1s 403us/step - loss: 979.7539 - accuracy: 0.0000e+00 - val_loss: 5777.9779 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "1981/1981 [==============================] - 1s 393us/step - loss: 979.4140 - accuracy: 0.0000e+00 - val_loss: 5775.9099 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "1981/1981 [==============================] - 1s 460us/step - loss: 979.0884 - accuracy: 0.0000e+00 - val_loss: 5773.9361 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/1000\n",
      "1981/1981 [==============================] - 1s 374us/step - loss: 978.7772 - accuracy: 0.0000e+00 - val_loss: 5772.0574 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/1000\n",
      "1981/1981 [==============================] - 1s 373us/step - loss: 978.4790 - accuracy: 0.0000e+00 - val_loss: 5770.2650 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/1000\n",
      "1981/1981 [==============================] - 1s 373us/step - loss: 978.1937 - accuracy: 0.0000e+00 - val_loss: 5768.5653 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/1000\n",
      "1981/1981 [==============================] - 1s 384us/step - loss: 977.9203 - accuracy: 0.0000e+00 - val_loss: 5766.9352 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/1000\n",
      "1981/1981 [==============================] - 1s 396us/step - loss: 977.6592 - accuracy: 0.0000e+00 - val_loss: 5765.4067 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/1000\n",
      "1981/1981 [==============================] - 1s 390us/step - loss: 977.4073 - accuracy: 0.0000e+00 - val_loss: 5763.9401 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/1000\n",
      "1981/1981 [==============================] - 1s 386us/step - loss: 977.1670 - accuracy: 0.0000e+00 - val_loss: 5762.5575 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/1000\n",
      "1981/1981 [==============================] - 1s 391us/step - loss: 976.9360 - accuracy: 0.0000e+00 - val_loss: 5761.2413 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/1000\n",
      "1981/1981 [==============================] - 1s 444us/step - loss: 976.7146 - accuracy: 0.0000e+00 - val_loss: 5760.0002 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/1000\n",
      "1981/1981 [==============================] - 1s 388us/step - loss: 976.5019 - accuracy: 0.0000e+00 - val_loss: 5758.8202 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/1000\n",
      "1981/1981 [==============================] - 1s 393us/step - loss: 976.2980 - accuracy: 0.0000e+00 - val_loss: 5757.7147 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/1000\n",
      "1981/1981 [==============================] - 1s 422us/step - loss: 976.1014 - accuracy: 0.0000e+00 - val_loss: 5756.6602 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/1000\n",
      "1981/1981 [==============================] - 1s 380us/step - loss: 975.9135 - accuracy: 0.0000e+00 - val_loss: 5755.6784 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/1000\n",
      "1981/1981 [==============================] - 1s 383us/step - loss: 975.7318 - accuracy: 0.0000e+00 - val_loss: 5754.7433 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/1000\n",
      "1981/1981 [==============================] - 1s 446us/step - loss: 975.5581 - accuracy: 0.0000e+00 - val_loss: 5753.8780 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/1000\n",
      "1981/1981 [==============================] - 1s 491us/step - loss: 975.3897 - accuracy: 0.0000e+00 - val_loss: 5753.0530 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/1000\n",
      "1981/1981 [==============================] - 1s 454us/step - loss: 975.2288 - accuracy: 0.0000e+00 - val_loss: 5752.2937 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/1000\n",
      "1981/1981 [==============================] - 1s 481us/step - loss: 975.0728 - accuracy: 0.0000e+00 - val_loss: 5751.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/1000\n",
      "1981/1981 [==============================] - 1s 469us/step - loss: 974.9237 - accuracy: 0.0000e+00 - val_loss: 5750.9131 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/1000\n",
      "1981/1981 [==============================] - 1s 553us/step - loss: 974.7786 - accuracy: 0.0000e+00 - val_loss: 5750.2888 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/1000\n",
      "1981/1981 [==============================] - 1s 581us/step - loss: 974.6398 - accuracy: 0.0000e+00 - val_loss: 5749.7230 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/1000\n",
      "1981/1981 [==============================] - 1s 573us/step - loss: 974.5046 - accuracy: 0.0000e+00 - val_loss: 5749.1858 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/1000\n",
      "1981/1981 [==============================] - 1s 517us/step - loss: 974.3754 - accuracy: 0.0000e+00 - val_loss: 5748.7034 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "1981/1981 [==============================] - 1s 567us/step - loss: 974.2495 - accuracy: 0.0000e+00 - val_loss: 5748.2511 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "1981/1981 [==============================] - 1s 470us/step - loss: 974.1286 - accuracy: 0.0000e+00 - val_loss: 5747.8484 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/1000\n",
      "1981/1981 [==============================] - 1s 491us/step - loss: 974.0113 - accuracy: 0.0000e+00 - val_loss: 5747.4754 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/1000\n",
      "1981/1981 [==============================] - 1s 424us/step - loss: 973.8981 - accuracy: 0.0000e+00 - val_loss: 5747.1480 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "1981/1981 [==============================] - 1s 396us/step - loss: 973.7880 - accuracy: 0.0000e+00 - val_loss: 5746.8446 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/1000\n",
      "1981/1981 [==============================] - 1s 424us/step - loss: 973.6819 - accuracy: 0.0000e+00 - val_loss: 5746.5868 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/1000\n",
      "1981/1981 [==============================] - 1s 423us/step - loss: 973.5785 - accuracy: 0.0000e+00 - val_loss: 5746.3485 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 1s 429us/step - loss: 973.4792 - accuracy: 0.0000e+00 - val_loss: 5746.1546 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/1000\n",
      "1981/1981 [==============================] - 1s 429us/step - loss: 973.3818 - accuracy: 0.0000e+00 - val_loss: 5745.9790 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/1000\n",
      "1981/1981 [==============================] - 1s 425us/step - loss: 973.2882 - accuracy: 0.0000e+00 - val_loss: 5745.8446 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/1000\n",
      "1981/1981 [==============================] - 1s 485us/step - loss: 973.1964 - accuracy: 0.0000e+00 - val_loss: 5745.7256 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/1000\n",
      "1981/1981 [==============================] - 1s 523us/step - loss: 973.1081 - accuracy: 0.0000e+00 - val_loss: 5745.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/1000\n",
      "1981/1981 [==============================] - 1s 495us/step - loss: 973.0215 - accuracy: 0.0000e+00 - val_loss: 5745.5807 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/1000\n",
      "1981/1981 [==============================] - 1s 529us/step - loss: 972.9380 - accuracy: 0.0000e+00 - val_loss: 5745.5509 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/1000\n",
      "1981/1981 [==============================] - 1s 533us/step - loss: 972.8561 - accuracy: 0.0000e+00 - val_loss: 5745.5351 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/1000\n",
      "1981/1981 [==============================] - 1s 561us/step - loss: 972.7769 - accuracy: 0.0000e+00 - val_loss: 5745.5524 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/1000\n",
      "1981/1981 [==============================] - 1s 582us/step - loss: 972.6992 - accuracy: 0.0000e+00 - val_loss: 5745.5813 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/1000\n",
      "1981/1981 [==============================] - 1s 577us/step - loss: 972.6241 - accuracy: 0.0000e+00 - val_loss: 5745.6421 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/1000\n",
      "1981/1981 [==============================] - 1s 530us/step - loss: 972.5502 - accuracy: 0.0000e+00 - val_loss: 5745.7108 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/1000\n",
      "1981/1981 [==============================] - 1s 511us/step - loss: 972.4792 - accuracy: 0.0000e+00 - val_loss: 5745.8137 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/1000\n",
      "1981/1981 [==============================] - 1s 482us/step - loss: 972.4086 - accuracy: 0.0000e+00 - val_loss: 5745.9240 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/1000\n",
      "1981/1981 [==============================] - 1s 466us/step - loss: 972.3407 - accuracy: 0.0000e+00 - val_loss: 5746.0613 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/1000\n",
      "1981/1981 [==============================] - 1s 459us/step - loss: 972.2736 - accuracy: 0.0000e+00 - val_loss: 5746.2080 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/1000\n",
      "1981/1981 [==============================] - 1s 436us/step - loss: 972.2087 - accuracy: 0.0000e+00 - val_loss: 5746.3813 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "1981/1981 [==============================] - 1s 433us/step - loss: 972.1446 - accuracy: 0.0000e+00 - val_loss: 5746.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/1000\n",
      "1981/1981 [==============================] - 1s 419us/step - loss: 972.0825 - accuracy: 0.0000e+00 - val_loss: 5746.7638 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/1000\n",
      "1981/1981 [==============================] - 1s 393us/step - loss: 972.0211 - accuracy: 0.0000e+00 - val_loss: 5746.9747 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "1981/1981 [==============================] - 1s 394us/step - loss: 971.9618 - accuracy: 0.0000e+00 - val_loss: 5747.2095 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/1000\n",
      "1981/1981 [==============================] - 1s 383us/step - loss: 971.9029 - accuracy: 0.0000e+00 - val_loss: 5747.4494 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "1981/1981 [==============================] - 1s 381us/step - loss: 971.8458 - accuracy: 0.0000e+00 - val_loss: 5747.7128 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "1981/1981 [==============================] - 1s 374us/step - loss: 971.7892 - accuracy: 0.0000e+00 - val_loss: 5747.9788 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "1981/1981 [==============================] - 1s 380us/step - loss: 971.7344 - accuracy: 0.0000e+00 - val_loss: 5748.2678 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "1981/1981 [==============================] - 1s 393us/step - loss: 971.6798 - accuracy: 0.0000e+00 - val_loss: 5748.5570 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "1981/1981 [==============================] - 1s 376us/step - loss: 971.6271 - accuracy: 0.0000e+00 - val_loss: 5748.8699 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "1981/1981 [==============================] - 1s 374us/step - loss: 971.5747 - accuracy: 0.0000e+00 - val_loss: 5749.1849 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "1981/1981 [==============================] - 1s 387us/step - loss: 971.5235 - accuracy: 0.0000e+00 - val_loss: 5749.5191 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "1981/1981 [==============================] - 1s 378us/step - loss: 971.4729 - accuracy: 0.0000e+00 - val_loss: 5749.8550 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "1981/1981 [==============================] - 1s 377us/step - loss: 971.4237 - accuracy: 0.0000e+00 - val_loss: 5750.2095 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "1981/1981 [==============================] - 1s 388us/step - loss: 971.3746 - accuracy: 0.0000e+00 - val_loss: 5750.5655 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "1981/1981 [==============================] - 1s 384us/step - loss: 971.3270 - accuracy: 0.0000e+00 - val_loss: 5750.9390 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "1981/1981 [==============================] - 1s 390us/step - loss: 971.2796 - accuracy: 0.0000e+00 - val_loss: 5751.3147 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/1000\n",
      "1981/1981 [==============================] - 1s 392us/step - loss: 971.2334 - accuracy: 0.0000e+00 - val_loss: 5751.7074 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "1981/1981 [==============================] - 1s 399us/step - loss: 971.1871 - accuracy: 0.0000e+00 - val_loss: 5752.0976 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/1000\n",
      "1981/1981 [==============================] - 1s 411us/step - loss: 971.1424 - accuracy: 0.0000e+00 - val_loss: 5752.5062 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "1981/1981 [==============================] - 1s 409us/step - loss: 971.0976 - accuracy: 0.0000e+00 - val_loss: 5752.9131 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/1000\n",
      "1981/1981 [==============================] - 1s 396us/step - loss: 971.0542 - accuracy: 0.0000e+00 - val_loss: 5753.3379 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "1981/1981 [==============================] - 1s 399us/step - loss: 971.0106 - accuracy: 0.0000e+00 - val_loss: 5753.7597 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/1000\n",
      "1981/1981 [==============================] - 1s 408us/step - loss: 970.9683 - accuracy: 0.0000e+00 - val_loss: 5754.1983 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "1981/1981 [==============================] - 1s 413us/step - loss: 970.9258 - accuracy: 0.0000e+00 - val_loss: 5754.6358 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "1981/1981 [==============================] - 1s 399us/step - loss: 970.8846 - accuracy: 0.0000e+00 - val_loss: 5755.0864 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "1981/1981 [==============================] - 1s 396us/step - loss: 970.8433 - accuracy: 0.0000e+00 - val_loss: 5755.5367 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "1981/1981 [==============================] - 1s 397us/step - loss: 970.8030 - accuracy: 0.0000e+00 - val_loss: 5756.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "1981/1981 [==============================] - 1s 387us/step - loss: 970.7627 - accuracy: 0.0000e+00 - val_loss: 5756.4629 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/1000\n",
      "1981/1981 [==============================] - 1s 406us/step - loss: 970.7234 - accuracy: 0.0000e+00 - val_loss: 5756.9392 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/1000\n",
      "1981/1981 [==============================] - 1s 397us/step - loss: 970.6839 - accuracy: 0.0000e+00 - val_loss: 5757.4117 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "1981/1981 [==============================] - 1s 389us/step - loss: 970.6455 - accuracy: 0.0000e+00 - val_loss: 5757.8993 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "1981/1981 [==============================] - 1s 392us/step - loss: 970.6069 - accuracy: 0.0000e+00 - val_loss: 5758.3828 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981/1981 [==============================] - 1s 396us/step - loss: 970.5695 - accuracy: 0.0000e+00 - val_loss: 5758.8818 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "1981/1981 [==============================] - 1s 382us/step - loss: 970.5315 - accuracy: 0.0000e+00 - val_loss: 5759.3752 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/1000\n",
      "1981/1981 [==============================] - 1s 379us/step - loss: 970.4948 - accuracy: 0.0000e+00 - val_loss: 5759.8826 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/1000\n",
      "1981/1981 [==============================] - 1s 388us/step - loss: 970.4578 - accuracy: 0.0000e+00 - val_loss: 5760.3875 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "1981/1981 [==============================] - 1s 370us/step - loss: 970.4216 - accuracy: 0.0000e+00 - val_loss: 5760.9032 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "1981/1981 [==============================] - 1s 384us/step - loss: 970.3854 - accuracy: 0.0000e+00 - val_loss: 5761.4166 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "1981/1981 [==============================] - 1s 388us/step - loss: 970.3499 - accuracy: 0.0000e+00 - val_loss: 5761.9400 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "1981/1981 [==============================] - 1s 381us/step - loss: 970.3145 - accuracy: 0.0000e+00 - val_loss: 5762.4635 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "1981/1981 [==============================] - 1s 379us/step - loss: 970.2795 - accuracy: 0.0000e+00 - val_loss: 5762.9957 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/1000\n",
      "1981/1981 [==============================] - 1s 442us/step - loss: 970.2445 - accuracy: 0.0000e+00 - val_loss: 5763.5244 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "1981/1981 [==============================] - 1s 474us/step - loss: 970.2106 - accuracy: 0.0000e+00 - val_loss: 5764.0672 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "1981/1981 [==============================] - 1s 487us/step - loss: 970.1760 - accuracy: 0.0000e+00 - val_loss: 5764.6059 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "1981/1981 [==============================] - 1s 468us/step - loss: 970.1423 - accuracy: 0.0000e+00 - val_loss: 5765.1547 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "1981/1981 [==============================] - 1s 450us/step - loss: 970.1083 - accuracy: 0.0000e+00 - val_loss: 5765.6981 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/1000\n",
      "1981/1981 [==============================] - 1s 462us/step - loss: 970.0753 - accuracy: 0.0000e+00 - val_loss: 5766.2543 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "1981/1981 [==============================] - 1s 457us/step - loss: 970.0418 - accuracy: 0.0000e+00 - val_loss: 5766.8065 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "1981/1981 [==============================] - 1s 435us/step - loss: 970.0094 - accuracy: 0.0000e+00 - val_loss: 5767.3709 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "1981/1981 [==============================] - 1s 444us/step - loss: 969.9765 - accuracy: 0.0000e+00 - val_loss: 5767.9303 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/1000\n",
      "1981/1981 [==============================] - 1s 463us/step - loss: 969.9444 - accuracy: 0.0000e+00 - val_loss: 5768.5004 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "1981/1981 [==============================] - 1s 451us/step - loss: 969.9119 - accuracy: 0.0000e+00 - val_loss: 5769.0650 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "1981/1981 [==============================] - 1s 437us/step - loss: 969.8803 - accuracy: 0.0000e+00 - val_loss: 5769.6435 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "1981/1981 [==============================] - 1s 472us/step - loss: 969.8483 - accuracy: 0.0000e+00 - val_loss: 5770.2151 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/1000\n",
      "1981/1981 [==============================] - 1s 444us/step - loss: 969.8168 - accuracy: 0.0000e+00 - val_loss: 5770.7957 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "1981/1981 [==============================] - 1s 418us/step - loss: 969.7855 - accuracy: 0.0000e+00 - val_loss: 5771.3752 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "1981/1981 [==============================] - 1s 427us/step - loss: 969.7545 - accuracy: 0.0000e+00 - val_loss: 5771.9666 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/1000\n",
      "1981/1981 [==============================] - 1s 421us/step - loss: 969.7232 - accuracy: 0.0000e+00 - val_loss: 5772.5504 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "1981/1981 [==============================] - 1s 423us/step - loss: 969.6928 - accuracy: 0.0000e+00 - val_loss: 5773.1449 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "1981/1981 [==============================] - 1s 404us/step - loss: 969.6621 - accuracy: 0.0000e+00 - val_loss: 5773.7374 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "1981/1981 [==============================] - 1s 400us/step - loss: 969.6318 - accuracy: 0.0000e+00 - val_loss: 5774.3389 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "1981/1981 [==============================] - 1s 393us/step - loss: 969.6014 - accuracy: 0.0000e+00 - val_loss: 5774.9362 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "1981/1981 [==============================] - 1s 408us/step - loss: 969.5715 - accuracy: 0.0000e+00 - val_loss: 5775.5449 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/1000\n",
      "1981/1981 [==============================] - 1s 413us/step - loss: 969.5412 - accuracy: 0.0000e+00 - val_loss: 5776.1464 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/1000\n",
      "1981/1981 [==============================] - 1s 414us/step - loss: 969.5119 - accuracy: 0.0000e+00 - val_loss: 5776.7625 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/1000\n",
      "1981/1981 [==============================] - 1s 406us/step - loss: 969.4820 - accuracy: 0.0000e+00 - val_loss: 5777.3710 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/1000\n",
      "1981/1981 [==============================] - 1s 395us/step - loss: 969.4529 - accuracy: 0.0000e+00 - val_loss: 5777.9940 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/1000\n",
      "1981/1981 [==============================] - 1s 408us/step - loss: 969.4231 - accuracy: 0.0000e+00 - val_loss: 5778.6093 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/1000\n",
      "1981/1981 [==============================] - 1s 397us/step - loss: 969.3942 - accuracy: 0.0000e+00 - val_loss: 5779.2342 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/1000\n",
      "1981/1981 [==============================] - 1s 413us/step - loss: 969.3650 - accuracy: 0.0000e+00 - val_loss: 5779.8562 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/1000\n",
      "1981/1981 [==============================] - 1s 396us/step - loss: 969.3364 - accuracy: 0.0000e+00 - val_loss: 5780.4895 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/1000\n",
      "1981/1981 [==============================] - 1s 416us/step - loss: 969.3074 - accuracy: 0.0000e+00 - val_loss: 5781.1197 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/1000\n",
      "1981/1981 [==============================] - 1s 421us/step - loss: 969.2788 - accuracy: 0.0000e+00 - val_loss: 5781.7576 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/1000\n",
      "1981/1981 [==============================] - 1s 406us/step - loss: 969.2502 - accuracy: 0.0000e+00 - val_loss: 5782.3947 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/1000\n",
      " 266/1981 [===>..........................] - ETA: 0s - loss: 3060.3708 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_1stacks_true_value, 7, stock_with_absolute, label_value_7d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=7)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test, batch_size=7)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)\n",
    "predictFrame = pd.DataFrame({'prediction': predictions.reshape(X_test.shape[0]), 'true_value': y_test.reshape(X_test.shape[0])})\n",
    "predictFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Best Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim:  6\n",
      "trim:  6\n",
      "trim:  4\n",
      "trim:  4\n",
      "trim:  1\n",
      "trim:  1\n",
      "(1981, 7, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_101 (LSTM)              (7, 64)                   40192     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (7, 20)                   1300      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (7, 1)                    21        \n",
      "=================================================================\n",
      "Total params: 41,513\n",
      "Trainable params: 41,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.551\n",
      "The Validation Accuracy  0.507\n",
      "The Test Accuracy   0.518\n",
      "AUC ROC : 0.481\n",
      "confusion matrix / precision recall scores\n",
      "[[114  21]\n",
      " [ 97  13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.84      0.66       135\n",
      "           1       0.38      0.12      0.18       110\n",
      "\n",
      "    accuracy                           0.52       245\n",
      "   macro avg       0.46      0.48      0.42       245\n",
      "weighted avg       0.47      0.52      0.44       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_1stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_1stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim:  6\n",
      "trim:  6\n",
      "trim:  4\n",
      "trim:  4\n",
      "trim:  1\n",
      "trim:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_104 (LSTM)              (7, 7, 64)                40192     \n",
      "_________________________________________________________________\n",
      "lstm_105 (LSTM)              (7, 64)                   33024     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (7, 64)                   0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (7, 20)                   1300      \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (7, 1)                    21        \n",
      "=================================================================\n",
      "Total params: 74,537\n",
      "Trainable params: 74,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00344: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.947\n",
      "The Validation Accuracy  0.525\n",
      "The Test Accuracy   0.490\n",
      "AUC ROC : 0.492\n",
      "confusion matrix / precision recall scores\n",
      "[[64 71]\n",
      " [54 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.47      0.51       135\n",
      "           1       0.44      0.51      0.47       110\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.49      0.49      0.49       245\n",
      "weighted avg       0.50      0.49      0.49       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_2stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_2stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim:  6\n",
      "trim:  6\n",
      "trim:  4\n",
      "trim:  4\n",
      "trim:  1\n",
      "trim:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_106 (LSTM)              (7, 7, 64)                40192     \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (7, 7, 64)                0         \n",
      "_________________________________________________________________\n",
      "lstm_107 (LSTM)              (7, 7, 64)                33024     \n",
      "_________________________________________________________________\n",
      "lstm_108 (LSTM)              (7, 64)                   33024     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (7, 64)                   0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (7, 20)                   1300      \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (7, 1)                    21        \n",
      "=================================================================\n",
      "Total params: 107,561\n",
      "Trainable params: 107,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00570: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.913\n",
      "The Validation Accuracy  0.442\n",
      "The Test Accuracy   0.531\n",
      "AUC ROC : 0.524\n",
      "confusion matrix / precision recall scores\n",
      "[[80 55]\n",
      " [60 50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.59      0.58       135\n",
      "           1       0.48      0.45      0.47       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.52      0.52      0.52       245\n",
      "weighted avg       0.53      0.53      0.53       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_3stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_3stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim:  6\n",
      "trim:  6\n",
      "trim:  4\n",
      "trim:  4\n",
      "trim:  1\n",
      "trim:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_109 (LSTM)              (7, 7, 64)                40192     \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (7, 7, 64)                0         \n",
      "_________________________________________________________________\n",
      "lstm_110 (LSTM)              (7, 7, 64)                33024     \n",
      "_________________________________________________________________\n",
      "lstm_111 (LSTM)              (7, 7, 64)                33024     \n",
      "_________________________________________________________________\n",
      "lstm_112 (LSTM)              (7, 64)                   33024     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (7, 64)                   0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (7, 20)                   1300      \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (7, 1)                    21        \n",
      "=================================================================\n",
      "Total params: 140,585\n",
      "Trainable params: 140,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00445: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.861\n",
      "The Validation Accuracy  0.475\n",
      "The Test Accuracy   0.469\n",
      "AUC ROC : 0.462\n",
      "confusion matrix / precision recall scores\n",
      "[[72 63]\n",
      " [67 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.53      0.53       135\n",
      "           1       0.41      0.39      0.40       110\n",
      "\n",
      "    accuracy                           0.47       245\n",
      "   macro avg       0.46      0.46      0.46       245\n",
      "weighted avg       0.47      0.47      0.47       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train(buildTrendModel_4stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_4stack_hid64\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Best Stacks (using loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1981, 7, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00242: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.562\n",
      "The Validation Accuracy  0.525\n",
      "The Test Accuracy   0.527\n",
      "AUC ROC : 0.493\n",
      "confusion matrix / precision recall scores\n",
      "[[111  24]\n",
      " [ 92  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.82      0.66       135\n",
      "           1       0.43      0.16      0.24       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.49      0.49      0.45       245\n",
      "weighted avg       0.49      0.53      0.47       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train_loss(buildTrendModel_1stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_1stack_hid64_loss\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00391: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.850\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.514\n",
      "AUC ROC : 0.513\n",
      "confusion matrix / precision recall scores\n",
      "[[71 64]\n",
      " [55 55]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.54       135\n",
      "           1       0.46      0.50      0.48       110\n",
      "\n",
      "    accuracy                           0.51       245\n",
      "   macro avg       0.51      0.51      0.51       245\n",
      "weighted avg       0.52      0.51      0.52       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train_loss(buildTrendModel_2stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_2stack_hid64_loss\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00224: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.539\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.544\n",
      "confusion matrix / precision recall scores\n",
      "[[83 52]\n",
      " [58 52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60       135\n",
      "           1       0.50      0.47      0.49       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.54      0.54      0.54       245\n",
      "weighted avg       0.55      0.55      0.55       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train_loss(buildTrendModel_3stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_3stack_hid64_loss\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00249: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.507\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.555\n",
      "confusion matrix / precision recall scores\n",
      "[[70 65]\n",
      " [45 65]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.52      0.56       135\n",
      "           1       0.50      0.59      0.54       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.55      0.55      0.55       245\n",
      "weighted avg       0.56      0.55      0.55       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model, X_train, y_train, X_valid, y_valid, X_test, y_test  = model_train_loss(buildTrendModel_4stacks, 7, stock_with_absolute, label_abs_1d, 64, batch_size)\n",
    "\n",
    "predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "print()\n",
    "result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "       \"./LSTM_RESULT/LSTM_7d_sequence_4stack_hid64_loss\", model.predict, clf_name=\"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Best Layer (using loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Predict  1d\n",
      "Layer:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00331: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.718\n",
      "The Validation Accuracy  0.484\n",
      "The Test Accuracy   0.539\n",
      "AUC ROC : 0.517\n",
      "confusion matrix / precision recall scores\n",
      "[[99 36]\n",
      " [77 33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64       135\n",
      "           1       0.48      0.30      0.37       110\n",
      "\n",
      "    accuracy                           0.54       245\n",
      "   macro avg       0.52      0.52      0.50       245\n",
      "weighted avg       0.52      0.54      0.52       245\n",
      "\n",
      "Layer:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00309: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.975\n",
      "The Validation Accuracy  0.475\n",
      "The Test Accuracy   0.514\n",
      "AUC ROC : 0.517\n",
      "confusion matrix / precision recall scores\n",
      "[[66 69]\n",
      " [50 60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.49      0.53       135\n",
      "           1       0.47      0.55      0.50       110\n",
      "\n",
      "    accuracy                           0.51       245\n",
      "   macro avg       0.52      0.52      0.51       245\n",
      "weighted avg       0.52      0.51      0.52       245\n",
      "\n",
      "Layer:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00214: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Start Predict  7d\n",
      "Layer:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00333: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.926\n",
      "The Validation Accuracy  0.558\n",
      "The Test Accuracy   0.600\n",
      "AUC ROC : 0.553\n",
      "confusion matrix / precision recall scores\n",
      "[[115  57]\n",
      " [ 41  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.67      0.70       172\n",
      "           1       0.36      0.44      0.40        73\n",
      "\n",
      "    accuracy                           0.60       245\n",
      "   macro avg       0.55      0.55      0.55       245\n",
      "weighted avg       0.62      0.60      0.61       245\n",
      "\n",
      "Layer:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00311: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.994\n",
      "The Validation Accuracy  0.594\n",
      "The Test Accuracy   0.478\n",
      "AUC ROC : 0.494\n",
      "confusion matrix / precision recall scores\n",
      "[[78 94]\n",
      " [34 39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.45      0.55       172\n",
      "           1       0.29      0.53      0.38        73\n",
      "\n",
      "    accuracy                           0.48       245\n",
      "   macro avg       0.49      0.49      0.46       245\n",
      "weighted avg       0.58      0.48      0.50       245\n",
      "\n",
      "Layer:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00240: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.571\n",
      "The Test Accuracy   0.543\n",
      "AUC ROC : 0.489\n",
      "confusion matrix / precision recall scores\n",
      "[[107  65]\n",
      " [ 47  26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.62      0.66       172\n",
      "           1       0.29      0.36      0.32        73\n",
      "\n",
      "    accuracy                           0.54       245\n",
      "   macro avg       0.49      0.49      0.49       245\n",
      "weighted avg       0.57      0.54      0.56       245\n",
      "\n",
      "Start Predict  30d\n",
      "Layer:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00322: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.947\n",
      "The Validation Accuracy  0.567\n",
      "The Test Accuracy   0.796\n",
      "AUC ROC : 0.547\n",
      "confusion matrix / precision recall scores\n",
      "[[189   4]\n",
      " [ 46   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88       193\n",
      "           1       0.60      0.12      0.19        52\n",
      "\n",
      "    accuracy                           0.80       245\n",
      "   macro avg       0.70      0.55      0.54       245\n",
      "weighted avg       0.76      0.80      0.74       245\n",
      "\n",
      "Layer:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00300: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.993\n",
      "The Validation Accuracy  0.659\n",
      "The Test Accuracy   0.755\n",
      "AUC ROC : 0.606\n",
      "confusion matrix / precision recall scores\n",
      "[[167  26]\n",
      " [ 34  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       193\n",
      "           1       0.41      0.35      0.38        52\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.62      0.61      0.61       245\n",
      "weighted avg       0.74      0.76      0.75       245\n",
      "\n",
      "Layer:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, batch_input_shape=(7, 7, 92), stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, stateful=True, kernel_initializer=\"RandomUniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "\n",
      "The Train Accuracy  0.673\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "hidden_layer_size = [32 , 64, 128]\n",
    "predict_day = [label_abs_1d, label_abs_7d, label_abs_30d]\n",
    "predict_day_str = [\"1d\", \"7d\", \"30d\"]\n",
    "\n",
    "for day in range(3):\n",
    "    print(\"Start Predict \", predict_day_str[day])\n",
    "    for layer_size in hidden_layer_size:\n",
    "        print(\"Layer: \", layer_size)\n",
    "        batch_size = 7\n",
    "        model, X_train, y_train, X_valid, y_valid, X_test, y_test = model_train_loss(buildTrendModel_3stacks, 7, \n",
    "        stock_with_absolute, predict_day[day], layer_size, batch_size)\n",
    "\n",
    "        predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "        predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "        predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "        print()\n",
    "        result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "               \"./LSTM_RESULT/best_layer_with_abs/LSTM_\"+predict_day_str[day]+\"_sequence_3stack_hid\"+str(layer_size)+\"_loss\", model.predict, clf_name=\"LSTM\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test All Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Predict  1d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n",
      "Epoch 00804: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.659\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.531\n",
      "AUC ROC : 0.494\n",
      "confusion matrix / precision recall scores\n",
      "[[115  20]\n",
      " [ 95  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.85      0.67       135\n",
      "           1       0.43      0.14      0.21       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.49      0.49      0.44       245\n",
      "weighted avg       0.49      0.53      0.46       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00357: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.729\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.482\n",
      "AUC ROC : 0.469\n",
      "confusion matrix / precision recall scores\n",
      "[[80 55]\n",
      " [72 38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.59      0.56       135\n",
      "           1       0.41      0.35      0.37       110\n",
      "\n",
      "    accuracy                           0.48       245\n",
      "   macro avg       0.47      0.47      0.47       245\n",
      "weighted avg       0.47      0.48      0.48       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00470: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.687\n",
      "The Validation Accuracy  0.465\n",
      "The Test Accuracy   0.486\n",
      "AUC ROC : 0.465\n",
      "confusion matrix / precision recall scores\n",
      "[[90 45]\n",
      " [81 29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.67      0.59       135\n",
      "           1       0.39      0.26      0.32       110\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.46      0.47      0.45       245\n",
      "weighted avg       0.47      0.49      0.47       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00326: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.617\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.539\n",
      "AUC ROC : 0.503\n",
      "confusion matrix / precision recall scores\n",
      "[[115  20]\n",
      " [ 93  17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.85      0.67       135\n",
      "           1       0.46      0.15      0.23       110\n",
      "\n",
      "    accuracy                           0.54       245\n",
      "   macro avg       0.51      0.50      0.45       245\n",
      "weighted avg       0.51      0.54      0.47       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00328: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.716\n",
      "The Validation Accuracy  0.479\n",
      "The Test Accuracy   0.508\n",
      "AUC ROC : 0.499\n",
      "confusion matrix / precision recall scores\n",
      "[[78 53]\n",
      " [64 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.60      0.57       131\n",
      "           1       0.45      0.40      0.42       107\n",
      "\n",
      "    accuracy                           0.51       238\n",
      "   macro avg       0.50      0.50      0.50       238\n",
      "weighted avg       0.50      0.51      0.50       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.526\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00281: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.903\n",
      "The Validation Accuracy  0.470\n",
      "The Test Accuracy   0.482\n",
      "AUC ROC : 0.482\n",
      "confusion matrix / precision recall scores\n",
      "[[65 70]\n",
      " [57 53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.48      0.51       135\n",
      "           1       0.43      0.48      0.45       110\n",
      "\n",
      "    accuracy                           0.48       245\n",
      "   macro avg       0.48      0.48      0.48       245\n",
      "weighted avg       0.49      0.48      0.48       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00258: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.953\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.559\n",
      "AUC ROC : 0.539\n",
      "confusion matrix / precision recall scores\n",
      "[[96 35]\n",
      " [70 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.65       131\n",
      "           1       0.51      0.35      0.41       107\n",
      "\n",
      "    accuracy                           0.56       238\n",
      "   macro avg       0.55      0.54      0.53       238\n",
      "weighted avg       0.55      0.56      0.54       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00211: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00470: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.939\n",
      "The Validation Accuracy  0.479\n",
      "The Test Accuracy   0.465\n",
      "AUC ROC : 0.463\n",
      "confusion matrix / precision recall scores\n",
      "[[65 70]\n",
      " [61 49]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.48      0.50       135\n",
      "           1       0.41      0.45      0.43       110\n",
      "\n",
      "    accuracy                           0.47       245\n",
      "   macro avg       0.46      0.46      0.46       245\n",
      "weighted avg       0.47      0.47      0.47       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00206: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.525\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00254: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.526\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00203: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.526\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.550\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[131   0]\n",
      " [107   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       131\n",
      "           1       0.00      0.00      0.00       107\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start Predict  7d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00511: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.646\n",
      "The Validation Accuracy  0.567\n",
      "The Test Accuracy   0.686\n",
      "AUC ROC : 0.512\n",
      "confusion matrix / precision recall scores\n",
      "[[162  10]\n",
      " [ 67   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.94      0.81       172\n",
      "           1       0.38      0.08      0.13        73\n",
      "\n",
      "    accuracy                           0.69       245\n",
      "   macro avg       0.54      0.51      0.47       245\n",
      "weighted avg       0.61      0.69      0.61       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00258: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.718\n",
      "The Validation Accuracy  0.576\n",
      "The Test Accuracy   0.588\n",
      "AUC ROC : 0.458\n",
      "confusion matrix / precision recall scores\n",
      "[[134  38]\n",
      " [ 63  10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73       172\n",
      "           1       0.21      0.14      0.17        73\n",
      "\n",
      "    accuracy                           0.59       245\n",
      "   macro avg       0.44      0.46      0.45       245\n",
      "weighted avg       0.54      0.59      0.56       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00262: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.880\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.392\n",
      "AUC ROC : 0.362\n",
      "confusion matrix / precision recall scores\n",
      "[[75 97]\n",
      " [52 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.44      0.50       172\n",
      "           1       0.18      0.29      0.22        73\n",
      "\n",
      "    accuracy                           0.39       245\n",
      "   macro avg       0.38      0.36      0.36       245\n",
      "weighted avg       0.47      0.39      0.42       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00253: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.966\n",
      "The Validation Accuracy  0.539\n",
      "The Test Accuracy   0.518\n",
      "AUC ROC : 0.480\n",
      "confusion matrix / precision recall scores\n",
      "[[99 73]\n",
      " [45 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.58      0.63       172\n",
      "           1       0.28      0.38      0.32        73\n",
      "\n",
      "    accuracy                           0.52       245\n",
      "   macro avg       0.48      0.48      0.47       245\n",
      "weighted avg       0.57      0.52      0.54       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00267: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.881\n",
      "The Validation Accuracy  0.539\n",
      "The Test Accuracy   0.529\n",
      "AUC ROC : 0.422\n",
      "confusion matrix / precision recall scores\n",
      "[[115  52]\n",
      " [ 60  11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.67       167\n",
      "           1       0.17      0.15      0.16        71\n",
      "\n",
      "    accuracy                           0.53       238\n",
      "   macro avg       0.42      0.42      0.42       238\n",
      "weighted avg       0.51      0.53      0.52       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00275: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.671\n",
      "The Validation Accuracy  0.548\n",
      "The Test Accuracy   0.673\n",
      "AUC ROC : 0.515\n",
      "confusion matrix / precision recall scores\n",
      "[[156  16]\n",
      " [ 64   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.91      0.80       172\n",
      "           1       0.36      0.12      0.18        73\n",
      "\n",
      "    accuracy                           0.67       245\n",
      "   macro avg       0.53      0.52      0.49       245\n",
      "weighted avg       0.61      0.67      0.61       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00234: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.907\n",
      "The Validation Accuracy  0.599\n",
      "The Test Accuracy   0.588\n",
      "AUC ROC : 0.525\n",
      "confusion matrix / precision recall scores\n",
      "[[117  55]\n",
      " [ 46  27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70       172\n",
      "           1       0.33      0.37      0.35        73\n",
      "\n",
      "    accuracy                           0.59       245\n",
      "   macro avg       0.52      0.53      0.52       245\n",
      "weighted avg       0.60      0.59      0.59       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.970\n",
      "The Validation Accuracy  0.502\n",
      "The Test Accuracy   0.616\n",
      "AUC ROC : 0.518\n",
      "confusion matrix / precision recall scores\n",
      "[[131  41]\n",
      " [ 53  20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.74       172\n",
      "           1       0.33      0.27      0.30        73\n",
      "\n",
      "    accuracy                           0.62       245\n",
      "   macro avg       0.52      0.52      0.52       245\n",
      "weighted avg       0.60      0.62      0.61       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00225: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.986\n",
      "The Validation Accuracy  0.419\n",
      "The Test Accuracy   0.494\n",
      "AUC ROC : 0.474\n",
      "confusion matrix / precision recall scores\n",
      "[[90 82]\n",
      " [42 31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.52      0.59       172\n",
      "           1       0.27      0.42      0.33        73\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.48      0.47      0.46       245\n",
      "weighted avg       0.56      0.49      0.52       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00313: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.948\n",
      "The Validation Accuracy  0.461\n",
      "The Test Accuracy   0.525\n",
      "AUC ROC : 0.467\n",
      "confusion matrix / precision recall scores\n",
      "[[102  65]\n",
      " [ 48  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.61      0.64       167\n",
      "           1       0.26      0.32      0.29        71\n",
      "\n",
      "    accuracy                           0.53       238\n",
      "   macro avg       0.47      0.47      0.47       238\n",
      "weighted avg       0.56      0.53      0.54       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00295: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.579\n",
      "The Validation Accuracy  0.567\n",
      "The Test Accuracy   0.694\n",
      "AUC ROC : 0.494\n",
      "confusion matrix / precision recall scores\n",
      "[[170   2]\n",
      " [ 73   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.99      0.82       172\n",
      "           1       0.00      0.00      0.00        73\n",
      "\n",
      "    accuracy                           0.69       245\n",
      "   macro avg       0.35      0.49      0.41       245\n",
      "weighted avg       0.49      0.69      0.58       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00330: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.978\n",
      "The Validation Accuracy  0.590\n",
      "The Test Accuracy   0.571\n",
      "AUC ROC : 0.478\n",
      "confusion matrix / precision recall scores\n",
      "[[122  50]\n",
      " [ 55  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70       172\n",
      "           1       0.26      0.25      0.26        73\n",
      "\n",
      "    accuracy                           0.57       245\n",
      "   macro avg       0.48      0.48      0.48       245\n",
      "weighted avg       0.56      0.57      0.57       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00239: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.964\n",
      "The Validation Accuracy  0.553\n",
      "The Test Accuracy   0.351\n",
      "AUC ROC : 0.368\n",
      "confusion matrix / precision recall scores\n",
      "[[ 56 116]\n",
      " [ 43  30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.33      0.41       172\n",
      "           1       0.21      0.41      0.27        73\n",
      "\n",
      "    accuracy                           0.35       245\n",
      "   macro avg       0.39      0.37      0.34       245\n",
      "weighted avg       0.46      0.35      0.37       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00290: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.994\n",
      "The Validation Accuracy  0.599\n",
      "The Test Accuracy   0.502\n",
      "AUC ROC : 0.476\n",
      "confusion matrix / precision recall scores\n",
      "[[93 79]\n",
      " [43 30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.54      0.60       172\n",
      "           1       0.28      0.41      0.33        73\n",
      "\n",
      "    accuracy                           0.50       245\n",
      "   macro avg       0.48      0.48      0.47       245\n",
      "weighted avg       0.56      0.50      0.52       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00284: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.581\n",
      "The Test Accuracy   0.588\n",
      "AUC ROC : 0.512\n",
      "confusion matrix / precision recall scores\n",
      "[[117  50]\n",
      " [ 48  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.70       167\n",
      "           1       0.32      0.32      0.32        71\n",
      "\n",
      "    accuracy                           0.59       238\n",
      "   macro avg       0.51      0.51      0.51       238\n",
      "weighted avg       0.59      0.59      0.59       238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start Predict  30d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00206: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.636\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00256: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.920\n",
      "The Validation Accuracy  0.484\n",
      "The Test Accuracy   0.494\n",
      "AUC ROC : 0.412\n",
      "confusion matrix / precision recall scores\n",
      "[[107  86]\n",
      " [ 38  14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.55      0.63       193\n",
      "           1       0.14      0.27      0.18        52\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.44      0.41      0.41       245\n",
      "weighted avg       0.61      0.49      0.54       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00305: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.963\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.759\n",
      "AUC ROC : 0.631\n",
      "confusion matrix / precision recall scores\n",
      "[[165  29]\n",
      " [ 30  21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       194\n",
      "           1       0.42      0.41      0.42        51\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.63      0.63      0.63       245\n",
      "weighted avg       0.76      0.76      0.76       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00450: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.964\n",
      "The Validation Accuracy  0.682\n",
      "The Test Accuracy   0.681\n",
      "AUC ROC : 0.563\n",
      "confusion matrix / precision recall scores\n",
      "[[144  44]\n",
      " [ 32  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.79       188\n",
      "           1       0.29      0.36      0.32        50\n",
      "\n",
      "    accuracy                           0.68       238\n",
      "   macro avg       0.55      0.56      0.56       238\n",
      "weighted avg       0.71      0.68      0.69       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00433: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.701\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.759\n",
      "AUC ROC : 0.482\n",
      "confusion matrix / precision recall scores\n",
      "[[186   7]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.96      0.86       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.39      0.48      0.43       245\n",
      "weighted avg       0.62      0.76      0.68       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00383: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.814\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.633\n",
      "AUC ROC : 0.444\n",
      "confusion matrix / precision recall scores\n",
      "[[149  44]\n",
      " [ 46   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77       193\n",
      "           1       0.12      0.12      0.12        52\n",
      "\n",
      "    accuracy                           0.63       245\n",
      "   macro avg       0.44      0.44      0.44       245\n",
      "weighted avg       0.63      0.63      0.63       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00306: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.820\n",
      "The Validation Accuracy  0.452\n",
      "The Test Accuracy   0.706\n",
      "AUC ROC : 0.596\n",
      "confusion matrix / precision recall scores\n",
      "[[152  41]\n",
      " [ 31  21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       193\n",
      "           1       0.34      0.40      0.37        52\n",
      "\n",
      "    accuracy                           0.71       245\n",
      "   macro avg       0.58      0.60      0.59       245\n",
      "weighted avg       0.73      0.71      0.72       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.630\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.792\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[194   0]\n",
      " [ 51   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       194\n",
      "           1       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.40      0.50      0.44       245\n",
      "weighted avg       0.63      0.79      0.70       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00352: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.899\n",
      "The Validation Accuracy  0.447\n",
      "The Test Accuracy   0.664\n",
      "AUC ROC : 0.428\n",
      "confusion matrix / precision recall scores\n",
      "[[157  31]\n",
      " [ 49   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80       188\n",
      "           1       0.03      0.02      0.02        50\n",
      "\n",
      "    accuracy                           0.66       238\n",
      "   macro avg       0.40      0.43      0.41       238\n",
      "weighted avg       0.61      0.66      0.63       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00231: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00214: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.630\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00202: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.630\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.792\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[194   0]\n",
      " [ 51   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       194\n",
      "           1       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.40      0.50      0.44       245\n",
      "weighted avg       0.63      0.79      0.70       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00202: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.625\n",
      "The Validation Accuracy  0.558\n",
      "The Test Accuracy   0.790\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[188   0]\n",
      " [ 50   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       188\n",
      "           1       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.79       238\n",
      "   macro avg       0.39      0.50      0.44       238\n",
      "weighted avg       0.62      0.79      0.70       238\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "hidden_layer_size = [32 , 64, 128]\n",
    "sequence_day = [1,3,7,15,30]\n",
    "predict_day = [label_abs_1d, label_abs_7d, label_abs_30d]\n",
    "predict_day_str = [\"1d\", \"7d\", \"30d\"]\n",
    "\n",
    "for day in range(3):\n",
    "    print(\"Start Predict \", predict_day_str[day])\n",
    "    print()\n",
    "    for layer_size in hidden_layer_size:\n",
    "        print(\"Layer: \", layer_size)\n",
    "        for past_day in sequence_day:\n",
    "            print(\"Sequence number of day: \", past_day)\n",
    "            model, X_train, y_train, X_valid, y_valid, X_test, y_test = model_train_loss(buildTrendModel_3stacks, past_day, \n",
    "            stock_with_absolute, predict_day[day], layer_size, batch_size)\n",
    "\n",
    "            predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "            predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "            predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "            print()\n",
    "            result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "                   \"./LSTM_RESULT/best_layer_seqnum_with_abs/LSTM_\"+predict_day_str[day]+\"_sequence_3stack_hid\"+str(layer_size)+\"_seq\"+str(past_day)+\"_loss\", model.predict, clf_name=\"LSTM\")\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Predict  1d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n",
      "Epoch 00211: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.681\n",
      "The Validation Accuracy  0.475\n",
      "The Test Accuracy   0.535\n",
      "AUC ROC : 0.510\n",
      "confusion matrix / precision recall scores\n",
      "[[101  34]\n",
      " [ 80  30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64       135\n",
      "           1       0.47      0.27      0.34       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.51      0.51      0.49       245\n",
      "weighted avg       0.52      0.53      0.51       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00213: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.841\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.543\n",
      "AUC ROC : 0.540\n",
      "confusion matrix / precision recall scores\n",
      "[[77 58]\n",
      " [54 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58       135\n",
      "           1       0.49      0.51      0.50       110\n",
      "\n",
      "    accuracy                           0.54       245\n",
      "   macro avg       0.54      0.54      0.54       245\n",
      "weighted avg       0.54      0.54      0.54       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00266: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.911\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.461\n",
      "AUC ROC : 0.466\n",
      "confusion matrix / precision recall scores\n",
      "[[57 78]\n",
      " [54 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.42      0.46       135\n",
      "           1       0.42      0.51      0.46       110\n",
      "\n",
      "    accuracy                           0.46       245\n",
      "   macro avg       0.47      0.47      0.46       245\n",
      "weighted avg       0.47      0.46      0.46       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00248: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.964\n",
      "The Validation Accuracy  0.576\n",
      "The Test Accuracy   0.559\n",
      "AUC ROC : 0.543\n",
      "confusion matrix / precision recall scores\n",
      "[[95 40]\n",
      " [68 42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.70      0.64       135\n",
      "           1       0.51      0.38      0.44       110\n",
      "\n",
      "    accuracy                           0.56       245\n",
      "   macro avg       0.55      0.54      0.54       245\n",
      "weighted avg       0.55      0.56      0.55       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00220: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.931\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.521\n",
      "AUC ROC : 0.518\n",
      "confusion matrix / precision recall scores\n",
      "[[72 59]\n",
      " [55 52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.55      0.56       131\n",
      "           1       0.47      0.49      0.48       107\n",
      "\n",
      "    accuracy                           0.52       238\n",
      "   macro avg       0.52      0.52      0.52       238\n",
      "weighted avg       0.52      0.52      0.52       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00270: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.808\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.535\n",
      "AUC ROC : 0.515\n",
      "confusion matrix / precision recall scores\n",
      "[[96 39]\n",
      " [75 35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.71      0.63       135\n",
      "           1       0.47      0.32      0.38       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.52      0.51      0.50       245\n",
      "weighted avg       0.52      0.53      0.52       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00222: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.940\n",
      "The Validation Accuracy  0.465\n",
      "The Test Accuracy   0.494\n",
      "AUC ROC : 0.491\n",
      "confusion matrix / precision recall scores\n",
      "[[70 65]\n",
      " [59 51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.52      0.53       135\n",
      "           1       0.44      0.46      0.45       110\n",
      "\n",
      "    accuracy                           0.49       245\n",
      "   macro avg       0.49      0.49      0.49       245\n",
      "weighted avg       0.50      0.49      0.49       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00215: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.512\n",
      "The Test Accuracy   0.531\n",
      "AUC ROC : 0.527\n",
      "confusion matrix / precision recall scores\n",
      "[[76 59]\n",
      " [56 54]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57       135\n",
      "           1       0.48      0.49      0.48       110\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.53      0.53      0.53       245\n",
      "weighted avg       0.53      0.53      0.53       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00234: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.990\n",
      "The Validation Accuracy  0.447\n",
      "The Test Accuracy   0.506\n",
      "AUC ROC : 0.506\n",
      "confusion matrix / precision recall scores\n",
      "[[68 67]\n",
      " [54 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53       135\n",
      "           1       0.46      0.51      0.48       110\n",
      "\n",
      "    accuracy                           0.51       245\n",
      "   macro avg       0.51      0.51      0.50       245\n",
      "weighted avg       0.51      0.51      0.51       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00210: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.999\n",
      "The Validation Accuracy  0.498\n",
      "The Test Accuracy   0.483\n",
      "AUC ROC : 0.477\n",
      "confusion matrix / precision recall scores\n",
      "[[71 60]\n",
      " [63 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.54      0.54       131\n",
      "           1       0.42      0.41      0.42       107\n",
      "\n",
      "    accuracy                           0.48       238\n",
      "   macro avg       0.48      0.48      0.48       238\n",
      "weighted avg       0.48      0.48      0.48       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00227: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.784\n",
      "The Validation Accuracy  0.516\n",
      "The Test Accuracy   0.547\n",
      "AUC ROC : 0.516\n",
      "confusion matrix / precision recall scores\n",
      "[[111  24]\n",
      " [ 87  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.82      0.67       135\n",
      "           1       0.49      0.21      0.29       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.52      0.52      0.48       245\n",
      "weighted avg       0.53      0.55      0.50       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00219: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.526\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[135   0]\n",
      " [110   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       135\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.28      0.50      0.36       245\n",
      "weighted avg       0.30      0.55      0.39       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00236: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.993\n",
      "The Validation Accuracy  0.544\n",
      "The Test Accuracy   0.437\n",
      "AUC ROC : 0.433\n",
      "confusion matrix / precision recall scores\n",
      "[[63 72]\n",
      " [66 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.47      0.48       135\n",
      "           1       0.38      0.40      0.39       110\n",
      "\n",
      "    accuracy                           0.44       245\n",
      "   macro avg       0.43      0.43      0.43       245\n",
      "weighted avg       0.44      0.44      0.44       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00243: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.999\n",
      "The Validation Accuracy  0.535\n",
      "The Test Accuracy   0.555\n",
      "AUC ROC : 0.556\n",
      "confusion matrix / precision recall scores\n",
      "[[74 61]\n",
      " [48 62]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.55      0.58       135\n",
      "           1       0.50      0.56      0.53       110\n",
      "\n",
      "    accuracy                           0.56       245\n",
      "   macro avg       0.56      0.56      0.55       245\n",
      "weighted avg       0.56      0.56      0.56       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00228: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  1.000\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.517\n",
      "AUC ROC : 0.520\n",
      "confusion matrix / precision recall scores\n",
      "[[64 67]\n",
      " [48 59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.49      0.53       131\n",
      "           1       0.47      0.55      0.51       107\n",
      "\n",
      "    accuracy                           0.52       238\n",
      "   macro avg       0.52      0.52      0.52       238\n",
      "weighted avg       0.53      0.52      0.52       238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start Predict  7d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00207: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.711\n",
      "The Validation Accuracy  0.558\n",
      "The Test Accuracy   0.584\n",
      "AUC ROC : 0.510\n",
      "confusion matrix / precision recall scores\n",
      "[[119  53]\n",
      " [ 49  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       172\n",
      "           1       0.31      0.33      0.32        73\n",
      "\n",
      "    accuracy                           0.58       245\n",
      "   macro avg       0.51      0.51      0.51       245\n",
      "weighted avg       0.59      0.58      0.59       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00212: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.820\n",
      "The Validation Accuracy  0.544\n",
      "The Test Accuracy   0.624\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[139  33]\n",
      " [ 59  14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.81      0.75       172\n",
      "           1       0.30      0.19      0.23        73\n",
      "\n",
      "    accuracy                           0.62       245\n",
      "   macro avg       0.50      0.50      0.49       245\n",
      "weighted avg       0.58      0.62      0.60       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00209: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.981\n",
      "The Validation Accuracy  0.479\n",
      "The Test Accuracy   0.612\n",
      "AUC ROC : 0.503\n",
      "confusion matrix / precision recall scores\n",
      "[[133  39]\n",
      " [ 56  17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.74       172\n",
      "           1       0.30      0.23      0.26        73\n",
      "\n",
      "    accuracy                           0.61       245\n",
      "   macro avg       0.50      0.50      0.50       245\n",
      "weighted avg       0.58      0.61      0.60       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00216: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.989\n",
      "The Validation Accuracy  0.581\n",
      "The Test Accuracy   0.596\n",
      "AUC ROC : 0.519\n",
      "confusion matrix / precision recall scores\n",
      "[[122  50]\n",
      " [ 49  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71       172\n",
      "           1       0.32      0.33      0.33        73\n",
      "\n",
      "    accuracy                           0.60       245\n",
      "   macro avg       0.52      0.52      0.52       245\n",
      "weighted avg       0.60      0.60      0.60       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00203: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.991\n",
      "The Validation Accuracy  0.516\n",
      "The Test Accuracy   0.634\n",
      "AUC ROC : 0.553\n",
      "confusion matrix / precision recall scores\n",
      "[[126  41]\n",
      " [ 46  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       167\n",
      "           1       0.38      0.35      0.36        71\n",
      "\n",
      "    accuracy                           0.63       238\n",
      "   macro avg       0.56      0.55      0.55       238\n",
      "weighted avg       0.63      0.63      0.63       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00206: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.808\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.633\n",
      "AUC ROC : 0.486\n",
      "confusion matrix / precision recall scores\n",
      "[[146  26]\n",
      " [ 64   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.85      0.76       172\n",
      "           1       0.26      0.12      0.17        73\n",
      "\n",
      "    accuracy                           0.63       245\n",
      "   macro avg       0.48      0.49      0.47       245\n",
      "weighted avg       0.56      0.63      0.59       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00208: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.942\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.592\n",
      "AUC ROC : 0.516\n",
      "confusion matrix / precision recall scores\n",
      "[[121  51]\n",
      " [ 49  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71       172\n",
      "           1       0.32      0.33      0.32        73\n",
      "\n",
      "    accuracy                           0.59       245\n",
      "   macro avg       0.52      0.52      0.52       245\n",
      "weighted avg       0.60      0.59      0.59       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00203: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.553\n",
      "The Test Accuracy   0.551\n",
      "AUC ROC : 0.523\n",
      "confusion matrix / precision recall scores\n",
      "[[102  70]\n",
      " [ 40  33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       172\n",
      "           1       0.32      0.45      0.38        73\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.52      0.52      0.51       245\n",
      "weighted avg       0.60      0.55      0.57       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.493\n",
      "The Test Accuracy   0.600\n",
      "AUC ROC : 0.553\n",
      "confusion matrix / precision recall scores\n",
      "[[115  57]\n",
      " [ 41  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.67      0.70       172\n",
      "           1       0.36      0.44      0.40        73\n",
      "\n",
      "    accuracy                           0.60       245\n",
      "   macro avg       0.55      0.55      0.55       245\n",
      "weighted avg       0.62      0.60      0.61       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.544\n",
      "The Test Accuracy   0.567\n",
      "AUC ROC : 0.534\n",
      "confusion matrix / precision recall scores\n",
      "[[103  64]\n",
      " [ 39  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.62      0.67       167\n",
      "           1       0.33      0.45      0.38        71\n",
      "\n",
      "    accuracy                           0.57       238\n",
      "   macro avg       0.53      0.53      0.52       238\n",
      "weighted avg       0.61      0.57      0.58       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00206: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.904\n",
      "The Validation Accuracy  0.539\n",
      "The Test Accuracy   0.637\n",
      "AUC ROC : 0.540\n",
      "confusion matrix / precision recall scores\n",
      "[[134  38]\n",
      " [ 51  22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75       172\n",
      "           1       0.37      0.30      0.33        73\n",
      "\n",
      "    accuracy                           0.64       245\n",
      "   macro avg       0.55      0.54      0.54       245\n",
      "weighted avg       0.62      0.64      0.63       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00221: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.995\n",
      "The Validation Accuracy  0.521\n",
      "The Test Accuracy   0.547\n",
      "AUC ROC : 0.520\n",
      "confusion matrix / precision recall scores\n",
      "[[101  71]\n",
      " [ 40  33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       172\n",
      "           1       0.32      0.45      0.37        73\n",
      "\n",
      "    accuracy                           0.55       245\n",
      "   macro avg       0.52      0.52      0.51       245\n",
      "weighted avg       0.60      0.55      0.56       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00212: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.999\n",
      "The Validation Accuracy  0.590\n",
      "The Test Accuracy   0.531\n",
      "AUC ROC : 0.508\n",
      "confusion matrix / precision recall scores\n",
      "[[97 75]\n",
      " [40 33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.56      0.63       172\n",
      "           1       0.31      0.45      0.36        73\n",
      "\n",
      "    accuracy                           0.53       245\n",
      "   macro avg       0.51      0.51      0.50       245\n",
      "weighted avg       0.59      0.53      0.55       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00213: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  1.000\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.567\n",
      "AUC ROC : 0.526\n",
      "confusion matrix / precision recall scores\n",
      "[[108  64]\n",
      " [ 42  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67       172\n",
      "           1       0.33      0.42      0.37        73\n",
      "\n",
      "    accuracy                           0.57       245\n",
      "   macro avg       0.52      0.53      0.52       245\n",
      "weighted avg       0.60      0.57      0.58       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00223: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.548\n",
      "The Test Accuracy   0.546\n",
      "AUC ROC : 0.450\n",
      "confusion matrix / precision recall scores\n",
      "[[115  52]\n",
      " [ 56  15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       167\n",
      "           1       0.22      0.21      0.22        71\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.45      0.45      0.45       238\n",
      "weighted avg       0.54      0.55      0.54       238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Start Predict  30d\n",
      "\n",
      "Layer:  32\n",
      "Sequence number of day:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00212: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.749\n",
      "The Validation Accuracy  0.585\n",
      "The Test Accuracy   0.796\n",
      "AUC ROC : 0.554\n",
      "confusion matrix / precision recall scores\n",
      "[[188   5]\n",
      " [ 45   7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88       193\n",
      "           1       0.58      0.13      0.22        52\n",
      "\n",
      "    accuracy                           0.80       245\n",
      "   macro avg       0.70      0.55      0.55       245\n",
      "weighted avg       0.76      0.80      0.74       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00230: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.926\n",
      "The Validation Accuracy  0.493\n",
      "The Test Accuracy   0.576\n",
      "AUC ROC : 0.443\n",
      "confusion matrix / precision recall scores\n",
      "[[130  63]\n",
      " [ 41  11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.67      0.71       193\n",
      "           1       0.15      0.21      0.17        52\n",
      "\n",
      "    accuracy                           0.58       245\n",
      "   macro avg       0.45      0.44      0.44       245\n",
      "weighted avg       0.63      0.58      0.60       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00226: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.996\n",
      "The Validation Accuracy  0.553\n",
      "The Test Accuracy   0.759\n",
      "AUC ROC : 0.538\n",
      "confusion matrix / precision recall scores\n",
      "[[178  15]\n",
      " [ 44   8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.86       193\n",
      "           1       0.35      0.15      0.21        52\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.57      0.54      0.54       245\n",
      "weighted avg       0.71      0.76      0.72       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00225: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.996\n",
      "The Validation Accuracy  0.608\n",
      "The Test Accuracy   0.514\n",
      "AUC ROC : 0.368\n",
      "confusion matrix / precision recall scores\n",
      "[[120  74]\n",
      " [ 45   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.62      0.67       194\n",
      "           1       0.07      0.12      0.09        51\n",
      "\n",
      "    accuracy                           0.51       245\n",
      "   macro avg       0.40      0.37      0.38       245\n",
      "weighted avg       0.59      0.51      0.55       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00234: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.991\n",
      "The Validation Accuracy  0.438\n",
      "The Test Accuracy   0.655\n",
      "AUC ROC : 0.620\n",
      "confusion matrix / precision recall scores\n",
      "[[128  60]\n",
      " [ 22  28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.68      0.76       188\n",
      "           1       0.32      0.56      0.41        50\n",
      "\n",
      "    accuracy                           0.66       238\n",
      "   macro avg       0.59      0.62      0.58       238\n",
      "weighted avg       0.74      0.66      0.68       238\n",
      "\n",
      "Layer:  64\n",
      "Sequence number of day:  1\n",
      "Epoch 00216: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.815\n",
      "The Validation Accuracy  0.553\n",
      "The Test Accuracy   0.763\n",
      "AUC ROC : 0.499\n",
      "confusion matrix / precision recall scores\n",
      "[[185   8]\n",
      " [ 50   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.96      0.86       193\n",
      "           1       0.20      0.04      0.06        52\n",
      "\n",
      "    accuracy                           0.76       245\n",
      "   macro avg       0.49      0.50      0.46       245\n",
      "weighted avg       0.66      0.76      0.69       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00231: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.977\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.641\n",
      "AUC ROC : 0.533\n",
      "confusion matrix / precision recall scores\n",
      "[[139  54]\n",
      " [ 34  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76       193\n",
      "           1       0.25      0.35      0.29        52\n",
      "\n",
      "    accuracy                           0.64       245\n",
      "   macro avg       0.53      0.53      0.52       245\n",
      "weighted avg       0.69      0.64      0.66       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00207: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  1.000\n",
      "The Validation Accuracy  0.530\n",
      "The Test Accuracy   0.771\n",
      "AUC ROC : 0.560\n",
      "confusion matrix / precision recall scores\n",
      "[[179  14]\n",
      " [ 42  10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.86       193\n",
      "           1       0.42      0.19      0.26        52\n",
      "\n",
      "    accuracy                           0.77       245\n",
      "   macro avg       0.61      0.56      0.56       245\n",
      "weighted avg       0.73      0.77      0.74       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00248: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.997\n",
      "The Validation Accuracy  0.576\n",
      "The Test Accuracy   0.727\n",
      "AUC ROC : 0.524\n",
      "confusion matrix / precision recall scores\n",
      "[[169  25]\n",
      " [ 42   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       194\n",
      "           1       0.26      0.18      0.21        51\n",
      "\n",
      "    accuracy                           0.73       245\n",
      "   macro avg       0.53      0.52      0.52       245\n",
      "weighted avg       0.69      0.73      0.70       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00230: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.999\n",
      "The Validation Accuracy  0.525\n",
      "The Test Accuracy   0.672\n",
      "AUC ROC : 0.492\n",
      "confusion matrix / precision recall scores\n",
      "[[151  37]\n",
      " [ 41   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79       188\n",
      "           1       0.20      0.18      0.19        50\n",
      "\n",
      "    accuracy                           0.67       238\n",
      "   macro avg       0.49      0.49      0.49       238\n",
      "weighted avg       0.66      0.67      0.67       238\n",
      "\n",
      "Layer:  128\n",
      "Sequence number of day:  1\n",
      "Epoch 00205: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  3\n",
      "Epoch 00201: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.629\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.788\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[193   0]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.39      0.50      0.44       245\n",
      "weighted avg       0.62      0.79      0.69       245\n",
      "\n",
      "Sequence number of day:  7\n",
      "Epoch 00267: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.995\n",
      "The Validation Accuracy  0.604\n",
      "The Test Accuracy   0.706\n",
      "AUC ROC : 0.448\n",
      "confusion matrix / precision recall scores\n",
      "[[173  20]\n",
      " [ 52   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83       193\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.71       245\n",
      "   macro avg       0.38      0.45      0.41       245\n",
      "weighted avg       0.61      0.71      0.65       245\n",
      "\n",
      "Sequence number of day:  15\n",
      "Epoch 00296: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.630\n",
      "The Validation Accuracy  0.562\n",
      "The Test Accuracy   0.792\n",
      "AUC ROC : 0.500\n",
      "confusion matrix / precision recall scores\n",
      "[[194   0]\n",
      " [ 51   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       194\n",
      "           1       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.79       245\n",
      "   macro avg       0.40      0.50      0.44       245\n",
      "weighted avg       0.63      0.79      0.70       245\n",
      "\n",
      "Sequence number of day:  30\n",
      "Epoch 00320: early stopping\n",
      "\n",
      "Results for  LSTM : \n",
      "The Train Accuracy  0.998\n",
      "The Validation Accuracy  0.484\n",
      "The Test Accuracy   0.685\n",
      "AUC ROC : 0.617\n",
      "confusion matrix / precision recall scores\n",
      "[[138  50]\n",
      " [ 25  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.79       188\n",
      "           1       0.33      0.50      0.40        50\n",
      "\n",
      "    accuracy                           0.68       238\n",
      "   macro avg       0.59      0.62      0.59       238\n",
      "weighted avg       0.74      0.68      0.71       238\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "hidden_layer_size = [32 , 64, 128]\n",
    "sequence_day = [1,3,7,15,30]\n",
    "predict_day = [label_abs_1d, label_abs_7d, label_abs_30d]\n",
    "predict_day_str = [\"1d\", \"7d\", \"30d\"]\n",
    "\n",
    "for day in range(3):\n",
    "    print(\"Start Predict \", predict_day_str[day])\n",
    "    print()\n",
    "    for layer_size in hidden_layer_size:\n",
    "        print(\"Layer: \", layer_size)\n",
    "        for past_day in sequence_day:\n",
    "            print(\"Sequence number of day: \", past_day)\n",
    "            model, X_train, y_train, X_valid, y_valid, X_test, y_test = model_train_loss(buildTrendModel_3stacks, past_day, \n",
    "            stock_without_absolute, predict_day[day], layer_size, batch_size)\n",
    "\n",
    "            predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "            predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "            predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "            print()\n",
    "            result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "                   \"./LSTM_RESULT/best_layer_seqnum_without_abs/LSTM_\"+predict_day_str[day]+\"_sequence_3stack_hid\"+str(layer_size)+\"_seq\"+str(past_day)+\"_loss\", model.predict, clf_name=\"LSTM\")\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 7\n",
    "sequence_day = 7 #???\n",
    "hidden_layer_size = [32 , 64, 128]\n",
    "predict_day = [label_abs_1d, label_abs_7d, label_abs_30d]\n",
    "predict_day_str = [\"1d\", \"7d\", \"30d\"]\n",
    "\n",
    "for day in range(3):\n",
    "    print(\"Start Predict \", predict_day_str[day])\n",
    "    for layer_size in hidden_layer_size:\n",
    "        print(\"Layer: \", layer_size)\n",
    "        batch_size = 7\n",
    "        model, X_train, y_train, X_valid, y_valid, X_test, y_test = model_train_loss(buildTrendModel_3stacks, sequence_day, \n",
    "        stock_without_absolute, predict_day[day], layer_size, batch_size)\n",
    "\n",
    "        predicted_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "        predicted_train = np.array(model.predict(X_train, batch_size=batch_size))\n",
    "        predicted_valid = np.array(model.predict(X_valid, batch_size=batch_size))\n",
    "        print()\n",
    "        result(np.where(predicted_test > 0.5, 1, 0), y_test, np.where(predicted_train > 0.5, 1, 0), y_train, np.where(predicted_valid > 0.5, 1, 0), y_valid,\n",
    "               \"./LSTM_RESULT/best_layer_with_abs/LSTM_\"+predict_day_str[day]+\"_sequence_3stack_hid\"+str(layer_size)+\"_loss\", model.predict, clf_name=\"LSTM\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = [32 , 64, 128]\n",
    "past_days = [1,2,4,7,14,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(1, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(1, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 1, 50)             28600     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 1, 75)             37800     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 1, 75)             0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1992 samples, validate on 222 samples\n",
      "Epoch 1/1000\n",
      "1992/1992 [==============================] - 1s 672us/step - loss: 0.6927 - accuracy: 0.5146 - val_loss: 0.6900 - val_accuracy: 0.5586\n",
      "Epoch 2/1000\n",
      "1992/1992 [==============================] - 0s 87us/step - loss: 0.6921 - accuracy: 0.5231 - val_loss: 0.6890 - val_accuracy: 0.5586\n",
      "Epoch 3/1000\n",
      "1992/1992 [==============================] - 0s 76us/step - loss: 0.6922 - accuracy: 0.5231 - val_loss: 0.6893 - val_accuracy: 0.5586\n",
      "Epoch 4/1000\n",
      "1992/1992 [==============================] - 0s 84us/step - loss: 0.6922 - accuracy: 0.5231 - val_loss: 0.6882 - val_accuracy: 0.5586\n",
      "Epoch 5/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6919 - accuracy: 0.5231 - val_loss: 0.6877 - val_accuracy: 0.5586\n",
      "Epoch 6/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6919 - accuracy: 0.5226 - val_loss: 0.6881 - val_accuracy: 0.5586\n",
      "Epoch 7/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6920 - accuracy: 0.5236 - val_loss: 0.6867 - val_accuracy: 0.5586\n",
      "Epoch 8/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6912 - accuracy: 0.5246 - val_loss: 0.6888 - val_accuracy: 0.5541\n",
      "Epoch 9/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6911 - accuracy: 0.5326 - val_loss: 0.6860 - val_accuracy: 0.5541\n",
      "Epoch 10/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6912 - accuracy: 0.5377 - val_loss: 0.6904 - val_accuracy: 0.5360\n",
      "Epoch 11/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6912 - accuracy: 0.5266 - val_loss: 0.6870 - val_accuracy: 0.5631\n",
      "Epoch 12/1000\n",
      "1992/1992 [==============================] - 0s 76us/step - loss: 0.6911 - accuracy: 0.5261 - val_loss: 0.6882 - val_accuracy: 0.5541\n",
      "Epoch 13/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6909 - accuracy: 0.5291 - val_loss: 0.6885 - val_accuracy: 0.5631\n",
      "Epoch 14/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6907 - accuracy: 0.5296 - val_loss: 0.6852 - val_accuracy: 0.5586\n",
      "Epoch 15/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6908 - accuracy: 0.5281 - val_loss: 0.6884 - val_accuracy: 0.5766\n",
      "Epoch 16/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6901 - accuracy: 0.5236 - val_loss: 0.6898 - val_accuracy: 0.5721\n",
      "Epoch 17/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6908 - accuracy: 0.5311 - val_loss: 0.6859 - val_accuracy: 0.5586\n",
      "Epoch 18/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6907 - accuracy: 0.5256 - val_loss: 0.6873 - val_accuracy: 0.5766\n",
      "Epoch 19/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6903 - accuracy: 0.5361 - val_loss: 0.6933 - val_accuracy: 0.5270\n",
      "Epoch 20/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6911 - accuracy: 0.5241 - val_loss: 0.6867 - val_accuracy: 0.5766\n",
      "Epoch 21/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6901 - accuracy: 0.5301 - val_loss: 0.6878 - val_accuracy: 0.5676\n",
      "Epoch 22/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6886 - accuracy: 0.5377 - val_loss: 0.6856 - val_accuracy: 0.5811\n",
      "Epoch 23/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6894 - accuracy: 0.5437 - val_loss: 0.6867 - val_accuracy: 0.5721\n",
      "Epoch 24/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6895 - accuracy: 0.5341 - val_loss: 0.6865 - val_accuracy: 0.5721\n",
      "Epoch 25/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6889 - accuracy: 0.5402 - val_loss: 0.6867 - val_accuracy: 0.5721\n",
      "Epoch 26/1000\n",
      "1992/1992 [==============================] - 0s 70us/step - loss: 0.6896 - accuracy: 0.5472 - val_loss: 0.6925 - val_accuracy: 0.5315\n",
      "Epoch 27/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6868 - accuracy: 0.5412 - val_loss: 0.6892 - val_accuracy: 0.5586\n",
      "Epoch 28/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6889 - accuracy: 0.5296 - val_loss: 0.6892 - val_accuracy: 0.5721\n",
      "Epoch 29/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6890 - accuracy: 0.5397 - val_loss: 0.6893 - val_accuracy: 0.5631\n",
      "Epoch 30/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6865 - accuracy: 0.5442 - val_loss: 0.6978 - val_accuracy: 0.5135\n",
      "Epoch 31/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6872 - accuracy: 0.5387 - val_loss: 0.6912 - val_accuracy: 0.5676\n",
      "Epoch 32/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6874 - accuracy: 0.5417 - val_loss: 0.6940 - val_accuracy: 0.5541\n",
      "Epoch 33/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6860 - accuracy: 0.5658 - val_loss: 0.6967 - val_accuracy: 0.5586\n",
      "Epoch 34/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6908 - accuracy: 0.5366 - val_loss: 0.6974 - val_accuracy: 0.5000\n",
      "Epoch 35/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6871 - accuracy: 0.5457 - val_loss: 0.6960 - val_accuracy: 0.5090\n",
      "Epoch 36/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6854 - accuracy: 0.5582 - val_loss: 0.6964 - val_accuracy: 0.4910\n",
      "Epoch 37/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6848 - accuracy: 0.5577 - val_loss: 0.7026 - val_accuracy: 0.4910\n",
      "Epoch 38/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6893 - accuracy: 0.5397 - val_loss: 0.7232 - val_accuracy: 0.4234\n",
      "Epoch 39/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6890 - accuracy: 0.5261 - val_loss: 0.6910 - val_accuracy: 0.5856\n",
      "Epoch 40/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6863 - accuracy: 0.5437 - val_loss: 0.6940 - val_accuracy: 0.5270\n",
      "Epoch 41/1000\n",
      "1992/1992 [==============================] - 0s 71us/step - loss: 0.6850 - accuracy: 0.5472 - val_loss: 0.6963 - val_accuracy: 0.5631\n",
      "Epoch 42/1000\n",
      "1992/1992 [==============================] - 0s 71us/step - loss: 0.6858 - accuracy: 0.5356 - val_loss: 0.6986 - val_accuracy: 0.5811\n",
      "Epoch 43/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6846 - accuracy: 0.5577 - val_loss: 0.7040 - val_accuracy: 0.4865\n",
      "Epoch 44/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6849 - accuracy: 0.5377 - val_loss: 0.7025 - val_accuracy: 0.5541\n",
      "Epoch 45/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6874 - accuracy: 0.5502 - val_loss: 0.6983 - val_accuracy: 0.5856\n",
      "Epoch 46/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6859 - accuracy: 0.5572 - val_loss: 0.6999 - val_accuracy: 0.5856\n",
      "Epoch 47/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6896 - accuracy: 0.5341 - val_loss: 0.6942 - val_accuracy: 0.5901\n",
      "Epoch 48/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6833 - accuracy: 0.5502 - val_loss: 0.7024 - val_accuracy: 0.4820\n",
      "Epoch 49/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6824 - accuracy: 0.5693 - val_loss: 0.7131 - val_accuracy: 0.4685\n",
      "Epoch 50/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6842 - accuracy: 0.5336 - val_loss: 0.7026 - val_accuracy: 0.4910\n",
      "Epoch 51/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6844 - accuracy: 0.5487 - val_loss: 0.7070 - val_accuracy: 0.4640\n",
      "Epoch 52/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6816 - accuracy: 0.5638 - val_loss: 0.7017 - val_accuracy: 0.5315\n",
      "Epoch 53/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6807 - accuracy: 0.5532 - val_loss: 0.7121 - val_accuracy: 0.4955\n",
      "Epoch 54/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6795 - accuracy: 0.5643 - val_loss: 0.7163 - val_accuracy: 0.4730\n",
      "Epoch 55/1000\n",
      "1992/1992 [==============================] - 0s 72us/step - loss: 0.6789 - accuracy: 0.5638 - val_loss: 0.7157 - val_accuracy: 0.4865\n",
      "Epoch 56/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6779 - accuracy: 0.5658 - val_loss: 0.7176 - val_accuracy: 0.4775\n",
      "Epoch 57/1000\n",
      "1992/1992 [==============================] - 0s 79us/step - loss: 0.6837 - accuracy: 0.5356 - val_loss: 0.7125 - val_accuracy: 0.5045\n",
      "Epoch 58/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6811 - accuracy: 0.5643 - val_loss: 0.7087 - val_accuracy: 0.4955\n",
      "Epoch 59/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6785 - accuracy: 0.5628 - val_loss: 0.7109 - val_accuracy: 0.5135\n",
      "Epoch 60/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6794 - accuracy: 0.5482 - val_loss: 0.7119 - val_accuracy: 0.5045\n",
      "Epoch 61/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6782 - accuracy: 0.5633 - val_loss: 0.7140 - val_accuracy: 0.5586\n",
      "Epoch 62/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6796 - accuracy: 0.5693 - val_loss: 0.7178 - val_accuracy: 0.4640\n",
      "Epoch 63/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6782 - accuracy: 0.5653 - val_loss: 0.7081 - val_accuracy: 0.5270\n",
      "Epoch 64/1000\n",
      "1992/1992 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.55 - 0s 66us/step - loss: 0.6758 - accuracy: 0.5572 - val_loss: 0.7181 - val_accuracy: 0.5090\n",
      "Epoch 65/1000\n",
      "1992/1992 [==============================] - 0s 76us/step - loss: 0.6805 - accuracy: 0.5683 - val_loss: 0.7083 - val_accuracy: 0.5225\n",
      "Epoch 66/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6775 - accuracy: 0.5678 - val_loss: 0.7230 - val_accuracy: 0.4505\n",
      "Epoch 67/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6778 - accuracy: 0.5517 - val_loss: 0.7158 - val_accuracy: 0.4775\n",
      "Epoch 68/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6811 - accuracy: 0.5587 - val_loss: 0.7080 - val_accuracy: 0.5045\n",
      "Epoch 69/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6768 - accuracy: 0.5622 - val_loss: 0.7215 - val_accuracy: 0.4459\n",
      "Epoch 70/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6764 - accuracy: 0.5617 - val_loss: 0.7196 - val_accuracy: 0.5090\n",
      "Epoch 71/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6766 - accuracy: 0.5633 - val_loss: 0.7122 - val_accuracy: 0.5135\n",
      "Epoch 72/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6761 - accuracy: 0.5552 - val_loss: 0.7191 - val_accuracy: 0.5270\n",
      "Epoch 73/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6764 - accuracy: 0.5688 - val_loss: 0.7139 - val_accuracy: 0.5225\n",
      "Epoch 74/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6740 - accuracy: 0.5602 - val_loss: 0.7216 - val_accuracy: 0.5405\n",
      "Epoch 75/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6754 - accuracy: 0.5693 - val_loss: 0.7096 - val_accuracy: 0.5045\n",
      "Epoch 76/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6747 - accuracy: 0.5668 - val_loss: 0.7186 - val_accuracy: 0.5360\n",
      "Epoch 77/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6702 - accuracy: 0.5798 - val_loss: 0.7370 - val_accuracy: 0.4640\n",
      "Epoch 78/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6742 - accuracy: 0.5673 - val_loss: 0.7131 - val_accuracy: 0.5360\n",
      "Epoch 79/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6750 - accuracy: 0.5587 - val_loss: 0.7141 - val_accuracy: 0.5225\n",
      "Epoch 80/1000\n",
      "1992/1992 [==============================] - 0s 74us/step - loss: 0.6740 - accuracy: 0.5668 - val_loss: 0.7125 - val_accuracy: 0.5315\n",
      "Epoch 81/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6707 - accuracy: 0.5653 - val_loss: 0.7374 - val_accuracy: 0.4505\n",
      "Epoch 82/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6734 - accuracy: 0.5688 - val_loss: 0.7255 - val_accuracy: 0.5405\n",
      "Epoch 83/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6763 - accuracy: 0.5567 - val_loss: 0.7110 - val_accuracy: 0.5270\n",
      "Epoch 84/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6661 - accuracy: 0.5808 - val_loss: 0.7432 - val_accuracy: 0.4640\n",
      "Epoch 85/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6698 - accuracy: 0.5582 - val_loss: 0.7202 - val_accuracy: 0.4640\n",
      "Epoch 86/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6717 - accuracy: 0.5733 - val_loss: 0.7174 - val_accuracy: 0.5450\n",
      "Epoch 87/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6700 - accuracy: 0.5773 - val_loss: 0.7176 - val_accuracy: 0.4955\n",
      "Epoch 88/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6689 - accuracy: 0.5763 - val_loss: 0.7167 - val_accuracy: 0.5225\n",
      "Epoch 89/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6654 - accuracy: 0.5783 - val_loss: 0.7213 - val_accuracy: 0.4910\n",
      "Epoch 90/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6640 - accuracy: 0.5753 - val_loss: 0.7435 - val_accuracy: 0.5495\n",
      "Epoch 91/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6682 - accuracy: 0.5713 - val_loss: 0.7318 - val_accuracy: 0.4369\n",
      "Epoch 92/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6653 - accuracy: 0.5663 - val_loss: 0.7276 - val_accuracy: 0.5180\n",
      "Epoch 93/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6667 - accuracy: 0.5818 - val_loss: 0.7204 - val_accuracy: 0.5360\n",
      "Epoch 94/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6632 - accuracy: 0.5838 - val_loss: 0.7387 - val_accuracy: 0.5180\n",
      "Epoch 95/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6719 - accuracy: 0.5547 - val_loss: 0.7116 - val_accuracy: 0.5315\n",
      "Epoch 96/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6632 - accuracy: 0.5873 - val_loss: 0.7278 - val_accuracy: 0.4865\n",
      "Epoch 97/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6651 - accuracy: 0.5798 - val_loss: 0.7198 - val_accuracy: 0.5225\n",
      "Epoch 98/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6632 - accuracy: 0.5838 - val_loss: 0.7297 - val_accuracy: 0.5180\n",
      "Epoch 99/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6614 - accuracy: 0.5803 - val_loss: 0.7279 - val_accuracy: 0.5450\n",
      "Epoch 100/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6618 - accuracy: 0.5949 - val_loss: 0.7454 - val_accuracy: 0.4414\n",
      "Epoch 101/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6656 - accuracy: 0.5753 - val_loss: 0.7161 - val_accuracy: 0.5495\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6610 - accuracy: 0.5863 - val_loss: 0.7390 - val_accuracy: 0.5270\n",
      "Epoch 103/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6605 - accuracy: 0.5813 - val_loss: 0.7369 - val_accuracy: 0.4595\n",
      "Epoch 104/1000\n",
      "1992/1992 [==============================] - 0s 69us/step - loss: 0.6604 - accuracy: 0.5848 - val_loss: 0.7326 - val_accuracy: 0.5360\n",
      "Epoch 105/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6601 - accuracy: 0.5949 - val_loss: 0.7259 - val_accuracy: 0.4955\n",
      "Epoch 106/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6611 - accuracy: 0.5863 - val_loss: 0.7187 - val_accuracy: 0.5315\n",
      "Epoch 107/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6657 - accuracy: 0.5713 - val_loss: 0.7192 - val_accuracy: 0.5180\n",
      "Epoch 108/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6565 - accuracy: 0.5959 - val_loss: 0.7592 - val_accuracy: 0.4730\n",
      "Epoch 109/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6577 - accuracy: 0.5914 - val_loss: 0.7507 - val_accuracy: 0.5270\n",
      "Epoch 110/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6536 - accuracy: 0.5964 - val_loss: 0.7417 - val_accuracy: 0.5180\n",
      "Epoch 111/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6545 - accuracy: 0.5984 - val_loss: 0.7448 - val_accuracy: 0.4865\n",
      "Epoch 112/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6555 - accuracy: 0.5863 - val_loss: 0.7509 - val_accuracy: 0.5360\n",
      "Epoch 113/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6575 - accuracy: 0.5868 - val_loss: 0.7547 - val_accuracy: 0.5045\n",
      "Epoch 114/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6642 - accuracy: 0.5798 - val_loss: 0.7130 - val_accuracy: 0.5225\n",
      "Epoch 115/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6573 - accuracy: 0.5959 - val_loss: 0.7538 - val_accuracy: 0.5045\n",
      "Epoch 116/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6512 - accuracy: 0.5934 - val_loss: 0.7384 - val_accuracy: 0.5405\n",
      "Epoch 117/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6613 - accuracy: 0.5848 - val_loss: 0.7428 - val_accuracy: 0.4820\n",
      "Epoch 118/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6620 - accuracy: 0.5818 - val_loss: 0.7342 - val_accuracy: 0.5225\n",
      "Epoch 119/1000\n",
      "1992/1992 [==============================] - 0s 62us/step - loss: 0.6545 - accuracy: 0.5939 - val_loss: 0.7390 - val_accuracy: 0.5225\n",
      "Epoch 120/1000\n",
      "1992/1992 [==============================] - 0s 69us/step - loss: 0.6556 - accuracy: 0.5979 - val_loss: 0.7444 - val_accuracy: 0.5045\n",
      "Epoch 121/1000\n",
      "1992/1992 [==============================] - 0s 69us/step - loss: 0.6503 - accuracy: 0.6019 - val_loss: 0.7497 - val_accuracy: 0.5225\n",
      "Epoch 122/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6552 - accuracy: 0.5964 - val_loss: 0.7327 - val_accuracy: 0.5541\n",
      "Epoch 123/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6564 - accuracy: 0.5879 - val_loss: 0.7505 - val_accuracy: 0.4955\n",
      "Epoch 124/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6565 - accuracy: 0.5989 - val_loss: 0.7390 - val_accuracy: 0.5045\n",
      "Epoch 125/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6529 - accuracy: 0.5924 - val_loss: 0.7382 - val_accuracy: 0.5090\n",
      "Epoch 126/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6472 - accuracy: 0.6145 - val_loss: 0.7779 - val_accuracy: 0.5180\n",
      "Epoch 127/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6533 - accuracy: 0.5984 - val_loss: 0.7587 - val_accuracy: 0.5225\n",
      "Epoch 128/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6474 - accuracy: 0.6094 - val_loss: 0.7508 - val_accuracy: 0.5315\n",
      "Epoch 129/1000\n",
      "1992/1992 [==============================] - 0s 72us/step - loss: 0.6517 - accuracy: 0.5934 - val_loss: 0.7539 - val_accuracy: 0.5000\n",
      "Epoch 130/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6508 - accuracy: 0.5929 - val_loss: 0.7695 - val_accuracy: 0.5045\n",
      "Epoch 131/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6501 - accuracy: 0.6029 - val_loss: 0.7406 - val_accuracy: 0.5045\n",
      "Epoch 132/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6498 - accuracy: 0.5994 - val_loss: 0.7387 - val_accuracy: 0.5135\n",
      "Epoch 133/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6553 - accuracy: 0.5873 - val_loss: 0.7631 - val_accuracy: 0.4865\n",
      "Epoch 134/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6546 - accuracy: 0.5868 - val_loss: 0.7362 - val_accuracy: 0.5000\n",
      "Epoch 135/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6653 - accuracy: 0.5758 - val_loss: 0.7165 - val_accuracy: 0.4955\n",
      "Epoch 136/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6538 - accuracy: 0.6004 - val_loss: 0.7423 - val_accuracy: 0.4910\n",
      "Epoch 137/1000\n",
      "1992/1992 [==============================] - 0s 71us/step - loss: 0.6474 - accuracy: 0.6019 - val_loss: 0.7507 - val_accuracy: 0.5000\n",
      "Epoch 138/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6474 - accuracy: 0.6084 - val_loss: 0.7550 - val_accuracy: 0.4955\n",
      "Epoch 139/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6443 - accuracy: 0.6029 - val_loss: 0.7653 - val_accuracy: 0.5360\n",
      "Epoch 140/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6454 - accuracy: 0.6004 - val_loss: 0.7384 - val_accuracy: 0.5090\n",
      "Epoch 141/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6501 - accuracy: 0.6019 - val_loss: 0.7672 - val_accuracy: 0.4505\n",
      "Epoch 142/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6435 - accuracy: 0.5999 - val_loss: 0.7440 - val_accuracy: 0.5090\n",
      "Epoch 143/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6486 - accuracy: 0.5929 - val_loss: 0.7686 - val_accuracy: 0.5090\n",
      "Epoch 144/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6421 - accuracy: 0.6135 - val_loss: 0.7930 - val_accuracy: 0.4865\n",
      "Epoch 145/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6478 - accuracy: 0.5924 - val_loss: 0.7442 - val_accuracy: 0.5090\n",
      "Epoch 146/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6561 - accuracy: 0.5884 - val_loss: 0.7432 - val_accuracy: 0.5045\n",
      "Epoch 147/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6426 - accuracy: 0.6165 - val_loss: 0.7678 - val_accuracy: 0.5180\n",
      "Epoch 148/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6483 - accuracy: 0.6054 - val_loss: 0.7547 - val_accuracy: 0.5135\n",
      "Epoch 149/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6426 - accuracy: 0.6064 - val_loss: 0.7625 - val_accuracy: 0.5045\n",
      "Epoch 150/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6435 - accuracy: 0.5969 - val_loss: 0.7482 - val_accuracy: 0.5135\n",
      "Epoch 151/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6415 - accuracy: 0.6124 - val_loss: 0.7544 - val_accuracy: 0.5180\n",
      "Epoch 152/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6424 - accuracy: 0.6104 - val_loss: 0.7831 - val_accuracy: 0.4955\n",
      "Epoch 153/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6446 - accuracy: 0.6059 - val_loss: 0.7689 - val_accuracy: 0.4685\n",
      "Epoch 154/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6464 - accuracy: 0.6064 - val_loss: 0.7492 - val_accuracy: 0.5045\n",
      "Epoch 155/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6425 - accuracy: 0.6054 - val_loss: 0.7478 - val_accuracy: 0.5180\n",
      "Epoch 156/1000\n",
      "1992/1992 [==============================] - 0s 72us/step - loss: 0.6416 - accuracy: 0.6044 - val_loss: 0.7819 - val_accuracy: 0.5225\n",
      "Epoch 157/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6387 - accuracy: 0.6170 - val_loss: 0.7849 - val_accuracy: 0.5270\n",
      "Epoch 158/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6424 - accuracy: 0.6049 - val_loss: 0.7858 - val_accuracy: 0.5000\n",
      "Epoch 159/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6400 - accuracy: 0.6044 - val_loss: 0.7672 - val_accuracy: 0.5270\n",
      "Epoch 160/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6401 - accuracy: 0.6049 - val_loss: 0.8022 - val_accuracy: 0.4910\n",
      "Epoch 161/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6412 - accuracy: 0.6160 - val_loss: 0.7493 - val_accuracy: 0.5180\n",
      "Epoch 162/1000\n",
      "1992/1992 [==============================] - 0s 76us/step - loss: 0.6389 - accuracy: 0.6150 - val_loss: 0.8286 - val_accuracy: 0.4820\n",
      "Epoch 163/1000\n",
      "1992/1992 [==============================] - 0s 82us/step - loss: 0.6408 - accuracy: 0.5989 - val_loss: 0.7817 - val_accuracy: 0.5450\n",
      "Epoch 164/1000\n",
      "1992/1992 [==============================] - 0s 79us/step - loss: 0.6349 - accuracy: 0.6180 - val_loss: 0.7751 - val_accuracy: 0.5000\n",
      "Epoch 165/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6452 - accuracy: 0.6034 - val_loss: 0.7482 - val_accuracy: 0.4640\n",
      "Epoch 166/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6404 - accuracy: 0.6029 - val_loss: 0.7665 - val_accuracy: 0.5405\n",
      "Epoch 167/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6431 - accuracy: 0.5974 - val_loss: 0.7735 - val_accuracy: 0.5225\n",
      "Epoch 168/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6345 - accuracy: 0.6145 - val_loss: 0.7886 - val_accuracy: 0.4910\n",
      "Epoch 169/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6355 - accuracy: 0.6135 - val_loss: 0.7796 - val_accuracy: 0.5225\n",
      "Epoch 170/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6349 - accuracy: 0.6104 - val_loss: 0.7967 - val_accuracy: 0.5000\n",
      "Epoch 171/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6340 - accuracy: 0.6044 - val_loss: 0.7949 - val_accuracy: 0.5270\n",
      "Epoch 172/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6323 - accuracy: 0.6155 - val_loss: 0.8035 - val_accuracy: 0.5000\n",
      "Epoch 173/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6347 - accuracy: 0.6064 - val_loss: 0.8007 - val_accuracy: 0.5135\n",
      "Epoch 174/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6373 - accuracy: 0.6064 - val_loss: 0.7750 - val_accuracy: 0.5315\n",
      "Epoch 175/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6400 - accuracy: 0.6014 - val_loss: 0.8038 - val_accuracy: 0.4865\n",
      "Epoch 176/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6370 - accuracy: 0.5994 - val_loss: 0.7609 - val_accuracy: 0.5360\n",
      "Epoch 177/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6338 - accuracy: 0.6215 - val_loss: 0.7797 - val_accuracy: 0.5225\n",
      "Epoch 178/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6328 - accuracy: 0.6175 - val_loss: 0.7945 - val_accuracy: 0.4775\n",
      "Epoch 179/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6386 - accuracy: 0.5889 - val_loss: 0.7480 - val_accuracy: 0.5270\n",
      "Epoch 180/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6404 - accuracy: 0.6064 - val_loss: 0.7837 - val_accuracy: 0.4865\n",
      "Epoch 181/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6351 - accuracy: 0.6059 - val_loss: 0.7851 - val_accuracy: 0.4955\n",
      "Epoch 182/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6294 - accuracy: 0.6094 - val_loss: 0.7707 - val_accuracy: 0.5270\n",
      "Epoch 183/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6325 - accuracy: 0.6104 - val_loss: 0.7797 - val_accuracy: 0.5045\n",
      "Epoch 184/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6279 - accuracy: 0.6109 - val_loss: 0.7672 - val_accuracy: 0.4550\n",
      "Epoch 185/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6355 - accuracy: 0.6155 - val_loss: 0.7885 - val_accuracy: 0.5045\n",
      "Epoch 186/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6295 - accuracy: 0.6245 - val_loss: 0.7766 - val_accuracy: 0.5045\n",
      "Epoch 187/1000\n",
      "1992/1992 [==============================] - 0s 66us/step - loss: 0.6270 - accuracy: 0.6295 - val_loss: 0.8240 - val_accuracy: 0.5000\n",
      "Epoch 188/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6317 - accuracy: 0.6140 - val_loss: 0.8110 - val_accuracy: 0.4820\n",
      "Epoch 189/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6270 - accuracy: 0.6175 - val_loss: 0.7992 - val_accuracy: 0.4955\n",
      "Epoch 190/1000\n",
      "1992/1992 [==============================] - 0s 63us/step - loss: 0.6298 - accuracy: 0.6195 - val_loss: 0.8207 - val_accuracy: 0.4865\n",
      "Epoch 191/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6243 - accuracy: 0.6270 - val_loss: 0.8311 - val_accuracy: 0.4685\n",
      "Epoch 192/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6307 - accuracy: 0.6260 - val_loss: 0.8166 - val_accuracy: 0.5090\n",
      "Epoch 193/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6291 - accuracy: 0.6205 - val_loss: 0.8536 - val_accuracy: 0.4550\n",
      "Epoch 194/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6380 - accuracy: 0.6185 - val_loss: 0.7622 - val_accuracy: 0.5135\n",
      "Epoch 195/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6327 - accuracy: 0.6215 - val_loss: 0.8067 - val_accuracy: 0.5135\n",
      "Epoch 196/1000\n",
      "1992/1992 [==============================] - 0s 65us/step - loss: 0.6261 - accuracy: 0.6295 - val_loss: 0.7621 - val_accuracy: 0.4955\n",
      "Epoch 197/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6217 - accuracy: 0.6220 - val_loss: 0.8431 - val_accuracy: 0.4775\n",
      "Epoch 198/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6290 - accuracy: 0.6160 - val_loss: 0.7951 - val_accuracy: 0.4955\n",
      "Epoch 199/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6277 - accuracy: 0.6089 - val_loss: 0.8024 - val_accuracy: 0.4910\n",
      "Epoch 200/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6329 - accuracy: 0.6150 - val_loss: 0.7714 - val_accuracy: 0.4775\n",
      "Epoch 201/1000\n",
      "1992/1992 [==============================] - 0s 64us/step - loss: 0.6224 - accuracy: 0.6320 - val_loss: 0.8259 - val_accuracy: 0.5045\n",
      "Epoch 202/1000\n",
      "1992/1992 [==============================] - 0s 67us/step - loss: 0.6274 - accuracy: 0.6220 - val_loss: 0.7602 - val_accuracy: 0.4595\n",
      "Epoch 203/1000\n",
      "1992/1992 [==============================] - 0s 68us/step - loss: 0.6240 - accuracy: 0.6245 - val_loss: 0.8787 - val_accuracy: 0.5000\n",
      "Epoch 204/1000\n",
      "1992/1992 [==============================] - 0s 97us/step - loss: 0.6271 - accuracy: 0.6064 - val_loss: 0.8038 - val_accuracy: 0.5180\n",
      "Epoch 205/1000\n",
      "1992/1992 [==============================] - 0s 88us/step - loss: 0.6254 - accuracy: 0.6225 - val_loss: 0.8270 - val_accuracy: 0.4550\n",
      "Epoch 206/1000\n",
      "1992/1992 [==============================] - 0s 83us/step - loss: 0.6280 - accuracy: 0.6135 - val_loss: 0.7928 - val_accuracy: 0.4955\n",
      "Epoch 207/1000\n",
      "1992/1992 [==============================] - 0s 85us/step - loss: 0.6195 - accuracy: 0.6205 - val_loss: 0.7941 - val_accuracy: 0.4730\n",
      "Epoch 208/1000\n",
      "1992/1992 [==============================] - 0s 75us/step - loss: 0.6163 - accuracy: 0.6330 - val_loss: 0.8188 - val_accuracy: 0.4775\n",
      "Epoch 209/1000\n",
      "1992/1992 [==============================] - 0s 75us/step - loss: 0.6217 - accuracy: 0.6099 - val_loss: 0.8215 - val_accuracy: 0.5045\n",
      "Epoch 210/1000\n",
      "1992/1992 [==============================] - 0s 77us/step - loss: 0.6323 - accuracy: 0.6094 - val_loss: 0.7974 - val_accuracy: 0.5045\n",
      "Epoch 211/1000\n",
      "1992/1992 [==============================] - 0s 94us/step - loss: 0.6188 - accuracy: 0.6175 - val_loss: 0.8645 - val_accuracy: 0.4865\n",
      "Epoch 212/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992/1992 [==============================] - 0s 69us/step - loss: 0.6227 - accuracy: 0.6205 - val_loss: 0.8381 - val_accuracy: 0.5360\n",
      "Epoch 213/1000\n",
      "1992/1992 [==============================] - 0s 70us/step - loss: 0.6167 - accuracy: 0.6235 - val_loss: 0.8567 - val_accuracy: 0.4640\n",
      "Epoch 214/1000\n",
      "1992/1992 [==============================] - 0s 79us/step - loss: 0.6206 - accuracy: 0.6265 - val_loss: 0.8102 - val_accuracy: 0.5000\n",
      "Epoch 00214: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a424eceb8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAST_DAYS = 1\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "246/246 [==============================] - 0s 49us/step\n",
      "test loss, test acc: [0.881428475787, 0.5325203537940979]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (246, 1)\n",
      "rmse: 0.5213076995516022\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Without Absolute Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(7, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(7, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 7, 50)             28600     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 7, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 7, 75)             37800     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 7, 75)             0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1987 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1987/1987 [==============================] - 2s 1ms/step - loss: 0.6939 - accuracy: 0.5048 - val_loss: 0.6880 - val_accuracy: 0.5747\n",
      "Epoch 2/1000\n",
      "1987/1987 [==============================] - 1s 271us/step - loss: 0.6935 - accuracy: 0.5118 - val_loss: 0.6922 - val_accuracy: 0.5837\n",
      "Epoch 3/1000\n",
      "1987/1987 [==============================] - 1s 308us/step - loss: 0.6933 - accuracy: 0.5239 - val_loss: 0.6900 - val_accuracy: 0.5747\n",
      "Epoch 4/1000\n",
      "1987/1987 [==============================] - 1s 254us/step - loss: 0.6929 - accuracy: 0.5133 - val_loss: 0.6856 - val_accuracy: 0.5747\n",
      "Epoch 5/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6922 - accuracy: 0.5219 - val_loss: 0.6874 - val_accuracy: 0.5747\n",
      "Epoch 6/1000\n",
      "1987/1987 [==============================] - 0s 249us/step - loss: 0.6933 - accuracy: 0.5038 - val_loss: 0.6869 - val_accuracy: 0.5747\n",
      "Epoch 7/1000\n",
      "1987/1987 [==============================] - 1s 262us/step - loss: 0.6936 - accuracy: 0.5229 - val_loss: 0.6902 - val_accuracy: 0.5747\n",
      "Epoch 8/1000\n",
      "1987/1987 [==============================] - 0s 249us/step - loss: 0.6922 - accuracy: 0.5219 - val_loss: 0.6854 - val_accuracy: 0.5747\n",
      "Epoch 9/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6928 - accuracy: 0.5219 - val_loss: 0.6875 - val_accuracy: 0.5747\n",
      "Epoch 10/1000\n",
      "1987/1987 [==============================] - 0s 234us/step - loss: 0.6922 - accuracy: 0.5219 - val_loss: 0.6868 - val_accuracy: 0.5747\n",
      "Epoch 11/1000\n",
      "1987/1987 [==============================] - 0s 246us/step - loss: 0.6922 - accuracy: 0.5219 - val_loss: 0.6859 - val_accuracy: 0.5747\n",
      "Epoch 12/1000\n",
      "1987/1987 [==============================] - 0s 245us/step - loss: 0.6929 - accuracy: 0.5184 - val_loss: 0.6875 - val_accuracy: 0.5747\n",
      "Epoch 13/1000\n",
      "1987/1987 [==============================] - 0s 243us/step - loss: 0.6924 - accuracy: 0.5219 - val_loss: 0.6873 - val_accuracy: 0.5747\n",
      "Epoch 14/1000\n",
      "1987/1987 [==============================] - 1s 256us/step - loss: 0.6919 - accuracy: 0.5239 - val_loss: 0.6890 - val_accuracy: 0.5792\n",
      "Epoch 15/1000\n",
      "1987/1987 [==============================] - 1s 262us/step - loss: 0.6913 - accuracy: 0.5335 - val_loss: 0.6858 - val_accuracy: 0.5747\n",
      "Epoch 16/1000\n",
      "1987/1987 [==============================] - 0s 235us/step - loss: 0.6917 - accuracy: 0.5234 - val_loss: 0.6874 - val_accuracy: 0.5656\n",
      "Epoch 17/1000\n",
      "1987/1987 [==============================] - 0s 241us/step - loss: 0.6924 - accuracy: 0.5204 - val_loss: 0.6859 - val_accuracy: 0.5747\n",
      "Epoch 18/1000\n",
      "1987/1987 [==============================] - 0s 230us/step - loss: 0.6926 - accuracy: 0.5224 - val_loss: 0.6904 - val_accuracy: 0.5747\n",
      "Epoch 19/1000\n",
      "1987/1987 [==============================] - 0s 243us/step - loss: 0.6920 - accuracy: 0.5234 - val_loss: 0.6884 - val_accuracy: 0.5792\n",
      "Epoch 20/1000\n",
      "1987/1987 [==============================] - 0s 244us/step - loss: 0.6916 - accuracy: 0.5264 - val_loss: 0.6858 - val_accuracy: 0.5747\n",
      "Epoch 21/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6916 - accuracy: 0.5345 - val_loss: 0.6866 - val_accuracy: 0.5701\n",
      "Epoch 22/1000\n",
      "1987/1987 [==============================] - 0s 249us/step - loss: 0.6915 - accuracy: 0.5330 - val_loss: 0.6927 - val_accuracy: 0.5294\n",
      "Epoch 23/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6919 - accuracy: 0.5204 - val_loss: 0.6874 - val_accuracy: 0.5656\n",
      "Epoch 24/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6911 - accuracy: 0.5350 - val_loss: 0.6888 - val_accuracy: 0.5611\n",
      "Epoch 25/1000\n",
      "1987/1987 [==============================] - 0s 245us/step - loss: 0.6918 - accuracy: 0.5315 - val_loss: 0.6880 - val_accuracy: 0.5656\n",
      "Epoch 26/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6915 - accuracy: 0.5249 - val_loss: 0.6873 - val_accuracy: 0.5747\n",
      "Epoch 27/1000\n",
      "1987/1987 [==============================] - 0s 247us/step - loss: 0.6912 - accuracy: 0.5299 - val_loss: 0.6894 - val_accuracy: 0.5701\n",
      "Epoch 28/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6860 - val_accuracy: 0.5747\n",
      "Epoch 29/1000\n",
      "1987/1987 [==============================] - 0s 247us/step - loss: 0.6909 - accuracy: 0.5375 - val_loss: 0.6864 - val_accuracy: 0.5656\n",
      "Epoch 30/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6917 - accuracy: 0.5335 - val_loss: 0.6860 - val_accuracy: 0.5656\n",
      "Epoch 31/1000\n",
      "1987/1987 [==============================] - 1s 256us/step - loss: 0.6906 - accuracy: 0.5390 - val_loss: 0.6894 - val_accuracy: 0.5656\n",
      "Epoch 32/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.6909 - accuracy: 0.5405 - val_loss: 0.6860 - val_accuracy: 0.5701\n",
      "Epoch 33/1000\n",
      "1987/1987 [==============================] - 1s 257us/step - loss: 0.6907 - accuracy: 0.5310 - val_loss: 0.6894 - val_accuracy: 0.5747\n",
      "Epoch 34/1000\n",
      "1987/1987 [==============================] - 1s 252us/step - loss: 0.6905 - accuracy: 0.5284 - val_loss: 0.6885 - val_accuracy: 0.5747\n",
      "Epoch 35/1000\n",
      "1987/1987 [==============================] - 1s 254us/step - loss: 0.6914 - accuracy: 0.5310 - val_loss: 0.6894 - val_accuracy: 0.5747\n",
      "Epoch 36/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6900 - accuracy: 0.5410 - val_loss: 0.6893 - val_accuracy: 0.5792\n",
      "Epoch 37/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6899 - accuracy: 0.5380 - val_loss: 0.6846 - val_accuracy: 0.5747\n",
      "Epoch 38/1000\n",
      "1987/1987 [==============================] - 1s 265us/step - loss: 0.6909 - accuracy: 0.5289 - val_loss: 0.6891 - val_accuracy: 0.5611\n",
      "Epoch 39/1000\n",
      "1987/1987 [==============================] - 1s 285us/step - loss: 0.6897 - accuracy: 0.5360 - val_loss: 0.6879 - val_accuracy: 0.5566\n",
      "Epoch 40/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.6900 - accuracy: 0.5370 - val_loss: 0.6860 - val_accuracy: 0.5747\n",
      "Epoch 41/1000\n",
      "1987/1987 [==============================] - 1s 278us/step - loss: 0.6910 - accuracy: 0.5320 - val_loss: 0.6890 - val_accuracy: 0.5747\n",
      "Epoch 42/1000\n",
      "1987/1987 [==============================] - 1s 273us/step - loss: 0.6898 - accuracy: 0.5395 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 43/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6895 - accuracy: 0.5460 - val_loss: 0.6875 - val_accuracy: 0.5747\n",
      "Epoch 44/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6890 - accuracy: 0.5325 - val_loss: 0.6888 - val_accuracy: 0.5701\n",
      "Epoch 45/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6883 - accuracy: 0.5415 - val_loss: 0.6930 - val_accuracy: 0.5566\n",
      "Epoch 46/1000\n",
      "1987/1987 [==============================] - 0s 245us/step - loss: 0.6891 - accuracy: 0.5355 - val_loss: 0.6918 - val_accuracy: 0.5566\n",
      "Epoch 47/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.6894 - accuracy: 0.5390 - val_loss: 0.6925 - val_accuracy: 0.5656\n",
      "Epoch 48/1000\n",
      "1987/1987 [==============================] - 0s 239us/step - loss: 0.6883 - accuracy: 0.5430 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 49/1000\n",
      "1987/1987 [==============================] - 0s 243us/step - loss: 0.6892 - accuracy: 0.5425 - val_loss: 0.6897 - val_accuracy: 0.5747\n",
      "Epoch 50/1000\n",
      "1987/1987 [==============================] - 0s 248us/step - loss: 0.6868 - accuracy: 0.5370 - val_loss: 0.6940 - val_accuracy: 0.5249\n",
      "Epoch 51/1000\n",
      "1987/1987 [==============================] - 0s 233us/step - loss: 0.6870 - accuracy: 0.5365 - val_loss: 0.6906 - val_accuracy: 0.5747\n",
      "Epoch 52/1000\n",
      "1987/1987 [==============================] - 0s 238us/step - loss: 0.6873 - accuracy: 0.5415 - val_loss: 0.6901 - val_accuracy: 0.5747\n",
      "Epoch 53/1000\n",
      "1987/1987 [==============================] - 0s 243us/step - loss: 0.6903 - accuracy: 0.5320 - val_loss: 0.6862 - val_accuracy: 0.5747\n",
      "Epoch 54/1000\n",
      "1987/1987 [==============================] - 0s 236us/step - loss: 0.6891 - accuracy: 0.5330 - val_loss: 0.6938 - val_accuracy: 0.5792\n",
      "Epoch 55/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6884 - accuracy: 0.5450 - val_loss: 0.6904 - val_accuracy: 0.5747\n",
      "Epoch 56/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6885 - accuracy: 0.5310 - val_loss: 0.6906 - val_accuracy: 0.5747\n",
      "Epoch 57/1000\n",
      "1987/1987 [==============================] - 1s 313us/step - loss: 0.6865 - accuracy: 0.5471 - val_loss: 0.6964 - val_accuracy: 0.5385\n",
      "Epoch 58/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6907 - accuracy: 0.5360 - val_loss: 0.6871 - val_accuracy: 0.5701\n",
      "Epoch 59/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6864 - accuracy: 0.5420 - val_loss: 0.6922 - val_accuracy: 0.5475\n",
      "Epoch 60/1000\n",
      "1987/1987 [==============================] - 1s 254us/step - loss: 0.6879 - accuracy: 0.5350 - val_loss: 0.6853 - val_accuracy: 0.5837\n",
      "Epoch 61/1000\n",
      "1987/1987 [==============================] - 1s 288us/step - loss: 0.6891 - accuracy: 0.5325 - val_loss: 0.6891 - val_accuracy: 0.5701\n",
      "Epoch 62/1000\n",
      "1987/1987 [==============================] - 1s 278us/step - loss: 0.6870 - accuracy: 0.5460 - val_loss: 0.6931 - val_accuracy: 0.5475\n",
      "Epoch 63/1000\n",
      "1987/1987 [==============================] - 1s 255us/step - loss: 0.6864 - accuracy: 0.5420 - val_loss: 0.6890 - val_accuracy: 0.5611\n",
      "Epoch 64/1000\n",
      "1987/1987 [==============================] - 0s 249us/step - loss: 0.6866 - accuracy: 0.5395 - val_loss: 0.6917 - val_accuracy: 0.5747\n",
      "Epoch 65/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6858 - accuracy: 0.5471 - val_loss: 0.6913 - val_accuracy: 0.5611\n",
      "Epoch 66/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6884 - accuracy: 0.5330 - val_loss: 0.6907 - val_accuracy: 0.5747\n",
      "Epoch 67/1000\n",
      "1987/1987 [==============================] - 0s 244us/step - loss: 0.6849 - accuracy: 0.5481 - val_loss: 0.6930 - val_accuracy: 0.5701\n",
      "Epoch 68/1000\n",
      "1987/1987 [==============================] - 0s 238us/step - loss: 0.6892 - accuracy: 0.5345 - val_loss: 0.6908 - val_accuracy: 0.5701\n",
      "Epoch 69/1000\n",
      "1987/1987 [==============================] - 0s 247us/step - loss: 0.6861 - accuracy: 0.5496 - val_loss: 0.6915 - val_accuracy: 0.5747\n",
      "Epoch 70/1000\n",
      "1987/1987 [==============================] - 0s 234us/step - loss: 0.6853 - accuracy: 0.5541 - val_loss: 0.7078 - val_accuracy: 0.4887\n",
      "Epoch 71/1000\n",
      "1987/1987 [==============================] - 0s 246us/step - loss: 0.6929 - accuracy: 0.5264 - val_loss: 0.6876 - val_accuracy: 0.5656\n",
      "Epoch 72/1000\n",
      "1987/1987 [==============================] - 0s 244us/step - loss: 0.6876 - accuracy: 0.5435 - val_loss: 0.6881 - val_accuracy: 0.5747\n",
      "Epoch 73/1000\n",
      "1987/1987 [==============================] - 1s 262us/step - loss: 0.6865 - accuracy: 0.5425 - val_loss: 0.6887 - val_accuracy: 0.5701\n",
      "Epoch 74/1000\n",
      "1987/1987 [==============================] - 1s 315us/step - loss: 0.6878 - accuracy: 0.5395 - val_loss: 0.6914 - val_accuracy: 0.5701\n",
      "Epoch 75/1000\n",
      "1987/1987 [==============================] - 1s 319us/step - loss: 0.6837 - accuracy: 0.5481 - val_loss: 0.6975 - val_accuracy: 0.5430\n",
      "Epoch 76/1000\n",
      "1987/1987 [==============================] - 1s 310us/step - loss: 0.6856 - accuracy: 0.5632 - val_loss: 0.6939 - val_accuracy: 0.5701\n",
      "Epoch 77/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6843 - accuracy: 0.5551 - val_loss: 0.6907 - val_accuracy: 0.5656\n",
      "Epoch 78/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6842 - accuracy: 0.5455 - val_loss: 0.6923 - val_accuracy: 0.5611\n",
      "Epoch 79/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.6836 - accuracy: 0.5541 - val_loss: 0.6913 - val_accuracy: 0.5611\n",
      "Epoch 80/1000\n",
      "1987/1987 [==============================] - 1s 268us/step - loss: 0.6843 - accuracy: 0.5511 - val_loss: 0.6939 - val_accuracy: 0.5747\n",
      "Epoch 81/1000\n",
      "1987/1987 [==============================] - 1s 293us/step - loss: 0.6863 - accuracy: 0.5435 - val_loss: 0.6936 - val_accuracy: 0.5656\n",
      "Epoch 82/1000\n",
      "1987/1987 [==============================] - 1s 255us/step - loss: 0.6851 - accuracy: 0.5415 - val_loss: 0.6984 - val_accuracy: 0.5294\n",
      "Epoch 83/1000\n",
      "1987/1987 [==============================] - 1s 260us/step - loss: 0.6864 - accuracy: 0.5486 - val_loss: 0.6930 - val_accuracy: 0.5747\n",
      "Epoch 84/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6843 - accuracy: 0.5445 - val_loss: 0.6992 - val_accuracy: 0.5249\n",
      "Epoch 85/1000\n",
      "1987/1987 [==============================] - 1s 272us/step - loss: 0.6845 - accuracy: 0.5440 - val_loss: 0.6947 - val_accuracy: 0.5611\n",
      "Epoch 86/1000\n",
      "1987/1987 [==============================] - 1s 275us/step - loss: 0.6837 - accuracy: 0.5526 - val_loss: 0.6998 - val_accuracy: 0.5339\n",
      "Epoch 87/1000\n",
      "1987/1987 [==============================] - 1s 270us/step - loss: 0.6844 - accuracy: 0.5491 - val_loss: 0.6950 - val_accuracy: 0.5656\n",
      "Epoch 88/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6834 - accuracy: 0.5420 - val_loss: 0.6962 - val_accuracy: 0.5701\n",
      "Epoch 89/1000\n",
      "1987/1987 [==============================] - 1s 296us/step - loss: 0.6809 - accuracy: 0.5551 - val_loss: 0.7071 - val_accuracy: 0.5430\n",
      "Epoch 90/1000\n",
      "1987/1987 [==============================] - 1s 293us/step - loss: 0.6837 - accuracy: 0.5476 - val_loss: 0.6963 - val_accuracy: 0.5747\n",
      "Epoch 91/1000\n",
      "1987/1987 [==============================] - 1s 271us/step - loss: 0.6812 - accuracy: 0.5657 - val_loss: 0.6962 - val_accuracy: 0.5430\n",
      "Epoch 92/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6796 - accuracy: 0.5556 - val_loss: 0.7010 - val_accuracy: 0.5611\n",
      "Epoch 93/1000\n",
      "1987/1987 [==============================] - 1s 256us/step - loss: 0.6803 - accuracy: 0.5531 - val_loss: 0.7016 - val_accuracy: 0.5113\n",
      "Epoch 94/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6859 - accuracy: 0.5435 - val_loss: 0.6929 - val_accuracy: 0.5520\n",
      "Epoch 95/1000\n",
      "1987/1987 [==============================] - 1s 258us/step - loss: 0.6834 - accuracy: 0.5486 - val_loss: 0.6977 - val_accuracy: 0.5611\n",
      "Epoch 96/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.6816 - accuracy: 0.5506 - val_loss: 0.6999 - val_accuracy: 0.5339\n",
      "Epoch 97/1000\n",
      "1987/1987 [==============================] - 1s 271us/step - loss: 0.6833 - accuracy: 0.5546 - val_loss: 0.6938 - val_accuracy: 0.5747\n",
      "Epoch 98/1000\n",
      "1987/1987 [==============================] - 1s 282us/step - loss: 0.6795 - accuracy: 0.5481 - val_loss: 0.7066 - val_accuracy: 0.4751\n",
      "Epoch 99/1000\n",
      "1987/1987 [==============================] - 1s 323us/step - loss: 0.6780 - accuracy: 0.5632 - val_loss: 0.7000 - val_accuracy: 0.5520\n",
      "Epoch 100/1000\n",
      "1987/1987 [==============================] - 1s 323us/step - loss: 0.6804 - accuracy: 0.5591 - val_loss: 0.7055 - val_accuracy: 0.5475\n",
      "Epoch 101/1000\n",
      "1987/1987 [==============================] - 1s 281us/step - loss: 0.6805 - accuracy: 0.5511 - val_loss: 0.7008 - val_accuracy: 0.5294\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987/1987 [==============================] - 1s 317us/step - loss: 0.6771 - accuracy: 0.5682 - val_loss: 0.7039 - val_accuracy: 0.5339\n",
      "Epoch 103/1000\n",
      "1987/1987 [==============================] - 1s 297us/step - loss: 0.6804 - accuracy: 0.5606 - val_loss: 0.7029 - val_accuracy: 0.5294\n",
      "Epoch 104/1000\n",
      "1987/1987 [==============================] - 1s 276us/step - loss: 0.6791 - accuracy: 0.5425 - val_loss: 0.7009 - val_accuracy: 0.5475\n",
      "Epoch 105/1000\n",
      "1987/1987 [==============================] - 1s 383us/step - loss: 0.6755 - accuracy: 0.5697 - val_loss: 0.7020 - val_accuracy: 0.5339\n",
      "Epoch 106/1000\n",
      "1987/1987 [==============================] - 1s 291us/step - loss: 0.6730 - accuracy: 0.5757 - val_loss: 0.7042 - val_accuracy: 0.5113\n",
      "Epoch 107/1000\n",
      "1987/1987 [==============================] - 1s 309us/step - loss: 0.6764 - accuracy: 0.5637 - val_loss: 0.7016 - val_accuracy: 0.5475\n",
      "Epoch 108/1000\n",
      "1987/1987 [==============================] - 1s 392us/step - loss: 0.6759 - accuracy: 0.5687 - val_loss: 0.7094 - val_accuracy: 0.5294\n",
      "Epoch 109/1000\n",
      "1987/1987 [==============================] - 1s 318us/step - loss: 0.6742 - accuracy: 0.5642 - val_loss: 0.7005 - val_accuracy: 0.5566\n",
      "Epoch 110/1000\n",
      "1987/1987 [==============================] - 1s 352us/step - loss: 0.6727 - accuracy: 0.5783 - val_loss: 0.7059 - val_accuracy: 0.5249\n",
      "Epoch 111/1000\n",
      "1987/1987 [==============================] - 1s 372us/step - loss: 0.6718 - accuracy: 0.5702 - val_loss: 0.7041 - val_accuracy: 0.5520\n",
      "Epoch 112/1000\n",
      "1987/1987 [==============================] - 1s 318us/step - loss: 0.6710 - accuracy: 0.5838 - val_loss: 0.7078 - val_accuracy: 0.5204\n",
      "Epoch 113/1000\n",
      "1987/1987 [==============================] - 1s 315us/step - loss: 0.6701 - accuracy: 0.5757 - val_loss: 0.7063 - val_accuracy: 0.5611\n",
      "Epoch 114/1000\n",
      "1987/1987 [==============================] - 1s 299us/step - loss: 0.6671 - accuracy: 0.5762 - val_loss: 0.7130 - val_accuracy: 0.4796\n",
      "Epoch 115/1000\n",
      "1987/1987 [==============================] - 1s 304us/step - loss: 0.6745 - accuracy: 0.5692 - val_loss: 0.7022 - val_accuracy: 0.5113\n",
      "Epoch 116/1000\n",
      "1987/1987 [==============================] - 1s 298us/step - loss: 0.6685 - accuracy: 0.5707 - val_loss: 0.7177 - val_accuracy: 0.5023\n",
      "Epoch 117/1000\n",
      "1987/1987 [==============================] - 1s 296us/step - loss: 0.6670 - accuracy: 0.5742 - val_loss: 0.7131 - val_accuracy: 0.5158\n",
      "Epoch 118/1000\n",
      "1987/1987 [==============================] - 1s 269us/step - loss: 0.6615 - accuracy: 0.6014 - val_loss: 0.7068 - val_accuracy: 0.5204\n",
      "Epoch 119/1000\n",
      "1987/1987 [==============================] - 1s 268us/step - loss: 0.6592 - accuracy: 0.6024 - val_loss: 0.7113 - val_accuracy: 0.4842\n",
      "Epoch 120/1000\n",
      "1987/1987 [==============================] - 1s 277us/step - loss: 0.6663 - accuracy: 0.5762 - val_loss: 0.7055 - val_accuracy: 0.5520\n",
      "Epoch 121/1000\n",
      "1987/1987 [==============================] - 1s 274us/step - loss: 0.6604 - accuracy: 0.5808 - val_loss: 0.7129 - val_accuracy: 0.5294\n",
      "Epoch 122/1000\n",
      "1987/1987 [==============================] - 1s 287us/step - loss: 0.6611 - accuracy: 0.5833 - val_loss: 0.6978 - val_accuracy: 0.5747\n",
      "Epoch 123/1000\n",
      "1987/1987 [==============================] - 1s 318us/step - loss: 0.6578 - accuracy: 0.5898 - val_loss: 0.7219 - val_accuracy: 0.5430\n",
      "Epoch 124/1000\n",
      "1987/1987 [==============================] - 1s 305us/step - loss: 0.6559 - accuracy: 0.5863 - val_loss: 0.7266 - val_accuracy: 0.4977\n",
      "Epoch 125/1000\n",
      "1987/1987 [==============================] - 1s 314us/step - loss: 0.6537 - accuracy: 0.5944 - val_loss: 0.7056 - val_accuracy: 0.5113\n",
      "Epoch 126/1000\n",
      "1987/1987 [==============================] - 1s 330us/step - loss: 0.6595 - accuracy: 0.5908 - val_loss: 0.7267 - val_accuracy: 0.5068\n",
      "Epoch 127/1000\n",
      "1987/1987 [==============================] - 1s 353us/step - loss: 0.6639 - accuracy: 0.5954 - val_loss: 0.7060 - val_accuracy: 0.5385\n",
      "Epoch 128/1000\n",
      "1987/1987 [==============================] - 1s 435us/step - loss: 0.6582 - accuracy: 0.5949 - val_loss: 0.7344 - val_accuracy: 0.5068\n",
      "Epoch 129/1000\n",
      "1987/1987 [==============================] - 1s 418us/step - loss: 0.6593 - accuracy: 0.5843 - val_loss: 0.7163 - val_accuracy: 0.5158\n",
      "Epoch 130/1000\n",
      "1987/1987 [==============================] - 1s 458us/step - loss: 0.6513 - accuracy: 0.6095 - val_loss: 0.7445 - val_accuracy: 0.5023\n",
      "Epoch 131/1000\n",
      "1987/1987 [==============================] - 1s 495us/step - loss: 0.6495 - accuracy: 0.5959 - val_loss: 0.7089 - val_accuracy: 0.5339\n",
      "Epoch 132/1000\n",
      "1987/1987 [==============================] - 1s 514us/step - loss: 0.6533 - accuracy: 0.5959 - val_loss: 0.7307 - val_accuracy: 0.5520\n",
      "Epoch 133/1000\n",
      "1987/1987 [==============================] - 1s 546us/step - loss: 0.6494 - accuracy: 0.6029 - val_loss: 0.7280 - val_accuracy: 0.4796\n",
      "Epoch 134/1000\n",
      "1987/1987 [==============================] - 1s 565us/step - loss: 0.6490 - accuracy: 0.6019 - val_loss: 0.7198 - val_accuracy: 0.5204\n",
      "Epoch 135/1000\n",
      "1987/1987 [==============================] - 1s 571us/step - loss: 0.6427 - accuracy: 0.6150 - val_loss: 0.7347 - val_accuracy: 0.5339\n",
      "Epoch 136/1000\n",
      "1987/1987 [==============================] - 1s 597us/step - loss: 0.6441 - accuracy: 0.6095 - val_loss: 0.7246 - val_accuracy: 0.4977\n",
      "Epoch 137/1000\n",
      "1987/1987 [==============================] - 1s 574us/step - loss: 0.6463 - accuracy: 0.6105 - val_loss: 0.7256 - val_accuracy: 0.5339\n",
      "Epoch 138/1000\n",
      "1987/1987 [==============================] - 1s 541us/step - loss: 0.6451 - accuracy: 0.6150 - val_loss: 0.7231 - val_accuracy: 0.4796\n",
      "Epoch 139/1000\n",
      "1987/1987 [==============================] - 1s 513us/step - loss: 0.6357 - accuracy: 0.6145 - val_loss: 0.7450 - val_accuracy: 0.5023\n",
      "Epoch 140/1000\n",
      "1987/1987 [==============================] - 1s 467us/step - loss: 0.6414 - accuracy: 0.6230 - val_loss: 0.7301 - val_accuracy: 0.5294\n",
      "Epoch 141/1000\n",
      "1987/1987 [==============================] - 1s 456us/step - loss: 0.6368 - accuracy: 0.6135 - val_loss: 0.7060 - val_accuracy: 0.5520\n",
      "Epoch 142/1000\n",
      "1987/1987 [==============================] - 1s 401us/step - loss: 0.6359 - accuracy: 0.6246 - val_loss: 0.7442 - val_accuracy: 0.5249\n",
      "Epoch 143/1000\n",
      "1987/1987 [==============================] - 1s 372us/step - loss: 0.6365 - accuracy: 0.6150 - val_loss: 0.7349 - val_accuracy: 0.5113\n",
      "Epoch 144/1000\n",
      "1987/1987 [==============================] - 1s 353us/step - loss: 0.6290 - accuracy: 0.6236 - val_loss: 0.7176 - val_accuracy: 0.5385\n",
      "Epoch 145/1000\n",
      "1987/1987 [==============================] - 1s 339us/step - loss: 0.6199 - accuracy: 0.6341 - val_loss: 0.7299 - val_accuracy: 0.5385\n",
      "Epoch 146/1000\n",
      "1987/1987 [==============================] - 1s 313us/step - loss: 0.6294 - accuracy: 0.6256 - val_loss: 0.7362 - val_accuracy: 0.5023\n",
      "Epoch 147/1000\n",
      "1987/1987 [==============================] - 1s 305us/step - loss: 0.6749 - accuracy: 0.5747 - val_loss: 0.6967 - val_accuracy: 0.5249\n",
      "Epoch 148/1000\n",
      "1987/1987 [==============================] - 1s 292us/step - loss: 0.6640 - accuracy: 0.5898 - val_loss: 0.7097 - val_accuracy: 0.5701\n",
      "Epoch 149/1000\n",
      "1987/1987 [==============================] - 1s 267us/step - loss: 0.6534 - accuracy: 0.6064 - val_loss: 0.7277 - val_accuracy: 0.5249\n",
      "Epoch 150/1000\n",
      "1987/1987 [==============================] - 1s 262us/step - loss: 0.6397 - accuracy: 0.6019 - val_loss: 0.7434 - val_accuracy: 0.5520\n",
      "Epoch 151/1000\n",
      "1987/1987 [==============================] - 1s 269us/step - loss: 0.6296 - accuracy: 0.6266 - val_loss: 0.7221 - val_accuracy: 0.5158\n",
      "Epoch 152/1000\n",
      "1987/1987 [==============================] - 1s 258us/step - loss: 0.6261 - accuracy: 0.6407 - val_loss: 0.7466 - val_accuracy: 0.5339\n",
      "Epoch 153/1000\n",
      "1987/1987 [==============================] - 1s 263us/step - loss: 0.6315 - accuracy: 0.6155 - val_loss: 0.7268 - val_accuracy: 0.5566\n",
      "Epoch 154/1000\n",
      "1987/1987 [==============================] - 1s 255us/step - loss: 0.6250 - accuracy: 0.6281 - val_loss: 0.7461 - val_accuracy: 0.5339\n",
      "Epoch 155/1000\n",
      "1987/1987 [==============================] - 1s 269us/step - loss: 0.6178 - accuracy: 0.6376 - val_loss: 0.7653 - val_accuracy: 0.5068\n",
      "Epoch 156/1000\n",
      "1987/1987 [==============================] - 1s 257us/step - loss: 0.6077 - accuracy: 0.6482 - val_loss: 0.8326 - val_accuracy: 0.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6502 - accuracy: 0.6170 - val_loss: 0.7057 - val_accuracy: 0.5204\n",
      "Epoch 158/1000\n",
      "1987/1987 [==============================] - 0s 251us/step - loss: 0.6180 - accuracy: 0.6402 - val_loss: 0.7523 - val_accuracy: 0.5747\n",
      "Epoch 159/1000\n",
      "1987/1987 [==============================] - 1s 269us/step - loss: 0.6173 - accuracy: 0.6381 - val_loss: 0.7351 - val_accuracy: 0.5249\n",
      "Epoch 160/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6044 - accuracy: 0.6573 - val_loss: 0.7496 - val_accuracy: 0.5294\n",
      "Epoch 161/1000\n",
      "1987/1987 [==============================] - 1s 259us/step - loss: 0.6015 - accuracy: 0.6527 - val_loss: 0.7290 - val_accuracy: 0.5475\n",
      "Epoch 162/1000\n",
      "1987/1987 [==============================] - 0s 250us/step - loss: 0.6005 - accuracy: 0.6623 - val_loss: 0.7611 - val_accuracy: 0.5385\n",
      "Epoch 163/1000\n",
      "1987/1987 [==============================] - 1s 255us/step - loss: 0.5928 - accuracy: 0.6507 - val_loss: 0.7640 - val_accuracy: 0.5204\n",
      "Epoch 164/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.5900 - accuracy: 0.6633 - val_loss: 0.7849 - val_accuracy: 0.5385\n",
      "Epoch 165/1000\n",
      "1987/1987 [==============================] - 1s 253us/step - loss: 0.5992 - accuracy: 0.6412 - val_loss: 0.7840 - val_accuracy: 0.5158\n",
      "Epoch 166/1000\n",
      "1987/1987 [==============================] - 1s 268us/step - loss: 0.5856 - accuracy: 0.6678 - val_loss: 0.7561 - val_accuracy: 0.5294\n",
      "Epoch 167/1000\n",
      "1987/1987 [==============================] - 1s 257us/step - loss: 0.5907 - accuracy: 0.6643 - val_loss: 0.7681 - val_accuracy: 0.5204\n",
      "Epoch 168/1000\n",
      "1987/1987 [==============================] - 1s 261us/step - loss: 0.6370 - accuracy: 0.6230 - val_loss: 0.7061 - val_accuracy: 0.5339\n",
      "Epoch 169/1000\n",
      "1987/1987 [==============================] - 1s 274us/step - loss: 0.6157 - accuracy: 0.6402 - val_loss: 0.7293 - val_accuracy: 0.5204\n",
      "Epoch 170/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.5858 - accuracy: 0.6648 - val_loss: 0.7769 - val_accuracy: 0.5339\n",
      "Epoch 171/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.5838 - accuracy: 0.6714 - val_loss: 0.7894 - val_accuracy: 0.5294\n",
      "Epoch 172/1000\n",
      "1987/1987 [==============================] - 1s 288us/step - loss: 0.5770 - accuracy: 0.6744 - val_loss: 0.7553 - val_accuracy: 0.5158\n",
      "Epoch 173/1000\n",
      "1987/1987 [==============================] - 1s 287us/step - loss: 0.6000 - accuracy: 0.6568 - val_loss: 0.7708 - val_accuracy: 0.5113\n",
      "Epoch 174/1000\n",
      "1987/1987 [==============================] - 1s 291us/step - loss: 0.5916 - accuracy: 0.6638 - val_loss: 0.7655 - val_accuracy: 0.5204\n",
      "Epoch 175/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.5843 - accuracy: 0.6709 - val_loss: 0.7819 - val_accuracy: 0.5023\n",
      "Epoch 176/1000\n",
      "1987/1987 [==============================] - 1s 316us/step - loss: 0.5836 - accuracy: 0.6563 - val_loss: 0.7969 - val_accuracy: 0.5113\n",
      "Epoch 177/1000\n",
      "1987/1987 [==============================] - 1s 306us/step - loss: 0.5960 - accuracy: 0.6699 - val_loss: 0.7996 - val_accuracy: 0.5339\n",
      "Epoch 178/1000\n",
      "1987/1987 [==============================] - 1s 322us/step - loss: 0.5671 - accuracy: 0.6779 - val_loss: 0.8114 - val_accuracy: 0.5294\n",
      "Epoch 179/1000\n",
      "1987/1987 [==============================] - 1s 339us/step - loss: 0.5632 - accuracy: 0.6819 - val_loss: 0.7744 - val_accuracy: 0.5023\n",
      "Epoch 180/1000\n",
      "1987/1987 [==============================] - 1s 330us/step - loss: 0.5684 - accuracy: 0.6704 - val_loss: 0.7744 - val_accuracy: 0.4842\n",
      "Epoch 181/1000\n",
      "1987/1987 [==============================] - 1s 323us/step - loss: 0.5738 - accuracy: 0.6739 - val_loss: 0.7621 - val_accuracy: 0.4887\n",
      "Epoch 182/1000\n",
      "1987/1987 [==============================] - 1s 340us/step - loss: 0.5655 - accuracy: 0.6875 - val_loss: 0.8070 - val_accuracy: 0.5113\n",
      "Epoch 183/1000\n",
      "1987/1987 [==============================] - 1s 326us/step - loss: 0.5811 - accuracy: 0.6653 - val_loss: 0.8564 - val_accuracy: 0.5158\n",
      "Epoch 184/1000\n",
      "1987/1987 [==============================] - 1s 349us/step - loss: 0.5624 - accuracy: 0.6860 - val_loss: 0.7865 - val_accuracy: 0.5113\n",
      "Epoch 185/1000\n",
      "1987/1987 [==============================] - 1s 369us/step - loss: 0.5463 - accuracy: 0.6935 - val_loss: 0.8398 - val_accuracy: 0.5204\n",
      "Epoch 186/1000\n",
      "1987/1987 [==============================] - 1s 325us/step - loss: 0.5649 - accuracy: 0.6860 - val_loss: 0.8380 - val_accuracy: 0.4842\n",
      "Epoch 187/1000\n",
      "1987/1987 [==============================] - 1s 312us/step - loss: 0.5365 - accuracy: 0.7021 - val_loss: 0.8407 - val_accuracy: 0.5339\n",
      "Epoch 188/1000\n",
      "1987/1987 [==============================] - 1s 304us/step - loss: 0.5467 - accuracy: 0.6975 - val_loss: 0.8346 - val_accuracy: 0.5113\n",
      "Epoch 189/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.5455 - accuracy: 0.6945 - val_loss: 0.8739 - val_accuracy: 0.5249\n",
      "Epoch 190/1000\n",
      "1987/1987 [==============================] - 1s 298us/step - loss: 0.5477 - accuracy: 0.6895 - val_loss: 0.7875 - val_accuracy: 0.5113\n",
      "Epoch 191/1000\n",
      "1987/1987 [==============================] - 1s 283us/step - loss: 0.5285 - accuracy: 0.7046 - val_loss: 0.8775 - val_accuracy: 0.5158\n",
      "Epoch 192/1000\n",
      "1987/1987 [==============================] - 1s 289us/step - loss: 0.5510 - accuracy: 0.6895 - val_loss: 0.8845 - val_accuracy: 0.4977\n",
      "Epoch 193/1000\n",
      "1987/1987 [==============================] - 1s 273us/step - loss: 0.5318 - accuracy: 0.7051 - val_loss: 0.9135 - val_accuracy: 0.5249\n",
      "Epoch 194/1000\n",
      "1987/1987 [==============================] - 1s 303us/step - loss: 0.5249 - accuracy: 0.7141 - val_loss: 0.8852 - val_accuracy: 0.4932\n",
      "Epoch 195/1000\n",
      "1987/1987 [==============================] - 1s 281us/step - loss: 0.5222 - accuracy: 0.7116 - val_loss: 0.8575 - val_accuracy: 0.5068\n",
      "Epoch 196/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.5338 - accuracy: 0.7076 - val_loss: 0.8968 - val_accuracy: 0.4751\n",
      "Epoch 197/1000\n",
      "1987/1987 [==============================] - 1s 278us/step - loss: 0.5217 - accuracy: 0.7162 - val_loss: 0.8780 - val_accuracy: 0.5023\n",
      "Epoch 198/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.5301 - accuracy: 0.7011 - val_loss: 0.8311 - val_accuracy: 0.5158\n",
      "Epoch 199/1000\n",
      "1987/1987 [==============================] - 1s 281us/step - loss: 0.5198 - accuracy: 0.7121 - val_loss: 0.9137 - val_accuracy: 0.4661\n",
      "Epoch 200/1000\n",
      "1987/1987 [==============================] - 1s 279us/step - loss: 0.5176 - accuracy: 0.7151 - val_loss: 0.8322 - val_accuracy: 0.5158\n",
      "Epoch 201/1000\n",
      "1987/1987 [==============================] - 1s 281us/step - loss: 0.5060 - accuracy: 0.7217 - val_loss: 0.8682 - val_accuracy: 0.4977\n",
      "Epoch 202/1000\n",
      "1987/1987 [==============================] - 1s 265us/step - loss: 0.5121 - accuracy: 0.7146 - val_loss: 0.8476 - val_accuracy: 0.4751\n",
      "Epoch 203/1000\n",
      "1987/1987 [==============================] - 1s 275us/step - loss: 0.5638 - accuracy: 0.6945 - val_loss: 0.7979 - val_accuracy: 0.5068\n",
      "Epoch 204/1000\n",
      "1987/1987 [==============================] - 1s 293us/step - loss: 0.5269 - accuracy: 0.7071 - val_loss: 0.8703 - val_accuracy: 0.4977\n",
      "Epoch 205/1000\n",
      "1987/1987 [==============================] - 1s 296us/step - loss: 0.5042 - accuracy: 0.7247 - val_loss: 0.9526 - val_accuracy: 0.4842\n",
      "Epoch 206/1000\n",
      "1987/1987 [==============================] - 1s 271us/step - loss: 0.5003 - accuracy: 0.7262 - val_loss: 0.9595 - val_accuracy: 0.4932\n",
      "Epoch 207/1000\n",
      "1987/1987 [==============================] - 1s 282us/step - loss: 0.4970 - accuracy: 0.7363 - val_loss: 0.9908 - val_accuracy: 0.4887\n",
      "Epoch 208/1000\n",
      "1987/1987 [==============================] - 1s 265us/step - loss: 0.5295 - accuracy: 0.7212 - val_loss: 0.9481 - val_accuracy: 0.4751\n",
      "Epoch 209/1000\n",
      "1987/1987 [==============================] - 1s 283us/step - loss: 0.5070 - accuracy: 0.7262 - val_loss: 0.9140 - val_accuracy: 0.4842\n",
      "Epoch 210/1000\n",
      "1987/1987 [==============================] - 1s 284us/step - loss: 0.4940 - accuracy: 0.7333 - val_loss: 0.9265 - val_accuracy: 0.5158\n",
      "Epoch 211/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.4856 - accuracy: 0.7408 - val_loss: 0.9935 - val_accuracy: 0.4932\n",
      "Epoch 212/1000\n",
      "1987/1987 [==============================] - 1s 300us/step - loss: 0.5655 - accuracy: 0.6834 - val_loss: 0.7787 - val_accuracy: 0.5113\n",
      "Epoch 213/1000\n",
      "1987/1987 [==============================] - 1s 293us/step - loss: 0.5318 - accuracy: 0.7121 - val_loss: 0.8526 - val_accuracy: 0.4842\n",
      "Epoch 214/1000\n",
      "1987/1987 [==============================] - 1s 305us/step - loss: 0.5062 - accuracy: 0.7323 - val_loss: 0.8638 - val_accuracy: 0.4842\n",
      "Epoch 215/1000\n",
      "1987/1987 [==============================] - 1s 285us/step - loss: 0.4975 - accuracy: 0.7277 - val_loss: 0.9237 - val_accuracy: 0.4615\n",
      "Epoch 216/1000\n",
      "1987/1987 [==============================] - 1s 301us/step - loss: 0.5225 - accuracy: 0.7217 - val_loss: 0.9193 - val_accuracy: 0.4977\n",
      "Epoch 217/1000\n",
      "1987/1987 [==============================] - 1s 290us/step - loss: 0.5070 - accuracy: 0.7187 - val_loss: 0.8652 - val_accuracy: 0.4977\n",
      "Epoch 218/1000\n",
      "1987/1987 [==============================] - 1s 296us/step - loss: 0.5012 - accuracy: 0.7242 - val_loss: 1.0037 - val_accuracy: 0.4977\n",
      "Epoch 219/1000\n",
      "1987/1987 [==============================] - 1s 298us/step - loss: 0.4877 - accuracy: 0.7343 - val_loss: 0.9405 - val_accuracy: 0.4842\n",
      "Epoch 220/1000\n",
      "1987/1987 [==============================] - 1s 297us/step - loss: 0.4842 - accuracy: 0.7338 - val_loss: 0.9529 - val_accuracy: 0.4887\n",
      "Epoch 221/1000\n",
      "1987/1987 [==============================] - 1s 304us/step - loss: 0.4911 - accuracy: 0.7237 - val_loss: 0.9158 - val_accuracy: 0.5113\n",
      "Epoch 222/1000\n",
      "1987/1987 [==============================] - 1s 338us/step - loss: 0.4770 - accuracy: 0.7393 - val_loss: 1.0016 - val_accuracy: 0.4661\n",
      "Epoch 223/1000\n",
      "1987/1987 [==============================] - 1s 408us/step - loss: 0.4632 - accuracy: 0.7443 - val_loss: 1.0417 - val_accuracy: 0.4796\n",
      "Epoch 224/1000\n",
      "1987/1987 [==============================] - 1s 330us/step - loss: 0.4619 - accuracy: 0.7458 - val_loss: 1.0757 - val_accuracy: 0.4570\n",
      "Epoch 225/1000\n",
      "1987/1987 [==============================] - 1s 338us/step - loss: 0.4670 - accuracy: 0.7464 - val_loss: 0.9633 - val_accuracy: 0.4977\n",
      "Epoch 226/1000\n",
      "1987/1987 [==============================] - 1s 309us/step - loss: 0.4681 - accuracy: 0.7620 - val_loss: 1.0417 - val_accuracy: 0.4796\n",
      "Epoch 227/1000\n",
      "1987/1987 [==============================] - 1s 304us/step - loss: 0.4579 - accuracy: 0.7428 - val_loss: 1.0274 - val_accuracy: 0.5113\n",
      "Epoch 228/1000\n",
      "1987/1987 [==============================] - 1s 305us/step - loss: 0.4567 - accuracy: 0.7529 - val_loss: 1.1008 - val_accuracy: 0.4887\n",
      "Epoch 229/1000\n",
      "1987/1987 [==============================] - 1s 323us/step - loss: 0.4355 - accuracy: 0.7635 - val_loss: 1.1809 - val_accuracy: 0.4887\n",
      "Epoch 230/1000\n",
      "1987/1987 [==============================] - 1s 300us/step - loss: 0.4510 - accuracy: 0.7464 - val_loss: 1.1243 - val_accuracy: 0.4977\n",
      "Epoch 231/1000\n",
      "1987/1987 [==============================] - 1s 312us/step - loss: 0.4368 - accuracy: 0.7645 - val_loss: 1.1382 - val_accuracy: 0.4887\n",
      "Epoch 232/1000\n",
      "1987/1987 [==============================] - 1s 311us/step - loss: 0.4485 - accuracy: 0.7630 - val_loss: 1.0484 - val_accuracy: 0.4796\n",
      "Epoch 233/1000\n",
      "1987/1987 [==============================] - 1s 295us/step - loss: 0.4341 - accuracy: 0.7670 - val_loss: 1.1743 - val_accuracy: 0.4796\n",
      "Epoch 234/1000\n",
      "1987/1987 [==============================] - 1s 297us/step - loss: 0.4244 - accuracy: 0.7740 - val_loss: 1.1755 - val_accuracy: 0.5113\n",
      "Epoch 235/1000\n",
      "1987/1987 [==============================] - 1s 289us/step - loss: 0.4162 - accuracy: 0.7765 - val_loss: 1.2933 - val_accuracy: 0.4887\n",
      "Epoch 236/1000\n",
      "1987/1987 [==============================] - 1s 280us/step - loss: 0.4156 - accuracy: 0.7735 - val_loss: 1.3154 - val_accuracy: 0.4977\n",
      "Epoch 237/1000\n",
      "1987/1987 [==============================] - 1s 283us/step - loss: 0.4201 - accuracy: 0.7715 - val_loss: 1.1477 - val_accuracy: 0.5204\n",
      "Epoch 00237: early stopping\n",
      "7 day\n",
      "\n",
      "# Evaluate on test data\n",
      "246/246 [==============================] - 0s 122us/step\n",
      "test loss, test acc: [1.2456587872854092, 0.577235758304596]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (246, 1)\n",
      "rmse: 0.5626497492768694\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 7\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "print(\"7 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(15, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(15, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 15, 50)            28600     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 15, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 15, 75)            37800     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 15, 75)            0         \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 2s 1ms/step - loss: 0.6941 - accuracy: 0.4965 - val_loss: 0.6907 - val_accuracy: 0.5656\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.6943 - accuracy: 0.5121 - val_loss: 0.6942 - val_accuracy: 0.4570\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.6917 - accuracy: 0.5222 - val_loss: 0.6857 - val_accuracy: 0.5701\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.6920 - accuracy: 0.5197 - val_loss: 0.6884 - val_accuracy: 0.5792\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.6924 - accuracy: 0.5086 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.6920 - accuracy: 0.5268 - val_loss: 0.6915 - val_accuracy: 0.5520\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 432us/step - loss: 0.6925 - accuracy: 0.5283 - val_loss: 0.6865 - val_accuracy: 0.5747\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.6921 - accuracy: 0.5242 - val_loss: 0.6894 - val_accuracy: 0.5928\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.6912 - accuracy: 0.5263 - val_loss: 0.6892 - val_accuracy: 0.5701\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.6913 - accuracy: 0.5126 - val_loss: 0.6908 - val_accuracy: 0.5611\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 490us/step - loss: 0.6920 - accuracy: 0.5283 - val_loss: 0.6892 - val_accuracy: 0.5656\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.6914 - accuracy: 0.5222 - val_loss: 0.6882 - val_accuracy: 0.5882\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.6905 - accuracy: 0.5258 - val_loss: 0.6902 - val_accuracy: 0.5882\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.6907 - accuracy: 0.5338 - val_loss: 0.6931 - val_accuracy: 0.5113\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.6907 - accuracy: 0.5273 - val_loss: 0.6917 - val_accuracy: 0.5611\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.6911 - accuracy: 0.5379 - val_loss: 0.6895 - val_accuracy: 0.5385\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.6899 - accuracy: 0.5247 - val_loss: 0.6911 - val_accuracy: 0.5294\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 455us/step - loss: 0.6913 - accuracy: 0.5227 - val_loss: 0.6907 - val_accuracy: 0.5294\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.6907 - accuracy: 0.5278 - val_loss: 0.6887 - val_accuracy: 0.5928\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.6898 - accuracy: 0.5293 - val_loss: 0.6924 - val_accuracy: 0.5113\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 475us/step - loss: 0.6905 - accuracy: 0.5273 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 500us/step - loss: 0.6909 - accuracy: 0.5207 - val_loss: 0.6880 - val_accuracy: 0.5656\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.6906 - accuracy: 0.5283 - val_loss: 0.6868 - val_accuracy: 0.5837\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.6913 - accuracy: 0.5076 - val_loss: 0.6873 - val_accuracy: 0.5792\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.6920 - accuracy: 0.5313 - val_loss: 0.6922 - val_accuracy: 0.5294\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.6893 - accuracy: 0.5298 - val_loss: 0.6932 - val_accuracy: 0.5747\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.6900 - accuracy: 0.5313 - val_loss: 0.6892 - val_accuracy: 0.5385\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.6898 - accuracy: 0.5298 - val_loss: 0.6909 - val_accuracy: 0.5566\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 482us/step - loss: 0.6898 - accuracy: 0.5338 - val_loss: 0.6910 - val_accuracy: 0.5294\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 513us/step - loss: 0.6893 - accuracy: 0.5399 - val_loss: 0.6893 - val_accuracy: 0.5611\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.6885 - accuracy: 0.5581 - val_loss: 0.6997 - val_accuracy: 0.4525\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 481us/step - loss: 0.6896 - accuracy: 0.5364 - val_loss: 0.6901 - val_accuracy: 0.5339\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 505us/step - loss: 0.6886 - accuracy: 0.5379 - val_loss: 0.6886 - val_accuracy: 0.5385\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6870 - accuracy: 0.5470 - val_loss: 0.6921 - val_accuracy: 0.5204\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6877 - accuracy: 0.5480 - val_loss: 0.6906 - val_accuracy: 0.5294\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.6927 - accuracy: 0.5202 - val_loss: 0.6858 - val_accuracy: 0.5701\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.6899 - accuracy: 0.5253 - val_loss: 0.6962 - val_accuracy: 0.4706\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.6907 - accuracy: 0.5197 - val_loss: 0.6902 - val_accuracy: 0.5792\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 452us/step - loss: 0.6899 - accuracy: 0.5253 - val_loss: 0.6890 - val_accuracy: 0.5837\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.6911 - accuracy: 0.5258 - val_loss: 0.6930 - val_accuracy: 0.5204\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6922 - accuracy: 0.5293 - val_loss: 0.6907 - val_accuracy: 0.5701\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.6895 - accuracy: 0.5364 - val_loss: 0.6908 - val_accuracy: 0.5701\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.6890 - accuracy: 0.5419 - val_loss: 0.6938 - val_accuracy: 0.5339\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6870 - accuracy: 0.5510 - val_loss: 0.6957 - val_accuracy: 0.5158\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6894 - accuracy: 0.5288 - val_loss: 0.6891 - val_accuracy: 0.5294\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 492us/step - loss: 0.6901 - accuracy: 0.5283 - val_loss: 0.6901 - val_accuracy: 0.5520\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6871 - accuracy: 0.5343 - val_loss: 0.6993 - val_accuracy: 0.4887\n",
      "Epoch 48/1000\n",
      "1980/1980 [==============================] - 1s 495us/step - loss: 0.6866 - accuracy: 0.5480 - val_loss: 0.6917 - val_accuracy: 0.5023\n",
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 1s 493us/step - loss: 0.6878 - accuracy: 0.5338 - val_loss: 0.6926 - val_accuracy: 0.5385\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 1s 475us/step - loss: 0.6842 - accuracy: 0.5505 - val_loss: 0.6994 - val_accuracy: 0.5113\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.6861 - accuracy: 0.5510 - val_loss: 0.6885 - val_accuracy: 0.5656\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 0.6867 - accuracy: 0.5434 - val_loss: 0.7049 - val_accuracy: 0.4661\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6864 - accuracy: 0.5444 - val_loss: 0.6892 - val_accuracy: 0.5385\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 1s 518us/step - loss: 0.6831 - accuracy: 0.5545 - val_loss: 0.7116 - val_accuracy: 0.4706\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 1s 511us/step - loss: 0.6861 - accuracy: 0.5601 - val_loss: 0.6894 - val_accuracy: 0.5701\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.6873 - accuracy: 0.5288 - val_loss: 0.6924 - val_accuracy: 0.5339\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.6860 - accuracy: 0.5470 - val_loss: 0.7078 - val_accuracy: 0.4842\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 1s 444us/step - loss: 0.6856 - accuracy: 0.5551 - val_loss: 0.7040 - val_accuracy: 0.4706\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 1s 448us/step - loss: 0.6852 - accuracy: 0.5530 - val_loss: 0.6897 - val_accuracy: 0.5339\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.6815 - accuracy: 0.5646 - val_loss: 0.7045 - val_accuracy: 0.5204\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.6800 - accuracy: 0.5581 - val_loss: 0.6944 - val_accuracy: 0.5520\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.6785 - accuracy: 0.5753 - val_loss: 0.7008 - val_accuracy: 0.5113\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.6768 - accuracy: 0.5631 - val_loss: 0.7177 - val_accuracy: 0.4480\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 1s 444us/step - loss: 0.6807 - accuracy: 0.5566 - val_loss: 0.7287 - val_accuracy: 0.5611\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.6833 - accuracy: 0.5490 - val_loss: 0.7013 - val_accuracy: 0.5068\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.6798 - accuracy: 0.5702 - val_loss: 0.7112 - val_accuracy: 0.5339\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.6805 - accuracy: 0.5525 - val_loss: 0.7184 - val_accuracy: 0.5023\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 505us/step - loss: 0.6779 - accuracy: 0.5621 - val_loss: 0.7071 - val_accuracy: 0.5430\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 481us/step - loss: 0.6799 - accuracy: 0.5662 - val_loss: 0.7033 - val_accuracy: 0.5294\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.6741 - accuracy: 0.5722 - val_loss: 0.7113 - val_accuracy: 0.5566\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.6767 - accuracy: 0.5672 - val_loss: 0.7077 - val_accuracy: 0.5113\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.6734 - accuracy: 0.5616 - val_loss: 0.7263 - val_accuracy: 0.5430\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 547us/step - loss: 0.6683 - accuracy: 0.5859 - val_loss: 0.7191 - val_accuracy: 0.5158\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.6801 - accuracy: 0.5490 - val_loss: 0.7092 - val_accuracy: 0.4751\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 488us/step - loss: 0.6749 - accuracy: 0.5687 - val_loss: 0.7071 - val_accuracy: 0.5566\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 495us/step - loss: 0.6719 - accuracy: 0.5808 - val_loss: 0.7090 - val_accuracy: 0.5430\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6733 - accuracy: 0.5667 - val_loss: 0.7180 - val_accuracy: 0.4842\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6661 - accuracy: 0.5808 - val_loss: 0.7226 - val_accuracy: 0.5023\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6702 - accuracy: 0.5914 - val_loss: 0.7085 - val_accuracy: 0.5158\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.6703 - accuracy: 0.5838 - val_loss: 0.7029 - val_accuracy: 0.5249\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 1s 450us/step - loss: 0.6700 - accuracy: 0.5808 - val_loss: 0.7221 - val_accuracy: 0.5475\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.6688 - accuracy: 0.5894 - val_loss: 0.7136 - val_accuracy: 0.4887\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 1s 448us/step - loss: 0.6622 - accuracy: 0.5955 - val_loss: 0.7403 - val_accuracy: 0.5339\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 1s 455us/step - loss: 0.6650 - accuracy: 0.5854 - val_loss: 0.7229 - val_accuracy: 0.5023\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6615 - accuracy: 0.5934 - val_loss: 0.7357 - val_accuracy: 0.5339\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.6590 - accuracy: 0.6015 - val_loss: 0.7600 - val_accuracy: 0.5113\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6562 - accuracy: 0.6071 - val_loss: 0.7328 - val_accuracy: 0.5520\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6595 - accuracy: 0.6061 - val_loss: 0.7304 - val_accuracy: 0.5385\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 473us/step - loss: 0.6597 - accuracy: 0.6040 - val_loss: 0.7203 - val_accuracy: 0.4796\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 517us/step - loss: 0.6652 - accuracy: 0.5899 - val_loss: 0.7160 - val_accuracy: 0.5294\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6605 - accuracy: 0.6020 - val_loss: 0.7332 - val_accuracy: 0.5385\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.6574 - accuracy: 0.6076 - val_loss: 0.7156 - val_accuracy: 0.5249\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 1s 485us/step - loss: 0.6668 - accuracy: 0.5929 - val_loss: 0.7310 - val_accuracy: 0.5294\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6610 - accuracy: 0.5965 - val_loss: 0.7023 - val_accuracy: 0.5475\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.6518 - accuracy: 0.6091 - val_loss: 0.7414 - val_accuracy: 0.4977\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6526 - accuracy: 0.6056 - val_loss: 0.7251 - val_accuracy: 0.5294\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.6548 - accuracy: 0.6035 - val_loss: 0.7231 - val_accuracy: 0.4932\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 1s 496us/step - loss: 0.6505 - accuracy: 0.6035 - val_loss: 0.7600 - val_accuracy: 0.5430\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.6540 - accuracy: 0.6056 - val_loss: 0.7244 - val_accuracy: 0.4977\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.6508 - accuracy: 0.6192 - val_loss: 0.7182 - val_accuracy: 0.5430\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6477 - accuracy: 0.6076 - val_loss: 0.7389 - val_accuracy: 0.5339\n",
      "Epoch 103/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.6428 - accuracy: 0.6232 - val_loss: 0.7570 - val_accuracy: 0.4842\n",
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.6520 - accuracy: 0.6081 - val_loss: 0.7199 - val_accuracy: 0.5339\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 452us/step - loss: 0.6478 - accuracy: 0.6253 - val_loss: 0.7652 - val_accuracy: 0.5339\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.6468 - accuracy: 0.6101 - val_loss: 0.7471 - val_accuracy: 0.5249\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 493us/step - loss: 0.6412 - accuracy: 0.6141 - val_loss: 0.7549 - val_accuracy: 0.5294\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.6401 - accuracy: 0.6192 - val_loss: 0.7186 - val_accuracy: 0.5520\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.6460 - accuracy: 0.6177 - val_loss: 0.7654 - val_accuracy: 0.5249\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.6375 - accuracy: 0.6384 - val_loss: 0.7439 - val_accuracy: 0.4751\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6449 - accuracy: 0.6253 - val_loss: 0.7351 - val_accuracy: 0.5611\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6349 - accuracy: 0.6394 - val_loss: 0.7293 - val_accuracy: 0.5339\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6383 - accuracy: 0.6318 - val_loss: 0.7678 - val_accuracy: 0.5385\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 482us/step - loss: 0.6347 - accuracy: 0.6288 - val_loss: 0.7701 - val_accuracy: 0.5158\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6376 - accuracy: 0.6202 - val_loss: 0.7609 - val_accuracy: 0.5339\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.6321 - accuracy: 0.6359 - val_loss: 0.7363 - val_accuracy: 0.5430\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 483us/step - loss: 0.6367 - accuracy: 0.6293 - val_loss: 0.7323 - val_accuracy: 0.5294\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 485us/step - loss: 0.6306 - accuracy: 0.6379 - val_loss: 0.7450 - val_accuracy: 0.5520\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 486us/step - loss: 0.6365 - accuracy: 0.6202 - val_loss: 0.7438 - val_accuracy: 0.5430\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6385 - accuracy: 0.6227 - val_loss: 0.7425 - val_accuracy: 0.5158\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 492us/step - loss: 0.6315 - accuracy: 0.6303 - val_loss: 0.7962 - val_accuracy: 0.5430\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 0.6330 - accuracy: 0.6212 - val_loss: 0.7482 - val_accuracy: 0.5294\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.6352 - accuracy: 0.6212 - val_loss: 0.7437 - val_accuracy: 0.5430\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 503us/step - loss: 0.6275 - accuracy: 0.6414 - val_loss: 0.7705 - val_accuracy: 0.5385\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6212 - accuracy: 0.6475 - val_loss: 0.8013 - val_accuracy: 0.5158\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.6192 - accuracy: 0.6434 - val_loss: 0.8084 - val_accuracy: 0.5204\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 445us/step - loss: 0.6296 - accuracy: 0.6389 - val_loss: 0.7367 - val_accuracy: 0.5294\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.6252 - accuracy: 0.6389 - val_loss: 0.7620 - val_accuracy: 0.5475\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.6172 - accuracy: 0.6500 - val_loss: 0.8043 - val_accuracy: 0.4977\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.6165 - accuracy: 0.6495 - val_loss: 0.7752 - val_accuracy: 0.5068\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.6097 - accuracy: 0.6576 - val_loss: 0.8002 - val_accuracy: 0.5113\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6100 - accuracy: 0.6561 - val_loss: 0.8088 - val_accuracy: 0.5023\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.6204 - accuracy: 0.6399 - val_loss: 0.7900 - val_accuracy: 0.5249\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.6271 - accuracy: 0.6288 - val_loss: 0.7815 - val_accuracy: 0.4706\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.6192 - accuracy: 0.6389 - val_loss: 0.7843 - val_accuracy: 0.5023\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 0.6292 - accuracy: 0.6424 - val_loss: 0.7324 - val_accuracy: 0.5430\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6234 - accuracy: 0.6505 - val_loss: 0.7743 - val_accuracy: 0.5385\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.6202 - accuracy: 0.6384 - val_loss: 0.7451 - val_accuracy: 0.5339\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 1s 487us/step - loss: 0.6182 - accuracy: 0.6500 - val_loss: 0.7643 - val_accuracy: 0.5475\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.6090 - accuracy: 0.6616 - val_loss: 0.7686 - val_accuracy: 0.4977\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.6099 - accuracy: 0.6566 - val_loss: 0.8167 - val_accuracy: 0.5158\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 1s 537us/step - loss: 0.6062 - accuracy: 0.6571 - val_loss: 0.8124 - val_accuracy: 0.4977\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 498us/step - loss: 0.6046 - accuracy: 0.6611 - val_loss: 0.7692 - val_accuracy: 0.5158\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 487us/step - loss: 0.6214 - accuracy: 0.6434 - val_loss: 0.7957 - val_accuracy: 0.5294\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 490us/step - loss: 0.6113 - accuracy: 0.6490 - val_loss: 0.8056 - val_accuracy: 0.4977\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.6022 - accuracy: 0.6545 - val_loss: 0.7709 - val_accuracy: 0.5068\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6032 - accuracy: 0.6636 - val_loss: 0.8103 - val_accuracy: 0.4887\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.6036 - accuracy: 0.6606 - val_loss: 0.8415 - val_accuracy: 0.5294\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.5914 - accuracy: 0.6616 - val_loss: 0.8308 - val_accuracy: 0.5430\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.5818 - accuracy: 0.6672 - val_loss: 0.8704 - val_accuracy: 0.5475\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 450us/step - loss: 0.5893 - accuracy: 0.6793 - val_loss: 0.8384 - val_accuracy: 0.5113\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.5832 - accuracy: 0.6742 - val_loss: 0.8534 - val_accuracy: 0.5701\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.5843 - accuracy: 0.6763 - val_loss: 0.8657 - val_accuracy: 0.4932\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.5960 - accuracy: 0.6530 - val_loss: 0.8467 - val_accuracy: 0.5204\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.5866 - accuracy: 0.6712 - val_loss: 0.8279 - val_accuracy: 0.5339\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.5739 - accuracy: 0.6955 - val_loss: 0.8605 - val_accuracy: 0.5430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.5880 - accuracy: 0.6707 - val_loss: 0.8288 - val_accuracy: 0.5385\n",
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.5827 - accuracy: 0.6702 - val_loss: 0.8477 - val_accuracy: 0.5249\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.5923 - accuracy: 0.6662 - val_loss: 0.8114 - val_accuracy: 0.5475\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 510us/step - loss: 0.5843 - accuracy: 0.6712 - val_loss: 0.9197 - val_accuracy: 0.5339\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.5781 - accuracy: 0.6864 - val_loss: 0.8542 - val_accuracy: 0.5566\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.5720 - accuracy: 0.6803 - val_loss: 0.8914 - val_accuracy: 0.5113\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.5756 - accuracy: 0.6763 - val_loss: 0.8331 - val_accuracy: 0.5249\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.5826 - accuracy: 0.6763 - val_loss: 0.9312 - val_accuracy: 0.5204\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 0.5610 - accuracy: 0.6924 - val_loss: 0.8937 - val_accuracy: 0.5339\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.5686 - accuracy: 0.6818 - val_loss: 0.8336 - val_accuracy: 0.4842\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.5733 - accuracy: 0.6813 - val_loss: 0.8587 - val_accuracy: 0.5249\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.5834 - accuracy: 0.6778 - val_loss: 0.8762 - val_accuracy: 0.5430\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.5546 - accuracy: 0.7015 - val_loss: 0.9457 - val_accuracy: 0.5249\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 455us/step - loss: 0.5466 - accuracy: 0.7061 - val_loss: 0.9469 - val_accuracy: 0.5294\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 1s 448us/step - loss: 0.5492 - accuracy: 0.6960 - val_loss: 1.0247 - val_accuracy: 0.5385\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.5464 - accuracy: 0.7025 - val_loss: 0.9551 - val_accuracy: 0.5068\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.5509 - accuracy: 0.7005 - val_loss: 0.9934 - val_accuracy: 0.5566\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.5479 - accuracy: 0.7111 - val_loss: 0.9013 - val_accuracy: 0.5249\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.5351 - accuracy: 0.7030 - val_loss: 0.9897 - val_accuracy: 0.5158\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.5615 - accuracy: 0.7010 - val_loss: 0.9220 - val_accuracy: 0.4706\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.5431 - accuracy: 0.6955 - val_loss: 0.9971 - val_accuracy: 0.5294\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 519us/step - loss: 0.5323 - accuracy: 0.7152 - val_loss: 0.9556 - val_accuracy: 0.5385\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.5323 - accuracy: 0.7030 - val_loss: 0.9186 - val_accuracy: 0.5566\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.5362 - accuracy: 0.7005 - val_loss: 0.9990 - val_accuracy: 0.5294\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 448us/step - loss: 0.5366 - accuracy: 0.7066 - val_loss: 1.0113 - val_accuracy: 0.5385\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.5322 - accuracy: 0.7141 - val_loss: 0.9777 - val_accuracy: 0.5113\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.5233 - accuracy: 0.7247 - val_loss: 0.9631 - val_accuracy: 0.4977\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.5341 - accuracy: 0.7061 - val_loss: 1.0024 - val_accuracy: 0.5023\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 469us/step - loss: 0.5039 - accuracy: 0.7298 - val_loss: 1.0680 - val_accuracy: 0.4932\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 0.5224 - accuracy: 0.7222 - val_loss: 1.0401 - val_accuracy: 0.5113\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.5283 - accuracy: 0.7152 - val_loss: 0.9962 - val_accuracy: 0.4887\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.5173 - accuracy: 0.7202 - val_loss: 1.0055 - val_accuracy: 0.4977\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 1s 470us/step - loss: 0.4975 - accuracy: 0.7495 - val_loss: 1.1952 - val_accuracy: 0.5339\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.5176 - accuracy: 0.7141 - val_loss: 1.0140 - val_accuracy: 0.5068\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.5027 - accuracy: 0.7298 - val_loss: 1.1324 - val_accuracy: 0.4932\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.5151 - accuracy: 0.7308 - val_loss: 1.0446 - val_accuracy: 0.5113\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.5043 - accuracy: 0.7278 - val_loss: 1.0746 - val_accuracy: 0.4842\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.5115 - accuracy: 0.7333 - val_loss: 1.1027 - val_accuracy: 0.5158\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.5009 - accuracy: 0.7348 - val_loss: 1.0993 - val_accuracy: 0.5113\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.5117 - accuracy: 0.7212 - val_loss: 1.0057 - val_accuracy: 0.4887\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 1s 513us/step - loss: 0.4866 - accuracy: 0.7460 - val_loss: 1.1617 - val_accuracy: 0.5430\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.4932 - accuracy: 0.7374 - val_loss: 1.1371 - val_accuracy: 0.5068\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.4731 - accuracy: 0.7384 - val_loss: 1.1227 - val_accuracy: 0.5204\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.4864 - accuracy: 0.7338 - val_loss: 1.1708 - val_accuracy: 0.5158\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 0.4841 - accuracy: 0.7389 - val_loss: 1.1601 - val_accuracy: 0.5113\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 0.4743 - accuracy: 0.7444 - val_loss: 1.2004 - val_accuracy: 0.4887\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.4718 - accuracy: 0.7419 - val_loss: 1.2425 - val_accuracy: 0.5023\n",
      "Epoch 00203: early stopping\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 145us/step\n",
      "test loss, test acc: [1.1414791734851137, 0.5224489569664001]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.5823113730505857\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(30, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(30, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 30, 50)            28600     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 30, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 30, 75)            37800     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 30, 75)            0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1968 samples, validate on 219 samples\n",
      "Epoch 1/1000\n",
      "1968/1968 [==============================] - 3s 2ms/step - loss: 0.6964 - accuracy: 0.5102 - val_loss: 0.6887 - val_accuracy: 0.5571\n",
      "Epoch 2/1000\n",
      "1968/1968 [==============================] - 2s 935us/step - loss: 0.6929 - accuracy: 0.5107 - val_loss: 0.6914 - val_accuracy: 0.5525\n",
      "Epoch 3/1000\n",
      "1968/1968 [==============================] - 2s 971us/step - loss: 0.6921 - accuracy: 0.5305 - val_loss: 0.6900 - val_accuracy: 0.5571\n",
      "Epoch 4/1000\n",
      "1968/1968 [==============================] - 2s 980us/step - loss: 0.6923 - accuracy: 0.5290 - val_loss: 0.6945 - val_accuracy: 0.5114\n",
      "Epoch 5/1000\n",
      "1968/1968 [==============================] - 2s 999us/step - loss: 0.6917 - accuracy: 0.5310 - val_loss: 0.6908 - val_accuracy: 0.5571\n",
      "Epoch 6/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6918 - accuracy: 0.5193 - val_loss: 0.6901 - val_accuracy: 0.5571\n",
      "Epoch 7/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6925 - accuracy: 0.5264 - val_loss: 0.6901 - val_accuracy: 0.5571\n",
      "Epoch 8/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6908 - accuracy: 0.5356 - val_loss: 0.6966 - val_accuracy: 0.5068\n",
      "Epoch 9/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6924 - accuracy: 0.5208 - val_loss: 0.6925 - val_accuracy: 0.5114\n",
      "Epoch 10/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6916 - accuracy: 0.5320 - val_loss: 0.6921 - val_accuracy: 0.5388\n",
      "Epoch 11/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6919 - accuracy: 0.5432 - val_loss: 0.6926 - val_accuracy: 0.5114\n",
      "Epoch 12/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6907 - accuracy: 0.5366 - val_loss: 0.6950 - val_accuracy: 0.5160\n",
      "Epoch 13/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6900 - accuracy: 0.5330 - val_loss: 0.6893 - val_accuracy: 0.5571\n",
      "Epoch 14/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6922 - accuracy: 0.5285 - val_loss: 0.6909 - val_accuracy: 0.5479\n",
      "Epoch 15/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6909 - accuracy: 0.5285 - val_loss: 0.6934 - val_accuracy: 0.5205\n",
      "Epoch 16/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6908 - accuracy: 0.5386 - val_loss: 0.6939 - val_accuracy: 0.5160\n",
      "Epoch 17/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6911 - accuracy: 0.5335 - val_loss: 0.6925 - val_accuracy: 0.5388\n",
      "Epoch 18/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6898 - accuracy: 0.5386 - val_loss: 0.6958 - val_accuracy: 0.5114\n",
      "Epoch 19/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6918 - accuracy: 0.5351 - val_loss: 0.6928 - val_accuracy: 0.5388\n",
      "Epoch 20/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6908 - accuracy: 0.5351 - val_loss: 0.6947 - val_accuracy: 0.5205\n",
      "Epoch 21/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6905 - accuracy: 0.5381 - val_loss: 0.6973 - val_accuracy: 0.5297\n",
      "Epoch 22/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6895 - accuracy: 0.5417 - val_loss: 0.6956 - val_accuracy: 0.5297\n",
      "Epoch 23/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6902 - accuracy: 0.5437 - val_loss: 0.6981 - val_accuracy: 0.4977\n",
      "Epoch 24/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6899 - accuracy: 0.5356 - val_loss: 0.6916 - val_accuracy: 0.5479\n",
      "Epoch 25/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6897 - accuracy: 0.5467 - val_loss: 0.6976 - val_accuracy: 0.5068\n",
      "Epoch 26/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6883 - accuracy: 0.5417 - val_loss: 0.6960 - val_accuracy: 0.5251\n",
      "Epoch 27/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6893 - accuracy: 0.5356 - val_loss: 0.6968 - val_accuracy: 0.5068\n",
      "Epoch 28/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6892 - accuracy: 0.5320 - val_loss: 0.6911 - val_accuracy: 0.5479\n",
      "Epoch 29/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6883 - accuracy: 0.5457 - val_loss: 0.7012 - val_accuracy: 0.5160\n",
      "Epoch 30/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6904 - accuracy: 0.5335 - val_loss: 0.6915 - val_accuracy: 0.5342\n",
      "Epoch 31/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6894 - accuracy: 0.5386 - val_loss: 0.6929 - val_accuracy: 0.5434\n",
      "Epoch 32/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6880 - accuracy: 0.5356 - val_loss: 0.6918 - val_accuracy: 0.5342\n",
      "Epoch 33/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6888 - accuracy: 0.5351 - val_loss: 0.6931 - val_accuracy: 0.5525\n",
      "Epoch 34/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6871 - accuracy: 0.5493 - val_loss: 0.6943 - val_accuracy: 0.5571\n",
      "Epoch 35/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6890 - accuracy: 0.5305 - val_loss: 0.6951 - val_accuracy: 0.4566\n",
      "Epoch 36/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6868 - accuracy: 0.5442 - val_loss: 0.6962 - val_accuracy: 0.4703\n",
      "Epoch 37/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6849 - accuracy: 0.5534 - val_loss: 0.7060 - val_accuracy: 0.4932\n",
      "Epoch 38/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6869 - accuracy: 0.5462 - val_loss: 0.7021 - val_accuracy: 0.4886\n",
      "Epoch 39/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6873 - accuracy: 0.5432 - val_loss: 0.6952 - val_accuracy: 0.5160\n",
      "Epoch 40/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6852 - accuracy: 0.5462 - val_loss: 0.6943 - val_accuracy: 0.5205\n",
      "Epoch 41/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6844 - accuracy: 0.5508 - val_loss: 0.7043 - val_accuracy: 0.4886\n",
      "Epoch 42/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6843 - accuracy: 0.5447 - val_loss: 0.7025 - val_accuracy: 0.4612\n",
      "Epoch 43/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6828 - accuracy: 0.5783 - val_loss: 0.6957 - val_accuracy: 0.5297\n",
      "Epoch 44/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6851 - accuracy: 0.5508 - val_loss: 0.7011 - val_accuracy: 0.4566\n",
      "Epoch 45/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6821 - accuracy: 0.5574 - val_loss: 0.7116 - val_accuracy: 0.4155\n",
      "Epoch 46/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6802 - accuracy: 0.5645 - val_loss: 0.6954 - val_accuracy: 0.5114\n",
      "Epoch 47/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6819 - accuracy: 0.5655 - val_loss: 0.6938 - val_accuracy: 0.4795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6812 - accuracy: 0.5549 - val_loss: 0.6952 - val_accuracy: 0.5114\n",
      "Epoch 49/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6771 - accuracy: 0.5681 - val_loss: 0.7124 - val_accuracy: 0.4840\n",
      "Epoch 50/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6822 - accuracy: 0.5534 - val_loss: 0.6886 - val_accuracy: 0.5205\n",
      "Epoch 51/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6781 - accuracy: 0.5757 - val_loss: 0.7014 - val_accuracy: 0.4840\n",
      "Epoch 52/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6784 - accuracy: 0.5630 - val_loss: 0.6873 - val_accuracy: 0.5251\n",
      "Epoch 53/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6788 - accuracy: 0.5549 - val_loss: 0.7087 - val_accuracy: 0.4886\n",
      "Epoch 54/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6770 - accuracy: 0.5696 - val_loss: 0.7097 - val_accuracy: 0.4384\n",
      "Epoch 55/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6750 - accuracy: 0.5843 - val_loss: 0.7035 - val_accuracy: 0.4977\n",
      "Epoch 56/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6767 - accuracy: 0.5610 - val_loss: 0.6976 - val_accuracy: 0.4977\n",
      "Epoch 57/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6703 - accuracy: 0.5818 - val_loss: 0.6998 - val_accuracy: 0.5023\n",
      "Epoch 58/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6750 - accuracy: 0.5727 - val_loss: 0.7002 - val_accuracy: 0.4932\n",
      "Epoch 59/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6694 - accuracy: 0.5823 - val_loss: 0.7209 - val_accuracy: 0.4521\n",
      "Epoch 60/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6731 - accuracy: 0.5833 - val_loss: 0.6865 - val_accuracy: 0.5068\n",
      "Epoch 61/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6686 - accuracy: 0.5996 - val_loss: 0.7200 - val_accuracy: 0.4886\n",
      "Epoch 62/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6703 - accuracy: 0.5864 - val_loss: 0.6955 - val_accuracy: 0.5388\n",
      "Epoch 63/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6646 - accuracy: 0.5981 - val_loss: 0.7042 - val_accuracy: 0.5068\n",
      "Epoch 64/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6648 - accuracy: 0.5879 - val_loss: 0.7040 - val_accuracy: 0.5114\n",
      "Epoch 65/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6617 - accuracy: 0.5915 - val_loss: 0.7338 - val_accuracy: 0.4566\n",
      "Epoch 66/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6614 - accuracy: 0.5971 - val_loss: 0.7173 - val_accuracy: 0.5023\n",
      "Epoch 67/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6597 - accuracy: 0.6062 - val_loss: 0.7427 - val_accuracy: 0.4475\n",
      "Epoch 68/1000\n",
      "1968/1968 [==============================] - 2s 1000us/step - loss: 0.6689 - accuracy: 0.5696 - val_loss: 0.7242 - val_accuracy: 0.4749\n",
      "Epoch 69/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6624 - accuracy: 0.5955 - val_loss: 0.7447 - val_accuracy: 0.4703\n",
      "Epoch 70/1000\n",
      "1968/1968 [==============================] - 2s 979us/step - loss: 0.6595 - accuracy: 0.6113 - val_loss: 0.7332 - val_accuracy: 0.4703\n",
      "Epoch 71/1000\n",
      "1968/1968 [==============================] - 2s 965us/step - loss: 0.6566 - accuracy: 0.6006 - val_loss: 0.7200 - val_accuracy: 0.4886\n",
      "Epoch 72/1000\n",
      "1968/1968 [==============================] - 2s 978us/step - loss: 0.6591 - accuracy: 0.5986 - val_loss: 0.7038 - val_accuracy: 0.4932\n",
      "Epoch 73/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6596 - accuracy: 0.6062 - val_loss: 0.7226 - val_accuracy: 0.5205\n",
      "Epoch 74/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6577 - accuracy: 0.6021 - val_loss: 0.7293 - val_accuracy: 0.4566\n",
      "Epoch 75/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6487 - accuracy: 0.6082 - val_loss: 0.7454 - val_accuracy: 0.5388\n",
      "Epoch 76/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6491 - accuracy: 0.6199 - val_loss: 0.7116 - val_accuracy: 0.5342\n",
      "Epoch 77/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6566 - accuracy: 0.6001 - val_loss: 0.7369 - val_accuracy: 0.4566\n",
      "Epoch 78/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6472 - accuracy: 0.6159 - val_loss: 0.7841 - val_accuracy: 0.4658\n",
      "Epoch 79/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6474 - accuracy: 0.6189 - val_loss: 0.7350 - val_accuracy: 0.4521\n",
      "Epoch 80/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6458 - accuracy: 0.6189 - val_loss: 0.7497 - val_accuracy: 0.4566\n",
      "Epoch 81/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6414 - accuracy: 0.6184 - val_loss: 0.7503 - val_accuracy: 0.4658\n",
      "Epoch 82/1000\n",
      "1968/1968 [==============================] - 2s 998us/step - loss: 0.6421 - accuracy: 0.6270 - val_loss: 0.7394 - val_accuracy: 0.4566\n",
      "Epoch 83/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6464 - accuracy: 0.6245 - val_loss: 0.7216 - val_accuracy: 0.4977\n",
      "Epoch 84/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6439 - accuracy: 0.6174 - val_loss: 0.7657 - val_accuracy: 0.4566\n",
      "Epoch 85/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6377 - accuracy: 0.6331 - val_loss: 0.7803 - val_accuracy: 0.4612\n",
      "Epoch 86/1000\n",
      "1968/1968 [==============================] - 2s 997us/step - loss: 0.6481 - accuracy: 0.6098 - val_loss: 0.7596 - val_accuracy: 0.4977\n",
      "Epoch 87/1000\n",
      "1968/1968 [==============================] - 2s 994us/step - loss: 0.6480 - accuracy: 0.6103 - val_loss: 0.7196 - val_accuracy: 0.4795\n",
      "Epoch 88/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6352 - accuracy: 0.6265 - val_loss: 0.7703 - val_accuracy: 0.4338\n",
      "Epoch 89/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6316 - accuracy: 0.6382 - val_loss: 0.7378 - val_accuracy: 0.4932\n",
      "Epoch 90/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6383 - accuracy: 0.6225 - val_loss: 0.7937 - val_accuracy: 0.4292\n",
      "Epoch 91/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6327 - accuracy: 0.6296 - val_loss: 0.7882 - val_accuracy: 0.4977\n",
      "Epoch 92/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6289 - accuracy: 0.6402 - val_loss: 0.7675 - val_accuracy: 0.4475\n",
      "Epoch 93/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6261 - accuracy: 0.6397 - val_loss: 0.7801 - val_accuracy: 0.4429\n",
      "Epoch 94/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6222 - accuracy: 0.6438 - val_loss: 0.7831 - val_accuracy: 0.4201\n",
      "Epoch 95/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6264 - accuracy: 0.6296 - val_loss: 0.8144 - val_accuracy: 0.4840\n",
      "Epoch 96/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6386 - accuracy: 0.6260 - val_loss: 0.7591 - val_accuracy: 0.4932\n",
      "Epoch 97/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6276 - accuracy: 0.6423 - val_loss: 0.7873 - val_accuracy: 0.4521\n",
      "Epoch 98/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6190 - accuracy: 0.6494 - val_loss: 0.8212 - val_accuracy: 0.4566\n",
      "Epoch 99/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6253 - accuracy: 0.6362 - val_loss: 0.7562 - val_accuracy: 0.4795\n",
      "Epoch 100/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6138 - accuracy: 0.6555 - val_loss: 0.7366 - val_accuracy: 0.5251\n",
      "Epoch 101/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6223 - accuracy: 0.6275 - val_loss: 0.7956 - val_accuracy: 0.5114\n",
      "Epoch 102/1000\n",
      "1968/1968 [==============================] - 2s 998us/step - loss: 0.6195 - accuracy: 0.6443 - val_loss: 0.7762 - val_accuracy: 0.4658\n",
      "Epoch 103/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6095 - accuracy: 0.6560 - val_loss: 0.8428 - val_accuracy: 0.4658\n",
      "Epoch 104/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6053 - accuracy: 0.6535 - val_loss: 0.8140 - val_accuracy: 0.4886\n",
      "Epoch 105/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6077 - accuracy: 0.6484 - val_loss: 0.7971 - val_accuracy: 0.4977\n",
      "Epoch 106/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6025 - accuracy: 0.6641 - val_loss: 0.8145 - val_accuracy: 0.4658\n",
      "Epoch 107/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6059 - accuracy: 0.6677 - val_loss: 0.8043 - val_accuracy: 0.4795\n",
      "Epoch 108/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6054 - accuracy: 0.6540 - val_loss: 0.7970 - val_accuracy: 0.4840\n",
      "Epoch 109/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6021 - accuracy: 0.6712 - val_loss: 0.8477 - val_accuracy: 0.5023\n",
      "Epoch 110/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5989 - accuracy: 0.6529 - val_loss: 0.7818 - val_accuracy: 0.5479\n",
      "Epoch 111/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6004 - accuracy: 0.6596 - val_loss: 0.8084 - val_accuracy: 0.4886\n",
      "Epoch 112/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6012 - accuracy: 0.6570 - val_loss: 0.8466 - val_accuracy: 0.4703\n",
      "Epoch 113/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6011 - accuracy: 0.6641 - val_loss: 0.8250 - val_accuracy: 0.4566\n",
      "Epoch 114/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5916 - accuracy: 0.6672 - val_loss: 0.7930 - val_accuracy: 0.4932\n",
      "Epoch 115/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5828 - accuracy: 0.6692 - val_loss: 0.8328 - val_accuracy: 0.5023\n",
      "Epoch 116/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5819 - accuracy: 0.6794 - val_loss: 0.8095 - val_accuracy: 0.4840\n",
      "Epoch 117/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5862 - accuracy: 0.6662 - val_loss: 0.8077 - val_accuracy: 0.4795\n",
      "Epoch 118/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5839 - accuracy: 0.6743 - val_loss: 0.9048 - val_accuracy: 0.4795\n",
      "Epoch 119/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5924 - accuracy: 0.6631 - val_loss: 0.8298 - val_accuracy: 0.4749\n",
      "Epoch 120/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5765 - accuracy: 0.6677 - val_loss: 0.8380 - val_accuracy: 0.4886\n",
      "Epoch 121/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5773 - accuracy: 0.6753 - val_loss: 0.8758 - val_accuracy: 0.4932\n",
      "Epoch 122/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5846 - accuracy: 0.6738 - val_loss: 0.8326 - val_accuracy: 0.4749\n",
      "Epoch 123/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5784 - accuracy: 0.6748 - val_loss: 0.8607 - val_accuracy: 0.4886\n",
      "Epoch 124/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5600 - accuracy: 0.6870 - val_loss: 0.9330 - val_accuracy: 0.4749\n",
      "Epoch 125/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5842 - accuracy: 0.6748 - val_loss: 0.8137 - val_accuracy: 0.4703\n",
      "Epoch 126/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5662 - accuracy: 0.6733 - val_loss: 0.9027 - val_accuracy: 0.4658\n",
      "Epoch 127/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5790 - accuracy: 0.6799 - val_loss: 0.8248 - val_accuracy: 0.4521\n",
      "Epoch 128/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5845 - accuracy: 0.6707 - val_loss: 0.9306 - val_accuracy: 0.4703\n",
      "Epoch 129/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5646 - accuracy: 0.6773 - val_loss: 0.8901 - val_accuracy: 0.4932\n",
      "Epoch 130/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5464 - accuracy: 0.6997 - val_loss: 0.9503 - val_accuracy: 0.4521\n",
      "Epoch 131/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5674 - accuracy: 0.6829 - val_loss: 0.8231 - val_accuracy: 0.5388\n",
      "Epoch 132/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5515 - accuracy: 0.6961 - val_loss: 0.9397 - val_accuracy: 0.4247\n",
      "Epoch 133/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5428 - accuracy: 0.6972 - val_loss: 0.9245 - val_accuracy: 0.4749\n",
      "Epoch 134/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5517 - accuracy: 0.6916 - val_loss: 0.9447 - val_accuracy: 0.4703\n",
      "Epoch 135/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5444 - accuracy: 0.6997 - val_loss: 0.9995 - val_accuracy: 0.5023\n",
      "Epoch 136/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5343 - accuracy: 0.7068 - val_loss: 0.9600 - val_accuracy: 0.4886\n",
      "Epoch 137/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5505 - accuracy: 0.6961 - val_loss: 0.8754 - val_accuracy: 0.4566\n",
      "Epoch 138/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5483 - accuracy: 0.6916 - val_loss: 0.9924 - val_accuracy: 0.4384\n",
      "Epoch 139/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5230 - accuracy: 0.7149 - val_loss: 1.0418 - val_accuracy: 0.4292\n",
      "Epoch 140/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5189 - accuracy: 0.7226 - val_loss: 0.9388 - val_accuracy: 0.4566\n",
      "Epoch 141/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5312 - accuracy: 0.7088 - val_loss: 1.0179 - val_accuracy: 0.4840\n",
      "Epoch 142/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5234 - accuracy: 0.7190 - val_loss: 1.0417 - val_accuracy: 0.4703\n",
      "Epoch 143/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5182 - accuracy: 0.7078 - val_loss: 1.0251 - val_accuracy: 0.4840\n",
      "Epoch 144/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5099 - accuracy: 0.7134 - val_loss: 1.0255 - val_accuracy: 0.5205\n",
      "Epoch 145/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5078 - accuracy: 0.7348 - val_loss: 1.0012 - val_accuracy: 0.5068\n",
      "Epoch 146/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5138 - accuracy: 0.7221 - val_loss: 1.0020 - val_accuracy: 0.4749\n",
      "Epoch 147/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5172 - accuracy: 0.7154 - val_loss: 0.9164 - val_accuracy: 0.5114\n",
      "Epoch 148/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5003 - accuracy: 0.7368 - val_loss: 0.9936 - val_accuracy: 0.4840\n",
      "Epoch 149/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5037 - accuracy: 0.7231 - val_loss: 1.0146 - val_accuracy: 0.4521\n",
      "Epoch 150/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5039 - accuracy: 0.7119 - val_loss: 1.0491 - val_accuracy: 0.4566\n",
      "Epoch 151/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5151 - accuracy: 0.7195 - val_loss: 1.0643 - val_accuracy: 0.4886\n",
      "Epoch 152/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4863 - accuracy: 0.7368 - val_loss: 1.0480 - val_accuracy: 0.4795\n",
      "Epoch 153/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4849 - accuracy: 0.7419 - val_loss: 1.1389 - val_accuracy: 0.4658\n",
      "Epoch 154/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5022 - accuracy: 0.7271 - val_loss: 1.0881 - val_accuracy: 0.4703\n",
      "Epoch 155/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5041 - accuracy: 0.7302 - val_loss: 0.9929 - val_accuracy: 0.4795\n",
      "Epoch 156/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.5067 - accuracy: 0.7266 - val_loss: 0.9939 - val_accuracy: 0.4612\n",
      "Epoch 157/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4880 - accuracy: 0.7368 - val_loss: 1.0701 - val_accuracy: 0.4795\n",
      "Epoch 158/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4812 - accuracy: 0.7449 - val_loss: 1.0531 - val_accuracy: 0.5297\n",
      "Epoch 159/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4714 - accuracy: 0.7434 - val_loss: 1.1627 - val_accuracy: 0.4247\n",
      "Epoch 160/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4797 - accuracy: 0.7419 - val_loss: 1.0976 - val_accuracy: 0.4886\n",
      "Epoch 161/1000\n",
      "1968/1968 [==============================] - 2s 998us/step - loss: 0.4768 - accuracy: 0.7434 - val_loss: 1.0153 - val_accuracy: 0.4703\n",
      "Epoch 162/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4597 - accuracy: 0.7586 - val_loss: 1.0393 - val_accuracy: 0.5205\n",
      "Epoch 163/1000\n",
      "1968/1968 [==============================] - 2s 991us/step - loss: 0.4739 - accuracy: 0.7388 - val_loss: 1.1401 - val_accuracy: 0.4932\n",
      "Epoch 164/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4579 - accuracy: 0.7571 - val_loss: 1.1024 - val_accuracy: 0.4795\n",
      "Epoch 165/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4373 - accuracy: 0.7703 - val_loss: 1.1491 - val_accuracy: 0.5023\n",
      "Epoch 166/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4459 - accuracy: 0.7607 - val_loss: 1.0578 - val_accuracy: 0.5434\n",
      "Epoch 167/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4596 - accuracy: 0.7576 - val_loss: 1.0483 - val_accuracy: 0.5023\n",
      "Epoch 168/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4587 - accuracy: 0.7627 - val_loss: 1.1779 - val_accuracy: 0.4475\n",
      "Epoch 169/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4340 - accuracy: 0.7724 - val_loss: 1.1747 - val_accuracy: 0.5160\n",
      "Epoch 170/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4159 - accuracy: 0.7739 - val_loss: 1.1933 - val_accuracy: 0.4703\n",
      "Epoch 171/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4285 - accuracy: 0.7703 - val_loss: 1.1129 - val_accuracy: 0.5160\n",
      "Epoch 172/1000\n",
      "1968/1968 [==============================] - 2s 996us/step - loss: 0.4268 - accuracy: 0.7734 - val_loss: 1.1134 - val_accuracy: 0.5023\n",
      "Epoch 173/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4331 - accuracy: 0.7658 - val_loss: 1.1759 - val_accuracy: 0.5114\n",
      "Epoch 174/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4188 - accuracy: 0.7810 - val_loss: 1.2701 - val_accuracy: 0.5023\n",
      "Epoch 175/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4299 - accuracy: 0.7703 - val_loss: 1.2755 - val_accuracy: 0.5068\n",
      "Epoch 176/1000\n",
      "1968/1968 [==============================] - 2s 992us/step - loss: 0.4512 - accuracy: 0.7541 - val_loss: 1.2346 - val_accuracy: 0.4932\n",
      "Epoch 177/1000\n",
      "1968/1968 [==============================] - 2s 989us/step - loss: 0.4184 - accuracy: 0.7830 - val_loss: 1.2015 - val_accuracy: 0.5160\n",
      "Epoch 178/1000\n",
      "1968/1968 [==============================] - 2s 996us/step - loss: 0.4205 - accuracy: 0.7876 - val_loss: 1.1858 - val_accuracy: 0.5434\n",
      "Epoch 179/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4322 - accuracy: 0.7779 - val_loss: 1.3149 - val_accuracy: 0.4886\n",
      "Epoch 180/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4115 - accuracy: 0.7891 - val_loss: 1.1450 - val_accuracy: 0.5160\n",
      "Epoch 181/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4042 - accuracy: 0.7825 - val_loss: 1.2264 - val_accuracy: 0.5297\n",
      "Epoch 182/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3996 - accuracy: 0.7937 - val_loss: 1.3582 - val_accuracy: 0.4977\n",
      "Epoch 183/1000\n",
      "1968/1968 [==============================] - 2s 996us/step - loss: 0.3979 - accuracy: 0.7912 - val_loss: 1.2519 - val_accuracy: 0.5160\n",
      "Epoch 184/1000\n",
      "1968/1968 [==============================] - 2s 990us/step - loss: 0.3922 - accuracy: 0.7967 - val_loss: 1.3490 - val_accuracy: 0.5297\n",
      "Epoch 185/1000\n",
      "1968/1968 [==============================] - 2s 984us/step - loss: 0.3855 - accuracy: 0.7962 - val_loss: 1.2943 - val_accuracy: 0.5251\n",
      "Epoch 186/1000\n",
      "1968/1968 [==============================] - 2s 970us/step - loss: 0.4000 - accuracy: 0.7947 - val_loss: 1.2458 - val_accuracy: 0.5205\n",
      "Epoch 187/1000\n",
      "1968/1968 [==============================] - 2s 988us/step - loss: 0.3943 - accuracy: 0.7835 - val_loss: 1.4841 - val_accuracy: 0.4977\n",
      "Epoch 188/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.4011 - accuracy: 0.7978 - val_loss: 1.3315 - val_accuracy: 0.5023\n",
      "Epoch 189/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3690 - accuracy: 0.8084 - val_loss: 1.3771 - val_accuracy: 0.5068\n",
      "Epoch 190/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3649 - accuracy: 0.8064 - val_loss: 1.3293 - val_accuracy: 0.5205\n",
      "Epoch 191/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3764 - accuracy: 0.8044 - val_loss: 1.3573 - val_accuracy: 0.4977\n",
      "Epoch 192/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3851 - accuracy: 0.7937 - val_loss: 1.3711 - val_accuracy: 0.4886\n",
      "Epoch 193/1000\n",
      "1968/1968 [==============================] - 2s 991us/step - loss: 0.3518 - accuracy: 0.8252 - val_loss: 1.3506 - val_accuracy: 0.5297\n",
      "Epoch 194/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3274 - accuracy: 0.8415 - val_loss: 1.5998 - val_accuracy: 0.5068\n",
      "Epoch 195/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3483 - accuracy: 0.8140 - val_loss: 1.5770 - val_accuracy: 0.4932\n",
      "Epoch 196/1000\n",
      "1968/1968 [==============================] - 2s 993us/step - loss: 0.3466 - accuracy: 0.8196 - val_loss: 1.4790 - val_accuracy: 0.4977\n",
      "Epoch 197/1000\n",
      "1968/1968 [==============================] - 2s 963us/step - loss: 0.3679 - accuracy: 0.8155 - val_loss: 1.6357 - val_accuracy: 0.5068\n",
      "Epoch 198/1000\n",
      "1968/1968 [==============================] - 2s 977us/step - loss: 0.3516 - accuracy: 0.8252 - val_loss: 1.5999 - val_accuracy: 0.4977\n",
      "Epoch 199/1000\n",
      "1968/1968 [==============================] - 2s 999us/step - loss: 0.3412 - accuracy: 0.8242 - val_loss: 1.4876 - val_accuracy: 0.5251\n",
      "Epoch 200/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3396 - accuracy: 0.8206 - val_loss: 1.4762 - val_accuracy: 0.5388\n",
      "Epoch 201/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3594 - accuracy: 0.8262 - val_loss: 1.4757 - val_accuracy: 0.5114\n",
      "Epoch 202/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3229 - accuracy: 0.8318 - val_loss: 1.6784 - val_accuracy: 0.5160\n",
      "Epoch 203/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3526 - accuracy: 0.8140 - val_loss: 1.3941 - val_accuracy: 0.5205\n",
      "Epoch 204/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3559 - accuracy: 0.8110 - val_loss: 1.4461 - val_accuracy: 0.5297\n",
      "Epoch 205/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3244 - accuracy: 0.8354 - val_loss: 1.5590 - val_accuracy: 0.5342\n",
      "Epoch 206/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.3079 - accuracy: 0.8425 - val_loss: 1.6024 - val_accuracy: 0.4840\n",
      "Epoch 207/1000\n",
      "1968/1968 [==============================] - 2s 967us/step - loss: 0.3327 - accuracy: 0.8445 - val_loss: 1.4888 - val_accuracy: 0.4977\n",
      "Epoch 208/1000\n",
      "1968/1968 [==============================] - 2s 989us/step - loss: 0.3302 - accuracy: 0.8349 - val_loss: 1.6240 - val_accuracy: 0.5388\n",
      "Epoch 209/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2904 - accuracy: 0.8598 - val_loss: 1.6600 - val_accuracy: 0.5205\n",
      "Epoch 210/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2974 - accuracy: 0.8537 - val_loss: 1.7274 - val_accuracy: 0.5342\n",
      "Epoch 211/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2783 - accuracy: 0.8633 - val_loss: 1.5896 - val_accuracy: 0.5205\n",
      "Epoch 212/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2854 - accuracy: 0.8674 - val_loss: 1.6866 - val_accuracy: 0.5342\n",
      "Epoch 213/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2762 - accuracy: 0.8694 - val_loss: 1.7907 - val_accuracy: 0.5068\n",
      "Epoch 214/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2707 - accuracy: 0.8755 - val_loss: 1.7822 - val_accuracy: 0.4977\n",
      "Epoch 215/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2693 - accuracy: 0.8786 - val_loss: 1.8179 - val_accuracy: 0.4977\n",
      "Epoch 216/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2868 - accuracy: 0.8659 - val_loss: 1.6669 - val_accuracy: 0.5114\n",
      "Epoch 217/1000\n",
      "1968/1968 [==============================] - 2s 982us/step - loss: 0.2781 - accuracy: 0.8618 - val_loss: 1.7804 - val_accuracy: 0.5297\n",
      "Epoch 218/1000\n",
      "1968/1968 [==============================] - 2s 976us/step - loss: 0.2990 - accuracy: 0.8521 - val_loss: 1.7877 - val_accuracy: 0.4977\n",
      "Epoch 219/1000\n",
      "1968/1968 [==============================] - 2s 967us/step - loss: 0.2665 - accuracy: 0.8699 - val_loss: 1.7481 - val_accuracy: 0.5662\n",
      "Epoch 220/1000\n",
      "1968/1968 [==============================] - 2s 975us/step - loss: 0.2736 - accuracy: 0.8694 - val_loss: 1.7509 - val_accuracy: 0.5114\n",
      "Epoch 221/1000\n",
      "1968/1968 [==============================] - 2s 982us/step - loss: 0.2900 - accuracy: 0.8577 - val_loss: 1.6904 - val_accuracy: 0.5297\n",
      "Epoch 222/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2688 - accuracy: 0.8720 - val_loss: 1.6633 - val_accuracy: 0.5708\n",
      "Epoch 223/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2661 - accuracy: 0.8740 - val_loss: 1.7691 - val_accuracy: 0.4795\n",
      "Epoch 224/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2615 - accuracy: 0.8775 - val_loss: 1.8142 - val_accuracy: 0.5205\n",
      "Epoch 225/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2551 - accuracy: 0.8831 - val_loss: 1.8624 - val_accuracy: 0.5023\n",
      "Epoch 226/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2606 - accuracy: 0.8755 - val_loss: 1.8634 - val_accuracy: 0.5616\n",
      "Epoch 227/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2847 - accuracy: 0.8633 - val_loss: 1.8080 - val_accuracy: 0.5160\n",
      "Epoch 228/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2975 - accuracy: 0.8577 - val_loss: 1.8082 - val_accuracy: 0.4840\n",
      "Epoch 229/1000\n",
      "1968/1968 [==============================] - 2s 971us/step - loss: 0.2702 - accuracy: 0.8750 - val_loss: 1.8521 - val_accuracy: 0.5068\n",
      "Epoch 230/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2984 - accuracy: 0.8532 - val_loss: 2.0011 - val_accuracy: 0.4749\n",
      "Epoch 231/1000\n",
      "1968/1968 [==============================] - 2s 990us/step - loss: 0.2355 - accuracy: 0.8938 - val_loss: 1.8958 - val_accuracy: 0.5160\n",
      "Epoch 232/1000\n",
      "1968/1968 [==============================] - 2s 994us/step - loss: 0.2433 - accuracy: 0.8933 - val_loss: 1.9126 - val_accuracy: 0.4840\n",
      "Epoch 233/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2105 - accuracy: 0.9090 - val_loss: 1.9795 - val_accuracy: 0.4977\n",
      "Epoch 234/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2161 - accuracy: 0.9055 - val_loss: 1.9267 - val_accuracy: 0.5205\n",
      "Epoch 235/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2206 - accuracy: 0.8999 - val_loss: 1.9807 - val_accuracy: 0.4795\n",
      "Epoch 236/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2243 - accuracy: 0.8963 - val_loss: 2.0644 - val_accuracy: 0.5023\n",
      "Epoch 237/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2138 - accuracy: 0.9065 - val_loss: 2.0891 - val_accuracy: 0.5114\n",
      "Epoch 238/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2024 - accuracy: 0.9141 - val_loss: 2.0806 - val_accuracy: 0.5023\n",
      "Epoch 239/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2073 - accuracy: 0.9090 - val_loss: 2.1486 - val_accuracy: 0.5160\n",
      "Epoch 240/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2003 - accuracy: 0.9157 - val_loss: 2.1830 - val_accuracy: 0.4658\n",
      "Epoch 241/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2008 - accuracy: 0.9085 - val_loss: 2.1165 - val_accuracy: 0.5023\n",
      "Epoch 242/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2086 - accuracy: 0.9065 - val_loss: 2.4454 - val_accuracy: 0.4475\n",
      "Epoch 243/1000\n",
      "1968/1968 [==============================] - 2s 998us/step - loss: 0.2189 - accuracy: 0.8958 - val_loss: 2.2649 - val_accuracy: 0.4977\n",
      "Epoch 244/1000\n",
      "1968/1968 [==============================] - 2s 999us/step - loss: 0.1900 - accuracy: 0.9126 - val_loss: 2.0695 - val_accuracy: 0.5160\n",
      "Epoch 245/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1982 - accuracy: 0.9116 - val_loss: 2.2297 - val_accuracy: 0.4886\n",
      "Epoch 246/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1891 - accuracy: 0.9182 - val_loss: 2.2463 - val_accuracy: 0.4886\n",
      "Epoch 247/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1886 - accuracy: 0.9126 - val_loss: 2.2135 - val_accuracy: 0.5068\n",
      "Epoch 248/1000\n",
      "1968/1968 [==============================] - 2s 989us/step - loss: 0.1721 - accuracy: 0.9243 - val_loss: 2.3821 - val_accuracy: 0.4658\n",
      "Epoch 249/1000\n",
      "1968/1968 [==============================] - 2s 985us/step - loss: 0.2092 - accuracy: 0.9045 - val_loss: 2.2001 - val_accuracy: 0.4977\n",
      "Epoch 250/1000\n",
      "1968/1968 [==============================] - 2s 992us/step - loss: 0.2243 - accuracy: 0.9004 - val_loss: 2.3416 - val_accuracy: 0.4658\n",
      "Epoch 251/1000\n",
      "1968/1968 [==============================] - 2s 995us/step - loss: 0.2734 - accuracy: 0.8750 - val_loss: 1.9938 - val_accuracy: 0.4932\n",
      "Epoch 252/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2247 - accuracy: 0.9045 - val_loss: 2.2144 - val_accuracy: 0.4795\n",
      "Epoch 253/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2035 - accuracy: 0.9162 - val_loss: 2.2341 - val_accuracy: 0.4932\n",
      "Epoch 254/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1762 - accuracy: 0.9172 - val_loss: 2.0819 - val_accuracy: 0.5251\n",
      "Epoch 255/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1841 - accuracy: 0.9212 - val_loss: 2.2314 - val_accuracy: 0.4703\n",
      "Epoch 256/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1652 - accuracy: 0.9319 - val_loss: 2.2624 - val_accuracy: 0.4612\n",
      "Epoch 257/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1415 - accuracy: 0.9441 - val_loss: 2.3877 - val_accuracy: 0.4932\n",
      "Epoch 258/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1642 - accuracy: 0.9319 - val_loss: 2.5571 - val_accuracy: 0.4566\n",
      "Epoch 259/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.1745 - accuracy: 0.9284 - val_loss: 2.4668 - val_accuracy: 0.4749\n",
      "Epoch 260/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.2187 - accuracy: 0.8989 - val_loss: 2.2726 - val_accuracy: 0.4932\n",
      "Epoch 00260: early stopping\n",
      "30 day\n",
      "\n",
      "# Evaluate on test data\n",
      "244/244 [==============================] - 0s 407us/step\n",
      "test loss, test acc: [2.709462697388696, 0.4549180269241333]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (244, 1)\n",
      "rmse: 0.6776633592510598\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 30\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "print(\"30 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, input_shape=(30, 92))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(75, return_sequences=True, input_shape=(30, 92))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_48 (LSTM)               (None, 30, 50)            28600     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 30, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 30, 75)            37800     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 30, 75)            0         \n",
      "_________________________________________________________________\n",
      "lstm_50 (LSTM)               (None, 100)               70400     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,901\n",
      "Trainable params: 136,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1968 samples, validate on 219 samples\n",
      "Epoch 1/1000\n",
      "1968/1968 [==============================] - 3s 2ms/step - loss: 0.6957 - accuracy: 0.4990 - val_loss: 0.6897 - val_accuracy: 0.5571\n",
      "Epoch 2/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.6920 - accuracy: 0.5188 - val_loss: 0.6897 - val_accuracy: 0.5571\n",
      "Epoch 3/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.6936 - accuracy: 0.5340 - val_loss: 0.6938 - val_accuracy: 0.5068\n",
      "Epoch 4/1000\n",
      "1968/1968 [==============================] - 2s 866us/step - loss: 0.6917 - accuracy: 0.5361 - val_loss: 0.6904 - val_accuracy: 0.5571\n",
      "Epoch 5/1000\n",
      "1968/1968 [==============================] - 2s 881us/step - loss: 0.6917 - accuracy: 0.5427 - val_loss: 0.6929 - val_accuracy: 0.5205\n",
      "Epoch 6/1000\n",
      "1968/1968 [==============================] - 2s 870us/step - loss: 0.6908 - accuracy: 0.5249 - val_loss: 0.6921 - val_accuracy: 0.5388\n",
      "Epoch 7/1000\n",
      "1968/1968 [==============================] - 2s 896us/step - loss: 0.6927 - accuracy: 0.5274 - val_loss: 0.6928 - val_accuracy: 0.5160\n",
      "Epoch 8/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.6907 - accuracy: 0.5310 - val_loss: 0.6910 - val_accuracy: 0.5525\n",
      "Epoch 9/1000\n",
      "1968/1968 [==============================] - 2s 842us/step - loss: 0.6907 - accuracy: 0.5274 - val_loss: 0.6926 - val_accuracy: 0.5205\n",
      "Epoch 10/1000\n",
      "1968/1968 [==============================] - 2s 850us/step - loss: 0.6904 - accuracy: 0.5422 - val_loss: 0.6958 - val_accuracy: 0.5251\n",
      "Epoch 11/1000\n",
      "1968/1968 [==============================] - 2s 865us/step - loss: 0.6908 - accuracy: 0.5346 - val_loss: 0.6998 - val_accuracy: 0.5160\n",
      "Epoch 12/1000\n",
      "1968/1968 [==============================] - 2s 840us/step - loss: 0.6913 - accuracy: 0.5366 - val_loss: 0.6957 - val_accuracy: 0.5114\n",
      "Epoch 13/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.6927 - accuracy: 0.5152 - val_loss: 0.6926 - val_accuracy: 0.5251\n",
      "Epoch 14/1000\n",
      "1968/1968 [==============================] - 2s 897us/step - loss: 0.6906 - accuracy: 0.5371 - val_loss: 0.6942 - val_accuracy: 0.5160\n",
      "Epoch 15/1000\n",
      "1968/1968 [==============================] - 2s 882us/step - loss: 0.6901 - accuracy: 0.5330 - val_loss: 0.7002 - val_accuracy: 0.4977\n",
      "Epoch 16/1000\n",
      "1968/1968 [==============================] - 2s 863us/step - loss: 0.6903 - accuracy: 0.5381 - val_loss: 0.6946 - val_accuracy: 0.5160\n",
      "Epoch 17/1000\n",
      "1968/1968 [==============================] - 2s 883us/step - loss: 0.6907 - accuracy: 0.5295 - val_loss: 0.6967 - val_accuracy: 0.5251\n",
      "Epoch 18/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.6884 - accuracy: 0.5483 - val_loss: 0.7020 - val_accuracy: 0.5068\n",
      "Epoch 19/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.6889 - accuracy: 0.5305 - val_loss: 0.7013 - val_accuracy: 0.5479\n",
      "Epoch 20/1000\n",
      "1968/1968 [==============================] - 2s 840us/step - loss: 0.6921 - accuracy: 0.5340 - val_loss: 0.6919 - val_accuracy: 0.5479\n",
      "Epoch 21/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.6917 - accuracy: 0.5340 - val_loss: 0.6941 - val_accuracy: 0.5342\n",
      "Epoch 22/1000\n",
      "1968/1968 [==============================] - 2s 815us/step - loss: 0.6904 - accuracy: 0.5361 - val_loss: 0.7016 - val_accuracy: 0.5068\n",
      "Epoch 23/1000\n",
      "1968/1968 [==============================] - 2s 813us/step - loss: 0.6896 - accuracy: 0.5432 - val_loss: 0.6987 - val_accuracy: 0.5114\n",
      "Epoch 24/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.6886 - accuracy: 0.5437 - val_loss: 0.7042 - val_accuracy: 0.4977\n",
      "Epoch 25/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6903 - accuracy: 0.5335 - val_loss: 0.6928 - val_accuracy: 0.5525\n",
      "Epoch 26/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6931 - accuracy: 0.5310 - val_loss: 0.6930 - val_accuracy: 0.5205\n",
      "Epoch 27/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6905 - accuracy: 0.5401 - val_loss: 0.6978 - val_accuracy: 0.5342\n",
      "Epoch 28/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6891 - accuracy: 0.5442 - val_loss: 0.6926 - val_accuracy: 0.5479\n",
      "Epoch 29/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.6890 - accuracy: 0.5488 - val_loss: 0.6983 - val_accuracy: 0.5160\n",
      "Epoch 30/1000\n",
      "1968/1968 [==============================] - 2s 843us/step - loss: 0.6893 - accuracy: 0.5356 - val_loss: 0.6953 - val_accuracy: 0.5205\n",
      "Epoch 31/1000\n",
      "1968/1968 [==============================] - 2s 815us/step - loss: 0.6883 - accuracy: 0.5407 - val_loss: 0.7001 - val_accuracy: 0.5160\n",
      "Epoch 32/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6910 - accuracy: 0.5229 - val_loss: 0.6902 - val_accuracy: 0.5525\n",
      "Epoch 33/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6928 - accuracy: 0.5295 - val_loss: 0.6921 - val_accuracy: 0.5297\n",
      "Epoch 34/1000\n",
      "1968/1968 [==============================] - 2s 822us/step - loss: 0.6882 - accuracy: 0.5386 - val_loss: 0.6957 - val_accuracy: 0.5251\n",
      "Epoch 35/1000\n",
      "1968/1968 [==============================] - 2s 826us/step - loss: 0.6885 - accuracy: 0.5330 - val_loss: 0.6991 - val_accuracy: 0.4612\n",
      "Epoch 36/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.6888 - accuracy: 0.5371 - val_loss: 0.6971 - val_accuracy: 0.5068\n",
      "Epoch 37/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6877 - accuracy: 0.5422 - val_loss: 0.7019 - val_accuracy: 0.5114\n",
      "Epoch 38/1000\n",
      "1968/1968 [==============================] - 2s 826us/step - loss: 0.6908 - accuracy: 0.5437 - val_loss: 0.6937 - val_accuracy: 0.5297\n",
      "Epoch 39/1000\n",
      "1968/1968 [==============================] - 2s 832us/step - loss: 0.6879 - accuracy: 0.5473 - val_loss: 0.6965 - val_accuracy: 0.5251\n",
      "Epoch 40/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6869 - accuracy: 0.5401 - val_loss: 0.7019 - val_accuracy: 0.4795\n",
      "Epoch 41/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6852 - accuracy: 0.5523 - val_loss: 0.7042 - val_accuracy: 0.4384\n",
      "Epoch 42/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.6894 - accuracy: 0.5340 - val_loss: 0.6938 - val_accuracy: 0.5434\n",
      "Epoch 43/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.6874 - accuracy: 0.5417 - val_loss: 0.6981 - val_accuracy: 0.5068\n",
      "Epoch 44/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6860 - accuracy: 0.5518 - val_loss: 0.6995 - val_accuracy: 0.5114\n",
      "Epoch 45/1000\n",
      "1968/1968 [==============================] - 2s 832us/step - loss: 0.6861 - accuracy: 0.5401 - val_loss: 0.7031 - val_accuracy: 0.4703\n",
      "Epoch 46/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6840 - accuracy: 0.5473 - val_loss: 0.7039 - val_accuracy: 0.5388\n",
      "Epoch 47/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6885 - accuracy: 0.5386 - val_loss: 0.6990 - val_accuracy: 0.4932\n",
      "Epoch 48/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.6880 - accuracy: 0.5462 - val_loss: 0.6973 - val_accuracy: 0.5114\n",
      "Epoch 49/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.6864 - accuracy: 0.5432 - val_loss: 0.6995 - val_accuracy: 0.5251\n",
      "Epoch 50/1000\n",
      "1968/1968 [==============================] - 2s 818us/step - loss: 0.6850 - accuracy: 0.5376 - val_loss: 0.7047 - val_accuracy: 0.4977\n",
      "Epoch 51/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.6833 - accuracy: 0.5320 - val_loss: 0.7008 - val_accuracy: 0.5068\n",
      "Epoch 52/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.6834 - accuracy: 0.5574 - val_loss: 0.7074 - val_accuracy: 0.5023\n",
      "Epoch 53/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6839 - accuracy: 0.5544 - val_loss: 0.7040 - val_accuracy: 0.4840\n",
      "Epoch 54/1000\n",
      "1968/1968 [==============================] - 2s 826us/step - loss: 0.6837 - accuracy: 0.5473 - val_loss: 0.6960 - val_accuracy: 0.4658\n",
      "Epoch 55/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6840 - accuracy: 0.5569 - val_loss: 0.6965 - val_accuracy: 0.4840\n",
      "Epoch 56/1000\n",
      "1968/1968 [==============================] - 2s 832us/step - loss: 0.6846 - accuracy: 0.5462 - val_loss: 0.6992 - val_accuracy: 0.5205\n",
      "Epoch 57/1000\n",
      "1968/1968 [==============================] - 2s 823us/step - loss: 0.6877 - accuracy: 0.5330 - val_loss: 0.6917 - val_accuracy: 0.5388\n",
      "Epoch 58/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6873 - accuracy: 0.5432 - val_loss: 0.6978 - val_accuracy: 0.5205\n",
      "Epoch 59/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.6839 - accuracy: 0.5401 - val_loss: 0.7047 - val_accuracy: 0.5205\n",
      "Epoch 60/1000\n",
      "1968/1968 [==============================] - 2s 849us/step - loss: 0.6874 - accuracy: 0.5432 - val_loss: 0.7043 - val_accuracy: 0.4658\n",
      "Epoch 61/1000\n",
      "1968/1968 [==============================] - 2s 869us/step - loss: 0.6824 - accuracy: 0.5447 - val_loss: 0.7136 - val_accuracy: 0.4658\n",
      "Epoch 62/1000\n",
      "1968/1968 [==============================] - 2s 889us/step - loss: 0.6816 - accuracy: 0.5610 - val_loss: 0.6970 - val_accuracy: 0.5205\n",
      "Epoch 63/1000\n",
      "1968/1968 [==============================] - 2s 935us/step - loss: 0.6789 - accuracy: 0.5676 - val_loss: 0.7144 - val_accuracy: 0.4201\n",
      "Epoch 64/1000\n",
      "1968/1968 [==============================] - 2s 964us/step - loss: 0.6797 - accuracy: 0.5645 - val_loss: 0.6968 - val_accuracy: 0.4886\n",
      "Epoch 65/1000\n",
      "1968/1968 [==============================] - 2s 984us/step - loss: 0.6803 - accuracy: 0.5493 - val_loss: 0.7025 - val_accuracy: 0.5068\n",
      "Epoch 66/1000\n",
      "1968/1968 [==============================] - 2s 992us/step - loss: 0.6770 - accuracy: 0.5696 - val_loss: 0.7111 - val_accuracy: 0.4566\n",
      "Epoch 67/1000\n",
      "1968/1968 [==============================] - 2s 1ms/step - loss: 0.6763 - accuracy: 0.5803 - val_loss: 0.7042 - val_accuracy: 0.4886\n",
      "Epoch 68/1000\n",
      "1968/1968 [==============================] - 2s 970us/step - loss: 0.6719 - accuracy: 0.5899 - val_loss: 0.7130 - val_accuracy: 0.4612\n",
      "Epoch 69/1000\n",
      "1968/1968 [==============================] - 2s 914us/step - loss: 0.6792 - accuracy: 0.5737 - val_loss: 0.6979 - val_accuracy: 0.4886\n",
      "Epoch 70/1000\n",
      "1968/1968 [==============================] - 2s 879us/step - loss: 0.6761 - accuracy: 0.5783 - val_loss: 0.7202 - val_accuracy: 0.4566\n",
      "Epoch 71/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.6731 - accuracy: 0.5747 - val_loss: 0.7134 - val_accuracy: 0.4612\n",
      "Epoch 72/1000\n",
      "1968/1968 [==============================] - 2s 863us/step - loss: 0.6751 - accuracy: 0.5772 - val_loss: 0.7136 - val_accuracy: 0.4795\n",
      "Epoch 73/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.6703 - accuracy: 0.5716 - val_loss: 0.7072 - val_accuracy: 0.4566\n",
      "Epoch 74/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6690 - accuracy: 0.5869 - val_loss: 0.7113 - val_accuracy: 0.5251\n",
      "Epoch 75/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.6719 - accuracy: 0.5996 - val_loss: 0.7335 - val_accuracy: 0.5023\n",
      "Epoch 76/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.6756 - accuracy: 0.5732 - val_loss: 0.7142 - val_accuracy: 0.5023\n",
      "Epoch 77/1000\n",
      "1968/1968 [==============================] - 2s 853us/step - loss: 0.6651 - accuracy: 0.6052 - val_loss: 0.7420 - val_accuracy: 0.5023\n",
      "Epoch 78/1000\n",
      "1968/1968 [==============================] - 2s 894us/step - loss: 0.6790 - accuracy: 0.5783 - val_loss: 0.6990 - val_accuracy: 0.5662\n",
      "Epoch 79/1000\n",
      "1968/1968 [==============================] - 2s 890us/step - loss: 0.6723 - accuracy: 0.5874 - val_loss: 0.7113 - val_accuracy: 0.5023\n",
      "Epoch 80/1000\n",
      "1968/1968 [==============================] - 2s 864us/step - loss: 0.6623 - accuracy: 0.6103 - val_loss: 0.7242 - val_accuracy: 0.4749\n",
      "Epoch 81/1000\n",
      "1968/1968 [==============================] - 2s 876us/step - loss: 0.6725 - accuracy: 0.5813 - val_loss: 0.7095 - val_accuracy: 0.5023\n",
      "Epoch 82/1000\n",
      "1968/1968 [==============================] - 2s 880us/step - loss: 0.6593 - accuracy: 0.6092 - val_loss: 0.7471 - val_accuracy: 0.4658\n",
      "Epoch 83/1000\n",
      "1968/1968 [==============================] - 2s 870us/step - loss: 0.6641 - accuracy: 0.6026 - val_loss: 0.7309 - val_accuracy: 0.4886\n",
      "Epoch 84/1000\n",
      "1968/1968 [==============================] - 2s 876us/step - loss: 0.6647 - accuracy: 0.5879 - val_loss: 0.7386 - val_accuracy: 0.4292\n",
      "Epoch 85/1000\n",
      "1968/1968 [==============================] - 2s 852us/step - loss: 0.6645 - accuracy: 0.5889 - val_loss: 0.7220 - val_accuracy: 0.4703\n",
      "Epoch 86/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.6530 - accuracy: 0.6113 - val_loss: 0.7448 - val_accuracy: 0.4247\n",
      "Epoch 87/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.6541 - accuracy: 0.6108 - val_loss: 0.7581 - val_accuracy: 0.4110\n",
      "Epoch 88/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6551 - accuracy: 0.5991 - val_loss: 0.7543 - val_accuracy: 0.4566\n",
      "Epoch 89/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6584 - accuracy: 0.6047 - val_loss: 0.7577 - val_accuracy: 0.4749\n",
      "Epoch 90/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6496 - accuracy: 0.6067 - val_loss: 0.7514 - val_accuracy: 0.4749\n",
      "Epoch 91/1000\n",
      "1968/1968 [==============================] - 2s 838us/step - loss: 0.6613 - accuracy: 0.6011 - val_loss: 0.7440 - val_accuracy: 0.4703\n",
      "Epoch 92/1000\n",
      "1968/1968 [==============================] - 2s 842us/step - loss: 0.6550 - accuracy: 0.6087 - val_loss: 0.7517 - val_accuracy: 0.4521\n",
      "Epoch 93/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.6444 - accuracy: 0.6260 - val_loss: 0.7499 - val_accuracy: 0.4475\n",
      "Epoch 94/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.6412 - accuracy: 0.6225 - val_loss: 0.7596 - val_accuracy: 0.4749\n",
      "Epoch 95/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.6441 - accuracy: 0.6148 - val_loss: 0.7537 - val_accuracy: 0.5205\n",
      "Epoch 96/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.6456 - accuracy: 0.6138 - val_loss: 0.7738 - val_accuracy: 0.4429\n",
      "Epoch 97/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.6426 - accuracy: 0.6286 - val_loss: 0.7509 - val_accuracy: 0.4658\n",
      "Epoch 98/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.6538 - accuracy: 0.6108 - val_loss: 0.7741 - val_accuracy: 0.4384\n",
      "Epoch 99/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.6481 - accuracy: 0.6047 - val_loss: 0.7479 - val_accuracy: 0.4612\n",
      "Epoch 100/1000\n",
      "1968/1968 [==============================] - 2s 835us/step - loss: 0.6384 - accuracy: 0.6367 - val_loss: 0.7766 - val_accuracy: 0.4932\n",
      "Epoch 101/1000\n",
      "1968/1968 [==============================] - 2s 818us/step - loss: 0.6424 - accuracy: 0.6286 - val_loss: 0.7432 - val_accuracy: 0.4749\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6328 - accuracy: 0.6341 - val_loss: 0.7608 - val_accuracy: 0.4521\n",
      "Epoch 103/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.6336 - accuracy: 0.6347 - val_loss: 0.7830 - val_accuracy: 0.4886\n",
      "Epoch 104/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.6333 - accuracy: 0.6336 - val_loss: 0.7324 - val_accuracy: 0.5160\n",
      "Epoch 105/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.6385 - accuracy: 0.6225 - val_loss: 0.7615 - val_accuracy: 0.4886\n",
      "Epoch 106/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6383 - accuracy: 0.6357 - val_loss: 0.7714 - val_accuracy: 0.4795\n",
      "Epoch 107/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6506 - accuracy: 0.6286 - val_loss: 0.7743 - val_accuracy: 0.4612\n",
      "Epoch 108/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.6287 - accuracy: 0.6357 - val_loss: 0.8073 - val_accuracy: 0.4475\n",
      "Epoch 109/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.6311 - accuracy: 0.6372 - val_loss: 0.7970 - val_accuracy: 0.4521\n",
      "Epoch 110/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6269 - accuracy: 0.6286 - val_loss: 0.7552 - val_accuracy: 0.4886\n",
      "Epoch 111/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.6376 - accuracy: 0.6240 - val_loss: 0.7952 - val_accuracy: 0.4840\n",
      "Epoch 112/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.6187 - accuracy: 0.6367 - val_loss: 0.7958 - val_accuracy: 0.4703\n",
      "Epoch 113/1000\n",
      "1968/1968 [==============================] - 2s 853us/step - loss: 0.6376 - accuracy: 0.6240 - val_loss: 0.7887 - val_accuracy: 0.4475\n",
      "Epoch 114/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.6245 - accuracy: 0.6418 - val_loss: 0.8041 - val_accuracy: 0.4795\n",
      "Epoch 115/1000\n",
      "1968/1968 [==============================] - 2s 852us/step - loss: 0.6135 - accuracy: 0.6529 - val_loss: 0.7895 - val_accuracy: 0.4886\n",
      "Epoch 116/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.6151 - accuracy: 0.6504 - val_loss: 0.7952 - val_accuracy: 0.4658\n",
      "Epoch 117/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6197 - accuracy: 0.6402 - val_loss: 0.8233 - val_accuracy: 0.4521\n",
      "Epoch 118/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6059 - accuracy: 0.6529 - val_loss: 0.8384 - val_accuracy: 0.4749\n",
      "Epoch 119/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.6201 - accuracy: 0.6408 - val_loss: 0.7981 - val_accuracy: 0.5068\n",
      "Epoch 120/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.6164 - accuracy: 0.6504 - val_loss: 0.8099 - val_accuracy: 0.4521\n",
      "Epoch 121/1000\n",
      "1968/1968 [==============================] - 2s 826us/step - loss: 0.6093 - accuracy: 0.6550 - val_loss: 0.8149 - val_accuracy: 0.4384\n",
      "Epoch 122/1000\n",
      "1968/1968 [==============================] - 2s 838us/step - loss: 0.6116 - accuracy: 0.6423 - val_loss: 0.8271 - val_accuracy: 0.4932\n",
      "Epoch 123/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.6140 - accuracy: 0.6443 - val_loss: 0.7971 - val_accuracy: 0.4703\n",
      "Epoch 124/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.6054 - accuracy: 0.6611 - val_loss: 0.8287 - val_accuracy: 0.4749\n",
      "Epoch 125/1000\n",
      "1968/1968 [==============================] - 2s 893us/step - loss: 0.6060 - accuracy: 0.6529 - val_loss: 0.8287 - val_accuracy: 0.4795\n",
      "Epoch 126/1000\n",
      "1968/1968 [==============================] - 2s 862us/step - loss: 0.6053 - accuracy: 0.6555 - val_loss: 0.8565 - val_accuracy: 0.4795\n",
      "Epoch 127/1000\n",
      "1968/1968 [==============================] - 2s 897us/step - loss: 0.6076 - accuracy: 0.6519 - val_loss: 0.8782 - val_accuracy: 0.4749\n",
      "Epoch 128/1000\n",
      "1968/1968 [==============================] - 2s 938us/step - loss: 0.6048 - accuracy: 0.6545 - val_loss: 0.8109 - val_accuracy: 0.4566\n",
      "Epoch 129/1000\n",
      "1968/1968 [==============================] - 2s 907us/step - loss: 0.6093 - accuracy: 0.6550 - val_loss: 0.8554 - val_accuracy: 0.4840\n",
      "Epoch 130/1000\n",
      "1968/1968 [==============================] - 2s 950us/step - loss: 0.5966 - accuracy: 0.6646 - val_loss: 0.8412 - val_accuracy: 0.4749\n",
      "Epoch 131/1000\n",
      "1968/1968 [==============================] - 2s 936us/step - loss: 0.5914 - accuracy: 0.6560 - val_loss: 0.9026 - val_accuracy: 0.4566\n",
      "Epoch 132/1000\n",
      "1968/1968 [==============================] - 2s 932us/step - loss: 0.6171 - accuracy: 0.6504 - val_loss: 0.8117 - val_accuracy: 0.4429\n",
      "Epoch 133/1000\n",
      "1968/1968 [==============================] - 2s 914us/step - loss: 0.6329 - accuracy: 0.6326 - val_loss: 0.8166 - val_accuracy: 0.4795\n",
      "Epoch 134/1000\n",
      "1968/1968 [==============================] - 2s 893us/step - loss: 0.6044 - accuracy: 0.6590 - val_loss: 0.8339 - val_accuracy: 0.4703\n",
      "Epoch 135/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.5940 - accuracy: 0.6606 - val_loss: 0.8725 - val_accuracy: 0.4566\n",
      "Epoch 136/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.5825 - accuracy: 0.6778 - val_loss: 0.8268 - val_accuracy: 0.4886\n",
      "Epoch 137/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.5957 - accuracy: 0.6641 - val_loss: 0.9111 - val_accuracy: 0.4566\n",
      "Epoch 138/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.5994 - accuracy: 0.6580 - val_loss: 0.8543 - val_accuracy: 0.4612\n",
      "Epoch 139/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.5775 - accuracy: 0.6738 - val_loss: 0.9325 - val_accuracy: 0.4703\n",
      "Epoch 140/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.5930 - accuracy: 0.6723 - val_loss: 0.8519 - val_accuracy: 0.4612\n",
      "Epoch 141/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.5842 - accuracy: 0.6707 - val_loss: 0.9138 - val_accuracy: 0.4795\n",
      "Epoch 142/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.5753 - accuracy: 0.6728 - val_loss: 0.8915 - val_accuracy: 0.4795\n",
      "Epoch 143/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.5846 - accuracy: 0.6677 - val_loss: 0.9122 - val_accuracy: 0.4749\n",
      "Epoch 144/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.5847 - accuracy: 0.6631 - val_loss: 0.8950 - val_accuracy: 0.4521\n",
      "Epoch 145/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.5615 - accuracy: 0.6865 - val_loss: 0.9262 - val_accuracy: 0.4749\n",
      "Epoch 146/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.5760 - accuracy: 0.6789 - val_loss: 0.9022 - val_accuracy: 0.4749\n",
      "Epoch 147/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.5751 - accuracy: 0.6789 - val_loss: 0.8634 - val_accuracy: 0.4658\n",
      "Epoch 148/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.5692 - accuracy: 0.6951 - val_loss: 0.9611 - val_accuracy: 0.4749\n",
      "Epoch 149/1000\n",
      "1968/1968 [==============================] - 2s 866us/step - loss: 0.5595 - accuracy: 0.6855 - val_loss: 0.9122 - val_accuracy: 0.4886\n",
      "Epoch 150/1000\n",
      "1968/1968 [==============================] - 2s 862us/step - loss: 0.5566 - accuracy: 0.6982 - val_loss: 0.9267 - val_accuracy: 0.5205\n",
      "Epoch 151/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.5674 - accuracy: 0.6819 - val_loss: 0.9543 - val_accuracy: 0.4886\n",
      "Epoch 152/1000\n",
      "1968/1968 [==============================] - 2s 862us/step - loss: 0.5711 - accuracy: 0.6753 - val_loss: 0.9196 - val_accuracy: 0.5068\n",
      "Epoch 153/1000\n",
      "1968/1968 [==============================] - 2s 852us/step - loss: 0.5670 - accuracy: 0.6839 - val_loss: 0.9617 - val_accuracy: 0.4384\n",
      "Epoch 154/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.5568 - accuracy: 0.6870 - val_loss: 0.9904 - val_accuracy: 0.4566\n",
      "Epoch 155/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.5616 - accuracy: 0.6855 - val_loss: 0.9484 - val_accuracy: 0.4566\n",
      "Epoch 156/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.5611 - accuracy: 0.6921 - val_loss: 0.9207 - val_accuracy: 0.4612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1000\n",
      "1968/1968 [==============================] - 2s 824us/step - loss: 0.5395 - accuracy: 0.7088 - val_loss: 0.9561 - val_accuracy: 0.4977\n",
      "Epoch 158/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.5673 - accuracy: 0.6839 - val_loss: 0.8926 - val_accuracy: 0.4977\n",
      "Epoch 159/1000\n",
      "1968/1968 [==============================] - 2s 835us/step - loss: 0.5560 - accuracy: 0.6905 - val_loss: 0.9696 - val_accuracy: 0.4886\n",
      "Epoch 160/1000\n",
      "1968/1968 [==============================] - 2s 822us/step - loss: 0.5541 - accuracy: 0.6987 - val_loss: 0.9650 - val_accuracy: 0.4977\n",
      "Epoch 161/1000\n",
      "1968/1968 [==============================] - 2s 800us/step - loss: 0.5534 - accuracy: 0.6855 - val_loss: 0.9668 - val_accuracy: 0.4886\n",
      "Epoch 162/1000\n",
      "1968/1968 [==============================] - 2s 808us/step - loss: 0.5436 - accuracy: 0.6900 - val_loss: 0.9495 - val_accuracy: 0.5251\n",
      "Epoch 163/1000\n",
      "1968/1968 [==============================] - 2s 810us/step - loss: 0.5314 - accuracy: 0.7170 - val_loss: 1.0137 - val_accuracy: 0.4658\n",
      "Epoch 164/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.5379 - accuracy: 0.6911 - val_loss: 0.9847 - val_accuracy: 0.4795\n",
      "Epoch 165/1000\n",
      "1968/1968 [==============================] - 2s 823us/step - loss: 0.5304 - accuracy: 0.7012 - val_loss: 1.0037 - val_accuracy: 0.4795\n",
      "Epoch 166/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.5193 - accuracy: 0.7134 - val_loss: 1.0028 - val_accuracy: 0.4840\n",
      "Epoch 167/1000\n",
      "1968/1968 [==============================] - 2s 872us/step - loss: 0.5388 - accuracy: 0.7002 - val_loss: 0.9625 - val_accuracy: 0.4977\n",
      "Epoch 168/1000\n",
      "1968/1968 [==============================] - 2s 913us/step - loss: 0.5602 - accuracy: 0.6865 - val_loss: 0.9463 - val_accuracy: 0.5068\n",
      "Epoch 169/1000\n",
      "1968/1968 [==============================] - 2s 888us/step - loss: 0.5309 - accuracy: 0.7017 - val_loss: 0.9803 - val_accuracy: 0.4612\n",
      "Epoch 170/1000\n",
      "1968/1968 [==============================] - 2s 941us/step - loss: 0.5229 - accuracy: 0.7124 - val_loss: 0.9674 - val_accuracy: 0.4932\n",
      "Epoch 171/1000\n",
      "1968/1968 [==============================] - 2s 913us/step - loss: 0.5392 - accuracy: 0.7068 - val_loss: 0.9828 - val_accuracy: 0.4749\n",
      "Epoch 172/1000\n",
      "1968/1968 [==============================] - 2s 909us/step - loss: 0.5209 - accuracy: 0.7139 - val_loss: 0.9817 - val_accuracy: 0.4886\n",
      "Epoch 173/1000\n",
      "1968/1968 [==============================] - 2s 896us/step - loss: 0.5189 - accuracy: 0.7165 - val_loss: 1.0129 - val_accuracy: 0.4612\n",
      "Epoch 174/1000\n",
      "1968/1968 [==============================] - 2s 932us/step - loss: 0.5154 - accuracy: 0.7185 - val_loss: 1.0282 - val_accuracy: 0.4840\n",
      "Epoch 175/1000\n",
      "1968/1968 [==============================] - 2s 899us/step - loss: 0.5181 - accuracy: 0.7215 - val_loss: 0.9806 - val_accuracy: 0.4932\n",
      "Epoch 176/1000\n",
      "1968/1968 [==============================] - 2s 844us/step - loss: 0.5184 - accuracy: 0.7109 - val_loss: 1.0228 - val_accuracy: 0.4566\n",
      "Epoch 177/1000\n",
      "1968/1968 [==============================] - 2s 842us/step - loss: 0.5091 - accuracy: 0.7165 - val_loss: 1.0438 - val_accuracy: 0.4749\n",
      "Epoch 178/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.4922 - accuracy: 0.7363 - val_loss: 1.0520 - val_accuracy: 0.4795\n",
      "Epoch 179/1000\n",
      "1968/1968 [==============================] - 2s 835us/step - loss: 0.4962 - accuracy: 0.7393 - val_loss: 0.9950 - val_accuracy: 0.4795\n",
      "Epoch 180/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.5087 - accuracy: 0.7322 - val_loss: 1.0440 - val_accuracy: 0.5023\n",
      "Epoch 181/1000\n",
      "1968/1968 [==============================] - 2s 868us/step - loss: 0.5088 - accuracy: 0.7190 - val_loss: 0.9913 - val_accuracy: 0.4977\n",
      "Epoch 182/1000\n",
      "1968/1968 [==============================] - 2s 860us/step - loss: 0.5132 - accuracy: 0.7231 - val_loss: 1.0882 - val_accuracy: 0.4795\n",
      "Epoch 183/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.5034 - accuracy: 0.7282 - val_loss: 1.0575 - val_accuracy: 0.5114\n",
      "Epoch 184/1000\n",
      "1968/1968 [==============================] - 2s 825us/step - loss: 0.5079 - accuracy: 0.7205 - val_loss: 1.0389 - val_accuracy: 0.4840\n",
      "Epoch 185/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.4786 - accuracy: 0.7383 - val_loss: 1.0879 - val_accuracy: 0.4840\n",
      "Epoch 186/1000\n",
      "1968/1968 [==============================] - 2s 848us/step - loss: 0.5008 - accuracy: 0.7246 - val_loss: 1.0731 - val_accuracy: 0.4795\n",
      "Epoch 187/1000\n",
      "1968/1968 [==============================] - 2s 850us/step - loss: 0.5093 - accuracy: 0.7205 - val_loss: 1.1027 - val_accuracy: 0.4749\n",
      "Epoch 188/1000\n",
      "1968/1968 [==============================] - 2s 908us/step - loss: 0.5045 - accuracy: 0.7287 - val_loss: 1.0590 - val_accuracy: 0.4612\n",
      "Epoch 189/1000\n",
      "1968/1968 [==============================] - 2s 912us/step - loss: 0.4958 - accuracy: 0.7327 - val_loss: 1.0680 - val_accuracy: 0.4977\n",
      "Epoch 190/1000\n",
      "1968/1968 [==============================] - 2s 860us/step - loss: 0.4621 - accuracy: 0.7536 - val_loss: 1.1250 - val_accuracy: 0.5251\n",
      "Epoch 191/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.4863 - accuracy: 0.7261 - val_loss: 1.0800 - val_accuracy: 0.4886\n",
      "Epoch 192/1000\n",
      "1968/1968 [==============================] - 2s 816us/step - loss: 0.4656 - accuracy: 0.7480 - val_loss: 1.0563 - val_accuracy: 0.4658\n",
      "Epoch 193/1000\n",
      "1968/1968 [==============================] - 2s 823us/step - loss: 0.4861 - accuracy: 0.7266 - val_loss: 1.1135 - val_accuracy: 0.5068\n",
      "Epoch 194/1000\n",
      "1968/1968 [==============================] - 2s 817us/step - loss: 0.4739 - accuracy: 0.7470 - val_loss: 1.0596 - val_accuracy: 0.5068\n",
      "Epoch 195/1000\n",
      "1968/1968 [==============================] - 2s 807us/step - loss: 0.4802 - accuracy: 0.7363 - val_loss: 1.1580 - val_accuracy: 0.5114\n",
      "Epoch 196/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.4934 - accuracy: 0.7409 - val_loss: 1.0704 - val_accuracy: 0.4521\n",
      "Epoch 197/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.4698 - accuracy: 0.7510 - val_loss: 1.0495 - val_accuracy: 0.5571\n",
      "Epoch 198/1000\n",
      "1968/1968 [==============================] - 2s 846us/step - loss: 0.4607 - accuracy: 0.7414 - val_loss: 1.1433 - val_accuracy: 0.4658\n",
      "Epoch 199/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.4575 - accuracy: 0.7520 - val_loss: 1.1522 - val_accuracy: 0.4749\n",
      "Epoch 200/1000\n",
      "1968/1968 [==============================] - 2s 848us/step - loss: 0.4566 - accuracy: 0.7612 - val_loss: 1.1552 - val_accuracy: 0.4840\n",
      "Epoch 201/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.4474 - accuracy: 0.7663 - val_loss: 1.1861 - val_accuracy: 0.4886\n",
      "Epoch 202/1000\n",
      "1968/1968 [==============================] - 2s 848us/step - loss: 0.4544 - accuracy: 0.7566 - val_loss: 1.1831 - val_accuracy: 0.4977\n",
      "Epoch 203/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.4423 - accuracy: 0.7678 - val_loss: 1.2241 - val_accuracy: 0.4795\n",
      "Epoch 204/1000\n",
      "1968/1968 [==============================] - 2s 847us/step - loss: 0.4455 - accuracy: 0.7520 - val_loss: 1.1502 - val_accuracy: 0.4612\n",
      "Epoch 205/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.4485 - accuracy: 0.7622 - val_loss: 1.2228 - val_accuracy: 0.4795\n",
      "Epoch 206/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.4438 - accuracy: 0.7617 - val_loss: 1.2404 - val_accuracy: 0.4886\n",
      "Epoch 207/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.4425 - accuracy: 0.7586 - val_loss: 1.2200 - val_accuracy: 0.4932\n",
      "Epoch 208/1000\n",
      "1968/1968 [==============================] - 2s 873us/step - loss: 0.4388 - accuracy: 0.7647 - val_loss: 1.2535 - val_accuracy: 0.4749\n",
      "Epoch 209/1000\n",
      "1968/1968 [==============================] - 2s 879us/step - loss: 0.4409 - accuracy: 0.7652 - val_loss: 1.1861 - val_accuracy: 0.4795\n",
      "Epoch 210/1000\n",
      "1968/1968 [==============================] - 2s 888us/step - loss: 0.4670 - accuracy: 0.7475 - val_loss: 1.1739 - val_accuracy: 0.4977\n",
      "Epoch 211/1000\n",
      "1968/1968 [==============================] - 2s 906us/step - loss: 0.4666 - accuracy: 0.7597 - val_loss: 1.2140 - val_accuracy: 0.4749\n",
      "Epoch 212/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.4641 - accuracy: 0.7637 - val_loss: 1.1837 - val_accuracy: 0.5023\n",
      "Epoch 213/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.4519 - accuracy: 0.7591 - val_loss: 1.1677 - val_accuracy: 0.5068\n",
      "Epoch 214/1000\n",
      "1968/1968 [==============================] - 2s 869us/step - loss: 0.4333 - accuracy: 0.7718 - val_loss: 1.1875 - val_accuracy: 0.4977\n",
      "Epoch 215/1000\n",
      "1968/1968 [==============================] - 2s 910us/step - loss: 0.4319 - accuracy: 0.7698 - val_loss: 1.1692 - val_accuracy: 0.4977\n",
      "Epoch 216/1000\n",
      "1968/1968 [==============================] - 2s 871us/step - loss: 0.4186 - accuracy: 0.7800 - val_loss: 1.2620 - val_accuracy: 0.5160\n",
      "Epoch 217/1000\n",
      "1968/1968 [==============================] - 2s 863us/step - loss: 0.4256 - accuracy: 0.7749 - val_loss: 1.1831 - val_accuracy: 0.5114\n",
      "Epoch 218/1000\n",
      "1968/1968 [==============================] - 2s 903us/step - loss: 0.4326 - accuracy: 0.7693 - val_loss: 1.2151 - val_accuracy: 0.4749\n",
      "Epoch 219/1000\n",
      "1968/1968 [==============================] - 2s 908us/step - loss: 0.4350 - accuracy: 0.7637 - val_loss: 1.1359 - val_accuracy: 0.4977\n",
      "Epoch 220/1000\n",
      "1968/1968 [==============================] - 2s 899us/step - loss: 0.4252 - accuracy: 0.7724 - val_loss: 1.2898 - val_accuracy: 0.4703\n",
      "Epoch 221/1000\n",
      "1968/1968 [==============================] - 2s 901us/step - loss: 0.4148 - accuracy: 0.7820 - val_loss: 1.2089 - val_accuracy: 0.4977\n",
      "Epoch 222/1000\n",
      "1968/1968 [==============================] - 2s 880us/step - loss: 0.3973 - accuracy: 0.7856 - val_loss: 1.3104 - val_accuracy: 0.4795\n",
      "Epoch 223/1000\n",
      "1968/1968 [==============================] - 2s 864us/step - loss: 0.3982 - accuracy: 0.7805 - val_loss: 1.2954 - val_accuracy: 0.4749\n",
      "Epoch 224/1000\n",
      "1968/1968 [==============================] - 2s 856us/step - loss: 0.3971 - accuracy: 0.7866 - val_loss: 1.3459 - val_accuracy: 0.4703\n",
      "Epoch 225/1000\n",
      "1968/1968 [==============================] - 2s 863us/step - loss: 0.4033 - accuracy: 0.7856 - val_loss: 1.2934 - val_accuracy: 0.4977\n",
      "Epoch 226/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.4017 - accuracy: 0.7942 - val_loss: 1.3117 - val_accuracy: 0.4749\n",
      "Epoch 227/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.3925 - accuracy: 0.7973 - val_loss: 1.4476 - val_accuracy: 0.4932\n",
      "Epoch 228/1000\n",
      "1968/1968 [==============================] - 2s 853us/step - loss: 0.3867 - accuracy: 0.7952 - val_loss: 1.3695 - val_accuracy: 0.5114\n",
      "Epoch 229/1000\n",
      "1968/1968 [==============================] - 2s 886us/step - loss: 0.3911 - accuracy: 0.7993 - val_loss: 1.3519 - val_accuracy: 0.5023\n",
      "Epoch 230/1000\n",
      "1968/1968 [==============================] - 2s 885us/step - loss: 0.4129 - accuracy: 0.7907 - val_loss: 1.3149 - val_accuracy: 0.4658\n",
      "Epoch 231/1000\n",
      "1968/1968 [==============================] - 2s 930us/step - loss: 0.4155 - accuracy: 0.7795 - val_loss: 1.3394 - val_accuracy: 0.4795\n",
      "Epoch 232/1000\n",
      "1968/1968 [==============================] - 2s 888us/step - loss: 0.4097 - accuracy: 0.7724 - val_loss: 1.3735 - val_accuracy: 0.4795\n",
      "Epoch 233/1000\n",
      "1968/1968 [==============================] - 2s 906us/step - loss: 0.3986 - accuracy: 0.7917 - val_loss: 1.2873 - val_accuracy: 0.4977\n",
      "Epoch 234/1000\n",
      "1968/1968 [==============================] - 2s 895us/step - loss: 0.3942 - accuracy: 0.7840 - val_loss: 1.3897 - val_accuracy: 0.4795\n",
      "Epoch 235/1000\n",
      "1968/1968 [==============================] - 2s 887us/step - loss: 0.3668 - accuracy: 0.8125 - val_loss: 1.4711 - val_accuracy: 0.4886\n",
      "Epoch 236/1000\n",
      "1968/1968 [==============================] - 2s 882us/step - loss: 0.3993 - accuracy: 0.7983 - val_loss: 1.3417 - val_accuracy: 0.4795\n",
      "Epoch 237/1000\n",
      "1968/1968 [==============================] - 2s 886us/step - loss: 0.3944 - accuracy: 0.7901 - val_loss: 1.4417 - val_accuracy: 0.4703\n",
      "Epoch 238/1000\n",
      "1968/1968 [==============================] - 2s 886us/step - loss: 0.3786 - accuracy: 0.8018 - val_loss: 1.3497 - val_accuracy: 0.5068\n",
      "Epoch 239/1000\n",
      "1968/1968 [==============================] - 2s 905us/step - loss: 0.3731 - accuracy: 0.8105 - val_loss: 1.4542 - val_accuracy: 0.4795\n",
      "Epoch 240/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.3559 - accuracy: 0.8105 - val_loss: 1.4862 - val_accuracy: 0.5205\n",
      "Epoch 241/1000\n",
      "1968/1968 [==============================] - 2s 851us/step - loss: 0.3544 - accuracy: 0.8084 - val_loss: 1.5261 - val_accuracy: 0.4749\n",
      "Epoch 242/1000\n",
      "1968/1968 [==============================] - 2s 893us/step - loss: 0.3403 - accuracy: 0.8298 - val_loss: 1.5243 - val_accuracy: 0.4749\n",
      "Epoch 243/1000\n",
      "1968/1968 [==============================] - 2s 878us/step - loss: 0.3637 - accuracy: 0.8237 - val_loss: 1.5587 - val_accuracy: 0.5068\n",
      "Epoch 244/1000\n",
      "1968/1968 [==============================] - 2s 890us/step - loss: 0.3672 - accuracy: 0.8150 - val_loss: 1.5526 - val_accuracy: 0.4566\n",
      "Epoch 245/1000\n",
      "1968/1968 [==============================] - 2s 865us/step - loss: 0.3953 - accuracy: 0.7962 - val_loss: 1.3736 - val_accuracy: 0.5205\n",
      "Epoch 246/1000\n",
      "1968/1968 [==============================] - 2s 848us/step - loss: 0.4112 - accuracy: 0.7744 - val_loss: 1.4752 - val_accuracy: 0.5068\n",
      "Epoch 247/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.3599 - accuracy: 0.8125 - val_loss: 1.5454 - val_accuracy: 0.4795\n",
      "Epoch 248/1000\n",
      "1968/1968 [==============================] - 2s 841us/step - loss: 0.3650 - accuracy: 0.8125 - val_loss: 1.5486 - val_accuracy: 0.5342\n",
      "Epoch 249/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.3738 - accuracy: 0.8034 - val_loss: 1.5095 - val_accuracy: 0.4795\n",
      "Epoch 250/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.3648 - accuracy: 0.8130 - val_loss: 1.4750 - val_accuracy: 0.4292\n",
      "Epoch 251/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3594 - accuracy: 0.8211 - val_loss: 1.4926 - val_accuracy: 0.4932\n",
      "Epoch 252/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3393 - accuracy: 0.8323 - val_loss: 1.5573 - val_accuracy: 0.4658\n",
      "Epoch 253/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.3314 - accuracy: 0.8272 - val_loss: 1.4927 - val_accuracy: 0.5251\n",
      "Epoch 254/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3300 - accuracy: 0.8277 - val_loss: 1.6011 - val_accuracy: 0.4977\n",
      "Epoch 255/1000\n",
      "1968/1968 [==============================] - 2s 850us/step - loss: 0.3300 - accuracy: 0.8277 - val_loss: 1.5259 - val_accuracy: 0.5023\n",
      "Epoch 256/1000\n",
      "1968/1968 [==============================] - 2s 833us/step - loss: 0.3508 - accuracy: 0.8201 - val_loss: 1.6204 - val_accuracy: 0.5023\n",
      "Epoch 257/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.3512 - accuracy: 0.8186 - val_loss: 1.5873 - val_accuracy: 0.4977\n",
      "Epoch 258/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3646 - accuracy: 0.8110 - val_loss: 1.4678 - val_accuracy: 0.5068\n",
      "Epoch 259/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.3414 - accuracy: 0.8303 - val_loss: 1.5475 - val_accuracy: 0.4840\n",
      "Epoch 260/1000\n",
      "1968/1968 [==============================] - 2s 830us/step - loss: 0.3326 - accuracy: 0.8328 - val_loss: 1.5628 - val_accuracy: 0.5023\n",
      "Epoch 261/1000\n",
      "1968/1968 [==============================] - 2s 839us/step - loss: 0.3253 - accuracy: 0.8369 - val_loss: 1.7393 - val_accuracy: 0.4840\n",
      "Epoch 262/1000\n",
      "1968/1968 [==============================] - 2s 858us/step - loss: 0.3390 - accuracy: 0.8277 - val_loss: 1.6044 - val_accuracy: 0.5251\n",
      "Epoch 263/1000\n",
      "1968/1968 [==============================] - 2s 855us/step - loss: 0.3314 - accuracy: 0.8293 - val_loss: 1.6872 - val_accuracy: 0.4795\n",
      "Epoch 264/1000\n",
      "1968/1968 [==============================] - 2s 864us/step - loss: 0.3379 - accuracy: 0.8267 - val_loss: 1.5901 - val_accuracy: 0.5251\n",
      "Epoch 265/1000\n",
      "1968/1968 [==============================] - 2s 861us/step - loss: 0.3317 - accuracy: 0.8308 - val_loss: 1.6206 - val_accuracy: 0.5297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.3159 - accuracy: 0.8389 - val_loss: 1.5660 - val_accuracy: 0.5023\n",
      "Epoch 267/1000\n",
      "1968/1968 [==============================] - 2s 837us/step - loss: 0.3217 - accuracy: 0.8420 - val_loss: 1.6273 - val_accuracy: 0.4886\n",
      "Epoch 268/1000\n",
      "1968/1968 [==============================] - 2s 831us/step - loss: 0.3070 - accuracy: 0.8491 - val_loss: 1.5968 - val_accuracy: 0.5434\n",
      "Epoch 269/1000\n",
      "1968/1968 [==============================] - 2s 836us/step - loss: 0.3025 - accuracy: 0.8511 - val_loss: 1.5670 - val_accuracy: 0.5297\n",
      "Epoch 270/1000\n",
      "1968/1968 [==============================] - 2s 818us/step - loss: 0.2864 - accuracy: 0.8572 - val_loss: 1.7966 - val_accuracy: 0.4703\n",
      "Epoch 271/1000\n",
      "1968/1968 [==============================] - 2s 805us/step - loss: 0.2970 - accuracy: 0.8592 - val_loss: 1.7786 - val_accuracy: 0.5205\n",
      "Epoch 272/1000\n",
      "1968/1968 [==============================] - 2s 809us/step - loss: 0.3225 - accuracy: 0.8379 - val_loss: 1.6613 - val_accuracy: 0.5160\n",
      "Epoch 273/1000\n",
      "1968/1968 [==============================] - 2s 819us/step - loss: 0.3350 - accuracy: 0.8277 - val_loss: 1.6824 - val_accuracy: 0.5297\n",
      "Epoch 274/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.3459 - accuracy: 0.8318 - val_loss: 1.6363 - val_accuracy: 0.5023\n",
      "Epoch 275/1000\n",
      "1968/1968 [==============================] - 2s 827us/step - loss: 0.3125 - accuracy: 0.8481 - val_loss: 1.7157 - val_accuracy: 0.5068\n",
      "Epoch 276/1000\n",
      "1968/1968 [==============================] - 2s 829us/step - loss: 0.3332 - accuracy: 0.8384 - val_loss: 1.6753 - val_accuracy: 0.5068\n",
      "Epoch 277/1000\n",
      "1968/1968 [==============================] - 2s 845us/step - loss: 0.3535 - accuracy: 0.8343 - val_loss: 1.6126 - val_accuracy: 0.4977\n",
      "Epoch 278/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.3524 - accuracy: 0.8171 - val_loss: 1.6800 - val_accuracy: 0.4840\n",
      "Epoch 279/1000\n",
      "1968/1968 [==============================] - 2s 857us/step - loss: 0.3147 - accuracy: 0.8374 - val_loss: 1.7155 - val_accuracy: 0.4886\n",
      "Epoch 280/1000\n",
      "1968/1968 [==============================] - 2s 854us/step - loss: 0.3001 - accuracy: 0.8577 - val_loss: 1.6451 - val_accuracy: 0.5160\n",
      "Epoch 281/1000\n",
      "1968/1968 [==============================] - 2s 834us/step - loss: 0.2821 - accuracy: 0.8587 - val_loss: 1.7062 - val_accuracy: 0.5251\n",
      "Epoch 282/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.2832 - accuracy: 0.8643 - val_loss: 1.6802 - val_accuracy: 0.5388\n",
      "Epoch 283/1000\n",
      "1968/1968 [==============================] - 2s 828us/step - loss: 0.2844 - accuracy: 0.8552 - val_loss: 1.6742 - val_accuracy: 0.5023\n",
      "Epoch 284/1000\n",
      "1968/1968 [==============================] - 2s 809us/step - loss: 0.2825 - accuracy: 0.8572 - val_loss: 1.7338 - val_accuracy: 0.5388\n",
      "Epoch 285/1000\n",
      "1968/1968 [==============================] - 2s 808us/step - loss: 0.2944 - accuracy: 0.8537 - val_loss: 1.6898 - val_accuracy: 0.5205\n",
      "Epoch 286/1000\n",
      "1968/1968 [==============================] - 2s 814us/step - loss: 0.2878 - accuracy: 0.8506 - val_loss: 1.8315 - val_accuracy: 0.5114\n",
      "Epoch 287/1000\n",
      "1968/1968 [==============================] - 2s 818us/step - loss: 0.2620 - accuracy: 0.8684 - val_loss: 1.8584 - val_accuracy: 0.5023\n",
      "Epoch 00287: early stopping\n",
      "30 day\n",
      "\n",
      "# Evaluate on test data\n",
      "244/244 [==============================] - 0s 355us/step\n",
      "test loss, test acc: [1.9669188026522026, 0.5409836173057556]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (244, 1)\n",
      "rmse: 0.6229645494364865\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "PAST_DAYS = 30\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_accuracy\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "print(\"30 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(15, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_44 (LSTM)               (None, 15, 128)           113152    \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 15, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_46 (LSTM)               (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_47 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 306,593\n",
      "Trainable params: 306,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 3s 2ms/step - loss: 0.6967 - accuracy: 0.4929 - val_loss: 0.6863 - val_accuracy: 0.5701\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 676us/step - loss: 0.6934 - accuracy: 0.5071 - val_loss: 0.6869 - val_accuracy: 0.5701\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6933 - accuracy: 0.5091 - val_loss: 0.6855 - val_accuracy: 0.5701\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 695us/step - loss: 0.6920 - accuracy: 0.5096 - val_loss: 0.6940 - val_accuracy: 0.4118\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 672us/step - loss: 0.6924 - accuracy: 0.5121 - val_loss: 0.6872 - val_accuracy: 0.5792\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.6927 - accuracy: 0.5187 - val_loss: 0.6858 - val_accuracy: 0.5701\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 671us/step - loss: 0.6924 - accuracy: 0.5212 - val_loss: 0.6907 - val_accuracy: 0.5747\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 671us/step - loss: 0.6928 - accuracy: 0.5207 - val_loss: 0.6901 - val_accuracy: 0.5747\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 686us/step - loss: 0.6923 - accuracy: 0.5237 - val_loss: 0.6897 - val_accuracy: 0.5747\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6920 - accuracy: 0.5227 - val_loss: 0.6896 - val_accuracy: 0.5928\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6924 - accuracy: 0.5237 - val_loss: 0.6884 - val_accuracy: 0.5701\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.6920 - accuracy: 0.5263 - val_loss: 0.6882 - val_accuracy: 0.5701\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 695us/step - loss: 0.6919 - accuracy: 0.5222 - val_loss: 0.6871 - val_accuracy: 0.5701\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6925 - accuracy: 0.5258 - val_loss: 0.6878 - val_accuracy: 0.5792\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6918 - accuracy: 0.5146 - val_loss: 0.6914 - val_accuracy: 0.5520\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6917 - accuracy: 0.5258 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6908 - accuracy: 0.5298 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6917 - accuracy: 0.5227 - val_loss: 0.6913 - val_accuracy: 0.5566\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6917 - accuracy: 0.5253 - val_loss: 0.6893 - val_accuracy: 0.5792\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6915 - accuracy: 0.5202 - val_loss: 0.6885 - val_accuracy: 0.5701\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 717us/step - loss: 0.6926 - accuracy: 0.5253 - val_loss: 0.6888 - val_accuracy: 0.5701\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 692us/step - loss: 0.6921 - accuracy: 0.5247 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6921 - accuracy: 0.5283 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6915 - accuracy: 0.5273 - val_loss: 0.6880 - val_accuracy: 0.5882\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6914 - accuracy: 0.5207 - val_loss: 0.6893 - val_accuracy: 0.5475\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6910 - accuracy: 0.5237 - val_loss: 0.6917 - val_accuracy: 0.5339\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.6921 - accuracy: 0.5157 - val_loss: 0.6872 - val_accuracy: 0.5701\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.6920 - accuracy: 0.5182 - val_loss: 0.6915 - val_accuracy: 0.5249\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 716us/step - loss: 0.6919 - accuracy: 0.5278 - val_loss: 0.6887 - val_accuracy: 0.5701\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6907 - accuracy: 0.5288 - val_loss: 0.6894 - val_accuracy: 0.5747\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6916 - accuracy: 0.5222 - val_loss: 0.6881 - val_accuracy: 0.5701\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6913 - accuracy: 0.5263 - val_loss: 0.6894 - val_accuracy: 0.5747\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6912 - accuracy: 0.5273 - val_loss: 0.6886 - val_accuracy: 0.5882\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 755us/step - loss: 0.6907 - accuracy: 0.5268 - val_loss: 0.6940 - val_accuracy: 0.4751\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 733us/step - loss: 0.6914 - accuracy: 0.5116 - val_loss: 0.6916 - val_accuracy: 0.5339\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 724us/step - loss: 0.6918 - accuracy: 0.5242 - val_loss: 0.6902 - val_accuracy: 0.5928\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6906 - accuracy: 0.5283 - val_loss: 0.6893 - val_accuracy: 0.5882\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.6899 - accuracy: 0.5323 - val_loss: 0.6920 - val_accuracy: 0.5068\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.6908 - accuracy: 0.5258 - val_loss: 0.6895 - val_accuracy: 0.5747\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 728us/step - loss: 0.6894 - accuracy: 0.5222 - val_loss: 0.6993 - val_accuracy: 0.4163\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.6902 - accuracy: 0.5146 - val_loss: 0.6934 - val_accuracy: 0.5701\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6913 - accuracy: 0.5187 - val_loss: 0.6886 - val_accuracy: 0.5701\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 715us/step - loss: 0.6915 - accuracy: 0.5268 - val_loss: 0.6901 - val_accuracy: 0.6018\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 724us/step - loss: 0.6906 - accuracy: 0.5308 - val_loss: 0.6873 - val_accuracy: 0.5882\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.6891 - accuracy: 0.5258 - val_loss: 0.6928 - val_accuracy: 0.5249\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 729us/step - loss: 0.6905 - accuracy: 0.5328 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 743us/step - loss: 0.6909 - accuracy: 0.5242 - val_loss: 0.6901 - val_accuracy: 0.5701\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.6896 - accuracy: 0.5172 - val_loss: 0.6929 - val_accuracy: 0.5747\n",
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 2s 787us/step - loss: 0.6900 - accuracy: 0.5258 - val_loss: 0.6902 - val_accuracy: 0.5339\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 2s 838us/step - loss: 0.6908 - accuracy: 0.5348 - val_loss: 0.6905 - val_accuracy: 0.5928\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.6909 - accuracy: 0.5530 - val_loss: 0.6899 - val_accuracy: 0.5747\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 2s 849us/step - loss: 0.6902 - accuracy: 0.5318 - val_loss: 0.6912 - val_accuracy: 0.5973\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 2s 790us/step - loss: 0.6892 - accuracy: 0.5354 - val_loss: 0.6953 - val_accuracy: 0.4932\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 2s 834us/step - loss: 0.6905 - accuracy: 0.5237 - val_loss: 0.6871 - val_accuracy: 0.5701\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 2s 762us/step - loss: 0.6888 - accuracy: 0.5429 - val_loss: 0.6945 - val_accuracy: 0.5294\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 2s 816us/step - loss: 0.6893 - accuracy: 0.5394 - val_loss: 0.6905 - val_accuracy: 0.5475\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 2s 882us/step - loss: 0.6891 - accuracy: 0.5399 - val_loss: 0.6972 - val_accuracy: 0.5204\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 2s 1ms/step - loss: 0.6896 - accuracy: 0.5308 - val_loss: 0.6889 - val_accuracy: 0.5656\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 3s 1ms/step - loss: 0.6891 - accuracy: 0.5414 - val_loss: 0.6979 - val_accuracy: 0.4887\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 3s 1ms/step - loss: 0.6882 - accuracy: 0.5419 - val_loss: 0.6949 - val_accuracy: 0.5385\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 3s 1ms/step - loss: 0.6888 - accuracy: 0.5333 - val_loss: 0.6963 - val_accuracy: 0.5294\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 3s 2ms/step - loss: 0.6867 - accuracy: 0.5596 - val_loss: 0.6944 - val_accuracy: 0.5339\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 2s 1ms/step - loss: 0.6876 - accuracy: 0.5545 - val_loss: 0.6891 - val_accuracy: 0.5792\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 2s 906us/step - loss: 0.6898 - accuracy: 0.5253 - val_loss: 0.6964 - val_accuracy: 0.4842\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 2s 808us/step - loss: 0.6859 - accuracy: 0.5399 - val_loss: 0.6941 - val_accuracy: 0.5656\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 2s 772us/step - loss: 0.6848 - accuracy: 0.5515 - val_loss: 0.6853 - val_accuracy: 0.5701\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 727us/step - loss: 0.6844 - accuracy: 0.5465 - val_loss: 0.7038 - val_accuracy: 0.4525\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6856 - accuracy: 0.5535 - val_loss: 0.6971 - val_accuracy: 0.5475\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6845 - accuracy: 0.5631 - val_loss: 0.7012 - val_accuracy: 0.5294\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 691us/step - loss: 0.6839 - accuracy: 0.5465 - val_loss: 0.6907 - val_accuracy: 0.5792\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.6833 - accuracy: 0.5535 - val_loss: 0.6905 - val_accuracy: 0.5792\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 686us/step - loss: 0.6802 - accuracy: 0.5636 - val_loss: 0.7041 - val_accuracy: 0.5204\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6789 - accuracy: 0.5561 - val_loss: 0.7000 - val_accuracy: 0.5385\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.6762 - accuracy: 0.5692 - val_loss: 0.6909 - val_accuracy: 0.5520\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 725us/step - loss: 0.6822 - accuracy: 0.5586 - val_loss: 0.7038 - val_accuracy: 0.5204\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 738us/step - loss: 0.6769 - accuracy: 0.5722 - val_loss: 0.6981 - val_accuracy: 0.5385\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 742us/step - loss: 0.6757 - accuracy: 0.5722 - val_loss: 0.6992 - val_accuracy: 0.5113\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 2s 759us/step - loss: 0.6741 - accuracy: 0.5667 - val_loss: 0.6949 - val_accuracy: 0.5475\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 754us/step - loss: 0.6827 - accuracy: 0.5556 - val_loss: 0.6997 - val_accuracy: 0.5430\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 2s 826us/step - loss: 0.6767 - accuracy: 0.5646 - val_loss: 0.7106 - val_accuracy: 0.4842\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 2s 808us/step - loss: 0.6749 - accuracy: 0.5722 - val_loss: 0.6996 - val_accuracy: 0.5113\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 2s 796us/step - loss: 0.6736 - accuracy: 0.5657 - val_loss: 0.6885 - val_accuracy: 0.5294\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 2s 814us/step - loss: 0.6699 - accuracy: 0.5753 - val_loss: 0.6967 - val_accuracy: 0.5430\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 2s 814us/step - loss: 0.6711 - accuracy: 0.5727 - val_loss: 0.6993 - val_accuracy: 0.5113\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 2s 813us/step - loss: 0.6723 - accuracy: 0.5783 - val_loss: 0.6890 - val_accuracy: 0.4977\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 2s 808us/step - loss: 0.6668 - accuracy: 0.5854 - val_loss: 0.6995 - val_accuracy: 0.5385\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 2s 806us/step - loss: 0.6686 - accuracy: 0.5758 - val_loss: 0.6931 - val_accuracy: 0.5701\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 2s 772us/step - loss: 0.6641 - accuracy: 0.5904 - val_loss: 0.7157 - val_accuracy: 0.5158\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 2s 781us/step - loss: 0.6571 - accuracy: 0.5924 - val_loss: 0.6901 - val_accuracy: 0.6063\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 754us/step - loss: 0.6624 - accuracy: 0.5874 - val_loss: 0.6888 - val_accuracy: 0.5566\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.6526 - accuracy: 0.6040 - val_loss: 0.7049 - val_accuracy: 0.5339\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 2s 772us/step - loss: 0.6645 - accuracy: 0.6061 - val_loss: 0.7067 - val_accuracy: 0.5204\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 2s 787us/step - loss: 0.6519 - accuracy: 0.6146 - val_loss: 0.7190 - val_accuracy: 0.5294\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 2s 794us/step - loss: 0.6692 - accuracy: 0.5753 - val_loss: 0.7172 - val_accuracy: 0.5204\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 2s 764us/step - loss: 0.6568 - accuracy: 0.6091 - val_loss: 0.6953 - val_accuracy: 0.5475\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 2s 760us/step - loss: 0.6513 - accuracy: 0.6217 - val_loss: 0.7083 - val_accuracy: 0.5249\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 2s 803us/step - loss: 0.6484 - accuracy: 0.6146 - val_loss: 0.7122 - val_accuracy: 0.5385\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 2s 780us/step - loss: 0.6487 - accuracy: 0.6217 - val_loss: 0.7172 - val_accuracy: 0.5204\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 2s 783us/step - loss: 0.6450 - accuracy: 0.6288 - val_loss: 0.7222 - val_accuracy: 0.5385\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 2s 812us/step - loss: 0.6432 - accuracy: 0.6207 - val_loss: 0.7317 - val_accuracy: 0.5113\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 2s 771us/step - loss: 0.6451 - accuracy: 0.6237 - val_loss: 0.7108 - val_accuracy: 0.5430\n",
      "Epoch 102/1000\n",
      "1980/1980 [==============================] - 2s 768us/step - loss: 0.6426 - accuracy: 0.6323 - val_loss: 0.7251 - val_accuracy: 0.5294\n",
      "Epoch 103/1000\n",
      "1980/1980 [==============================] - 2s 777us/step - loss: 0.6353 - accuracy: 0.6293 - val_loss: 0.7245 - val_accuracy: 0.5430\n",
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 2s 765us/step - loss: 0.6354 - accuracy: 0.6338 - val_loss: 0.7351 - val_accuracy: 0.5158\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.6415 - accuracy: 0.6258 - val_loss: 0.7089 - val_accuracy: 0.5294\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.6411 - accuracy: 0.6217 - val_loss: 0.7175 - val_accuracy: 0.5068\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.6369 - accuracy: 0.6434 - val_loss: 0.7257 - val_accuracy: 0.5385\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 720us/step - loss: 0.6293 - accuracy: 0.6374 - val_loss: 0.7175 - val_accuracy: 0.5158\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 718us/step - loss: 0.6277 - accuracy: 0.6359 - val_loss: 0.7400 - val_accuracy: 0.5339\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.6277 - accuracy: 0.6455 - val_loss: 0.7567 - val_accuracy: 0.5249\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 720us/step - loss: 0.6243 - accuracy: 0.6460 - val_loss: 0.7501 - val_accuracy: 0.5249\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 714us/step - loss: 0.6186 - accuracy: 0.6530 - val_loss: 0.7276 - val_accuracy: 0.5294\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.6251 - accuracy: 0.6525 - val_loss: 0.7655 - val_accuracy: 0.5158\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.6204 - accuracy: 0.6535 - val_loss: 0.7249 - val_accuracy: 0.5339\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 738us/step - loss: 0.6180 - accuracy: 0.6535 - val_loss: 0.7499 - val_accuracy: 0.5385\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 736us/step - loss: 0.6175 - accuracy: 0.6470 - val_loss: 0.7494 - val_accuracy: 0.5158\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.6095 - accuracy: 0.6606 - val_loss: 0.7806 - val_accuracy: 0.4887\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 736us/step - loss: 0.6151 - accuracy: 0.6520 - val_loss: 0.7326 - val_accuracy: 0.5430\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 735us/step - loss: 0.6169 - accuracy: 0.6586 - val_loss: 0.7501 - val_accuracy: 0.5068\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.6062 - accuracy: 0.6611 - val_loss: 0.7766 - val_accuracy: 0.5204\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.6068 - accuracy: 0.6611 - val_loss: 0.7383 - val_accuracy: 0.5339\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 738us/step - loss: 0.6081 - accuracy: 0.6616 - val_loss: 0.7738 - val_accuracy: 0.5158\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 745us/step - loss: 0.6111 - accuracy: 0.6626 - val_loss: 0.7623 - val_accuracy: 0.5113\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 729us/step - loss: 0.6002 - accuracy: 0.6636 - val_loss: 0.7923 - val_accuracy: 0.5113\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 724us/step - loss: 0.5951 - accuracy: 0.6758 - val_loss: 0.7731 - val_accuracy: 0.5294\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.5968 - accuracy: 0.6621 - val_loss: 0.8039 - val_accuracy: 0.4977\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.6023 - accuracy: 0.6722 - val_loss: 0.7574 - val_accuracy: 0.5339\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 732us/step - loss: 0.5998 - accuracy: 0.6687 - val_loss: 0.7869 - val_accuracy: 0.5113\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.5872 - accuracy: 0.6828 - val_loss: 0.8159 - val_accuracy: 0.5204\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 2s 775us/step - loss: 0.5906 - accuracy: 0.6793 - val_loss: 0.8422 - val_accuracy: 0.5249\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 757us/step - loss: 0.5912 - accuracy: 0.6727 - val_loss: 0.7782 - val_accuracy: 0.4977\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.5903 - accuracy: 0.6808 - val_loss: 0.7907 - val_accuracy: 0.4977\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 2s 765us/step - loss: 0.5813 - accuracy: 0.6773 - val_loss: 0.8088 - val_accuracy: 0.5068\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 2s 788us/step - loss: 0.5855 - accuracy: 0.6793 - val_loss: 0.8396 - val_accuracy: 0.4796\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 2s 794us/step - loss: 0.5847 - accuracy: 0.6894 - val_loss: 0.8020 - val_accuracy: 0.5023\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 2s 812us/step - loss: 0.5806 - accuracy: 0.6914 - val_loss: 0.8455 - val_accuracy: 0.5113\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 2s 787us/step - loss: 0.5910 - accuracy: 0.6717 - val_loss: 0.7862 - val_accuracy: 0.5068\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 2s 758us/step - loss: 0.5747 - accuracy: 0.6909 - val_loss: 0.8529 - val_accuracy: 0.5158\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 2s 848us/step - loss: 0.5778 - accuracy: 0.6798 - val_loss: 0.7885 - val_accuracy: 0.4932\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.5684 - accuracy: 0.6924 - val_loss: 0.8988 - val_accuracy: 0.5113\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 2s 788us/step - loss: 0.5805 - accuracy: 0.6773 - val_loss: 0.8753 - val_accuracy: 0.4842\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 2s 770us/step - loss: 0.5778 - accuracy: 0.6833 - val_loss: 0.8230 - val_accuracy: 0.5430\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.5720 - accuracy: 0.6919 - val_loss: 0.8867 - val_accuracy: 0.5249\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.5622 - accuracy: 0.6924 - val_loss: 0.8770 - val_accuracy: 0.4796\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.5606 - accuracy: 0.7035 - val_loss: 0.8475 - val_accuracy: 0.5204\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 751us/step - loss: 0.5538 - accuracy: 0.7071 - val_loss: 0.8763 - val_accuracy: 0.5339\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 725us/step - loss: 0.5488 - accuracy: 0.7061 - val_loss: 0.8389 - val_accuracy: 0.5294\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 735us/step - loss: 0.5618 - accuracy: 0.6919 - val_loss: 0.9009 - val_accuracy: 0.4842\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.5583 - accuracy: 0.6944 - val_loss: 0.9129 - val_accuracy: 0.5158\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.5505 - accuracy: 0.7015 - val_loss: 0.8595 - val_accuracy: 0.4842\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 743us/step - loss: 0.5479 - accuracy: 0.7096 - val_loss: 0.9260 - val_accuracy: 0.5294\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 757us/step - loss: 0.5532 - accuracy: 0.6965 - val_loss: 0.8989 - val_accuracy: 0.5023\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 2s 761us/step - loss: 0.5508 - accuracy: 0.7116 - val_loss: 0.8962 - val_accuracy: 0.5204\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 2s 761us/step - loss: 0.5360 - accuracy: 0.7101 - val_loss: 0.9402 - val_accuracy: 0.5430\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 2s 800us/step - loss: 0.5438 - accuracy: 0.7081 - val_loss: 0.9361 - val_accuracy: 0.5294\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 2s 788us/step - loss: 0.5257 - accuracy: 0.7202 - val_loss: 1.0042 - val_accuracy: 0.5068\n",
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 2s 761us/step - loss: 0.5389 - accuracy: 0.7187 - val_loss: 0.8843 - val_accuracy: 0.4977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.5280 - accuracy: 0.7222 - val_loss: 1.0194 - val_accuracy: 0.5023\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.5174 - accuracy: 0.7313 - val_loss: 0.9869 - val_accuracy: 0.5113\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.5119 - accuracy: 0.7343 - val_loss: 1.0292 - val_accuracy: 0.5113\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 2s 781us/step - loss: 0.5231 - accuracy: 0.7247 - val_loss: 0.9317 - val_accuracy: 0.5113\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.5065 - accuracy: 0.7328 - val_loss: 1.0171 - val_accuracy: 0.5068\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.5268 - accuracy: 0.7146 - val_loss: 0.9725 - val_accuracy: 0.4932\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 732us/step - loss: 0.5293 - accuracy: 0.7177 - val_loss: 0.9744 - val_accuracy: 0.5113\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.5053 - accuracy: 0.7293 - val_loss: 1.0547 - val_accuracy: 0.5113\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 711us/step - loss: 0.4992 - accuracy: 0.7369 - val_loss: 1.0429 - val_accuracy: 0.5204\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.4891 - accuracy: 0.7374 - val_loss: 1.0528 - val_accuracy: 0.5158\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.4953 - accuracy: 0.7328 - val_loss: 1.0722 - val_accuracy: 0.4751\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 724us/step - loss: 0.4933 - accuracy: 0.7318 - val_loss: 1.0180 - val_accuracy: 0.5023\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.5007 - accuracy: 0.7374 - val_loss: 1.0650 - val_accuracy: 0.4932\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 2s 774us/step - loss: 0.4999 - accuracy: 0.7323 - val_loss: 1.0369 - val_accuracy: 0.4661\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 753us/step - loss: 0.5043 - accuracy: 0.7374 - val_loss: 1.0463 - val_accuracy: 0.5113\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.4903 - accuracy: 0.7434 - val_loss: 1.0553 - val_accuracy: 0.4751\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 727us/step - loss: 0.4693 - accuracy: 0.7556 - val_loss: 1.1027 - val_accuracy: 0.5023\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 730us/step - loss: 0.4793 - accuracy: 0.7525 - val_loss: 1.0004 - val_accuracy: 0.4932\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 745us/step - loss: 0.4713 - accuracy: 0.7510 - val_loss: 1.0791 - val_accuracy: 0.5339\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.4620 - accuracy: 0.7566 - val_loss: 1.1467 - val_accuracy: 0.5068\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 734us/step - loss: 0.4881 - accuracy: 0.7460 - val_loss: 1.0704 - val_accuracy: 0.4932\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 723us/step - loss: 0.4895 - accuracy: 0.7414 - val_loss: 1.1188 - val_accuracy: 0.5158\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 716us/step - loss: 0.4768 - accuracy: 0.7540 - val_loss: 1.0692 - val_accuracy: 0.5158\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.4779 - accuracy: 0.7540 - val_loss: 1.0650 - val_accuracy: 0.4977\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.4549 - accuracy: 0.7556 - val_loss: 1.1927 - val_accuracy: 0.4977\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.4569 - accuracy: 0.7611 - val_loss: 1.2231 - val_accuracy: 0.4977\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.4480 - accuracy: 0.7732 - val_loss: 1.1786 - val_accuracy: 0.5158\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.4408 - accuracy: 0.7712 - val_loss: 1.2422 - val_accuracy: 0.4887\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 715us/step - loss: 0.4475 - accuracy: 0.7732 - val_loss: 1.2549 - val_accuracy: 0.4932\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 725us/step - loss: 0.4480 - accuracy: 0.7657 - val_loss: 1.1641 - val_accuracy: 0.5158\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 2s 782us/step - loss: 0.4363 - accuracy: 0.7742 - val_loss: 1.1419 - val_accuracy: 0.5113\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 2s 843us/step - loss: 0.4151 - accuracy: 0.7965 - val_loss: 1.2176 - val_accuracy: 0.4751\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 2s 799us/step - loss: 0.4112 - accuracy: 0.7995 - val_loss: 1.1642 - val_accuracy: 0.4887\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 2s 780us/step - loss: 0.4193 - accuracy: 0.7874 - val_loss: 1.2339 - val_accuracy: 0.4887\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 2s 776us/step - loss: 0.4034 - accuracy: 0.8061 - val_loss: 1.2567 - val_accuracy: 0.5294\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 2s 780us/step - loss: 0.4117 - accuracy: 0.7975 - val_loss: 1.2610 - val_accuracy: 0.4932\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 2s 814us/step - loss: 0.3873 - accuracy: 0.8066 - val_loss: 1.2896 - val_accuracy: 0.5249\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 2s 796us/step - loss: 0.4106 - accuracy: 0.7889 - val_loss: 1.2751 - val_accuracy: 0.5023\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 2s 794us/step - loss: 0.4027 - accuracy: 0.8005 - val_loss: 1.3872 - val_accuracy: 0.4887\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 2s 762us/step - loss: 0.4306 - accuracy: 0.7803 - val_loss: 1.2440 - val_accuracy: 0.4977\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 2s 776us/step - loss: 0.3951 - accuracy: 0.8091 - val_loss: 1.3066 - val_accuracy: 0.4977\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 735us/step - loss: 0.3726 - accuracy: 0.8111 - val_loss: 1.3649 - val_accuracy: 0.4842\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 730us/step - loss: 0.3659 - accuracy: 0.8253 - val_loss: 1.3630 - val_accuracy: 0.5023\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.3674 - accuracy: 0.8232 - val_loss: 1.2867 - val_accuracy: 0.5158\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 727us/step - loss: 0.3876 - accuracy: 0.8081 - val_loss: 1.3318 - val_accuracy: 0.4887\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 750us/step - loss: 0.3699 - accuracy: 0.8131 - val_loss: 1.4252 - val_accuracy: 0.4932\n",
      "Epoch 204/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.3577 - accuracy: 0.8263 - val_loss: 1.4640 - val_accuracy: 0.5023\n",
      "Epoch 205/1000\n",
      "1980/1980 [==============================] - 1s 734us/step - loss: 0.3496 - accuracy: 0.8247 - val_loss: 1.5660 - val_accuracy: 0.5158\n",
      "Epoch 206/1000\n",
      "1980/1980 [==============================] - 1s 739us/step - loss: 0.3567 - accuracy: 0.8167 - val_loss: 1.3442 - val_accuracy: 0.5068\n",
      "Epoch 207/1000\n",
      "1980/1980 [==============================] - 2s 763us/step - loss: 0.3357 - accuracy: 0.8348 - val_loss: 1.6185 - val_accuracy: 0.4977\n",
      "Epoch 208/1000\n",
      "1980/1980 [==============================] - 2s 794us/step - loss: 0.3263 - accuracy: 0.8409 - val_loss: 1.7237 - val_accuracy: 0.4796\n",
      "Epoch 209/1000\n",
      "1980/1980 [==============================] - 2s 807us/step - loss: 0.3685 - accuracy: 0.8146 - val_loss: 1.6410 - val_accuracy: 0.4751\n",
      "Epoch 210/1000\n",
      "1980/1980 [==============================] - 2s 797us/step - loss: 0.3596 - accuracy: 0.8222 - val_loss: 1.4877 - val_accuracy: 0.4887\n",
      "Epoch 211/1000\n",
      "1980/1980 [==============================] - 2s 785us/step - loss: 0.3335 - accuracy: 0.8369 - val_loss: 1.5569 - val_accuracy: 0.4977\n",
      "Epoch 212/1000\n",
      "1980/1980 [==============================] - 2s 765us/step - loss: 0.3153 - accuracy: 0.8444 - val_loss: 1.6393 - val_accuracy: 0.4932\n",
      "Epoch 213/1000\n",
      "1980/1980 [==============================] - 2s 786us/step - loss: 0.3041 - accuracy: 0.8601 - val_loss: 1.5607 - val_accuracy: 0.5023\n",
      "Epoch 214/1000\n",
      "1980/1980 [==============================] - 2s 770us/step - loss: 0.3020 - accuracy: 0.8520 - val_loss: 1.6371 - val_accuracy: 0.5158\n",
      "Epoch 215/1000\n",
      "1980/1980 [==============================] - 2s 773us/step - loss: 0.3234 - accuracy: 0.8455 - val_loss: 1.6779 - val_accuracy: 0.4842\n",
      "Epoch 216/1000\n",
      "1980/1980 [==============================] - 1s 747us/step - loss: 0.3357 - accuracy: 0.8333 - val_loss: 1.6805 - val_accuracy: 0.5023\n",
      "Epoch 217/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.3017 - accuracy: 0.8540 - val_loss: 1.6009 - val_accuracy: 0.4887\n",
      "Epoch 218/1000\n",
      "1980/1980 [==============================] - 1s 737us/step - loss: 0.3033 - accuracy: 0.8530 - val_loss: 1.6027 - val_accuracy: 0.5158\n",
      "Epoch 219/1000\n",
      "1980/1980 [==============================] - 1s 753us/step - loss: 0.2820 - accuracy: 0.8682 - val_loss: 1.6346 - val_accuracy: 0.5249\n",
      "Epoch 220/1000\n",
      "1980/1980 [==============================] - 1s 734us/step - loss: 0.2812 - accuracy: 0.8641 - val_loss: 1.8202 - val_accuracy: 0.4977\n",
      "Epoch 221/1000\n",
      "1980/1980 [==============================] - 1s 727us/step - loss: 0.2884 - accuracy: 0.8631 - val_loss: 1.6566 - val_accuracy: 0.5339\n",
      "Epoch 222/1000\n",
      "1980/1980 [==============================] - 1s 729us/step - loss: 0.2882 - accuracy: 0.8596 - val_loss: 1.7409 - val_accuracy: 0.4887\n",
      "Epoch 223/1000\n",
      "1980/1980 [==============================] - 1s 743us/step - loss: 0.3056 - accuracy: 0.8576 - val_loss: 1.6516 - val_accuracy: 0.5430\n",
      "Epoch 224/1000\n",
      "1980/1980 [==============================] - 2s 799us/step - loss: 0.2890 - accuracy: 0.8621 - val_loss: 1.6045 - val_accuracy: 0.5385\n",
      "Epoch 225/1000\n",
      "1980/1980 [==============================] - 1s 746us/step - loss: 0.3049 - accuracy: 0.8515 - val_loss: 1.6774 - val_accuracy: 0.5023\n",
      "Epoch 226/1000\n",
      "1980/1980 [==============================] - 1s 752us/step - loss: 0.2614 - accuracy: 0.8758 - val_loss: 1.7667 - val_accuracy: 0.4977\n",
      "Epoch 227/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.2736 - accuracy: 0.8763 - val_loss: 1.9110 - val_accuracy: 0.5158\n",
      "Epoch 228/1000\n",
      "1980/1980 [==============================] - 2s 766us/step - loss: 0.3029 - accuracy: 0.8530 - val_loss: 1.8039 - val_accuracy: 0.5339\n",
      "Epoch 229/1000\n",
      "1980/1980 [==============================] - 2s 766us/step - loss: 0.2755 - accuracy: 0.8778 - val_loss: 1.7083 - val_accuracy: 0.5249\n",
      "Epoch 230/1000\n",
      "1980/1980 [==============================] - 1s 756us/step - loss: 0.2649 - accuracy: 0.8803 - val_loss: 1.9422 - val_accuracy: 0.4977\n",
      "Epoch 231/1000\n",
      "1980/1980 [==============================] - 2s 760us/step - loss: 0.2383 - accuracy: 0.8869 - val_loss: 1.9374 - val_accuracy: 0.5113\n",
      "Epoch 232/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.2887 - accuracy: 0.8601 - val_loss: 1.7558 - val_accuracy: 0.5204\n",
      "Epoch 233/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.2505 - accuracy: 0.8768 - val_loss: 1.9668 - val_accuracy: 0.5204\n",
      "Epoch 234/1000\n",
      "1980/1980 [==============================] - 2s 771us/step - loss: 0.2320 - accuracy: 0.8864 - val_loss: 2.0264 - val_accuracy: 0.4887\n",
      "Epoch 235/1000\n",
      "1980/1980 [==============================] - 1s 752us/step - loss: 0.2204 - accuracy: 0.8990 - val_loss: 1.8920 - val_accuracy: 0.5294\n",
      "Epoch 236/1000\n",
      "1980/1980 [==============================] - 1s 747us/step - loss: 0.2420 - accuracy: 0.8955 - val_loss: 2.2158 - val_accuracy: 0.4932\n",
      "Epoch 237/1000\n",
      "1980/1980 [==============================] - 1s 736us/step - loss: 0.2312 - accuracy: 0.8975 - val_loss: 2.1214 - val_accuracy: 0.5068\n",
      "Epoch 238/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.2259 - accuracy: 0.8970 - val_loss: 1.9555 - val_accuracy: 0.5204\n",
      "Epoch 239/1000\n",
      "1980/1980 [==============================] - 1s 726us/step - loss: 0.1962 - accuracy: 0.9086 - val_loss: 2.2549 - val_accuracy: 0.4977\n",
      "Epoch 240/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.2060 - accuracy: 0.9035 - val_loss: 2.0961 - val_accuracy: 0.5249\n",
      "Epoch 241/1000\n",
      "1980/1980 [==============================] - 1s 720us/step - loss: 0.2255 - accuracy: 0.8955 - val_loss: 2.0969 - val_accuracy: 0.5158\n",
      "Epoch 242/1000\n",
      "1980/1980 [==============================] - 1s 716us/step - loss: 0.2218 - accuracy: 0.8965 - val_loss: 2.3155 - val_accuracy: 0.4932\n",
      "Epoch 243/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.2112 - accuracy: 0.9025 - val_loss: 2.0053 - val_accuracy: 0.5385\n",
      "Epoch 244/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.1976 - accuracy: 0.9106 - val_loss: 2.1227 - val_accuracy: 0.5158\n",
      "Epoch 245/1000\n",
      "1980/1980 [==============================] - 1s 711us/step - loss: 0.2173 - accuracy: 0.8985 - val_loss: 2.2244 - val_accuracy: 0.5023\n",
      "Epoch 246/1000\n",
      "1980/1980 [==============================] - 1s 719us/step - loss: 0.2345 - accuracy: 0.8914 - val_loss: 2.1127 - val_accuracy: 0.4977\n",
      "Epoch 247/1000\n",
      "1980/1980 [==============================] - 1s 731us/step - loss: 0.2382 - accuracy: 0.8904 - val_loss: 1.9043 - val_accuracy: 0.5339\n",
      "Epoch 248/1000\n",
      "1980/1980 [==============================] - 1s 744us/step - loss: 0.1885 - accuracy: 0.9111 - val_loss: 2.1699 - val_accuracy: 0.5249\n",
      "Epoch 249/1000\n",
      "1980/1980 [==============================] - 1s 749us/step - loss: 0.2150 - accuracy: 0.9066 - val_loss: 2.1473 - val_accuracy: 0.5385\n",
      "Epoch 250/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.1700 - accuracy: 0.9247 - val_loss: 2.3778 - val_accuracy: 0.5339\n",
      "Epoch 251/1000\n",
      "1980/1980 [==============================] - 1s 752us/step - loss: 0.1882 - accuracy: 0.9177 - val_loss: 2.3431 - val_accuracy: 0.5158\n",
      "Epoch 252/1000\n",
      "1980/1980 [==============================] - 2s 761us/step - loss: 0.2008 - accuracy: 0.9096 - val_loss: 2.1108 - val_accuracy: 0.5158\n",
      "Epoch 253/1000\n",
      "1980/1980 [==============================] - 1s 735us/step - loss: 0.2066 - accuracy: 0.9025 - val_loss: 2.2538 - val_accuracy: 0.5385\n",
      "Epoch 254/1000\n",
      "1980/1980 [==============================] - 1s 748us/step - loss: 0.2114 - accuracy: 0.9071 - val_loss: 2.3005 - val_accuracy: 0.5068\n",
      "Epoch 255/1000\n",
      "1980/1980 [==============================] - 1s 740us/step - loss: 0.1731 - accuracy: 0.9202 - val_loss: 2.3615 - val_accuracy: 0.5339\n",
      "Epoch 256/1000\n",
      "1980/1980 [==============================] - 1s 720us/step - loss: 0.1731 - accuracy: 0.9227 - val_loss: 2.3164 - val_accuracy: 0.5430\n",
      "Epoch 257/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.2212 - accuracy: 0.9010 - val_loss: 2.3155 - val_accuracy: 0.4932\n",
      "Epoch 258/1000\n",
      "1980/1980 [==============================] - 1s 721us/step - loss: 0.2113 - accuracy: 0.9126 - val_loss: 2.1804 - val_accuracy: 0.5520\n",
      "Epoch 259/1000\n",
      "1980/1980 [==============================] - 1s 733us/step - loss: 0.1680 - accuracy: 0.9258 - val_loss: 2.3547 - val_accuracy: 0.5158\n",
      "Epoch 260/1000\n",
      "1980/1980 [==============================] - 1s 718us/step - loss: 0.1623 - accuracy: 0.9359 - val_loss: 2.3483 - val_accuracy: 0.5430\n",
      "Epoch 261/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.1562 - accuracy: 0.9308 - val_loss: 2.3043 - val_accuracy: 0.5158\n",
      "Epoch 262/1000\n",
      "1980/1980 [==============================] - 1s 722us/step - loss: 0.1305 - accuracy: 0.9434 - val_loss: 2.6284 - val_accuracy: 0.5158\n",
      "Epoch 263/1000\n",
      "1980/1980 [==============================] - 1s 725us/step - loss: 0.1295 - accuracy: 0.9439 - val_loss: 2.5280 - val_accuracy: 0.5520\n",
      "Epoch 264/1000\n",
      "1980/1980 [==============================] - 1s 715us/step - loss: 0.1200 - accuracy: 0.9505 - val_loss: 2.5515 - val_accuracy: 0.5249\n",
      "Epoch 265/1000\n",
      "1980/1980 [==============================] - 1s 717us/step - loss: 0.1347 - accuracy: 0.9434 - val_loss: 2.4661 - val_accuracy: 0.5385\n",
      "Epoch 266/1000\n",
      "1980/1980 [==============================] - 1s 749us/step - loss: 0.1246 - accuracy: 0.9470 - val_loss: 2.5362 - val_accuracy: 0.5294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00266: early stopping\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 203us/step\n",
      "test loss, test acc: [2.2715577407759064, 0.5142857432365417]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.6361083635720066\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1, mode=\"min\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(15, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_55 (LSTM)               (None, 15, 128)           113152    \n",
      "_________________________________________________________________\n",
      "lstm_56 (LSTM)               (None, 15, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 306,593\n",
      "Trainable params: 306,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 3s 1ms/step - loss: 0.6950 - accuracy: 0.5116 - val_loss: 0.6833 - val_accuracy: 0.5701\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6953 - accuracy: 0.5217 - val_loss: 0.6939 - val_accuracy: 0.4661\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 674us/step - loss: 0.6927 - accuracy: 0.5268 - val_loss: 0.6898 - val_accuracy: 0.5701\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.6935 - accuracy: 0.5141 - val_loss: 0.6856 - val_accuracy: 0.5701\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6926 - accuracy: 0.5182 - val_loss: 0.6901 - val_accuracy: 0.5701\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 686us/step - loss: 0.6919 - accuracy: 0.5202 - val_loss: 0.6928 - val_accuracy: 0.5339\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 673us/step - loss: 0.6924 - accuracy: 0.5202 - val_loss: 0.6882 - val_accuracy: 0.5701\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 670us/step - loss: 0.6925 - accuracy: 0.5182 - val_loss: 0.6888 - val_accuracy: 0.5747\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.6924 - accuracy: 0.5232 - val_loss: 0.6885 - val_accuracy: 0.5747\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6915 - accuracy: 0.5298 - val_loss: 0.6892 - val_accuracy: 0.5747\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6921 - accuracy: 0.5141 - val_loss: 0.6864 - val_accuracy: 0.5747\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 731us/step - loss: 0.6922 - accuracy: 0.5167 - val_loss: 0.6870 - val_accuracy: 0.5701\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6913 - accuracy: 0.5268 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6910 - accuracy: 0.5278 - val_loss: 0.6926 - val_accuracy: 0.5158\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6926 - accuracy: 0.5040 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.6916 - accuracy: 0.5207 - val_loss: 0.6895 - val_accuracy: 0.5792\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6917 - accuracy: 0.5303 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6916 - accuracy: 0.5237 - val_loss: 0.6891 - val_accuracy: 0.5701\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.6918 - accuracy: 0.5288 - val_loss: 0.6873 - val_accuracy: 0.5882\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6920 - accuracy: 0.5258 - val_loss: 0.6873 - val_accuracy: 0.5837\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6922 - accuracy: 0.5232 - val_loss: 0.6893 - val_accuracy: 0.5701\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.6911 - accuracy: 0.5268 - val_loss: 0.6894 - val_accuracy: 0.5837\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6912 - accuracy: 0.5293 - val_loss: 0.6907 - val_accuracy: 0.5656\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6919 - accuracy: 0.5237 - val_loss: 0.6878 - val_accuracy: 0.5701\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6923 - accuracy: 0.5258 - val_loss: 0.6889 - val_accuracy: 0.5701\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6909 - accuracy: 0.5227 - val_loss: 0.6916 - val_accuracy: 0.5973\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6918 - accuracy: 0.5187 - val_loss: 0.6903 - val_accuracy: 0.5792\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 718us/step - loss: 0.6923 - accuracy: 0.5232 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6917 - accuracy: 0.5283 - val_loss: 0.6875 - val_accuracy: 0.5928\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6916 - accuracy: 0.5258 - val_loss: 0.6890 - val_accuracy: 0.5611\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6918 - accuracy: 0.5338 - val_loss: 0.6876 - val_accuracy: 0.5837\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6906 - accuracy: 0.5278 - val_loss: 0.6869 - val_accuracy: 0.5701\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 675us/step - loss: 0.6908 - accuracy: 0.5288 - val_loss: 0.6904 - val_accuracy: 0.5566\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6905 - accuracy: 0.5207 - val_loss: 0.6882 - val_accuracy: 0.5882\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6899 - accuracy: 0.5258 - val_loss: 0.6885 - val_accuracy: 0.5747\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6905 - accuracy: 0.5348 - val_loss: 0.6900 - val_accuracy: 0.5430\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6902 - accuracy: 0.5258 - val_loss: 0.6906 - val_accuracy: 0.5385\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6904 - accuracy: 0.5525 - val_loss: 0.6970 - val_accuracy: 0.5701\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6979 - accuracy: 0.5010 - val_loss: 0.6918 - val_accuracy: 0.5701\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6953 - accuracy: 0.5131 - val_loss: 0.6842 - val_accuracy: 0.5701\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6967 - accuracy: 0.5061 - val_loss: 0.6941 - val_accuracy: 0.4299\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.6943 - accuracy: 0.5177 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.6921 - accuracy: 0.5253 - val_loss: 0.6852 - val_accuracy: 0.5701\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6932 - accuracy: 0.5227 - val_loss: 0.6881 - val_accuracy: 0.5701\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 712us/step - loss: 0.6943 - accuracy: 0.5040 - val_loss: 0.6858 - val_accuracy: 0.5701\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 708us/step - loss: 0.6933 - accuracy: 0.5192 - val_loss: 0.6872 - val_accuracy: 0.5701\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6914 - accuracy: 0.5217 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6945 - accuracy: 0.4975 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6921 - accuracy: 0.5146 - val_loss: 0.6919 - val_accuracy: 0.5701\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 1s 691us/step - loss: 0.6916 - accuracy: 0.5268 - val_loss: 0.6879 - val_accuracy: 0.5701\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6913 - accuracy: 0.5298 - val_loss: 0.6910 - val_accuracy: 0.5475\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6927 - accuracy: 0.5081 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 1s 714us/step - loss: 0.6925 - accuracy: 0.5247 - val_loss: 0.6858 - val_accuracy: 0.5701\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6919 - accuracy: 0.5232 - val_loss: 0.6934 - val_accuracy: 0.4208\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6926 - accuracy: 0.5076 - val_loss: 0.6859 - val_accuracy: 0.5701\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.6927 - accuracy: 0.5222 - val_loss: 0.6881 - val_accuracy: 0.5701\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6933 - accuracy: 0.5212 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6928 - accuracy: 0.5227 - val_loss: 0.6861 - val_accuracy: 0.5701\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6928 - accuracy: 0.5066 - val_loss: 0.6860 - val_accuracy: 0.5701\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6931 - accuracy: 0.5247 - val_loss: 0.6869 - val_accuracy: 0.5701\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.6927 - accuracy: 0.5202 - val_loss: 0.6879 - val_accuracy: 0.5701\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 1s 672us/step - loss: 0.6920 - accuracy: 0.5212 - val_loss: 0.6877 - val_accuracy: 0.5701\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6931 - accuracy: 0.5247 - val_loss: 0.6865 - val_accuracy: 0.5701\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6926 - accuracy: 0.5242 - val_loss: 0.6879 - val_accuracy: 0.5701\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6922 - accuracy: 0.5242 - val_loss: 0.6869 - val_accuracy: 0.5701\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6924 - accuracy: 0.5242 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.6918 - accuracy: 0.5222 - val_loss: 0.6865 - val_accuracy: 0.5701\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 691us/step - loss: 0.6921 - accuracy: 0.5242 - val_loss: 0.6917 - val_accuracy: 0.5611\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6918 - accuracy: 0.5212 - val_loss: 0.6884 - val_accuracy: 0.5701\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6916 - accuracy: 0.5288 - val_loss: 0.6895 - val_accuracy: 0.5973\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6912 - accuracy: 0.5222 - val_loss: 0.6884 - val_accuracy: 0.5701\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6911 - accuracy: 0.5232 - val_loss: 0.6930 - val_accuracy: 0.4887\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6917 - accuracy: 0.5263 - val_loss: 0.6905 - val_accuracy: 0.5656\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6909 - accuracy: 0.5237 - val_loss: 0.6916 - val_accuracy: 0.5294\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6906 - accuracy: 0.5293 - val_loss: 0.6953 - val_accuracy: 0.5113\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6908 - accuracy: 0.5308 - val_loss: 0.6903 - val_accuracy: 0.5430\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6895 - accuracy: 0.5323 - val_loss: 0.6897 - val_accuracy: 0.5837\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6891 - accuracy: 0.5273 - val_loss: 0.6889 - val_accuracy: 0.5792\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.6924 - accuracy: 0.5237 - val_loss: 0.6858 - val_accuracy: 0.5837\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6916 - accuracy: 0.5197 - val_loss: 0.6868 - val_accuracy: 0.5837\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 1s 684us/step - loss: 0.6894 - accuracy: 0.5419 - val_loss: 0.6930 - val_accuracy: 0.5204\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6914 - accuracy: 0.5354 - val_loss: 0.6922 - val_accuracy: 0.5339\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6909 - accuracy: 0.5293 - val_loss: 0.6903 - val_accuracy: 0.5747\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6902 - accuracy: 0.5283 - val_loss: 0.6942 - val_accuracy: 0.5294\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 1s 718us/step - loss: 0.6891 - accuracy: 0.5429 - val_loss: 0.6933 - val_accuracy: 0.5294\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 1s 708us/step - loss: 0.6918 - accuracy: 0.5126 - val_loss: 0.6894 - val_accuracy: 0.5339\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6885 - accuracy: 0.5323 - val_loss: 0.7009 - val_accuracy: 0.4570\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6892 - accuracy: 0.5303 - val_loss: 0.6927 - val_accuracy: 0.5113\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6886 - accuracy: 0.5389 - val_loss: 0.6914 - val_accuracy: 0.5294\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.6905 - accuracy: 0.5273 - val_loss: 0.6893 - val_accuracy: 0.5701\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6896 - accuracy: 0.5288 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6900 - accuracy: 0.5237 - val_loss: 0.6955 - val_accuracy: 0.4977\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.6886 - accuracy: 0.5389 - val_loss: 0.6933 - val_accuracy: 0.5113\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6892 - accuracy: 0.5242 - val_loss: 0.6935 - val_accuracy: 0.5339\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 1s 671us/step - loss: 0.6882 - accuracy: 0.5323 - val_loss: 0.7009 - val_accuracy: 0.5158\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.6890 - accuracy: 0.5338 - val_loss: 0.6976 - val_accuracy: 0.5068\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.6905 - accuracy: 0.5197 - val_loss: 0.6891 - val_accuracy: 0.5294\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6878 - accuracy: 0.5333 - val_loss: 0.6966 - val_accuracy: 0.5113\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.6890 - accuracy: 0.5308 - val_loss: 0.6929 - val_accuracy: 0.5249\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6873 - accuracy: 0.5404 - val_loss: 0.6896 - val_accuracy: 0.5973\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6864 - accuracy: 0.5354 - val_loss: 0.6997 - val_accuracy: 0.4932\n",
      "Epoch 102/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6868 - accuracy: 0.5510 - val_loss: 0.6892 - val_accuracy: 0.5566\n",
      "Epoch 103/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6855 - accuracy: 0.5394 - val_loss: 0.6987 - val_accuracy: 0.5249\n",
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6867 - accuracy: 0.5404 - val_loss: 0.6899 - val_accuracy: 0.5701\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6845 - accuracy: 0.5465 - val_loss: 0.6929 - val_accuracy: 0.5023\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6896 - accuracy: 0.5278 - val_loss: 0.6889 - val_accuracy: 0.5520\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6869 - accuracy: 0.5505 - val_loss: 0.6906 - val_accuracy: 0.5158\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6860 - accuracy: 0.5389 - val_loss: 0.6952 - val_accuracy: 0.5566\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.6818 - accuracy: 0.5515 - val_loss: 0.7088 - val_accuracy: 0.5158\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 677us/step - loss: 0.6834 - accuracy: 0.5449 - val_loss: 0.6841 - val_accuracy: 0.5566\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6852 - accuracy: 0.5581 - val_loss: 0.6976 - val_accuracy: 0.5656\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6838 - accuracy: 0.5379 - val_loss: 0.6925 - val_accuracy: 0.5520\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6830 - accuracy: 0.5566 - val_loss: 0.7038 - val_accuracy: 0.5158\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 684us/step - loss: 0.6825 - accuracy: 0.5505 - val_loss: 0.6925 - val_accuracy: 0.4977\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6831 - accuracy: 0.5591 - val_loss: 0.7003 - val_accuracy: 0.5837\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 711us/step - loss: 0.6875 - accuracy: 0.5444 - val_loss: 0.6841 - val_accuracy: 0.5611\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6800 - accuracy: 0.5505 - val_loss: 0.7002 - val_accuracy: 0.5158\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6775 - accuracy: 0.5540 - val_loss: 0.6872 - val_accuracy: 0.5656\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6785 - accuracy: 0.5525 - val_loss: 0.7072 - val_accuracy: 0.5249\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 669us/step - loss: 0.6815 - accuracy: 0.5530 - val_loss: 0.6895 - val_accuracy: 0.5385\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6743 - accuracy: 0.5697 - val_loss: 0.6981 - val_accuracy: 0.5294\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6817 - accuracy: 0.5566 - val_loss: 0.7006 - val_accuracy: 0.4796\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.6749 - accuracy: 0.5641 - val_loss: 0.6867 - val_accuracy: 0.5611\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6718 - accuracy: 0.5808 - val_loss: 0.7167 - val_accuracy: 0.5204\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.6731 - accuracy: 0.5672 - val_loss: 0.6919 - val_accuracy: 0.5747\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.6681 - accuracy: 0.5808 - val_loss: 0.7019 - val_accuracy: 0.5385\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.6798 - accuracy: 0.5702 - val_loss: 0.6907 - val_accuracy: 0.5339\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.6731 - accuracy: 0.5783 - val_loss: 0.7015 - val_accuracy: 0.5339\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 687us/step - loss: 0.6703 - accuracy: 0.5712 - val_loss: 0.6947 - val_accuracy: 0.5385\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.6711 - accuracy: 0.5621 - val_loss: 0.7021 - val_accuracy: 0.5294\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 713us/step - loss: 0.6662 - accuracy: 0.5939 - val_loss: 0.7027 - val_accuracy: 0.5204\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6697 - accuracy: 0.5783 - val_loss: 0.6991 - val_accuracy: 0.5566\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6614 - accuracy: 0.5919 - val_loss: 0.7129 - val_accuracy: 0.5023\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 1s 679us/step - loss: 0.6628 - accuracy: 0.5929 - val_loss: 0.7079 - val_accuracy: 0.5701\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6712 - accuracy: 0.5874 - val_loss: 0.6977 - val_accuracy: 0.5566\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.6616 - accuracy: 0.5864 - val_loss: 0.7000 - val_accuracy: 0.5701\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6588 - accuracy: 0.5894 - val_loss: 0.6954 - val_accuracy: 0.5973\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 1s 686us/step - loss: 0.6688 - accuracy: 0.5854 - val_loss: 0.6896 - val_accuracy: 0.5520\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 1s 680us/step - loss: 0.6674 - accuracy: 0.5864 - val_loss: 0.7034 - val_accuracy: 0.5747\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.6535 - accuracy: 0.5960 - val_loss: 0.7098 - val_accuracy: 0.5339\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6565 - accuracy: 0.5939 - val_loss: 0.7112 - val_accuracy: 0.5475\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6509 - accuracy: 0.6071 - val_loss: 0.7537 - val_accuracy: 0.4842\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6486 - accuracy: 0.6192 - val_loss: 0.7316 - val_accuracy: 0.5339\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.6473 - accuracy: 0.6237 - val_loss: 0.7208 - val_accuracy: 0.5701\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.6465 - accuracy: 0.6187 - val_loss: 0.7355 - val_accuracy: 0.5701\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6397 - accuracy: 0.6308 - val_loss: 0.7472 - val_accuracy: 0.4751\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.6451 - accuracy: 0.6141 - val_loss: 0.7264 - val_accuracy: 0.5385\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6429 - accuracy: 0.6212 - val_loss: 0.7229 - val_accuracy: 0.5611\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 675us/step - loss: 0.6425 - accuracy: 0.6348 - val_loss: 0.7306 - val_accuracy: 0.5747\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6479 - accuracy: 0.6182 - val_loss: 0.7179 - val_accuracy: 0.5475\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.6342 - accuracy: 0.6313 - val_loss: 0.7212 - val_accuracy: 0.5701\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6273 - accuracy: 0.6328 - val_loss: 0.7538 - val_accuracy: 0.5204\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 1s 706us/step - loss: 0.6298 - accuracy: 0.6369 - val_loss: 0.7654 - val_accuracy: 0.5430\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.6235 - accuracy: 0.6394 - val_loss: 0.7585 - val_accuracy: 0.5520\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 1s 689us/step - loss: 0.6260 - accuracy: 0.6429 - val_loss: 0.7407 - val_accuracy: 0.5294\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6313 - accuracy: 0.6399 - val_loss: 0.7527 - val_accuracy: 0.5249\n",
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6274 - accuracy: 0.6449 - val_loss: 0.7424 - val_accuracy: 0.5339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 684us/step - loss: 0.6205 - accuracy: 0.6439 - val_loss: 0.7506 - val_accuracy: 0.5249\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.6163 - accuracy: 0.6540 - val_loss: 0.7758 - val_accuracy: 0.5249\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.6176 - accuracy: 0.6485 - val_loss: 0.7511 - val_accuracy: 0.5339\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.6161 - accuracy: 0.6571 - val_loss: 0.7716 - val_accuracy: 0.5475\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.6133 - accuracy: 0.6500 - val_loss: 0.7491 - val_accuracy: 0.5430\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.6108 - accuracy: 0.6591 - val_loss: 0.7591 - val_accuracy: 0.5294\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.6063 - accuracy: 0.6626 - val_loss: 0.7908 - val_accuracy: 0.5385\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 698us/step - loss: 0.6018 - accuracy: 0.6601 - val_loss: 0.7598 - val_accuracy: 0.5520\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 707us/step - loss: 0.6020 - accuracy: 0.6652 - val_loss: 0.7488 - val_accuracy: 0.5747\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.6055 - accuracy: 0.6727 - val_loss: 0.8003 - val_accuracy: 0.5294\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.6023 - accuracy: 0.6682 - val_loss: 0.8029 - val_accuracy: 0.5249\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.5879 - accuracy: 0.6646 - val_loss: 0.8314 - val_accuracy: 0.5566\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.5971 - accuracy: 0.6616 - val_loss: 0.8081 - val_accuracy: 0.5520\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.5963 - accuracy: 0.6626 - val_loss: 0.8169 - val_accuracy: 0.5385\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.5834 - accuracy: 0.6742 - val_loss: 0.8450 - val_accuracy: 0.5520\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 677us/step - loss: 0.5781 - accuracy: 0.6763 - val_loss: 0.8439 - val_accuracy: 0.5520\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 687us/step - loss: 0.5766 - accuracy: 0.6717 - val_loss: 0.8659 - val_accuracy: 0.5566\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 708us/step - loss: 0.5810 - accuracy: 0.6773 - val_loss: 0.8380 - val_accuracy: 0.5520\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.5807 - accuracy: 0.6768 - val_loss: 0.8130 - val_accuracy: 0.5430\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.5785 - accuracy: 0.6768 - val_loss: 0.8621 - val_accuracy: 0.5701\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.5685 - accuracy: 0.6889 - val_loss: 0.8105 - val_accuracy: 0.5294\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.5550 - accuracy: 0.6864 - val_loss: 0.8542 - val_accuracy: 0.5520\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.5738 - accuracy: 0.6854 - val_loss: 0.8616 - val_accuracy: 0.5656\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.5556 - accuracy: 0.6965 - val_loss: 0.8819 - val_accuracy: 0.5520\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 711us/step - loss: 0.5704 - accuracy: 0.6944 - val_loss: 0.8780 - val_accuracy: 0.5520\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.5719 - accuracy: 0.6869 - val_loss: 0.8727 - val_accuracy: 0.5339\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 692us/step - loss: 0.5510 - accuracy: 0.6894 - val_loss: 0.9056 - val_accuracy: 0.5430\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 704us/step - loss: 0.5461 - accuracy: 0.7005 - val_loss: 0.9128 - val_accuracy: 0.5656\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 697us/step - loss: 0.5659 - accuracy: 0.6773 - val_loss: 0.8803 - val_accuracy: 0.5204\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 678us/step - loss: 0.5416 - accuracy: 0.7025 - val_loss: 0.9363 - val_accuracy: 0.5656\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.5383 - accuracy: 0.7081 - val_loss: 0.9504 - val_accuracy: 0.5430\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.5308 - accuracy: 0.7096 - val_loss: 0.9477 - val_accuracy: 0.5475\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.5353 - accuracy: 0.7081 - val_loss: 0.9392 - val_accuracy: 0.5430\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.5576 - accuracy: 0.6889 - val_loss: 0.9063 - val_accuracy: 0.5158\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 1s 677us/step - loss: 0.5276 - accuracy: 0.7172 - val_loss: 0.9742 - val_accuracy: 0.5475\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 1s 685us/step - loss: 0.5279 - accuracy: 0.7066 - val_loss: 0.9867 - val_accuracy: 0.5339\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 1s 703us/step - loss: 0.5242 - accuracy: 0.7061 - val_loss: 0.9264 - val_accuracy: 0.5656\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 1s 702us/step - loss: 0.5308 - accuracy: 0.7056 - val_loss: 1.0330 - val_accuracy: 0.5385\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.5170 - accuracy: 0.7172 - val_loss: 1.0706 - val_accuracy: 0.5385\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.5019 - accuracy: 0.7278 - val_loss: 1.0464 - val_accuracy: 0.5430\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 1s 710us/step - loss: 0.4933 - accuracy: 0.7313 - val_loss: 1.0084 - val_accuracy: 0.5385\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.5262 - accuracy: 0.7076 - val_loss: 1.0643 - val_accuracy: 0.5204\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 691us/step - loss: 0.5232 - accuracy: 0.7106 - val_loss: 1.0392 - val_accuracy: 0.5520\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 676us/step - loss: 0.5070 - accuracy: 0.7278 - val_loss: 0.9728 - val_accuracy: 0.5566\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 714us/step - loss: 0.4925 - accuracy: 0.7222 - val_loss: 1.1698 - val_accuracy: 0.5385\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 695us/step - loss: 0.4989 - accuracy: 0.7242 - val_loss: 1.0610 - val_accuracy: 0.5611\n",
      "Epoch 204/1000\n",
      "1980/1980 [==============================] - 1s 709us/step - loss: 0.4845 - accuracy: 0.7354 - val_loss: 1.0922 - val_accuracy: 0.5566\n",
      "Epoch 205/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.4858 - accuracy: 0.7470 - val_loss: 1.1857 - val_accuracy: 0.5339\n",
      "Epoch 206/1000\n",
      "1980/1980 [==============================] - 1s 681us/step - loss: 0.4794 - accuracy: 0.7434 - val_loss: 1.1001 - val_accuracy: 0.5656\n",
      "Epoch 207/1000\n",
      "1980/1980 [==============================] - 1s 690us/step - loss: 0.4962 - accuracy: 0.7303 - val_loss: 1.1714 - val_accuracy: 0.5701\n",
      "Epoch 208/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.4662 - accuracy: 0.7495 - val_loss: 1.1227 - val_accuracy: 0.5611\n",
      "Epoch 209/1000\n",
      "1980/1980 [==============================] - 1s 700us/step - loss: 0.4789 - accuracy: 0.7460 - val_loss: 1.1989 - val_accuracy: 0.5611\n",
      "Epoch 210/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.4607 - accuracy: 0.7606 - val_loss: 1.2102 - val_accuracy: 0.5113\n",
      "Epoch 211/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.4651 - accuracy: 0.7460 - val_loss: 1.1555 - val_accuracy: 0.5611\n",
      "Epoch 212/1000\n",
      "1980/1980 [==============================] - 1s 687us/step - loss: 0.4480 - accuracy: 0.7606 - val_loss: 1.2356 - val_accuracy: 0.5656\n",
      "Epoch 213/1000\n",
      "1980/1980 [==============================] - 1s 708us/step - loss: 0.4445 - accuracy: 0.7722 - val_loss: 1.2333 - val_accuracy: 0.5385\n",
      "Epoch 214/1000\n",
      "1980/1980 [==============================] - 1s 696us/step - loss: 0.4656 - accuracy: 0.7571 - val_loss: 1.1043 - val_accuracy: 0.5747\n",
      "Epoch 215/1000\n",
      "1980/1980 [==============================] - 1s 694us/step - loss: 0.4660 - accuracy: 0.7545 - val_loss: 1.2337 - val_accuracy: 0.5339\n",
      "Epoch 216/1000\n",
      "1980/1980 [==============================] - 1s 683us/step - loss: 0.4625 - accuracy: 0.7586 - val_loss: 1.2626 - val_accuracy: 0.5566\n",
      "Epoch 217/1000\n",
      "1980/1980 [==============================] - 1s 693us/step - loss: 0.4430 - accuracy: 0.7616 - val_loss: 1.2500 - val_accuracy: 0.5611\n",
      "Epoch 218/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.4238 - accuracy: 0.7768 - val_loss: 1.1823 - val_accuracy: 0.5520\n",
      "Epoch 219/1000\n",
      "1980/1980 [==============================] - 1s 692us/step - loss: 0.4425 - accuracy: 0.7667 - val_loss: 1.2763 - val_accuracy: 0.5475\n",
      "Epoch 220/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.4366 - accuracy: 0.7646 - val_loss: 1.2646 - val_accuracy: 0.5882\n",
      "Epoch 221/1000\n",
      "1980/1980 [==============================] - 1s 688us/step - loss: 0.4241 - accuracy: 0.7707 - val_loss: 1.3088 - val_accuracy: 0.5475\n",
      "Epoch 222/1000\n",
      "1980/1980 [==============================] - 1s 705us/step - loss: 0.4241 - accuracy: 0.7702 - val_loss: 1.3572 - val_accuracy: 0.5385\n",
      "Epoch 223/1000\n",
      "1980/1980 [==============================] - 1s 699us/step - loss: 0.4181 - accuracy: 0.7768 - val_loss: 1.3453 - val_accuracy: 0.5611\n",
      "Epoch 224/1000\n",
      "1980/1980 [==============================] - 1s 682us/step - loss: 0.4174 - accuracy: 0.7687 - val_loss: 1.2781 - val_accuracy: 0.5611\n",
      "Epoch 225/1000\n",
      "1980/1980 [==============================] - 1s 684us/step - loss: 0.3968 - accuracy: 0.7904 - val_loss: 1.3586 - val_accuracy: 0.5566\n",
      "Epoch 226/1000\n",
      "1980/1980 [==============================] - 1s 701us/step - loss: 0.3844 - accuracy: 0.7899 - val_loss: 1.4924 - val_accuracy: 0.5339\n",
      "Epoch 00226: early stopping\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 193us/step\n",
      "test loss, test acc: [1.3268592109485549, 0.5142857432365417]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.6006114553838006\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_accuracy\", patience=200, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(15, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_67 (LSTM)               (None, 15, 128)           113152    \n",
      "_________________________________________________________________\n",
      "lstm_68 (LSTM)               (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_69 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 175,009\n",
      "Trainable params: 175,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 2s 935us/step - loss: 0.6975 - accuracy: 0.5152 - val_loss: 0.6924 - val_accuracy: 0.4977\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6938 - accuracy: 0.5111 - val_loss: 0.6939 - val_accuracy: 0.4796\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6933 - accuracy: 0.5157 - val_loss: 0.6883 - val_accuracy: 0.5747\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6914 - accuracy: 0.5197 - val_loss: 0.6905 - val_accuracy: 0.5385\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.6909 - accuracy: 0.5268 - val_loss: 0.6890 - val_accuracy: 0.5656\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6920 - accuracy: 0.5202 - val_loss: 0.6883 - val_accuracy: 0.5747\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6931 - accuracy: 0.5101 - val_loss: 0.6920 - val_accuracy: 0.5475\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6923 - accuracy: 0.5162 - val_loss: 0.7005 - val_accuracy: 0.4344\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6933 - accuracy: 0.5187 - val_loss: 0.6857 - val_accuracy: 0.5701\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6915 - accuracy: 0.5217 - val_loss: 0.6927 - val_accuracy: 0.5520\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.6928 - accuracy: 0.5298 - val_loss: 0.6887 - val_accuracy: 0.5701\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6913 - accuracy: 0.5293 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6922 - accuracy: 0.5141 - val_loss: 0.6926 - val_accuracy: 0.5475\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6921 - accuracy: 0.5157 - val_loss: 0.6911 - val_accuracy: 0.5701\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6911 - accuracy: 0.5232 - val_loss: 0.6916 - val_accuracy: 0.5566\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6929 - accuracy: 0.5157 - val_loss: 0.6860 - val_accuracy: 0.5701\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6913 - accuracy: 0.5217 - val_loss: 0.6899 - val_accuracy: 0.5973\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6909 - accuracy: 0.5338 - val_loss: 0.6892 - val_accuracy: 0.5928\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6916 - accuracy: 0.5258 - val_loss: 0.6872 - val_accuracy: 0.5792\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6904 - accuracy: 0.5263 - val_loss: 0.6930 - val_accuracy: 0.5475\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.6903 - accuracy: 0.5333 - val_loss: 0.6878 - val_accuracy: 0.5882\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.6910 - accuracy: 0.5268 - val_loss: 0.6883 - val_accuracy: 0.5928\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.6915 - accuracy: 0.5141 - val_loss: 0.6908 - val_accuracy: 0.5656\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.6912 - accuracy: 0.5121 - val_loss: 0.6901 - val_accuracy: 0.5385\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.6924 - accuracy: 0.5283 - val_loss: 0.6881 - val_accuracy: 0.5928\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.6915 - accuracy: 0.5263 - val_loss: 0.6930 - val_accuracy: 0.5385\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.6913 - accuracy: 0.5237 - val_loss: 0.6899 - val_accuracy: 0.5656\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.6912 - accuracy: 0.5146 - val_loss: 0.6929 - val_accuracy: 0.5792\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.6910 - accuracy: 0.5278 - val_loss: 0.6901 - val_accuracy: 0.5747\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6903 - accuracy: 0.5278 - val_loss: 0.6915 - val_accuracy: 0.5882\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6901 - accuracy: 0.5273 - val_loss: 0.6907 - val_accuracy: 0.5611\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6902 - accuracy: 0.5258 - val_loss: 0.6922 - val_accuracy: 0.5249\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.6908 - accuracy: 0.5258 - val_loss: 0.6922 - val_accuracy: 0.5158\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6905 - accuracy: 0.5187 - val_loss: 0.6953 - val_accuracy: 0.5385\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.6899 - accuracy: 0.5308 - val_loss: 0.6924 - val_accuracy: 0.5204\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.6910 - accuracy: 0.5364 - val_loss: 0.6873 - val_accuracy: 0.5928\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.6905 - accuracy: 0.5232 - val_loss: 0.6873 - val_accuracy: 0.5882\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.6911 - accuracy: 0.5323 - val_loss: 0.6927 - val_accuracy: 0.5294\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.6904 - accuracy: 0.5354 - val_loss: 0.6889 - val_accuracy: 0.5792\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.6899 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5249\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6917 - accuracy: 0.5182 - val_loss: 0.6985 - val_accuracy: 0.4389\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.6885 - accuracy: 0.5343 - val_loss: 0.6900 - val_accuracy: 0.5928\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.6886 - accuracy: 0.5313 - val_loss: 0.6958 - val_accuracy: 0.5158\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6929 - val_accuracy: 0.5882\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6905 - accuracy: 0.5354 - val_loss: 0.6933 - val_accuracy: 0.5520\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6900 - accuracy: 0.5247 - val_loss: 0.6918 - val_accuracy: 0.5204\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6889 - accuracy: 0.5273 - val_loss: 0.6956 - val_accuracy: 0.5882\n",
      "Epoch 48/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6886 - accuracy: 0.5338 - val_loss: 0.6891 - val_accuracy: 0.5475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6900 - accuracy: 0.5298 - val_loss: 0.6921 - val_accuracy: 0.5113\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6915 - accuracy: 0.5308 - val_loss: 0.6953 - val_accuracy: 0.5882\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6892 - accuracy: 0.5232 - val_loss: 0.6926 - val_accuracy: 0.5837\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6890 - accuracy: 0.5283 - val_loss: 0.6890 - val_accuracy: 0.5520\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6908 - accuracy: 0.5364 - val_loss: 0.6853 - val_accuracy: 0.5928\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6883 - accuracy: 0.5354 - val_loss: 0.6976 - val_accuracy: 0.5294\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6881 - accuracy: 0.5429 - val_loss: 0.6932 - val_accuracy: 0.5747\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6892 - accuracy: 0.5293 - val_loss: 0.6923 - val_accuracy: 0.5339\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6893 - accuracy: 0.5379 - val_loss: 0.6939 - val_accuracy: 0.5339\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6864 - accuracy: 0.5505 - val_loss: 0.6919 - val_accuracy: 0.5701\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6842 - accuracy: 0.5525 - val_loss: 0.6908 - val_accuracy: 0.5430\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6876 - accuracy: 0.5369 - val_loss: 0.6878 - val_accuracy: 0.5611\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.6891 - accuracy: 0.5369 - val_loss: 0.6914 - val_accuracy: 0.5701\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6917 - accuracy: 0.5136 - val_loss: 0.6876 - val_accuracy: 0.5837\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6872 - accuracy: 0.5333 - val_loss: 0.6986 - val_accuracy: 0.5294\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6866 - accuracy: 0.5379 - val_loss: 0.6940 - val_accuracy: 0.5656\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.6849 - accuracy: 0.5520 - val_loss: 0.7049 - val_accuracy: 0.5566\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6886 - accuracy: 0.5374 - val_loss: 0.6883 - val_accuracy: 0.5520\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6867 - accuracy: 0.5551 - val_loss: 0.6968 - val_accuracy: 0.5520\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6846 - accuracy: 0.5551 - val_loss: 0.6915 - val_accuracy: 0.5520\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6869 - accuracy: 0.5348 - val_loss: 0.6947 - val_accuracy: 0.5294\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6876 - accuracy: 0.5384 - val_loss: 0.6898 - val_accuracy: 0.5701\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6854 - accuracy: 0.5424 - val_loss: 0.6975 - val_accuracy: 0.5113\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6854 - accuracy: 0.5389 - val_loss: 0.6876 - val_accuracy: 0.5566\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6836 - accuracy: 0.5535 - val_loss: 0.6971 - val_accuracy: 0.5520\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6812 - accuracy: 0.5747 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6835 - accuracy: 0.5626 - val_loss: 0.6968 - val_accuracy: 0.5294\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6838 - accuracy: 0.5520 - val_loss: 0.6983 - val_accuracy: 0.5068\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6817 - accuracy: 0.5712 - val_loss: 0.6985 - val_accuracy: 0.5430\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6810 - accuracy: 0.5758 - val_loss: 0.6959 - val_accuracy: 0.5520\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6824 - accuracy: 0.5616 - val_loss: 0.6930 - val_accuracy: 0.5339\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6784 - accuracy: 0.5823 - val_loss: 0.7026 - val_accuracy: 0.5475\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6821 - accuracy: 0.5545 - val_loss: 0.6876 - val_accuracy: 0.5520\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6774 - accuracy: 0.5682 - val_loss: 0.7030 - val_accuracy: 0.5656\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6787 - accuracy: 0.5707 - val_loss: 0.6956 - val_accuracy: 0.5792\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6758 - accuracy: 0.5793 - val_loss: 0.7073 - val_accuracy: 0.5068\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6802 - accuracy: 0.5586 - val_loss: 0.6947 - val_accuracy: 0.5294\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6792 - accuracy: 0.5742 - val_loss: 0.6879 - val_accuracy: 0.5611\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6739 - accuracy: 0.5808 - val_loss: 0.7037 - val_accuracy: 0.5520\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6726 - accuracy: 0.5899 - val_loss: 0.6997 - val_accuracy: 0.5701\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6732 - accuracy: 0.5742 - val_loss: 0.6979 - val_accuracy: 0.5475\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6732 - accuracy: 0.5823 - val_loss: 0.7138 - val_accuracy: 0.5430\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6712 - accuracy: 0.5697 - val_loss: 0.6985 - val_accuracy: 0.5430\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6676 - accuracy: 0.5919 - val_loss: 0.6969 - val_accuracy: 0.5928\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6721 - accuracy: 0.5828 - val_loss: 0.7159 - val_accuracy: 0.5339\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6721 - accuracy: 0.5747 - val_loss: 0.7092 - val_accuracy: 0.5385\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.6703 - accuracy: 0.5879 - val_loss: 0.7166 - val_accuracy: 0.5339\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6677 - accuracy: 0.5924 - val_loss: 0.7042 - val_accuracy: 0.5656\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6653 - accuracy: 0.5990 - val_loss: 0.7138 - val_accuracy: 0.5520\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6663 - accuracy: 0.6020 - val_loss: 0.7065 - val_accuracy: 0.5430\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6668 - accuracy: 0.5848 - val_loss: 0.7117 - val_accuracy: 0.5113\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6619 - accuracy: 0.5929 - val_loss: 0.7077 - val_accuracy: 0.5520\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6671 - accuracy: 0.5788 - val_loss: 0.7036 - val_accuracy: 0.5339\n",
      "Epoch 102/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6598 - accuracy: 0.5970 - val_loss: 0.7196 - val_accuracy: 0.5204\n",
      "Epoch 103/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6722 - accuracy: 0.5692 - val_loss: 0.6995 - val_accuracy: 0.5430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6580 - accuracy: 0.6066 - val_loss: 0.7340 - val_accuracy: 0.5294\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.6626 - accuracy: 0.5929 - val_loss: 0.7023 - val_accuracy: 0.5430\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6601 - accuracy: 0.5980 - val_loss: 0.7202 - val_accuracy: 0.5339\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6595 - accuracy: 0.5909 - val_loss: 0.7251 - val_accuracy: 0.5385\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6509 - accuracy: 0.6121 - val_loss: 0.7326 - val_accuracy: 0.5566\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6518 - accuracy: 0.6126 - val_loss: 0.7391 - val_accuracy: 0.5385\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6545 - accuracy: 0.6081 - val_loss: 0.7235 - val_accuracy: 0.5339\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6535 - accuracy: 0.6091 - val_loss: 0.7186 - val_accuracy: 0.5792\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6428 - accuracy: 0.6298 - val_loss: 0.7363 - val_accuracy: 0.5249\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6437 - accuracy: 0.6167 - val_loss: 0.7358 - val_accuracy: 0.5475\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6441 - accuracy: 0.6157 - val_loss: 0.7318 - val_accuracy: 0.5475\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6507 - accuracy: 0.6131 - val_loss: 0.7265 - val_accuracy: 0.5566\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6409 - accuracy: 0.6298 - val_loss: 0.7357 - val_accuracy: 0.5611\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6510 - accuracy: 0.6136 - val_loss: 0.7088 - val_accuracy: 0.5475\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6493 - accuracy: 0.6212 - val_loss: 0.7412 - val_accuracy: 0.5430\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6394 - accuracy: 0.6308 - val_loss: 0.7442 - val_accuracy: 0.5068\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6408 - accuracy: 0.6283 - val_loss: 0.7483 - val_accuracy: 0.5475\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6408 - accuracy: 0.6318 - val_loss: 0.7547 - val_accuracy: 0.5204\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6372 - accuracy: 0.6227 - val_loss: 0.7393 - val_accuracy: 0.5249\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6436 - accuracy: 0.6298 - val_loss: 0.7296 - val_accuracy: 0.5249\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.6324 - accuracy: 0.6364 - val_loss: 0.7313 - val_accuracy: 0.5430\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6307 - accuracy: 0.6384 - val_loss: 0.7551 - val_accuracy: 0.5068\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6317 - accuracy: 0.6399 - val_loss: 0.7361 - val_accuracy: 0.4977\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6276 - accuracy: 0.6348 - val_loss: 0.7648 - val_accuracy: 0.5158\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.6213 - accuracy: 0.6571 - val_loss: 0.7614 - val_accuracy: 0.5068\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.6212 - accuracy: 0.6465 - val_loss: 0.7639 - val_accuracy: 0.5249\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.6168 - accuracy: 0.6525 - val_loss: 0.7690 - val_accuracy: 0.5158\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 442us/step - loss: 0.6211 - accuracy: 0.6429 - val_loss: 0.7939 - val_accuracy: 0.4977\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.6181 - accuracy: 0.6455 - val_loss: 0.7792 - val_accuracy: 0.5385\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6187 - accuracy: 0.6333 - val_loss: 0.7603 - val_accuracy: 0.5294\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.6082 - accuracy: 0.6586 - val_loss: 0.7662 - val_accuracy: 0.5339\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6055 - accuracy: 0.6530 - val_loss: 0.7660 - val_accuracy: 0.5339\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6048 - accuracy: 0.6515 - val_loss: 0.7772 - val_accuracy: 0.5566\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6061 - accuracy: 0.6606 - val_loss: 0.7850 - val_accuracy: 0.5249\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.6056 - accuracy: 0.6677 - val_loss: 0.7646 - val_accuracy: 0.5158\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 0.5923 - accuracy: 0.6722 - val_loss: 0.7948 - val_accuracy: 0.5294\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.6023 - accuracy: 0.6722 - val_loss: 0.8319 - val_accuracy: 0.5068\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.6118 - accuracy: 0.6394 - val_loss: 0.7552 - val_accuracy: 0.5566\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 0.5944 - accuracy: 0.6753 - val_loss: 0.7646 - val_accuracy: 0.5339\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 476us/step - loss: 0.5861 - accuracy: 0.6864 - val_loss: 0.8209 - val_accuracy: 0.5023\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.5798 - accuracy: 0.6763 - val_loss: 0.8689 - val_accuracy: 0.5158\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.5784 - accuracy: 0.6823 - val_loss: 0.7959 - val_accuracy: 0.5068\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.5750 - accuracy: 0.6914 - val_loss: 0.8604 - val_accuracy: 0.5294\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.5970 - accuracy: 0.6631 - val_loss: 0.7804 - val_accuracy: 0.4977\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.5700 - accuracy: 0.6924 - val_loss: 0.8448 - val_accuracy: 0.5249\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.5632 - accuracy: 0.6990 - val_loss: 0.8540 - val_accuracy: 0.4751\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.5647 - accuracy: 0.6960 - val_loss: 0.8295 - val_accuracy: 0.5113\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.5665 - accuracy: 0.7040 - val_loss: 0.8717 - val_accuracy: 0.5204\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 0.5579 - accuracy: 0.7035 - val_loss: 0.8904 - val_accuracy: 0.5068\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.5447 - accuracy: 0.7101 - val_loss: 0.8972 - val_accuracy: 0.5068\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.5401 - accuracy: 0.7101 - val_loss: 0.8638 - val_accuracy: 0.5385\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 0.5440 - accuracy: 0.7061 - val_loss: 0.9389 - val_accuracy: 0.5068\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.5445 - accuracy: 0.7076 - val_loss: 0.9119 - val_accuracy: 0.5566\n",
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.5669 - accuracy: 0.6854 - val_loss: 0.8544 - val_accuracy: 0.4706\n",
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.5458 - accuracy: 0.7040 - val_loss: 0.9288 - val_accuracy: 0.5158\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5421 - accuracy: 0.7091 - val_loss: 0.8208 - val_accuracy: 0.5249\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5381 - accuracy: 0.7066 - val_loss: 0.9520 - val_accuracy: 0.4977\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.5218 - accuracy: 0.7172 - val_loss: 1.0018 - val_accuracy: 0.5158\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.5190 - accuracy: 0.7177 - val_loss: 0.9534 - val_accuracy: 0.5339\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.5056 - accuracy: 0.7323 - val_loss: 0.8805 - val_accuracy: 0.5158\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.5241 - accuracy: 0.7283 - val_loss: 1.0091 - val_accuracy: 0.5023\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.5201 - accuracy: 0.7253 - val_loss: 0.9885 - val_accuracy: 0.4932\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.5069 - accuracy: 0.7409 - val_loss: 1.0342 - val_accuracy: 0.5068\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.4936 - accuracy: 0.7480 - val_loss: 0.9918 - val_accuracy: 0.5158\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.4855 - accuracy: 0.7414 - val_loss: 0.9764 - val_accuracy: 0.5294\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.4971 - accuracy: 0.7364 - val_loss: 0.9619 - val_accuracy: 0.5204\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.4839 - accuracy: 0.7601 - val_loss: 1.1171 - val_accuracy: 0.5068\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.4771 - accuracy: 0.7551 - val_loss: 1.1282 - val_accuracy: 0.5520\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.4744 - accuracy: 0.7626 - val_loss: 1.0966 - val_accuracy: 0.5294\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.4630 - accuracy: 0.7606 - val_loss: 1.1558 - val_accuracy: 0.5068\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.4633 - accuracy: 0.7641 - val_loss: 1.0788 - val_accuracy: 0.5385\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.4819 - accuracy: 0.7566 - val_loss: 1.0725 - val_accuracy: 0.5339\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.4670 - accuracy: 0.7581 - val_loss: 1.1519 - val_accuracy: 0.5385\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.4502 - accuracy: 0.7732 - val_loss: 1.1732 - val_accuracy: 0.4932\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.4582 - accuracy: 0.7551 - val_loss: 1.1928 - val_accuracy: 0.4887\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.4659 - accuracy: 0.7641 - val_loss: 1.2115 - val_accuracy: 0.5385\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.4473 - accuracy: 0.7818 - val_loss: 1.2027 - val_accuracy: 0.4932\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.4305 - accuracy: 0.7833 - val_loss: 1.2374 - val_accuracy: 0.5294\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.4258 - accuracy: 0.7818 - val_loss: 1.3364 - val_accuracy: 0.5068\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.4269 - accuracy: 0.7838 - val_loss: 1.2318 - val_accuracy: 0.5294\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.4067 - accuracy: 0.7944 - val_loss: 1.3818 - val_accuracy: 0.4977\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.3847 - accuracy: 0.8066 - val_loss: 1.3333 - val_accuracy: 0.5520\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.3786 - accuracy: 0.8091 - val_loss: 1.4752 - val_accuracy: 0.5113\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.3869 - accuracy: 0.8040 - val_loss: 1.3600 - val_accuracy: 0.4977\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.3918 - accuracy: 0.8000 - val_loss: 1.5129 - val_accuracy: 0.5339\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.3864 - accuracy: 0.8061 - val_loss: 1.4342 - val_accuracy: 0.5475\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.3615 - accuracy: 0.8222 - val_loss: 1.5128 - val_accuracy: 0.4977\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.3756 - accuracy: 0.8141 - val_loss: 1.4514 - val_accuracy: 0.5339\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.3888 - accuracy: 0.7995 - val_loss: 1.4158 - val_accuracy: 0.5249\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.3591 - accuracy: 0.8247 - val_loss: 1.5545 - val_accuracy: 0.5430\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.3362 - accuracy: 0.8359 - val_loss: 1.5190 - val_accuracy: 0.5294\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3394 - accuracy: 0.8359 - val_loss: 1.5563 - val_accuracy: 0.4887\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.3563 - accuracy: 0.8258 - val_loss: 1.4738 - val_accuracy: 0.5204\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.3397 - accuracy: 0.8369 - val_loss: 1.6621 - val_accuracy: 0.5249\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.3123 - accuracy: 0.8485 - val_loss: 1.6972 - val_accuracy: 0.5068\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.3127 - accuracy: 0.8490 - val_loss: 1.6821 - val_accuracy: 0.5204\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.3245 - accuracy: 0.8439 - val_loss: 1.6441 - val_accuracy: 0.5294\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.3296 - accuracy: 0.8354 - val_loss: 1.6980 - val_accuracy: 0.4977\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.3125 - accuracy: 0.8465 - val_loss: 1.7211 - val_accuracy: 0.5204\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.2911 - accuracy: 0.8601 - val_loss: 1.8065 - val_accuracy: 0.5294\n",
      "Epoch 204/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.2935 - accuracy: 0.8586 - val_loss: 1.9126 - val_accuracy: 0.5204\n",
      "Epoch 205/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.3127 - accuracy: 0.8515 - val_loss: 1.8263 - val_accuracy: 0.5249\n",
      "Epoch 206/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.3037 - accuracy: 0.8581 - val_loss: 1.8042 - val_accuracy: 0.5023\n",
      "Epoch 207/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.2738 - accuracy: 0.8697 - val_loss: 1.9345 - val_accuracy: 0.4796\n",
      "Epoch 208/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.2749 - accuracy: 0.8657 - val_loss: 1.8673 - val_accuracy: 0.4977\n",
      "Epoch 209/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.2553 - accuracy: 0.8742 - val_loss: 1.9912 - val_accuracy: 0.5249\n",
      "Epoch 210/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.2422 - accuracy: 0.8833 - val_loss: 2.0095 - val_accuracy: 0.5339\n",
      "Epoch 211/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.2557 - accuracy: 0.8798 - val_loss: 1.9129 - val_accuracy: 0.5385\n",
      "Epoch 212/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.2534 - accuracy: 0.8833 - val_loss: 2.0013 - val_accuracy: 0.5339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.2354 - accuracy: 0.8914 - val_loss: 2.0587 - val_accuracy: 0.4977\n",
      "Epoch 214/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.2389 - accuracy: 0.8848 - val_loss: 2.0053 - val_accuracy: 0.5204\n",
      "Epoch 215/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.2644 - accuracy: 0.8717 - val_loss: 2.0772 - val_accuracy: 0.5068\n",
      "Epoch 216/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.2962 - accuracy: 0.8611 - val_loss: 2.0445 - val_accuracy: 0.5249\n",
      "Epoch 217/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.2709 - accuracy: 0.8737 - val_loss: 2.0396 - val_accuracy: 0.5294\n",
      "Epoch 218/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.2589 - accuracy: 0.8758 - val_loss: 2.0587 - val_accuracy: 0.5158\n",
      "Epoch 219/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.2486 - accuracy: 0.8838 - val_loss: 2.1186 - val_accuracy: 0.5068\n",
      "Epoch 220/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.2069 - accuracy: 0.8970 - val_loss: 2.2319 - val_accuracy: 0.5068\n",
      "Epoch 221/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.1866 - accuracy: 0.9152 - val_loss: 2.1985 - val_accuracy: 0.5249\n",
      "Epoch 222/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.1998 - accuracy: 0.9076 - val_loss: 2.1599 - val_accuracy: 0.4977\n",
      "Epoch 223/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 0.1989 - accuracy: 0.9146 - val_loss: 2.2302 - val_accuracy: 0.5158\n",
      "Epoch 224/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.2020 - accuracy: 0.9045 - val_loss: 2.2720 - val_accuracy: 0.5023\n",
      "Epoch 225/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.1922 - accuracy: 0.9111 - val_loss: 2.2978 - val_accuracy: 0.5113\n",
      "Epoch 226/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.1719 - accuracy: 0.9222 - val_loss: 2.2723 - val_accuracy: 0.5158\n",
      "Epoch 227/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.1543 - accuracy: 0.9359 - val_loss: 2.4087 - val_accuracy: 0.5249\n",
      "Epoch 228/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.1423 - accuracy: 0.9364 - val_loss: 2.5071 - val_accuracy: 0.4932\n",
      "Epoch 229/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.1365 - accuracy: 0.9439 - val_loss: 2.6410 - val_accuracy: 0.5204\n",
      "Epoch 230/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.1345 - accuracy: 0.9394 - val_loss: 2.5859 - val_accuracy: 0.5204\n",
      "Epoch 231/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.1301 - accuracy: 0.9460 - val_loss: 2.6546 - val_accuracy: 0.5068\n",
      "Epoch 232/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 0.1267 - accuracy: 0.9444 - val_loss: 2.6756 - val_accuracy: 0.5158\n",
      "Epoch 233/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.1413 - accuracy: 0.9399 - val_loss: 2.6298 - val_accuracy: 0.5158\n",
      "Epoch 234/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.1507 - accuracy: 0.9298 - val_loss: 2.6892 - val_accuracy: 0.5430\n",
      "Epoch 235/1000\n",
      "1980/1980 [==============================] - 1s 450us/step - loss: 0.1485 - accuracy: 0.9389 - val_loss: 2.7717 - val_accuracy: 0.5385\n",
      "Epoch 236/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 0.1745 - accuracy: 0.9288 - val_loss: 2.6269 - val_accuracy: 0.5204\n",
      "Epoch 237/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.1945 - accuracy: 0.9136 - val_loss: 2.6327 - val_accuracy: 0.4842\n",
      "Epoch 238/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.4646 - accuracy: 0.8192 - val_loss: 2.0030 - val_accuracy: 0.5294\n",
      "Epoch 239/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.3557 - accuracy: 0.8419 - val_loss: 1.8578 - val_accuracy: 0.4977\n",
      "Epoch 240/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 0.2592 - accuracy: 0.8848 - val_loss: 2.0640 - val_accuracy: 0.5023\n",
      "Epoch 241/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.2010 - accuracy: 0.9167 - val_loss: 2.2767 - val_accuracy: 0.4661\n",
      "Epoch 242/1000\n",
      "1980/1980 [==============================] - 1s 455us/step - loss: 0.1976 - accuracy: 0.9192 - val_loss: 2.2955 - val_accuracy: 0.5068\n",
      "Epoch 243/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.1854 - accuracy: 0.9202 - val_loss: 2.3144 - val_accuracy: 0.5068\n",
      "Epoch 244/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.1628 - accuracy: 0.9313 - val_loss: 2.2824 - val_accuracy: 0.5249\n",
      "Epoch 245/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.1253 - accuracy: 0.9449 - val_loss: 2.4906 - val_accuracy: 0.5385\n",
      "Epoch 246/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.1261 - accuracy: 0.9495 - val_loss: 2.4151 - val_accuracy: 0.5113\n",
      "Epoch 247/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.1129 - accuracy: 0.9525 - val_loss: 2.4748 - val_accuracy: 0.5113\n",
      "Epoch 248/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.1039 - accuracy: 0.9535 - val_loss: 2.6152 - val_accuracy: 0.5158\n",
      "Epoch 249/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0902 - accuracy: 0.9631 - val_loss: 2.7056 - val_accuracy: 0.4706\n",
      "Epoch 250/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0975 - accuracy: 0.9566 - val_loss: 2.7876 - val_accuracy: 0.4932\n",
      "Epoch 251/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0947 - accuracy: 0.9581 - val_loss: 2.8144 - val_accuracy: 0.4932\n",
      "Epoch 252/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0902 - accuracy: 0.9652 - val_loss: 2.8037 - val_accuracy: 0.4842\n",
      "Epoch 253/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0881 - accuracy: 0.9682 - val_loss: 2.9407 - val_accuracy: 0.5068\n",
      "Epoch 254/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0826 - accuracy: 0.9662 - val_loss: 2.9173 - val_accuracy: 0.4796\n",
      "Epoch 255/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0958 - accuracy: 0.9606 - val_loss: 2.8381 - val_accuracy: 0.4932\n",
      "Epoch 256/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.1176 - accuracy: 0.9561 - val_loss: 2.6710 - val_accuracy: 0.5023\n",
      "Epoch 257/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.1065 - accuracy: 0.9581 - val_loss: 2.8828 - val_accuracy: 0.5023\n",
      "Epoch 258/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.1069 - accuracy: 0.9561 - val_loss: 2.9378 - val_accuracy: 0.4932\n",
      "Epoch 259/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0966 - accuracy: 0.9586 - val_loss: 3.0305 - val_accuracy: 0.4932\n",
      "Epoch 260/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.1140 - accuracy: 0.9485 - val_loss: 3.0161 - val_accuracy: 0.5023\n",
      "Epoch 261/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.1144 - accuracy: 0.9556 - val_loss: 2.9876 - val_accuracy: 0.5113\n",
      "Epoch 262/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.1295 - accuracy: 0.9485 - val_loss: 2.8745 - val_accuracy: 0.5204\n",
      "Epoch 263/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.1421 - accuracy: 0.9475 - val_loss: 2.8843 - val_accuracy: 0.5068\n",
      "Epoch 264/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.1532 - accuracy: 0.9394 - val_loss: 2.9049 - val_accuracy: 0.4977\n",
      "Epoch 265/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0975 - accuracy: 0.9596 - val_loss: 2.7899 - val_accuracy: 0.5249\n",
      "Epoch 266/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.1000 - accuracy: 0.9611 - val_loss: 2.9149 - val_accuracy: 0.5339\n",
      "Epoch 267/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.1012 - accuracy: 0.9576 - val_loss: 2.9209 - val_accuracy: 0.5023\n",
      "Epoch 268/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0989 - accuracy: 0.9652 - val_loss: 3.0198 - val_accuracy: 0.5023\n",
      "Epoch 269/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.0812 - accuracy: 0.9692 - val_loss: 3.1038 - val_accuracy: 0.5113\n",
      "Epoch 270/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.0600 - accuracy: 0.9773 - val_loss: 3.0849 - val_accuracy: 0.5068\n",
      "Epoch 271/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.0517 - accuracy: 0.9803 - val_loss: 3.1899 - val_accuracy: 0.4977\n",
      "Epoch 272/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.0577 - accuracy: 0.9803 - val_loss: 3.0984 - val_accuracy: 0.5068\n",
      "Epoch 273/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0484 - accuracy: 0.9838 - val_loss: 3.2340 - val_accuracy: 0.5068\n",
      "Epoch 274/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0441 - accuracy: 0.9823 - val_loss: 3.1909 - val_accuracy: 0.4887\n",
      "Epoch 275/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0464 - accuracy: 0.9808 - val_loss: 3.2788 - val_accuracy: 0.4842\n",
      "Epoch 276/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0424 - accuracy: 0.9859 - val_loss: 3.1996 - val_accuracy: 0.5204\n",
      "Epoch 277/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0384 - accuracy: 0.9828 - val_loss: 3.3957 - val_accuracy: 0.5113\n",
      "Epoch 278/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0426 - accuracy: 0.9833 - val_loss: 3.3389 - val_accuracy: 0.4932\n",
      "Epoch 279/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0556 - accuracy: 0.9793 - val_loss: 3.2212 - val_accuracy: 0.5204\n",
      "Epoch 280/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0553 - accuracy: 0.9778 - val_loss: 3.2072 - val_accuracy: 0.5158\n",
      "Epoch 281/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0395 - accuracy: 0.9838 - val_loss: 3.2922 - val_accuracy: 0.5158\n",
      "Epoch 282/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0319 - accuracy: 0.9889 - val_loss: 3.3423 - val_accuracy: 0.5023\n",
      "Epoch 283/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0288 - accuracy: 0.9894 - val_loss: 3.3991 - val_accuracy: 0.5068\n",
      "Epoch 284/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0308 - accuracy: 0.9899 - val_loss: 3.5092 - val_accuracy: 0.5158\n",
      "Epoch 285/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0374 - accuracy: 0.9848 - val_loss: 3.4288 - val_accuracy: 0.4977\n",
      "Epoch 286/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0423 - accuracy: 0.9843 - val_loss: 3.5127 - val_accuracy: 0.5023\n",
      "Epoch 287/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0404 - accuracy: 0.9838 - val_loss: 3.4716 - val_accuracy: 0.4977\n",
      "Epoch 288/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0410 - accuracy: 0.9823 - val_loss: 3.4943 - val_accuracy: 0.4932\n",
      "Epoch 289/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0502 - accuracy: 0.9833 - val_loss: 3.5545 - val_accuracy: 0.4796\n",
      "Epoch 290/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0486 - accuracy: 0.9813 - val_loss: 3.5671 - val_accuracy: 0.4796\n",
      "Epoch 291/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0370 - accuracy: 0.9874 - val_loss: 3.4479 - val_accuracy: 0.5068\n",
      "Epoch 292/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0481 - accuracy: 0.9823 - val_loss: 3.6070 - val_accuracy: 0.4751\n",
      "Epoch 293/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0474 - accuracy: 0.9848 - val_loss: 3.5494 - val_accuracy: 0.5204\n",
      "Epoch 294/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.1100 - accuracy: 0.9611 - val_loss: 3.4459 - val_accuracy: 0.5158\n",
      "Epoch 295/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.1496 - accuracy: 0.9444 - val_loss: 3.2868 - val_accuracy: 0.5430\n",
      "Epoch 296/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.2528 - accuracy: 0.9056 - val_loss: 3.1059 - val_accuracy: 0.4977\n",
      "Epoch 297/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.3109 - accuracy: 0.8742 - val_loss: 2.7555 - val_accuracy: 0.5294\n",
      "Epoch 298/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.2498 - accuracy: 0.9061 - val_loss: 2.8320 - val_accuracy: 0.5204\n",
      "Epoch 299/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.1551 - accuracy: 0.9444 - val_loss: 2.6536 - val_accuracy: 0.5204\n",
      "Epoch 300/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.1318 - accuracy: 0.9475 - val_loss: 2.8749 - val_accuracy: 0.5068\n",
      "Epoch 301/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0915 - accuracy: 0.9626 - val_loss: 2.9827 - val_accuracy: 0.5113\n",
      "Epoch 302/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0495 - accuracy: 0.9869 - val_loss: 3.1086 - val_accuracy: 0.5068\n",
      "Epoch 303/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0418 - accuracy: 0.9874 - val_loss: 3.1253 - val_accuracy: 0.4977\n",
      "Epoch 304/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0301 - accuracy: 0.9919 - val_loss: 3.2209 - val_accuracy: 0.4977\n",
      "Epoch 305/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0298 - accuracy: 0.9909 - val_loss: 3.2197 - val_accuracy: 0.5113\n",
      "Epoch 306/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0253 - accuracy: 0.9929 - val_loss: 3.2534 - val_accuracy: 0.5023\n",
      "Epoch 307/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0261 - accuracy: 0.9919 - val_loss: 3.3209 - val_accuracy: 0.5023\n",
      "Epoch 308/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0299 - accuracy: 0.9904 - val_loss: 3.3394 - val_accuracy: 0.5113\n",
      "Epoch 309/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0300 - accuracy: 0.9919 - val_loss: 3.2736 - val_accuracy: 0.5385\n",
      "Epoch 310/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0217 - accuracy: 0.9934 - val_loss: 3.3846 - val_accuracy: 0.5204\n",
      "Epoch 311/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.0199 - accuracy: 0.9924 - val_loss: 3.3626 - val_accuracy: 0.5430\n",
      "Epoch 312/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.0169 - accuracy: 0.9934 - val_loss: 3.4421 - val_accuracy: 0.5204\n",
      "Epoch 313/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.0150 - accuracy: 0.9955 - val_loss: 3.4656 - val_accuracy: 0.5204\n",
      "Epoch 314/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.0143 - accuracy: 0.9970 - val_loss: 3.5155 - val_accuracy: 0.5249\n",
      "Epoch 315/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.0137 - accuracy: 0.9960 - val_loss: 3.5321 - val_accuracy: 0.5249\n",
      "Epoch 316/1000\n",
      "1980/1980 [==============================] - 1s 445us/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 3.5188 - val_accuracy: 0.5294\n",
      "Epoch 317/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0130 - accuracy: 0.9965 - val_loss: 3.5616 - val_accuracy: 0.5294\n",
      "Epoch 318/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 3.5648 - val_accuracy: 0.5204\n",
      "Epoch 319/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.0120 - accuracy: 0.9975 - val_loss: 3.6191 - val_accuracy: 0.5249\n",
      "Epoch 320/1000\n",
      "1980/1980 [==============================] - 1s 442us/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 3.6236 - val_accuracy: 0.5249\n",
      "Epoch 321/1000\n",
      "1980/1980 [==============================] - 1s 434us/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 3.6576 - val_accuracy: 0.5204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0100 - accuracy: 0.9980 - val_loss: 3.6366 - val_accuracy: 0.5204\n",
      "Epoch 323/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 3.6418 - val_accuracy: 0.5204\n",
      "Epoch 324/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0113 - accuracy: 0.9980 - val_loss: 3.6757 - val_accuracy: 0.5158\n",
      "Epoch 325/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0107 - accuracy: 0.9975 - val_loss: 3.6912 - val_accuracy: 0.5249\n",
      "Epoch 326/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 3.6837 - val_accuracy: 0.5023\n",
      "Epoch 327/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0373 - accuracy: 0.9838 - val_loss: 3.5474 - val_accuracy: 0.5113\n",
      "Epoch 328/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0476 - accuracy: 0.9838 - val_loss: 3.5010 - val_accuracy: 0.5023\n",
      "Epoch 329/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 3.5921 - val_accuracy: 0.4977\n",
      "Epoch 330/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0323 - accuracy: 0.9904 - val_loss: 3.5354 - val_accuracy: 0.5294\n",
      "Epoch 331/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0419 - accuracy: 0.9828 - val_loss: 3.5831 - val_accuracy: 0.5204\n",
      "Epoch 332/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0787 - accuracy: 0.9763 - val_loss: 3.4942 - val_accuracy: 0.5204\n",
      "Epoch 333/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0826 - accuracy: 0.9727 - val_loss: 3.5440 - val_accuracy: 0.4796\n",
      "Epoch 334/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.1319 - accuracy: 0.9500 - val_loss: 3.2290 - val_accuracy: 0.5249\n",
      "Epoch 335/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.2290 - accuracy: 0.9141 - val_loss: 3.0691 - val_accuracy: 0.4887\n",
      "Epoch 336/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.1398 - accuracy: 0.9465 - val_loss: 3.3033 - val_accuracy: 0.4932\n",
      "Epoch 337/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0970 - accuracy: 0.9646 - val_loss: 3.1425 - val_accuracy: 0.5385\n",
      "Epoch 338/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.1072 - accuracy: 0.9596 - val_loss: 2.9493 - val_accuracy: 0.5294\n",
      "Epoch 339/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.0797 - accuracy: 0.9727 - val_loss: 2.9790 - val_accuracy: 0.5294\n",
      "Epoch 340/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.0590 - accuracy: 0.9818 - val_loss: 3.3450 - val_accuracy: 0.5158\n",
      "Epoch 341/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0413 - accuracy: 0.9869 - val_loss: 3.3407 - val_accuracy: 0.5204\n",
      "Epoch 342/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0461 - accuracy: 0.9874 - val_loss: 3.4201 - val_accuracy: 0.5158\n",
      "Epoch 343/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0527 - accuracy: 0.9818 - val_loss: 3.3576 - val_accuracy: 0.5158\n",
      "Epoch 344/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0526 - accuracy: 0.9813 - val_loss: 3.3450 - val_accuracy: 0.5113\n",
      "Epoch 345/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0573 - accuracy: 0.9818 - val_loss: 3.2906 - val_accuracy: 0.4977\n",
      "Epoch 346/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0581 - accuracy: 0.9798 - val_loss: 3.2666 - val_accuracy: 0.5204\n",
      "Epoch 347/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0481 - accuracy: 0.9833 - val_loss: 3.3048 - val_accuracy: 0.5294\n",
      "Epoch 348/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0414 - accuracy: 0.9899 - val_loss: 3.3529 - val_accuracy: 0.5158\n",
      "Epoch 349/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0380 - accuracy: 0.9874 - val_loss: 3.3703 - val_accuracy: 0.5158\n",
      "Epoch 350/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0299 - accuracy: 0.9919 - val_loss: 3.4872 - val_accuracy: 0.4977\n",
      "Epoch 351/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0291 - accuracy: 0.9899 - val_loss: 3.4251 - val_accuracy: 0.4977\n",
      "Epoch 352/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0220 - accuracy: 0.9944 - val_loss: 3.5410 - val_accuracy: 0.4932\n",
      "Epoch 353/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0192 - accuracy: 0.9949 - val_loss: 3.5235 - val_accuracy: 0.5113\n",
      "Epoch 354/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.0160 - accuracy: 0.9955 - val_loss: 3.5822 - val_accuracy: 0.5113\n",
      "Epoch 355/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0142 - accuracy: 0.9970 - val_loss: 3.6152 - val_accuracy: 0.5068\n",
      "Epoch 356/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0108 - accuracy: 0.9975 - val_loss: 3.6528 - val_accuracy: 0.5158\n",
      "Epoch 357/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.0116 - accuracy: 0.9975 - val_loss: 3.6210 - val_accuracy: 0.5113\n",
      "Epoch 358/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 3.6453 - val_accuracy: 0.5158\n",
      "Epoch 359/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0078 - accuracy: 0.9990 - val_loss: 3.6717 - val_accuracy: 0.5113\n",
      "Epoch 360/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 3.6977 - val_accuracy: 0.5158\n",
      "Epoch 361/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0072 - accuracy: 0.9990 - val_loss: 3.7227 - val_accuracy: 0.5113\n",
      "Epoch 362/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 3.7482 - val_accuracy: 0.5113\n",
      "Epoch 363/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0063 - accuracy: 0.9995 - val_loss: 3.7814 - val_accuracy: 0.5158\n",
      "Epoch 364/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 3.7851 - val_accuracy: 0.5158\n",
      "Epoch 365/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0060 - accuracy: 0.9990 - val_loss: 3.8080 - val_accuracy: 0.5113\n",
      "Epoch 366/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 3.8216 - val_accuracy: 0.5158\n",
      "Epoch 367/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0054 - accuracy: 0.9995 - val_loss: 3.8525 - val_accuracy: 0.5113\n",
      "Epoch 368/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0053 - accuracy: 0.9995 - val_loss: 3.8683 - val_accuracy: 0.5113\n",
      "Epoch 369/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 3.8835 - val_accuracy: 0.5158\n",
      "Epoch 370/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 3.8814 - val_accuracy: 0.5113\n",
      "Epoch 371/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.8929 - val_accuracy: 0.5158\n",
      "Epoch 372/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 3.9230 - val_accuracy: 0.5113\n",
      "Epoch 373/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 3.9229 - val_accuracy: 0.5068\n",
      "Epoch 374/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.9409 - val_accuracy: 0.5113\n",
      "Epoch 375/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.9587 - val_accuracy: 0.5113\n",
      "Epoch 376/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.9779 - val_accuracy: 0.5113\n",
      "Epoch 377/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.9913 - val_accuracy: 0.5113\n",
      "Epoch 378/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.9925 - val_accuracy: 0.5113\n",
      "Epoch 379/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.0007 - val_accuracy: 0.5113\n",
      "Epoch 380/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.9997 - val_accuracy: 0.5158\n",
      "Epoch 381/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.0078 - val_accuracy: 0.5113\n",
      "Epoch 382/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.0238 - val_accuracy: 0.5113\n",
      "Epoch 383/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.0352 - val_accuracy: 0.5113\n",
      "Epoch 384/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0071 - accuracy: 0.9990 - val_loss: 3.9719 - val_accuracy: 0.5339\n",
      "Epoch 385/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0071 - accuracy: 0.9990 - val_loss: 3.9995 - val_accuracy: 0.5113\n",
      "Epoch 386/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0133 - accuracy: 0.9975 - val_loss: 4.0244 - val_accuracy: 0.5158\n",
      "Epoch 387/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 3.9228 - val_accuracy: 0.5158\n",
      "Epoch 388/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0161 - accuracy: 0.9955 - val_loss: 3.8478 - val_accuracy: 0.5249\n",
      "Epoch 389/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0447 - accuracy: 0.9848 - val_loss: 4.0461 - val_accuracy: 0.4977\n",
      "Epoch 390/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.1312 - accuracy: 0.9581 - val_loss: 3.6467 - val_accuracy: 0.4842\n",
      "Epoch 391/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.2685 - accuracy: 0.9131 - val_loss: 3.3102 - val_accuracy: 0.4932\n",
      "Epoch 392/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.2924 - accuracy: 0.9066 - val_loss: 3.0629 - val_accuracy: 0.4706\n",
      "Epoch 393/1000\n",
      "1980/1980 [==============================] - 1s 444us/step - loss: 0.2293 - accuracy: 0.9141 - val_loss: 2.8114 - val_accuracy: 0.5023\n",
      "Epoch 394/1000\n",
      "1980/1980 [==============================] - 1s 479us/step - loss: 0.1809 - accuracy: 0.9313 - val_loss: 2.9029 - val_accuracy: 0.5249\n",
      "Epoch 395/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 0.1088 - accuracy: 0.9586 - val_loss: 2.9928 - val_accuracy: 0.5385\n",
      "Epoch 396/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.0694 - accuracy: 0.9768 - val_loss: 2.9254 - val_accuracy: 0.5068\n",
      "Epoch 397/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.0491 - accuracy: 0.9859 - val_loss: 3.0718 - val_accuracy: 0.5249\n",
      "Epoch 398/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0343 - accuracy: 0.9884 - val_loss: 3.2232 - val_accuracy: 0.5113\n",
      "Epoch 399/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0202 - accuracy: 0.9960 - val_loss: 3.2844 - val_accuracy: 0.5294\n",
      "Epoch 400/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0149 - accuracy: 0.9980 - val_loss: 3.3366 - val_accuracy: 0.5249\n",
      "Epoch 401/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0111 - accuracy: 0.9990 - val_loss: 3.3542 - val_accuracy: 0.5294\n",
      "Epoch 402/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.4290 - val_accuracy: 0.5249\n",
      "Epoch 403/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0076 - accuracy: 0.9995 - val_loss: 3.4749 - val_accuracy: 0.5339\n",
      "Epoch 404/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0076 - accuracy: 0.9990 - val_loss: 3.5000 - val_accuracy: 0.5339\n",
      "Epoch 405/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0065 - accuracy: 0.9995 - val_loss: 3.5261 - val_accuracy: 0.5385\n",
      "Epoch 406/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.5443 - val_accuracy: 0.5339\n",
      "Epoch 407/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 3.5778 - val_accuracy: 0.5294\n",
      "Epoch 408/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.5928 - val_accuracy: 0.5339\n",
      "Epoch 409/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 3.6040 - val_accuracy: 0.5294\n",
      "Epoch 410/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0053 - accuracy: 0.9995 - val_loss: 3.6331 - val_accuracy: 0.5204\n",
      "Epoch 411/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.6580 - val_accuracy: 0.5249\n",
      "Epoch 412/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.6741 - val_accuracy: 0.5249\n",
      "Epoch 413/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 3.6890 - val_accuracy: 0.5249\n",
      "Epoch 414/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.7051 - val_accuracy: 0.5158\n",
      "Epoch 415/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.7057 - val_accuracy: 0.5249\n",
      "Epoch 416/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 3.7218 - val_accuracy: 0.5113\n",
      "Epoch 417/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.7439 - val_accuracy: 0.5158\n",
      "Epoch 418/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.7634 - val_accuracy: 0.5113\n",
      "Epoch 419/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.7703 - val_accuracy: 0.5113\n",
      "Epoch 420/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.7809 - val_accuracy: 0.5158\n",
      "Epoch 421/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.7983 - val_accuracy: 0.5158\n",
      "Epoch 422/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.8078 - val_accuracy: 0.5158\n",
      "Epoch 423/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 3.8183 - val_accuracy: 0.5113\n",
      "Epoch 424/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 3.8336 - val_accuracy: 0.5204\n",
      "Epoch 425/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 3.8216 - val_accuracy: 0.5294\n",
      "Epoch 426/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.8353 - val_accuracy: 0.5113\n",
      "Epoch 427/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 3.8961 - val_accuracy: 0.5158\n",
      "Epoch 428/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.9043 - val_accuracy: 0.5158\n",
      "Epoch 429/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.9105 - val_accuracy: 0.5158\n",
      "Epoch 430/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.8997 - val_accuracy: 0.5249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 431/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 3.8491 - val_accuracy: 0.5158\n",
      "Epoch 432/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.0236 - accuracy: 0.9934 - val_loss: 4.0066 - val_accuracy: 0.5068\n",
      "Epoch 433/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0178 - accuracy: 0.9934 - val_loss: 3.9399 - val_accuracy: 0.5068\n",
      "Epoch 434/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0234 - accuracy: 0.9919 - val_loss: 3.9600 - val_accuracy: 0.5158\n",
      "Epoch 435/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0320 - accuracy: 0.9894 - val_loss: 3.7841 - val_accuracy: 0.4842\n",
      "Epoch 436/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.1076 - accuracy: 0.9662 - val_loss: 3.6086 - val_accuracy: 0.4842\n",
      "Epoch 437/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.1749 - accuracy: 0.9460 - val_loss: 3.2644 - val_accuracy: 0.5249\n",
      "Epoch 438/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.1700 - accuracy: 0.9394 - val_loss: 3.2758 - val_accuracy: 0.4977\n",
      "Epoch 439/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.1161 - accuracy: 0.9556 - val_loss: 3.2292 - val_accuracy: 0.5294\n",
      "Epoch 440/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0915 - accuracy: 0.9667 - val_loss: 3.2472 - val_accuracy: 0.4887\n",
      "Epoch 441/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0673 - accuracy: 0.9778 - val_loss: 3.1858 - val_accuracy: 0.5204\n",
      "Epoch 442/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0793 - accuracy: 0.9753 - val_loss: 3.2931 - val_accuracy: 0.4977\n",
      "Epoch 443/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0651 - accuracy: 0.9763 - val_loss: 3.3064 - val_accuracy: 0.5475\n",
      "Epoch 444/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0723 - accuracy: 0.9753 - val_loss: 3.2604 - val_accuracy: 0.5339\n",
      "Epoch 445/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0720 - accuracy: 0.9763 - val_loss: 3.1978 - val_accuracy: 0.5158\n",
      "Epoch 446/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.0399 - accuracy: 0.9854 - val_loss: 3.4925 - val_accuracy: 0.5249\n",
      "Epoch 447/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0342 - accuracy: 0.9884 - val_loss: 3.4570 - val_accuracy: 0.5339\n",
      "Epoch 448/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0277 - accuracy: 0.9919 - val_loss: 3.5054 - val_accuracy: 0.5249\n",
      "Epoch 449/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0171 - accuracy: 0.9970 - val_loss: 3.5537 - val_accuracy: 0.5113\n",
      "Epoch 450/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 3.6272 - val_accuracy: 0.5023\n",
      "Epoch 451/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.0087 - accuracy: 0.9995 - val_loss: 3.6359 - val_accuracy: 0.5158\n",
      "Epoch 452/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0064 - accuracy: 0.9995 - val_loss: 3.7101 - val_accuracy: 0.5113\n",
      "Epoch 453/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 3.7403 - val_accuracy: 0.5068\n",
      "Epoch 454/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.7574 - val_accuracy: 0.5068\n",
      "Epoch 455/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 3.7694 - val_accuracy: 0.5113\n",
      "Epoch 456/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.7883 - val_accuracy: 0.5158\n",
      "Epoch 457/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 3.7997 - val_accuracy: 0.5158\n",
      "Epoch 458/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.8077 - val_accuracy: 0.5204\n",
      "Epoch 459/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.8269 - val_accuracy: 0.5204\n",
      "Epoch 460/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.8416 - val_accuracy: 0.5113\n",
      "Epoch 461/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.8568 - val_accuracy: 0.5158\n",
      "Epoch 462/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.8636 - val_accuracy: 0.5158\n",
      "Epoch 463/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.8711 - val_accuracy: 0.5158\n",
      "Epoch 464/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.8791 - val_accuracy: 0.5158\n",
      "Epoch 465/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.8892 - val_accuracy: 0.5113\n",
      "Epoch 466/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.8941 - val_accuracy: 0.5113\n",
      "Epoch 467/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9066 - val_accuracy: 0.5113\n",
      "Epoch 468/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.9145 - val_accuracy: 0.5113\n",
      "Epoch 469/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9248 - val_accuracy: 0.5113\n",
      "Epoch 470/1000\n",
      "1980/1980 [==============================] - 1s 474us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9363 - val_accuracy: 0.5204\n",
      "Epoch 471/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.9399 - val_accuracy: 0.5204\n",
      "Epoch 472/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.9476 - val_accuracy: 0.5158\n",
      "Epoch 473/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.9559 - val_accuracy: 0.5204\n",
      "Epoch 474/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9653 - val_accuracy: 0.5158\n",
      "Epoch 475/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.9788 - val_accuracy: 0.5158\n",
      "Epoch 476/1000\n",
      "1980/1980 [==============================] - 1s 478us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.9844 - val_accuracy: 0.5158\n",
      "Epoch 477/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9931 - val_accuracy: 0.5204\n",
      "Epoch 478/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.9990 - val_accuracy: 0.5204\n",
      "Epoch 479/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0018 - val_accuracy: 0.5204\n",
      "Epoch 480/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0087 - val_accuracy: 0.5158\n",
      "Epoch 481/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0178 - val_accuracy: 0.5158\n",
      "Epoch 482/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0161 - val_accuracy: 0.5204\n",
      "Epoch 483/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0231 - val_accuracy: 0.5249\n",
      "Epoch 484/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.0322 - val_accuracy: 0.5249\n",
      "Epoch 485/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0412 - val_accuracy: 0.5249\n",
      "Epoch 486/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0512 - val_accuracy: 0.5249\n",
      "Epoch 487/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0556 - val_accuracy: 0.5249\n",
      "Epoch 488/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.0699 - val_accuracy: 0.5249\n",
      "Epoch 489/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0761 - val_accuracy: 0.5204\n",
      "Epoch 490/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0836 - val_accuracy: 0.5294\n",
      "Epoch 491/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.0920 - val_accuracy: 0.5249\n",
      "Epoch 492/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.1023 - val_accuracy: 0.5204\n",
      "Epoch 493/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.1087 - val_accuracy: 0.5204\n",
      "Epoch 494/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1177 - val_accuracy: 0.5158\n",
      "Epoch 495/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 9.7856e-04 - accuracy: 1.0000 - val_loss: 4.1244 - val_accuracy: 0.5249\n",
      "Epoch 496/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1328 - val_accuracy: 0.5249\n",
      "Epoch 497/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 9.1380e-04 - accuracy: 1.0000 - val_loss: 4.1379 - val_accuracy: 0.5294\n",
      "Epoch 498/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 8.5368e-04 - accuracy: 1.0000 - val_loss: 4.1424 - val_accuracy: 0.5204\n",
      "Epoch 499/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 8.1501e-04 - accuracy: 1.0000 - val_loss: 4.1485 - val_accuracy: 0.5204\n",
      "Epoch 500/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 7.9253e-04 - accuracy: 1.0000 - val_loss: 4.1545 - val_accuracy: 0.5249\n",
      "Epoch 501/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 7.8596e-04 - accuracy: 1.0000 - val_loss: 4.1617 - val_accuracy: 0.5249\n",
      "Epoch 502/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 8.1835e-04 - accuracy: 1.0000 - val_loss: 4.1719 - val_accuracy: 0.5249\n",
      "Epoch 503/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 8.5035e-04 - accuracy: 1.0000 - val_loss: 4.1769 - val_accuracy: 0.5204\n",
      "Epoch 504/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 7.6960e-04 - accuracy: 1.0000 - val_loss: 4.1827 - val_accuracy: 0.5249\n",
      "Epoch 505/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 7.8759e-04 - accuracy: 1.0000 - val_loss: 4.1883 - val_accuracy: 0.5249\n",
      "Epoch 506/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 8.5047e-04 - accuracy: 1.0000 - val_loss: 4.1931 - val_accuracy: 0.5249\n",
      "Epoch 507/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 7.9088e-04 - accuracy: 1.0000 - val_loss: 4.1988 - val_accuracy: 0.5249\n",
      "Epoch 508/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 8.4043e-04 - accuracy: 1.0000 - val_loss: 4.2058 - val_accuracy: 0.5249\n",
      "Epoch 509/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 8.0218e-04 - accuracy: 1.0000 - val_loss: 4.2170 - val_accuracy: 0.5249\n",
      "Epoch 510/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 7.8775e-04 - accuracy: 1.0000 - val_loss: 4.2246 - val_accuracy: 0.5249\n",
      "Epoch 511/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.8437e-04 - accuracy: 1.0000 - val_loss: 4.2304 - val_accuracy: 0.5294\n",
      "Epoch 512/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.8536e-04 - accuracy: 1.0000 - val_loss: 4.2363 - val_accuracy: 0.5294\n",
      "Epoch 513/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 7.3302e-04 - accuracy: 1.0000 - val_loss: 4.2446 - val_accuracy: 0.5249\n",
      "Epoch 514/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 8.8574e-04 - accuracy: 1.0000 - val_loss: 4.2479 - val_accuracy: 0.5249\n",
      "Epoch 515/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 7.5674e-04 - accuracy: 1.0000 - val_loss: 4.2535 - val_accuracy: 0.5249\n",
      "Epoch 516/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 6.7465e-04 - accuracy: 1.0000 - val_loss: 4.2607 - val_accuracy: 0.5249\n",
      "Epoch 517/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 7.0747e-04 - accuracy: 1.0000 - val_loss: 4.2686 - val_accuracy: 0.5249\n",
      "Epoch 518/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 6.6312e-04 - accuracy: 1.0000 - val_loss: 4.2686 - val_accuracy: 0.5294\n",
      "Epoch 519/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.9239e-04 - accuracy: 1.0000 - val_loss: 4.2727 - val_accuracy: 0.5249\n",
      "Epoch 520/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 6.4421e-04 - accuracy: 1.0000 - val_loss: 4.2785 - val_accuracy: 0.5294\n",
      "Epoch 521/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 6.5251e-04 - accuracy: 1.0000 - val_loss: 4.2847 - val_accuracy: 0.5294\n",
      "Epoch 522/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 5.7913e-04 - accuracy: 1.0000 - val_loss: 4.2940 - val_accuracy: 0.5294\n",
      "Epoch 523/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 5.8530e-04 - accuracy: 1.0000 - val_loss: 4.2993 - val_accuracy: 0.5294\n",
      "Epoch 524/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 5.4532e-04 - accuracy: 1.0000 - val_loss: 4.3045 - val_accuracy: 0.5294\n",
      "Epoch 525/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 6.1229e-04 - accuracy: 1.0000 - val_loss: 4.3105 - val_accuracy: 0.5294\n",
      "Epoch 526/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 5.6428e-04 - accuracy: 1.0000 - val_loss: 4.3148 - val_accuracy: 0.5294\n",
      "Epoch 527/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 5.5125e-04 - accuracy: 1.0000 - val_loss: 4.3207 - val_accuracy: 0.5294\n",
      "Epoch 528/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 7.1370e-04 - accuracy: 1.0000 - val_loss: 4.3256 - val_accuracy: 0.5204\n",
      "Epoch 529/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.3463e-04 - accuracy: 1.0000 - val_loss: 4.3326 - val_accuracy: 0.5294\n",
      "Epoch 530/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 4.9634e-04 - accuracy: 1.0000 - val_loss: 4.3402 - val_accuracy: 0.5249\n",
      "Epoch 531/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.1673e-04 - accuracy: 1.0000 - val_loss: 4.3457 - val_accuracy: 0.5204\n",
      "Epoch 532/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 5.3808e-04 - accuracy: 1.0000 - val_loss: 4.3518 - val_accuracy: 0.5249\n",
      "Epoch 533/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 5.1902e-04 - accuracy: 1.0000 - val_loss: 4.3571 - val_accuracy: 0.5294\n",
      "Epoch 534/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 5.9164e-04 - accuracy: 1.0000 - val_loss: 4.3611 - val_accuracy: 0.5294\n",
      "Epoch 535/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 4.7213e-04 - accuracy: 1.0000 - val_loss: 4.3696 - val_accuracy: 0.5249\n",
      "Epoch 536/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 5.3595e-04 - accuracy: 1.0000 - val_loss: 4.3717 - val_accuracy: 0.5249\n",
      "Epoch 537/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 5.2043e-04 - accuracy: 1.0000 - val_loss: 4.3745 - val_accuracy: 0.5294\n",
      "Epoch 538/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 382us/step - loss: 4.9537e-04 - accuracy: 1.0000 - val_loss: 4.3790 - val_accuracy: 0.5294\n",
      "Epoch 539/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 4.9277e-04 - accuracy: 1.0000 - val_loss: 4.3864 - val_accuracy: 0.5294\n",
      "Epoch 540/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 5.1057e-04 - accuracy: 1.0000 - val_loss: 4.3894 - val_accuracy: 0.5294\n",
      "Epoch 541/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 4.2059e-04 - accuracy: 1.0000 - val_loss: 4.3945 - val_accuracy: 0.5294\n",
      "Epoch 542/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 4.2908e-04 - accuracy: 1.0000 - val_loss: 4.4004 - val_accuracy: 0.5294\n",
      "Epoch 543/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 4.0866e-04 - accuracy: 1.0000 - val_loss: 4.4057 - val_accuracy: 0.5294\n",
      "Epoch 544/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 4.4751e-04 - accuracy: 1.0000 - val_loss: 4.4099 - val_accuracy: 0.5294\n",
      "Epoch 545/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 4.1373e-04 - accuracy: 1.0000 - val_loss: 4.4163 - val_accuracy: 0.5249\n",
      "Epoch 546/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 4.6780e-04 - accuracy: 1.0000 - val_loss: 4.4225 - val_accuracy: 0.5294\n",
      "Epoch 547/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 3.6128e-04 - accuracy: 1.0000 - val_loss: 4.4287 - val_accuracy: 0.5294\n",
      "Epoch 548/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 4.9714e-04 - accuracy: 1.0000 - val_loss: 4.4368 - val_accuracy: 0.5249\n",
      "Epoch 549/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 4.4498e-04 - accuracy: 1.0000 - val_loss: 4.4416 - val_accuracy: 0.5294\n",
      "Epoch 550/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 3.7650e-04 - accuracy: 1.0000 - val_loss: 4.4429 - val_accuracy: 0.5249\n",
      "Epoch 551/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.7915e-04 - accuracy: 1.0000 - val_loss: 4.4453 - val_accuracy: 0.5294\n",
      "Epoch 552/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 4.1719e-04 - accuracy: 1.0000 - val_loss: 4.4512 - val_accuracy: 0.5249\n",
      "Epoch 553/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 3.5240e-04 - accuracy: 1.0000 - val_loss: 4.4581 - val_accuracy: 0.5294\n",
      "Epoch 554/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 4.4657e-04 - accuracy: 1.0000 - val_loss: 4.4618 - val_accuracy: 0.5249\n",
      "Epoch 555/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 4.0647e-04 - accuracy: 1.0000 - val_loss: 4.4696 - val_accuracy: 0.5249\n",
      "Epoch 556/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 4.2570e-04 - accuracy: 1.0000 - val_loss: 4.4768 - val_accuracy: 0.5204\n",
      "Epoch 557/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.4575e-04 - accuracy: 1.0000 - val_loss: 4.4793 - val_accuracy: 0.5249\n",
      "Epoch 558/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 3.2348e-04 - accuracy: 1.0000 - val_loss: 4.4855 - val_accuracy: 0.5249\n",
      "Epoch 559/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 4.3150e-04 - accuracy: 1.0000 - val_loss: 4.4882 - val_accuracy: 0.5249\n",
      "Epoch 560/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 3.6815e-04 - accuracy: 1.0000 - val_loss: 4.4931 - val_accuracy: 0.5249\n",
      "Epoch 561/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 4.0862e-04 - accuracy: 1.0000 - val_loss: 4.4973 - val_accuracy: 0.5294\n",
      "Epoch 562/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 3.5234e-04 - accuracy: 1.0000 - val_loss: 4.5043 - val_accuracy: 0.5294\n",
      "Epoch 563/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 3.0946e-04 - accuracy: 1.0000 - val_loss: 4.5106 - val_accuracy: 0.5249\n",
      "Epoch 564/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 3.4500e-04 - accuracy: 1.0000 - val_loss: 4.5125 - val_accuracy: 0.5294\n",
      "Epoch 565/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.7617e-04 - accuracy: 1.0000 - val_loss: 4.5169 - val_accuracy: 0.5294\n",
      "Epoch 566/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.1327e-04 - accuracy: 1.0000 - val_loss: 4.5214 - val_accuracy: 0.5294\n",
      "Epoch 567/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 2.9279e-04 - accuracy: 1.0000 - val_loss: 4.5255 - val_accuracy: 0.5294\n",
      "Epoch 568/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 3.4059e-04 - accuracy: 1.0000 - val_loss: 4.5324 - val_accuracy: 0.5249\n",
      "Epoch 569/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.4555e-04 - accuracy: 1.0000 - val_loss: 4.5392 - val_accuracy: 0.5249\n",
      "Epoch 570/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 3.3834e-04 - accuracy: 1.0000 - val_loss: 4.5470 - val_accuracy: 0.5249\n",
      "Epoch 571/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 3.2121e-04 - accuracy: 1.0000 - val_loss: 4.5521 - val_accuracy: 0.5249\n",
      "Epoch 572/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 2.6805e-04 - accuracy: 1.0000 - val_loss: 4.5525 - val_accuracy: 0.5249\n",
      "Epoch 573/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 3.7275e-04 - accuracy: 1.0000 - val_loss: 4.5551 - val_accuracy: 0.5249\n",
      "Epoch 574/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.9869e-04 - accuracy: 1.0000 - val_loss: 4.5606 - val_accuracy: 0.5249\n",
      "Epoch 575/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.1135e-04 - accuracy: 1.0000 - val_loss: 4.5676 - val_accuracy: 0.5249\n",
      "Epoch 576/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.8021e-04 - accuracy: 1.0000 - val_loss: 4.5707 - val_accuracy: 0.5294\n",
      "Epoch 577/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 2.7722e-04 - accuracy: 1.0000 - val_loss: 4.5743 - val_accuracy: 0.5294\n",
      "Epoch 578/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 2.8763e-04 - accuracy: 1.0000 - val_loss: 4.5785 - val_accuracy: 0.5249\n",
      "Epoch 579/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 2.8847e-04 - accuracy: 1.0000 - val_loss: 4.5826 - val_accuracy: 0.5249\n",
      "Epoch 580/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 2.5314e-04 - accuracy: 1.0000 - val_loss: 4.5865 - val_accuracy: 0.5294\n",
      "Epoch 581/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 2.6544e-04 - accuracy: 1.0000 - val_loss: 4.5878 - val_accuracy: 0.5249\n",
      "Epoch 582/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 2.2670e-04 - accuracy: 1.0000 - val_loss: 4.5910 - val_accuracy: 0.5204\n",
      "Epoch 583/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 2.9470e-04 - accuracy: 1.0000 - val_loss: 4.5967 - val_accuracy: 0.5294\n",
      "Epoch 584/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.6079e-04 - accuracy: 1.0000 - val_loss: 4.6018 - val_accuracy: 0.5294\n",
      "Epoch 585/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.7198e-04 - accuracy: 1.0000 - val_loss: 4.6044 - val_accuracy: 0.5294\n",
      "Epoch 586/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 2.1981e-04 - accuracy: 1.0000 - val_loss: 4.6086 - val_accuracy: 0.5249\n",
      "Epoch 587/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 3.2904e-04 - accuracy: 1.0000 - val_loss: 4.6092 - val_accuracy: 0.5249\n",
      "Epoch 588/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 2.4370e-04 - accuracy: 1.0000 - val_loss: 4.6173 - val_accuracy: 0.5249\n",
      "Epoch 589/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.3240e-04 - accuracy: 1.0000 - val_loss: 4.6255 - val_accuracy: 0.5249\n",
      "Epoch 590/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 2.2359e-04 - accuracy: 1.0000 - val_loss: 4.6269 - val_accuracy: 0.5294\n",
      "Epoch 591/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.3260e-04 - accuracy: 1.0000 - val_loss: 4.6300 - val_accuracy: 0.5249\n",
      "Epoch 592/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.0962e-04 - accuracy: 1.0000 - val_loss: 4.6370 - val_accuracy: 0.5294\n",
      "Epoch 593/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.7363e-04 - accuracy: 1.0000 - val_loss: 4.6449 - val_accuracy: 0.5294\n",
      "Epoch 594/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 2.4521e-04 - accuracy: 1.0000 - val_loss: 4.6518 - val_accuracy: 0.5294\n",
      "Epoch 595/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.4533e-04 - accuracy: 1.0000 - val_loss: 4.6557 - val_accuracy: 0.5294\n",
      "Epoch 596/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.0824e-04 - accuracy: 1.0000 - val_loss: 4.6602 - val_accuracy: 0.5294\n",
      "Epoch 597/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 1.8699e-04 - accuracy: 1.0000 - val_loss: 4.6671 - val_accuracy: 0.5249\n",
      "Epoch 598/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.8665e-04 - accuracy: 1.0000 - val_loss: 4.6678 - val_accuracy: 0.5249\n",
      "Epoch 599/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.0056e-04 - accuracy: 1.0000 - val_loss: 4.6710 - val_accuracy: 0.5249\n",
      "Epoch 600/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 2.3457e-04 - accuracy: 1.0000 - val_loss: 4.6782 - val_accuracy: 0.5249\n",
      "Epoch 601/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.2169e-04 - accuracy: 1.0000 - val_loss: 4.6821 - val_accuracy: 0.5249\n",
      "Epoch 602/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 1.9907e-04 - accuracy: 1.0000 - val_loss: 4.6844 - val_accuracy: 0.5249\n",
      "Epoch 603/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 1.8273e-04 - accuracy: 1.0000 - val_loss: 4.6892 - val_accuracy: 0.5249\n",
      "Epoch 604/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 2.0810e-04 - accuracy: 1.0000 - val_loss: 4.6924 - val_accuracy: 0.5249\n",
      "Epoch 605/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 1.8801e-04 - accuracy: 1.0000 - val_loss: 4.6984 - val_accuracy: 0.5294\n",
      "Epoch 606/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 2.0889e-04 - accuracy: 1.0000 - val_loss: 4.7006 - val_accuracy: 0.5294\n",
      "Epoch 607/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 1.9712e-04 - accuracy: 1.0000 - val_loss: 4.6995 - val_accuracy: 0.5249\n",
      "Epoch 608/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 1.9796e-04 - accuracy: 1.0000 - val_loss: 4.7074 - val_accuracy: 0.5249\n",
      "Epoch 609/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 1.9393e-04 - accuracy: 1.0000 - val_loss: 4.7142 - val_accuracy: 0.5249\n",
      "Epoch 610/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 1.7060e-04 - accuracy: 1.0000 - val_loss: 4.7194 - val_accuracy: 0.5249\n",
      "Epoch 611/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 1.8711e-04 - accuracy: 1.0000 - val_loss: 4.7216 - val_accuracy: 0.5249\n",
      "Epoch 612/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 1.5741e-04 - accuracy: 1.0000 - val_loss: 4.7312 - val_accuracy: 0.5249\n",
      "Epoch 613/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 1.9109e-04 - accuracy: 1.0000 - val_loss: 4.7348 - val_accuracy: 0.5249\n",
      "Epoch 614/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 2.5086e-04 - accuracy: 1.0000 - val_loss: 4.7372 - val_accuracy: 0.5249\n",
      "Epoch 615/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 2.0625e-04 - accuracy: 1.0000 - val_loss: 4.7410 - val_accuracy: 0.5249\n",
      "Epoch 616/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.8887e-04 - accuracy: 1.0000 - val_loss: 4.7427 - val_accuracy: 0.5249\n",
      "Epoch 617/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 1.7449e-04 - accuracy: 1.0000 - val_loss: 4.7456 - val_accuracy: 0.5249\n",
      "Epoch 618/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.7342e-04 - accuracy: 1.0000 - val_loss: 4.7513 - val_accuracy: 0.5249\n",
      "Epoch 619/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.6101e-04 - accuracy: 1.0000 - val_loss: 4.7615 - val_accuracy: 0.5249\n",
      "Epoch 620/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 1.8198e-04 - accuracy: 1.0000 - val_loss: 4.7693 - val_accuracy: 0.5249\n",
      "Epoch 621/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 2.1261e-04 - accuracy: 1.0000 - val_loss: 4.7730 - val_accuracy: 0.5294\n",
      "Epoch 622/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 1.8666e-04 - accuracy: 1.0000 - val_loss: 4.7741 - val_accuracy: 0.5294\n",
      "Epoch 623/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 2.3180e-04 - accuracy: 1.0000 - val_loss: 4.7740 - val_accuracy: 0.5294\n",
      "Epoch 624/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 1.9071e-04 - accuracy: 1.0000 - val_loss: 4.7854 - val_accuracy: 0.5294\n",
      "Epoch 625/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.5290e-04 - accuracy: 1.0000 - val_loss: 4.7926 - val_accuracy: 0.5294\n",
      "Epoch 626/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.2741e-04 - accuracy: 1.0000 - val_loss: 4.7970 - val_accuracy: 0.5294\n",
      "Epoch 627/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 1.4088e-04 - accuracy: 1.0000 - val_loss: 4.8043 - val_accuracy: 0.5294\n",
      "Epoch 628/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 1.6968e-04 - accuracy: 1.0000 - val_loss: 4.8052 - val_accuracy: 0.5339\n",
      "Epoch 629/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 1.5308e-04 - accuracy: 1.0000 - val_loss: 4.8141 - val_accuracy: 0.5339\n",
      "Epoch 630/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.5780e-04 - accuracy: 1.0000 - val_loss: 4.8182 - val_accuracy: 0.5294\n",
      "Epoch 631/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.5174e-04 - accuracy: 1.0000 - val_loss: 4.8205 - val_accuracy: 0.5294\n",
      "Epoch 632/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.3202e-04 - accuracy: 1.0000 - val_loss: 4.8225 - val_accuracy: 0.5294\n",
      "Epoch 633/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.5907e-04 - accuracy: 1.0000 - val_loss: 4.8208 - val_accuracy: 0.5294\n",
      "Epoch 634/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.4313e-04 - accuracy: 1.0000 - val_loss: 4.8274 - val_accuracy: 0.5294\n",
      "Epoch 635/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 1.2470e-04 - accuracy: 1.0000 - val_loss: 4.8322 - val_accuracy: 0.5294\n",
      "Epoch 636/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 1.3947e-04 - accuracy: 1.0000 - val_loss: 4.8392 - val_accuracy: 0.5249\n",
      "Epoch 637/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.6716e-04 - accuracy: 1.0000 - val_loss: 4.8517 - val_accuracy: 0.5294\n",
      "Epoch 638/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.2438e-04 - accuracy: 1.0000 - val_loss: 4.8563 - val_accuracy: 0.5294\n",
      "Epoch 639/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.7434e-04 - accuracy: 1.0000 - val_loss: 4.8578 - val_accuracy: 0.5294\n",
      "Epoch 640/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.2489e-04 - accuracy: 1.0000 - val_loss: 4.8551 - val_accuracy: 0.5294\n",
      "Epoch 641/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.1600e-04 - accuracy: 1.0000 - val_loss: 4.8649 - val_accuracy: 0.5294\n",
      "Epoch 642/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 1.2189e-04 - accuracy: 1.0000 - val_loss: 4.8661 - val_accuracy: 0.5294\n",
      "Epoch 643/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.1740e-04 - accuracy: 1.0000 - val_loss: 4.8694 - val_accuracy: 0.5294\n",
      "Epoch 644/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 382us/step - loss: 1.1513e-04 - accuracy: 1.0000 - val_loss: 4.8765 - val_accuracy: 0.5294\n",
      "Epoch 645/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.1094e-04 - accuracy: 1.0000 - val_loss: 4.8807 - val_accuracy: 0.5249\n",
      "Epoch 646/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 1.1768e-04 - accuracy: 1.0000 - val_loss: 4.8826 - val_accuracy: 0.5249\n",
      "Epoch 647/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 1.0854e-04 - accuracy: 1.0000 - val_loss: 4.8861 - val_accuracy: 0.5249\n",
      "Epoch 648/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 1.3173e-04 - accuracy: 1.0000 - val_loss: 4.8904 - val_accuracy: 0.5294\n",
      "Epoch 649/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 1.3463e-04 - accuracy: 1.0000 - val_loss: 4.8870 - val_accuracy: 0.5294\n",
      "Epoch 650/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 1.2148e-04 - accuracy: 1.0000 - val_loss: 4.8889 - val_accuracy: 0.5294\n",
      "Epoch 651/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.1243e-04 - accuracy: 1.0000 - val_loss: 4.8960 - val_accuracy: 0.5294\n",
      "Epoch 652/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.4580e-04 - accuracy: 1.0000 - val_loss: 4.9035 - val_accuracy: 0.5294\n",
      "Epoch 653/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 1.1799e-04 - accuracy: 1.0000 - val_loss: 4.9046 - val_accuracy: 0.5294\n",
      "Epoch 654/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 9.5992e-05 - accuracy: 1.0000 - val_loss: 4.9103 - val_accuracy: 0.5294\n",
      "Epoch 655/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 1.1512e-04 - accuracy: 1.0000 - val_loss: 4.9172 - val_accuracy: 0.5294\n",
      "Epoch 656/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 1.1705e-04 - accuracy: 1.0000 - val_loss: 4.9142 - val_accuracy: 0.5294\n",
      "Epoch 657/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 8.5023e-05 - accuracy: 1.0000 - val_loss: 4.9147 - val_accuracy: 0.5294\n",
      "Epoch 658/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 1.1203e-04 - accuracy: 1.0000 - val_loss: 4.9213 - val_accuracy: 0.5294\n",
      "Epoch 659/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 1.0081e-04 - accuracy: 1.0000 - val_loss: 4.9257 - val_accuracy: 0.5294\n",
      "Epoch 660/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 9.1350e-05 - accuracy: 1.0000 - val_loss: 4.9309 - val_accuracy: 0.5339\n",
      "Epoch 661/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 1.0686e-04 - accuracy: 1.0000 - val_loss: 4.9315 - val_accuracy: 0.5294\n",
      "Epoch 662/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 1.1264e-04 - accuracy: 1.0000 - val_loss: 4.9265 - val_accuracy: 0.5294\n",
      "Epoch 663/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 9.6257e-05 - accuracy: 1.0000 - val_loss: 4.9299 - val_accuracy: 0.5294\n",
      "Epoch 664/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.1278e-04 - accuracy: 1.0000 - val_loss: 4.9291 - val_accuracy: 0.5294\n",
      "Epoch 665/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.3079e-05 - accuracy: 1.0000 - val_loss: 4.9403 - val_accuracy: 0.5294\n",
      "Epoch 666/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 9.8621e-05 - accuracy: 1.0000 - val_loss: 4.9495 - val_accuracy: 0.5249\n",
      "Epoch 667/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 9.7245e-05 - accuracy: 1.0000 - val_loss: 4.9538 - val_accuracy: 0.5294\n",
      "Epoch 668/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 7.9553e-05 - accuracy: 1.0000 - val_loss: 4.9572 - val_accuracy: 0.5294\n",
      "Epoch 669/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.2096e-04 - accuracy: 1.0000 - val_loss: 4.9620 - val_accuracy: 0.5294\n",
      "Epoch 670/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 8.5868e-05 - accuracy: 1.0000 - val_loss: 4.9666 - val_accuracy: 0.5249\n",
      "Epoch 671/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.8569e-05 - accuracy: 1.0000 - val_loss: 4.9753 - val_accuracy: 0.5249\n",
      "Epoch 672/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 7.5832e-05 - accuracy: 1.0000 - val_loss: 4.9760 - val_accuracy: 0.5249\n",
      "Epoch 673/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.6583e-05 - accuracy: 1.0000 - val_loss: 4.9813 - val_accuracy: 0.5339\n",
      "Epoch 674/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.1263e-04 - accuracy: 1.0000 - val_loss: 4.9840 - val_accuracy: 0.5294\n",
      "Epoch 675/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 1.2336e-04 - accuracy: 1.0000 - val_loss: 4.9850 - val_accuracy: 0.5339\n",
      "Epoch 676/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 9.8741e-05 - accuracy: 1.0000 - val_loss: 4.9907 - val_accuracy: 0.5339\n",
      "Epoch 677/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 8.2069e-05 - accuracy: 1.0000 - val_loss: 4.9952 - val_accuracy: 0.5339\n",
      "Epoch 678/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 7.1230e-05 - accuracy: 1.0000 - val_loss: 5.0009 - val_accuracy: 0.5339\n",
      "Epoch 679/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 1.0039e-04 - accuracy: 1.0000 - val_loss: 4.9997 - val_accuracy: 0.5339\n",
      "Epoch 680/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 7.7208e-05 - accuracy: 1.0000 - val_loss: 5.0009 - val_accuracy: 0.5339\n",
      "Epoch 681/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 9.3711e-05 - accuracy: 1.0000 - val_loss: 5.0027 - val_accuracy: 0.5339\n",
      "Epoch 682/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 7.0141e-05 - accuracy: 1.0000 - val_loss: 5.0019 - val_accuracy: 0.5339\n",
      "Epoch 683/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 8.5511e-05 - accuracy: 1.0000 - val_loss: 5.0058 - val_accuracy: 0.5294\n",
      "Epoch 684/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 7.4058e-05 - accuracy: 1.0000 - val_loss: 5.0231 - val_accuracy: 0.5339\n",
      "Epoch 685/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.1390e-05 - accuracy: 1.0000 - val_loss: 5.0295 - val_accuracy: 0.5339\n",
      "Epoch 686/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 7.8584e-05 - accuracy: 1.0000 - val_loss: 5.0399 - val_accuracy: 0.5294\n",
      "Epoch 687/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.9908e-05 - accuracy: 1.0000 - val_loss: 5.0526 - val_accuracy: 0.5249\n",
      "Epoch 688/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 9.5287e-05 - accuracy: 1.0000 - val_loss: 5.0561 - val_accuracy: 0.5249\n",
      "Epoch 689/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.3022e-04 - accuracy: 1.0000 - val_loss: 5.0511 - val_accuracy: 0.5339\n",
      "Epoch 690/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 9.0337e-05 - accuracy: 1.0000 - val_loss: 5.0516 - val_accuracy: 0.5339\n",
      "Epoch 691/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.3094e-05 - accuracy: 1.0000 - val_loss: 5.0534 - val_accuracy: 0.5294\n",
      "Epoch 692/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 7.4294e-05 - accuracy: 1.0000 - val_loss: 5.0671 - val_accuracy: 0.5294\n",
      "Epoch 693/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 8.2684e-05 - accuracy: 1.0000 - val_loss: 5.0642 - val_accuracy: 0.5294\n",
      "Epoch 694/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 6.4612e-05 - accuracy: 1.0000 - val_loss: 5.0604 - val_accuracy: 0.5339\n",
      "Epoch 695/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 5.8934e-05 - accuracy: 1.0000 - val_loss: 5.0669 - val_accuracy: 0.5339\n",
      "Epoch 696/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 7.8584e-05 - accuracy: 1.0000 - val_loss: 5.0661 - val_accuracy: 0.5339\n",
      "Epoch 697/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 6.6313e-05 - accuracy: 1.0000 - val_loss: 5.0725 - val_accuracy: 0.5339\n",
      "Epoch 698/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 5.8266e-05 - accuracy: 1.0000 - val_loss: 5.0768 - val_accuracy: 0.5294\n",
      "Epoch 699/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 7.0609e-05 - accuracy: 1.0000 - val_loss: 5.0861 - val_accuracy: 0.5294\n",
      "Epoch 700/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 7.9973e-05 - accuracy: 1.0000 - val_loss: 5.0889 - val_accuracy: 0.5294\n",
      "Epoch 701/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 7.7056e-05 - accuracy: 1.0000 - val_loss: 5.0856 - val_accuracy: 0.5294\n",
      "Epoch 702/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 4.8358e-05 - accuracy: 1.0000 - val_loss: 5.0942 - val_accuracy: 0.5294\n",
      "Epoch 703/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 8.0698e-05 - accuracy: 1.0000 - val_loss: 5.0979 - val_accuracy: 0.5249\n",
      "Epoch 704/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 5.8626e-05 - accuracy: 1.0000 - val_loss: 5.1010 - val_accuracy: 0.5294\n",
      "Epoch 705/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 5.9776e-05 - accuracy: 1.0000 - val_loss: 5.1121 - val_accuracy: 0.5294\n",
      "Epoch 706/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.9416e-05 - accuracy: 1.0000 - val_loss: 5.1255 - val_accuracy: 0.5339\n",
      "Epoch 707/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 5.2410e-05 - accuracy: 1.0000 - val_loss: 5.1272 - val_accuracy: 0.5339\n",
      "Epoch 708/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.1834e-05 - accuracy: 1.0000 - val_loss: 5.1292 - val_accuracy: 0.5339\n",
      "Epoch 709/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.5632e-05 - accuracy: 1.0000 - val_loss: 5.1329 - val_accuracy: 0.5294\n",
      "Epoch 710/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 6.3164e-05 - accuracy: 1.0000 - val_loss: 5.1366 - val_accuracy: 0.5339\n",
      "Epoch 711/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 6.1092e-05 - accuracy: 1.0000 - val_loss: 5.1335 - val_accuracy: 0.5249\n",
      "Epoch 712/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 6.5845e-05 - accuracy: 1.0000 - val_loss: 5.1370 - val_accuracy: 0.5249\n",
      "Epoch 713/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 6.4447e-05 - accuracy: 1.0000 - val_loss: 5.1416 - val_accuracy: 0.5249\n",
      "Epoch 714/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 6.2067e-05 - accuracy: 1.0000 - val_loss: 5.1491 - val_accuracy: 0.5249\n",
      "Epoch 715/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 5.4280e-05 - accuracy: 1.0000 - val_loss: 5.1516 - val_accuracy: 0.5249\n",
      "Epoch 716/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 4.8240e-05 - accuracy: 1.0000 - val_loss: 5.1567 - val_accuracy: 0.5249\n",
      "Epoch 717/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 4.7097e-05 - accuracy: 1.0000 - val_loss: 5.1556 - val_accuracy: 0.5249\n",
      "Epoch 718/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 5.3694e-05 - accuracy: 1.0000 - val_loss: 5.1681 - val_accuracy: 0.5249\n",
      "Epoch 719/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.9792e-05 - accuracy: 1.0000 - val_loss: 5.1725 - val_accuracy: 0.5249\n",
      "Epoch 720/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 6.9634e-05 - accuracy: 1.0000 - val_loss: 5.1790 - val_accuracy: 0.5294\n",
      "Epoch 721/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 5.0030e-05 - accuracy: 1.0000 - val_loss: 5.1783 - val_accuracy: 0.5294\n",
      "Epoch 722/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 6.1076e-05 - accuracy: 1.0000 - val_loss: 5.1781 - val_accuracy: 0.5249\n",
      "Epoch 723/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 5.2324e-05 - accuracy: 1.0000 - val_loss: 5.1795 - val_accuracy: 0.5249\n",
      "Epoch 724/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 4.1741e-05 - accuracy: 1.0000 - val_loss: 5.1869 - val_accuracy: 0.5294\n",
      "Epoch 725/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 4.4267e-05 - accuracy: 1.0000 - val_loss: 5.1957 - val_accuracy: 0.5249\n",
      "Epoch 726/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 4.4623e-05 - accuracy: 1.0000 - val_loss: 5.2001 - val_accuracy: 0.5294\n",
      "Epoch 727/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 4.0527e-05 - accuracy: 1.0000 - val_loss: 5.2139 - val_accuracy: 0.5249\n",
      "Epoch 728/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 4.6746e-05 - accuracy: 1.0000 - val_loss: 5.2202 - val_accuracy: 0.5294\n",
      "Epoch 729/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 4.2073e-05 - accuracy: 1.0000 - val_loss: 5.2182 - val_accuracy: 0.5339\n",
      "Epoch 730/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 4.1612e-05 - accuracy: 1.0000 - val_loss: 5.2223 - val_accuracy: 0.5339\n",
      "Epoch 731/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 6.1752e-05 - accuracy: 1.0000 - val_loss: 5.2184 - val_accuracy: 0.5385\n",
      "Epoch 732/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 4.4769e-05 - accuracy: 1.0000 - val_loss: 5.2381 - val_accuracy: 0.5385\n",
      "Epoch 733/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 4.3580e-05 - accuracy: 1.0000 - val_loss: 5.2502 - val_accuracy: 0.5294\n",
      "Epoch 734/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 5.2678e-05 - accuracy: 1.0000 - val_loss: 5.2376 - val_accuracy: 0.5385\n",
      "Epoch 735/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 5.0813e-05 - accuracy: 1.0000 - val_loss: 5.2383 - val_accuracy: 0.5339\n",
      "Epoch 736/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 6.2761e-05 - accuracy: 1.0000 - val_loss: 5.2436 - val_accuracy: 0.5385\n",
      "Epoch 737/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 3.9047e-05 - accuracy: 1.0000 - val_loss: 5.2472 - val_accuracy: 0.5385\n",
      "Epoch 738/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 1.1878e-04 - accuracy: 1.0000 - val_loss: 5.2666 - val_accuracy: 0.5204\n",
      "Epoch 739/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.5838 - accuracy: 0.8753 - val_loss: 2.8707 - val_accuracy: 0.5249\n",
      "Epoch 740/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.9385 - accuracy: 0.5586 - val_loss: 0.7067 - val_accuracy: 0.4615\n",
      "Epoch 741/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.7114 - accuracy: 0.5273 - val_loss: 0.6910 - val_accuracy: 0.5204\n",
      "Epoch 742/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.6924 - accuracy: 0.5328 - val_loss: 0.6908 - val_accuracy: 0.5566\n",
      "Epoch 743/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 0.6845 - accuracy: 0.5636 - val_loss: 0.6926 - val_accuracy: 0.5566\n",
      "Epoch 744/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.6812 - accuracy: 0.5687 - val_loss: 0.7002 - val_accuracy: 0.5204\n",
      "Epoch 745/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6722 - accuracy: 0.5742 - val_loss: 0.7103 - val_accuracy: 0.5973\n",
      "Epoch 746/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.6582 - accuracy: 0.5924 - val_loss: 0.7162 - val_accuracy: 0.5249\n",
      "Epoch 747/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6480 - accuracy: 0.6121 - val_loss: 0.7684 - val_accuracy: 0.5294\n",
      "Epoch 748/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.6603 - accuracy: 0.6015 - val_loss: 0.7072 - val_accuracy: 0.5430\n",
      "Epoch 749/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.6446 - accuracy: 0.6192 - val_loss: 0.7745 - val_accuracy: 0.5113\n",
      "Epoch 750/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.6245 - accuracy: 0.6490 - val_loss: 0.7751 - val_accuracy: 0.5475\n",
      "Epoch 751/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6056 - accuracy: 0.6636 - val_loss: 0.7951 - val_accuracy: 0.5249\n",
      "Epoch 752/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.5976 - accuracy: 0.6611 - val_loss: 0.8116 - val_accuracy: 0.4977\n",
      "Epoch 753/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.5863 - accuracy: 0.6773 - val_loss: 0.7872 - val_accuracy: 0.4887\n",
      "Epoch 754/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.5641 - accuracy: 0.7000 - val_loss: 0.8477 - val_accuracy: 0.5249\n",
      "Epoch 755/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.5517 - accuracy: 0.7040 - val_loss: 0.8729 - val_accuracy: 0.5113\n",
      "Epoch 756/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.5538 - accuracy: 0.7020 - val_loss: 0.8459 - val_accuracy: 0.5249\n",
      "Epoch 757/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 0.5303 - accuracy: 0.7308 - val_loss: 0.8958 - val_accuracy: 0.4932\n",
      "Epoch 758/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.5173 - accuracy: 0.7394 - val_loss: 1.0009 - val_accuracy: 0.4977\n",
      "Epoch 759/1000\n",
      "1980/1980 [==============================] - 1s 434us/step - loss: 0.5083 - accuracy: 0.7424 - val_loss: 0.9662 - val_accuracy: 0.4977\n",
      "Epoch 760/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 0.5090 - accuracy: 0.7359 - val_loss: 0.9485 - val_accuracy: 0.4932\n",
      "Epoch 761/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.4873 - accuracy: 0.7465 - val_loss: 0.9645 - val_accuracy: 0.5023\n",
      "Epoch 762/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.4799 - accuracy: 0.7636 - val_loss: 1.1291 - val_accuracy: 0.5158\n",
      "Epoch 763/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.4606 - accuracy: 0.7778 - val_loss: 1.0844 - val_accuracy: 0.5204\n",
      "Epoch 764/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.4411 - accuracy: 0.7798 - val_loss: 1.1157 - val_accuracy: 0.5113\n",
      "Epoch 765/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.4206 - accuracy: 0.8000 - val_loss: 1.2123 - val_accuracy: 0.4977\n",
      "Epoch 766/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.4176 - accuracy: 0.7919 - val_loss: 1.1523 - val_accuracy: 0.4932\n",
      "Epoch 767/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.4143 - accuracy: 0.7919 - val_loss: 1.1933 - val_accuracy: 0.5385\n",
      "Epoch 768/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 0.4071 - accuracy: 0.7965 - val_loss: 1.2158 - val_accuracy: 0.4887\n",
      "Epoch 769/1000\n",
      "1980/1980 [==============================] - 1s 432us/step - loss: 0.4185 - accuracy: 0.7939 - val_loss: 1.2044 - val_accuracy: 0.4977\n",
      "Epoch 770/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.4236 - accuracy: 0.7934 - val_loss: 1.1487 - val_accuracy: 0.4751\n",
      "Epoch 771/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.3724 - accuracy: 0.8258 - val_loss: 1.2650 - val_accuracy: 0.5068\n",
      "Epoch 772/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.3620 - accuracy: 0.8242 - val_loss: 1.2873 - val_accuracy: 0.4887\n",
      "Epoch 773/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.3433 - accuracy: 0.8399 - val_loss: 1.3871 - val_accuracy: 0.5113\n",
      "Epoch 774/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.3184 - accuracy: 0.8606 - val_loss: 1.4744 - val_accuracy: 0.5113\n",
      "Epoch 775/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.2914 - accuracy: 0.8662 - val_loss: 1.5276 - val_accuracy: 0.5113\n",
      "Epoch 776/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.3121 - accuracy: 0.8556 - val_loss: 1.6229 - val_accuracy: 0.5023\n",
      "Epoch 777/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.3088 - accuracy: 0.8561 - val_loss: 1.6021 - val_accuracy: 0.4887\n",
      "Epoch 778/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.2861 - accuracy: 0.8641 - val_loss: 1.5994 - val_accuracy: 0.4887\n",
      "Epoch 779/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.2672 - accuracy: 0.8823 - val_loss: 1.7068 - val_accuracy: 0.4842\n",
      "Epoch 780/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.2837 - accuracy: 0.8646 - val_loss: 1.7650 - val_accuracy: 0.4570\n",
      "Epoch 781/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.2726 - accuracy: 0.8808 - val_loss: 1.8715 - val_accuracy: 0.4977\n",
      "Epoch 782/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.2483 - accuracy: 0.8929 - val_loss: 1.7285 - val_accuracy: 0.5113\n",
      "Epoch 783/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.2018 - accuracy: 0.9182 - val_loss: 1.9913 - val_accuracy: 0.4887\n",
      "Epoch 784/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.1721 - accuracy: 0.9258 - val_loss: 2.0978 - val_accuracy: 0.4751\n",
      "Epoch 785/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.1611 - accuracy: 0.9288 - val_loss: 2.1272 - val_accuracy: 0.5158\n",
      "Epoch 786/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.1817 - accuracy: 0.9242 - val_loss: 2.1473 - val_accuracy: 0.4932\n",
      "Epoch 787/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.1571 - accuracy: 0.9384 - val_loss: 2.2784 - val_accuracy: 0.5023\n",
      "Epoch 788/1000\n",
      "1980/1980 [==============================] - 1s 447us/step - loss: 0.1872 - accuracy: 0.9258 - val_loss: 2.2344 - val_accuracy: 0.4887\n",
      "Epoch 789/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.2126 - accuracy: 0.9106 - val_loss: 2.0478 - val_accuracy: 0.4842\n",
      "Epoch 790/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.1739 - accuracy: 0.9253 - val_loss: 2.0860 - val_accuracy: 0.5113\n",
      "Epoch 791/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.1610 - accuracy: 0.9354 - val_loss: 2.0336 - val_accuracy: 0.5249\n",
      "Epoch 792/1000\n",
      "1980/1980 [==============================] - 1s 427us/step - loss: 0.1479 - accuracy: 0.9389 - val_loss: 2.1412 - val_accuracy: 0.4932\n",
      "Epoch 793/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.1118 - accuracy: 0.9606 - val_loss: 2.2683 - val_accuracy: 0.5113\n",
      "Epoch 794/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.1247 - accuracy: 0.9561 - val_loss: 2.2795 - val_accuracy: 0.4796\n",
      "Epoch 795/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.1384 - accuracy: 0.9424 - val_loss: 2.4768 - val_accuracy: 0.4842\n",
      "Epoch 796/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.1247 - accuracy: 0.9515 - val_loss: 2.3566 - val_accuracy: 0.5113\n",
      "Epoch 797/1000\n",
      "1980/1980 [==============================] - 1s 442us/step - loss: 0.1077 - accuracy: 0.9566 - val_loss: 2.5211 - val_accuracy: 0.5294\n",
      "Epoch 798/1000\n",
      "1980/1980 [==============================] - 1s 446us/step - loss: 0.1132 - accuracy: 0.9561 - val_loss: 2.3895 - val_accuracy: 0.5113\n",
      "Epoch 799/1000\n",
      "1980/1980 [==============================] - 1s 437us/step - loss: 0.1534 - accuracy: 0.9424 - val_loss: 2.4924 - val_accuracy: 0.5158\n",
      "Epoch 800/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 0.1688 - accuracy: 0.9333 - val_loss: 2.4820 - val_accuracy: 0.5068\n",
      "Epoch 801/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.1240 - accuracy: 0.9525 - val_loss: 2.4416 - val_accuracy: 0.5113\n",
      "Epoch 802/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.1285 - accuracy: 0.9530 - val_loss: 2.3516 - val_accuracy: 0.5430\n",
      "Epoch 803/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.1229 - accuracy: 0.9510 - val_loss: 2.3826 - val_accuracy: 0.5294\n",
      "Epoch 804/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.1062 - accuracy: 0.9586 - val_loss: 2.5245 - val_accuracy: 0.5475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 805/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0843 - accuracy: 0.9727 - val_loss: 2.5873 - val_accuracy: 0.5430\n",
      "Epoch 806/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0608 - accuracy: 0.9798 - val_loss: 2.6669 - val_accuracy: 0.5339\n",
      "Epoch 807/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0788 - accuracy: 0.9727 - val_loss: 2.6578 - val_accuracy: 0.5430\n",
      "Epoch 808/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0636 - accuracy: 0.9798 - val_loss: 2.6772 - val_accuracy: 0.4932\n",
      "Epoch 809/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0538 - accuracy: 0.9813 - val_loss: 2.6693 - val_accuracy: 0.5385\n",
      "Epoch 810/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0516 - accuracy: 0.9838 - val_loss: 2.9242 - val_accuracy: 0.4796\n",
      "Epoch 811/1000\n",
      "1980/1980 [==============================] - 1s 434us/step - loss: 0.0470 - accuracy: 0.9864 - val_loss: 2.8499 - val_accuracy: 0.5158\n",
      "Epoch 812/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.0341 - accuracy: 0.9919 - val_loss: 2.9541 - val_accuracy: 0.5204\n",
      "Epoch 813/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0339 - accuracy: 0.9889 - val_loss: 3.0187 - val_accuracy: 0.5204\n",
      "Epoch 814/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0315 - accuracy: 0.9904 - val_loss: 3.0593 - val_accuracy: 0.5294\n",
      "Epoch 815/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0273 - accuracy: 0.9914 - val_loss: 3.0402 - val_accuracy: 0.5294\n",
      "Epoch 816/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0214 - accuracy: 0.9934 - val_loss: 3.0616 - val_accuracy: 0.5113\n",
      "Epoch 817/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0219 - accuracy: 0.9924 - val_loss: 3.0955 - val_accuracy: 0.5294\n",
      "Epoch 818/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0189 - accuracy: 0.9955 - val_loss: 3.1491 - val_accuracy: 0.5204\n",
      "Epoch 819/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0296 - accuracy: 0.9889 - val_loss: 3.2800 - val_accuracy: 0.5113\n",
      "Epoch 820/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0221 - accuracy: 0.9944 - val_loss: 3.1100 - val_accuracy: 0.5249\n",
      "Epoch 821/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0213 - accuracy: 0.9919 - val_loss: 3.2137 - val_accuracy: 0.5249\n",
      "Epoch 822/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0177 - accuracy: 0.9934 - val_loss: 3.3040 - val_accuracy: 0.5204\n",
      "Epoch 823/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0148 - accuracy: 0.9955 - val_loss: 3.2616 - val_accuracy: 0.5339\n",
      "Epoch 824/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0150 - accuracy: 0.9944 - val_loss: 3.2975 - val_accuracy: 0.5294\n",
      "Epoch 825/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 3.3609 - val_accuracy: 0.5158\n",
      "Epoch 826/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0097 - accuracy: 0.9975 - val_loss: 3.3466 - val_accuracy: 0.5294\n",
      "Epoch 827/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 3.3862 - val_accuracy: 0.5339\n",
      "Epoch 828/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0063 - accuracy: 0.9995 - val_loss: 3.4201 - val_accuracy: 0.5249\n",
      "Epoch 829/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 3.4488 - val_accuracy: 0.5249\n",
      "Epoch 830/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 3.4680 - val_accuracy: 0.5249\n",
      "Epoch 831/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 3.5070 - val_accuracy: 0.5158\n",
      "Epoch 832/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 3.4884 - val_accuracy: 0.5249\n",
      "Epoch 833/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0120 - accuracy: 0.9955 - val_loss: 3.3887 - val_accuracy: 0.5249\n",
      "Epoch 834/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.0364 - accuracy: 0.9909 - val_loss: 3.3392 - val_accuracy: 0.5294\n",
      "Epoch 835/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.1670 - accuracy: 0.9455 - val_loss: 2.5809 - val_accuracy: 0.5385\n",
      "Epoch 836/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.3241 - accuracy: 0.8899 - val_loss: 2.4325 - val_accuracy: 0.5339\n",
      "Epoch 837/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.3197 - accuracy: 0.8687 - val_loss: 1.9190 - val_accuracy: 0.5158\n",
      "Epoch 838/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.2262 - accuracy: 0.9096 - val_loss: 2.1099 - val_accuracy: 0.4751\n",
      "Epoch 839/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.1506 - accuracy: 0.9394 - val_loss: 2.2013 - val_accuracy: 0.5204\n",
      "Epoch 840/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0748 - accuracy: 0.9753 - val_loss: 2.6121 - val_accuracy: 0.5113\n",
      "Epoch 841/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0543 - accuracy: 0.9823 - val_loss: 2.6046 - val_accuracy: 0.5158\n",
      "Epoch 842/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0418 - accuracy: 0.9919 - val_loss: 2.7542 - val_accuracy: 0.5113\n",
      "Epoch 843/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.0308 - accuracy: 0.9919 - val_loss: 2.8150 - val_accuracy: 0.5204\n",
      "Epoch 844/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0225 - accuracy: 0.9960 - val_loss: 2.8894 - val_accuracy: 0.5113\n",
      "Epoch 845/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0148 - accuracy: 0.9970 - val_loss: 2.8413 - val_accuracy: 0.5158\n",
      "Epoch 846/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0113 - accuracy: 0.9975 - val_loss: 2.9110 - val_accuracy: 0.5249\n",
      "Epoch 847/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0082 - accuracy: 0.9995 - val_loss: 2.9515 - val_accuracy: 0.5339\n",
      "Epoch 848/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 3.0216 - val_accuracy: 0.5339\n",
      "Epoch 849/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0062 - accuracy: 0.9995 - val_loss: 3.0391 - val_accuracy: 0.5339\n",
      "Epoch 850/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 3.0464 - val_accuracy: 0.5294\n",
      "Epoch 851/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0055 - accuracy: 0.9990 - val_loss: 3.0798 - val_accuracy: 0.5339\n",
      "Epoch 852/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 3.1403 - val_accuracy: 0.5294\n",
      "Epoch 853/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 3.1685 - val_accuracy: 0.5294\n",
      "Epoch 854/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 3.1873 - val_accuracy: 0.5249\n",
      "Epoch 855/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 3.2165 - val_accuracy: 0.5113\n",
      "Epoch 856/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.0048 - accuracy: 0.9995 - val_loss: 3.1938 - val_accuracy: 0.5339\n",
      "Epoch 857/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 3.2665 - val_accuracy: 0.5068\n",
      "Epoch 858/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 3.2338 - val_accuracy: 0.5294\n",
      "Epoch 859/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 3.2500 - val_accuracy: 0.5294\n",
      "Epoch 860/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 3.2628 - val_accuracy: 0.5339\n",
      "Epoch 861/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 3.2880 - val_accuracy: 0.5294\n",
      "Epoch 862/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 3.3313 - val_accuracy: 0.5204\n",
      "Epoch 863/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 3.3454 - val_accuracy: 0.5204\n",
      "Epoch 864/1000\n",
      "1980/1980 [==============================] - 1s 437us/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 3.3602 - val_accuracy: 0.5204\n",
      "Epoch 865/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 3.3915 - val_accuracy: 0.5204\n",
      "Epoch 866/1000\n",
      "1980/1980 [==============================] - 1s 506us/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 3.3964 - val_accuracy: 0.5249\n",
      "Epoch 867/1000\n",
      "1980/1980 [==============================] - 1s 441us/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 3.4052 - val_accuracy: 0.5249\n",
      "Epoch 868/1000\n",
      "1980/1980 [==============================] - 1s 482us/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 3.4120 - val_accuracy: 0.5249\n",
      "Epoch 869/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 3.4456 - val_accuracy: 0.5249\n",
      "Epoch 870/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 3.4389 - val_accuracy: 0.5249\n",
      "Epoch 871/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 3.4790 - val_accuracy: 0.4977\n",
      "Epoch 872/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 3.4873 - val_accuracy: 0.5113\n",
      "Epoch 873/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 3.4684 - val_accuracy: 0.5249\n",
      "Epoch 874/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 3.4626 - val_accuracy: 0.5113\n",
      "Epoch 875/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 3.4950 - val_accuracy: 0.5158\n",
      "Epoch 876/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 3.5079 - val_accuracy: 0.5204\n",
      "Epoch 877/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 3.4973 - val_accuracy: 0.5294\n",
      "Epoch 878/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 3.4814 - val_accuracy: 0.5158\n",
      "Epoch 879/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 3.4927 - val_accuracy: 0.5249\n",
      "Epoch 880/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 3.4503 - val_accuracy: 0.5385\n",
      "Epoch 881/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 3.4560 - val_accuracy: 0.5204\n",
      "Epoch 882/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 3.5073 - val_accuracy: 0.5294\n",
      "Epoch 883/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.0113 - accuracy: 0.9980 - val_loss: 3.5724 - val_accuracy: 0.5249\n",
      "Epoch 884/1000\n",
      "1980/1980 [==============================] - 1s 511us/step - loss: 0.0095 - accuracy: 0.9980 - val_loss: 3.4679 - val_accuracy: 0.5385\n",
      "Epoch 885/1000\n",
      "1980/1980 [==============================] - 1s 494us/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 3.4221 - val_accuracy: 0.5339\n",
      "Epoch 886/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 3.5351 - val_accuracy: 0.5158\n",
      "Epoch 887/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 3.6278 - val_accuracy: 0.5204\n",
      "Epoch 888/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 3.6348 - val_accuracy: 0.5294\n",
      "Epoch 889/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 3.6366 - val_accuracy: 0.5249\n",
      "Epoch 890/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.6401 - val_accuracy: 0.5113\n",
      "Epoch 891/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 3.6529 - val_accuracy: 0.5158\n",
      "Epoch 892/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.6688 - val_accuracy: 0.5204\n",
      "Epoch 893/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 3.6578 - val_accuracy: 0.5158\n",
      "Epoch 894/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.6826 - val_accuracy: 0.5113\n",
      "Epoch 895/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.6889 - val_accuracy: 0.5068\n",
      "Epoch 896/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.7195 - val_accuracy: 0.5294\n",
      "Epoch 897/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.6798 - val_accuracy: 0.5113\n",
      "Epoch 898/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 3.6962 - val_accuracy: 0.5204\n",
      "Epoch 899/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 3.6890 - val_accuracy: 0.5204\n",
      "Epoch 900/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.7463 - val_accuracy: 0.5113\n",
      "Epoch 901/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 3.7442 - val_accuracy: 0.5113\n",
      "Epoch 902/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.7747 - val_accuracy: 0.5068\n",
      "Epoch 903/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.7830 - val_accuracy: 0.5068\n",
      "Epoch 904/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.7631 - val_accuracy: 0.5113\n",
      "Epoch 905/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 3.8107 - val_accuracy: 0.5158\n",
      "Epoch 906/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 3.8108 - val_accuracy: 0.5113\n",
      "Epoch 907/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 8.4579e-04 - accuracy: 1.0000 - val_loss: 3.8063 - val_accuracy: 0.5113\n",
      "Epoch 908/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 9.4917e-04 - accuracy: 1.0000 - val_loss: 3.7980 - val_accuracy: 0.5158\n",
      "Epoch 909/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.8378 - val_accuracy: 0.5023\n",
      "Epoch 910/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 3.8879 - val_accuracy: 0.4932\n",
      "Epoch 911/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0348 - accuracy: 0.9879 - val_loss: 3.6188 - val_accuracy: 0.5204\n",
      "Epoch 912/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.2631 - accuracy: 0.9217 - val_loss: 2.8251 - val_accuracy: 0.5204\n",
      "Epoch 913/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5170 - accuracy: 0.8096 - val_loss: 1.6536 - val_accuracy: 0.5294\n",
      "Epoch 914/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.4131 - accuracy: 0.8056 - val_loss: 1.4678 - val_accuracy: 0.5158\n",
      "Epoch 915/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.2522 - accuracy: 0.8955 - val_loss: 1.7906 - val_accuracy: 0.5294\n",
      "Epoch 916/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.1432 - accuracy: 0.9470 - val_loss: 2.2057 - val_accuracy: 0.5023\n",
      "Epoch 917/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.0847 - accuracy: 0.9712 - val_loss: 2.4741 - val_accuracy: 0.4977\n",
      "Epoch 918/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.0705 - accuracy: 0.9793 - val_loss: 2.7232 - val_accuracy: 0.4751\n",
      "Epoch 919/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0628 - accuracy: 0.9818 - val_loss: 2.7354 - val_accuracy: 0.5068\n",
      "Epoch 920/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0494 - accuracy: 0.9828 - val_loss: 2.9072 - val_accuracy: 0.4842\n",
      "Epoch 921/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0509 - accuracy: 0.9848 - val_loss: 2.9838 - val_accuracy: 0.4796\n",
      "Epoch 922/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0407 - accuracy: 0.9859 - val_loss: 3.0142 - val_accuracy: 0.5068\n",
      "Epoch 923/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.0408 - accuracy: 0.9919 - val_loss: 2.9985 - val_accuracy: 0.5204\n",
      "Epoch 924/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0473 - accuracy: 0.9854 - val_loss: 3.2176 - val_accuracy: 0.5068\n",
      "Epoch 925/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0359 - accuracy: 0.9894 - val_loss: 3.1681 - val_accuracy: 0.5385\n",
      "Epoch 926/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0313 - accuracy: 0.9919 - val_loss: 2.9945 - val_accuracy: 0.5204\n",
      "Epoch 927/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0452 - accuracy: 0.9874 - val_loss: 3.0250 - val_accuracy: 0.5068\n",
      "Epoch 928/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0598 - accuracy: 0.9823 - val_loss: 3.0680 - val_accuracy: 0.5339\n",
      "Epoch 929/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0532 - accuracy: 0.9808 - val_loss: 2.9217 - val_accuracy: 0.5113\n",
      "Epoch 930/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0237 - accuracy: 0.9944 - val_loss: 3.1024 - val_accuracy: 0.5023\n",
      "Epoch 931/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0122 - accuracy: 0.9965 - val_loss: 3.1319 - val_accuracy: 0.5023\n",
      "Epoch 932/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 3.2356 - val_accuracy: 0.4977\n",
      "Epoch 933/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 3.3001 - val_accuracy: 0.4887\n",
      "Epoch 934/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 3.3491 - val_accuracy: 0.4977\n",
      "Epoch 935/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.3826 - val_accuracy: 0.5023\n",
      "Epoch 936/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 3.4033 - val_accuracy: 0.5023\n",
      "Epoch 937/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.4515 - val_accuracy: 0.4977\n",
      "Epoch 938/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 3.4824 - val_accuracy: 0.4932\n",
      "Epoch 939/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.4908 - val_accuracy: 0.4932\n",
      "Epoch 940/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.5221 - val_accuracy: 0.4977\n",
      "Epoch 941/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 3.5311 - val_accuracy: 0.4977\n",
      "Epoch 942/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.5386 - val_accuracy: 0.4932\n",
      "Epoch 943/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.5601 - val_accuracy: 0.4932\n",
      "Epoch 944/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.6059 - val_accuracy: 0.4932\n",
      "Epoch 945/1000\n",
      "1980/1980 [==============================] - 1s 486us/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 3.5769 - val_accuracy: 0.5068\n",
      "Epoch 946/1000\n",
      "1980/1980 [==============================] - 1s 515us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.6172 - val_accuracy: 0.4977\n",
      "Epoch 947/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.6460 - val_accuracy: 0.4977\n",
      "Epoch 948/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 3.6415 - val_accuracy: 0.4932\n",
      "Epoch 949/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 3.6722 - val_accuracy: 0.4932\n",
      "Epoch 950/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 3.5391 - val_accuracy: 0.4977\n",
      "Epoch 951/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 3.5561 - val_accuracy: 0.5023\n",
      "Epoch 952/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.5603 - val_accuracy: 0.4977\n",
      "Epoch 953/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.6243 - val_accuracy: 0.4932\n",
      "Epoch 954/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 3.6492 - val_accuracy: 0.4977\n",
      "Epoch 955/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.6762 - val_accuracy: 0.4932\n",
      "Epoch 956/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.6912 - val_accuracy: 0.4932\n",
      "Epoch 957/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 3.7032 - val_accuracy: 0.4932\n",
      "Epoch 958/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.7251 - val_accuracy: 0.4932\n",
      "Epoch 959/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.7443 - val_accuracy: 0.4932\n",
      "Epoch 960/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 8.6859e-04 - accuracy: 1.0000 - val_loss: 3.7584 - val_accuracy: 0.4932\n",
      "Epoch 961/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.7673 - val_accuracy: 0.4932\n",
      "Epoch 962/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.7953 - val_accuracy: 0.4977\n",
      "Epoch 963/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.7750 - val_accuracy: 0.4932\n",
      "Epoch 964/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 3.7891 - val_accuracy: 0.4887\n",
      "Epoch 965/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 3.8153 - val_accuracy: 0.5023\n",
      "Epoch 966/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 3.8139 - val_accuracy: 0.4977\n",
      "Epoch 967/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8074 - val_accuracy: 0.4977\n",
      "Epoch 968/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8325 - val_accuracy: 0.4977\n",
      "Epoch 969/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 6.9661e-04 - accuracy: 1.0000 - val_loss: 3.8506 - val_accuracy: 0.5023\n",
      "Epoch 970/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 7.3905e-04 - accuracy: 1.0000 - val_loss: 3.8590 - val_accuracy: 0.4977\n",
      "Epoch 971/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 7.1609e-04 - accuracy: 1.0000 - val_loss: 3.8687 - val_accuracy: 0.4977\n",
      "Epoch 972/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 7.0511e-04 - accuracy: 1.0000 - val_loss: 3.8748 - val_accuracy: 0.4977\n",
      "Epoch 973/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 6.2173e-04 - accuracy: 1.0000 - val_loss: 3.8933 - val_accuracy: 0.4977\n",
      "Epoch 974/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 6.8076e-04 - accuracy: 1.0000 - val_loss: 3.9034 - val_accuracy: 0.4977\n",
      "Epoch 975/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 7.1596e-04 - accuracy: 1.0000 - val_loss: 3.9192 - val_accuracy: 0.4977\n",
      "Epoch 976/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 9.7363e-04 - accuracy: 0.9995 - val_loss: 3.9133 - val_accuracy: 0.4977\n",
      "Epoch 977/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 3.9422 - val_accuracy: 0.4796\n",
      "Epoch 978/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.0776 - accuracy: 0.9808 - val_loss: 3.3180 - val_accuracy: 0.5158\n",
      "Epoch 979/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.1929 - accuracy: 0.9359 - val_loss: 3.1067 - val_accuracy: 0.5158\n",
      "Epoch 980/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.2566 - accuracy: 0.9086 - val_loss: 2.4783 - val_accuracy: 0.5249\n",
      "Epoch 981/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.1758 - accuracy: 0.9348 - val_loss: 2.4859 - val_accuracy: 0.5158\n",
      "Epoch 982/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1202 - accuracy: 0.9540 - val_loss: 2.6665 - val_accuracy: 0.4887\n",
      "Epoch 983/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.1188 - accuracy: 0.9561 - val_loss: 2.8790 - val_accuracy: 0.4887\n",
      "Epoch 984/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0652 - accuracy: 0.9773 - val_loss: 2.8873 - val_accuracy: 0.5023\n",
      "Epoch 985/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0539 - accuracy: 0.9828 - val_loss: 2.8268 - val_accuracy: 0.4932\n",
      "Epoch 986/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0396 - accuracy: 0.9889 - val_loss: 3.0282 - val_accuracy: 0.5294\n",
      "Epoch 987/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.0268 - accuracy: 0.9914 - val_loss: 3.1946 - val_accuracy: 0.4932\n",
      "Epoch 988/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0198 - accuracy: 0.9949 - val_loss: 3.0869 - val_accuracy: 0.4977\n",
      "Epoch 989/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0146 - accuracy: 0.9965 - val_loss: 3.2986 - val_accuracy: 0.5068\n",
      "Epoch 990/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 3.2902 - val_accuracy: 0.5113\n",
      "Epoch 991/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0054 - accuracy: 0.9995 - val_loss: 3.3589 - val_accuracy: 0.5023\n",
      "Epoch 992/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 3.4088 - val_accuracy: 0.5068\n",
      "Epoch 993/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.4398 - val_accuracy: 0.5023\n",
      "Epoch 994/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.4599 - val_accuracy: 0.5068\n",
      "Epoch 995/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.4806 - val_accuracy: 0.5113\n",
      "Epoch 996/1000\n",
      "1980/1980 [==============================] - 1s 358us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.5101 - val_accuracy: 0.5068\n",
      "Epoch 997/1000\n",
      "1980/1980 [==============================] - 1s 363us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.5426 - val_accuracy: 0.5023\n",
      "Epoch 998/1000\n",
      "1980/1980 [==============================] - 1s 359us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.5575 - val_accuracy: 0.5113\n",
      "Epoch 999/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.5886 - val_accuracy: 0.5023\n",
      "Epoch 1000/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.6081 - val_accuracy: 0.5068\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 127us/step\n",
      "test loss, test acc: [3.4057431941129725, 0.5020408034324646]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.6852301146750568\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=500, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(15, 64), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_70 (LSTM)               (None, 15, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_72 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 160,673\n",
      "Trainable params: 160,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1980 samples, validate on 221 samples\n",
      "Epoch 1/1000\n",
      "1980/1980 [==============================] - 2s 911us/step - loss: 0.6968 - accuracy: 0.5066 - val_loss: 0.6900 - val_accuracy: 0.5701\n",
      "Epoch 2/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6920 - accuracy: 0.5308 - val_loss: 0.6933 - val_accuracy: 0.4932\n",
      "Epoch 3/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6926 - accuracy: 0.5232 - val_loss: 0.6889 - val_accuracy: 0.5566\n",
      "Epoch 4/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6918 - accuracy: 0.5197 - val_loss: 0.6856 - val_accuracy: 0.5701\n",
      "Epoch 5/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6950 - accuracy: 0.4985 - val_loss: 0.6875 - val_accuracy: 0.5701\n",
      "Epoch 6/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6918 - accuracy: 0.5237 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 7/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6933 - accuracy: 0.5187 - val_loss: 0.6871 - val_accuracy: 0.5701\n",
      "Epoch 8/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6936 - accuracy: 0.5217 - val_loss: 0.6914 - val_accuracy: 0.5475\n",
      "Epoch 9/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6921 - accuracy: 0.5268 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 10/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.6934 - accuracy: 0.5258 - val_loss: 0.6894 - val_accuracy: 0.5701\n",
      "Epoch 11/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 12/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6917 - accuracy: 0.5232 - val_loss: 0.6878 - val_accuracy: 0.5701\n",
      "Epoch 13/1000\n",
      "1980/1980 [==============================] - 1s 361us/step - loss: 0.6922 - accuracy: 0.5187 - val_loss: 0.6889 - val_accuracy: 0.5701\n",
      "Epoch 14/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.6920 - accuracy: 0.5146 - val_loss: 0.6887 - val_accuracy: 0.5656\n",
      "Epoch 15/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6913 - accuracy: 0.5242 - val_loss: 0.6876 - val_accuracy: 0.5566\n",
      "Epoch 16/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6905 - accuracy: 0.5207 - val_loss: 0.6957 - val_accuracy: 0.4525\n",
      "Epoch 17/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 18/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6911 - accuracy: 0.5283 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 19/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6921 - accuracy: 0.5202 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 20/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6919 - accuracy: 0.5222 - val_loss: 0.6910 - val_accuracy: 0.5294\n",
      "Epoch 21/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6922 - accuracy: 0.5091 - val_loss: 0.6893 - val_accuracy: 0.5385\n",
      "Epoch 22/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6933 - accuracy: 0.5232 - val_loss: 0.6891 - val_accuracy: 0.5701\n",
      "Epoch 23/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6911 - accuracy: 0.5258 - val_loss: 0.6887 - val_accuracy: 0.5656\n",
      "Epoch 24/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6912 - accuracy: 0.5278 - val_loss: 0.6890 - val_accuracy: 0.5475\n",
      "Epoch 25/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6913 - accuracy: 0.5172 - val_loss: 0.7009 - val_accuracy: 0.4389\n",
      "Epoch 26/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6939 - accuracy: 0.5091 - val_loss: 0.6862 - val_accuracy: 0.5701\n",
      "Epoch 27/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.6919 - accuracy: 0.5232 - val_loss: 0.6884 - val_accuracy: 0.5701\n",
      "Epoch 28/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6926 - accuracy: 0.5197 - val_loss: 0.6902 - val_accuracy: 0.5701\n",
      "Epoch 29/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6912 - accuracy: 0.5253 - val_loss: 0.6882 - val_accuracy: 0.5701\n",
      "Epoch 30/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6916 - accuracy: 0.5232 - val_loss: 0.6887 - val_accuracy: 0.5656\n",
      "Epoch 31/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.6919 - accuracy: 0.5288 - val_loss: 0.6905 - val_accuracy: 0.5656\n",
      "Epoch 32/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6923 - accuracy: 0.5192 - val_loss: 0.6877 - val_accuracy: 0.5701\n",
      "Epoch 33/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6911 - accuracy: 0.5273 - val_loss: 0.6897 - val_accuracy: 0.5430\n",
      "Epoch 34/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6915 - accuracy: 0.5253 - val_loss: 0.6915 - val_accuracy: 0.5204\n",
      "Epoch 35/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6909 - accuracy: 0.5303 - val_loss: 0.6892 - val_accuracy: 0.5566\n",
      "Epoch 36/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6938 - accuracy: 0.5172 - val_loss: 0.6895 - val_accuracy: 0.5701\n",
      "Epoch 37/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6916 - accuracy: 0.5253 - val_loss: 0.6877 - val_accuracy: 0.5701\n",
      "Epoch 38/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6917 - accuracy: 0.5197 - val_loss: 0.6902 - val_accuracy: 0.5475\n",
      "Epoch 39/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6915 - accuracy: 0.5237 - val_loss: 0.6892 - val_accuracy: 0.5656\n",
      "Epoch 40/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6911 - accuracy: 0.5247 - val_loss: 0.6910 - val_accuracy: 0.5339\n",
      "Epoch 41/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6918 - accuracy: 0.5232 - val_loss: 0.6910 - val_accuracy: 0.5339\n",
      "Epoch 42/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6910 - accuracy: 0.5374 - val_loss: 0.6908 - val_accuracy: 0.5249\n",
      "Epoch 43/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6917 - accuracy: 0.5313 - val_loss: 0.6888 - val_accuracy: 0.5475\n",
      "Epoch 44/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6908 - accuracy: 0.5278 - val_loss: 0.6889 - val_accuracy: 0.5747\n",
      "Epoch 45/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6905 - accuracy: 0.5247 - val_loss: 0.6903 - val_accuracy: 0.5249\n",
      "Epoch 46/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6898 - accuracy: 0.5374 - val_loss: 0.6921 - val_accuracy: 0.4932\n",
      "Epoch 47/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6910 - accuracy: 0.5141 - val_loss: 0.6877 - val_accuracy: 0.5701\n",
      "Epoch 48/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6923 - accuracy: 0.5232 - val_loss: 0.6952 - val_accuracy: 0.4253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6925 - accuracy: 0.5187 - val_loss: 0.6868 - val_accuracy: 0.5701\n",
      "Epoch 50/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.6911 - accuracy: 0.5227 - val_loss: 0.6902 - val_accuracy: 0.5520\n",
      "Epoch 51/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6910 - accuracy: 0.5247 - val_loss: 0.6899 - val_accuracy: 0.5475\n",
      "Epoch 52/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6905 - accuracy: 0.5222 - val_loss: 0.6917 - val_accuracy: 0.5339\n",
      "Epoch 53/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6914 - accuracy: 0.5227 - val_loss: 0.6932 - val_accuracy: 0.5113\n",
      "Epoch 54/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6896 - accuracy: 0.5278 - val_loss: 0.6906 - val_accuracy: 0.5656\n",
      "Epoch 55/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6912 - accuracy: 0.5212 - val_loss: 0.6918 - val_accuracy: 0.5158\n",
      "Epoch 56/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.6922 - accuracy: 0.5217 - val_loss: 0.6887 - val_accuracy: 0.5701\n",
      "Epoch 57/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 0.6913 - accuracy: 0.5237 - val_loss: 0.6888 - val_accuracy: 0.5701\n",
      "Epoch 58/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.6906 - accuracy: 0.5338 - val_loss: 0.6897 - val_accuracy: 0.5430\n",
      "Epoch 59/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6901 - accuracy: 0.5328 - val_loss: 0.6916 - val_accuracy: 0.5249\n",
      "Epoch 60/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6902 - accuracy: 0.5333 - val_loss: 0.6936 - val_accuracy: 0.4977\n",
      "Epoch 61/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6915 - accuracy: 0.5343 - val_loss: 0.6873 - val_accuracy: 0.5701\n",
      "Epoch 62/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6916 - accuracy: 0.5212 - val_loss: 0.6893 - val_accuracy: 0.5701\n",
      "Epoch 63/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6904 - accuracy: 0.5288 - val_loss: 0.6909 - val_accuracy: 0.5475\n",
      "Epoch 64/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6909 - accuracy: 0.5197 - val_loss: 0.6935 - val_accuracy: 0.5068\n",
      "Epoch 65/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6892 - accuracy: 0.5424 - val_loss: 0.6919 - val_accuracy: 0.5339\n",
      "Epoch 66/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6910 - accuracy: 0.5258 - val_loss: 0.6934 - val_accuracy: 0.5339\n",
      "Epoch 67/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 0.6899 - accuracy: 0.5364 - val_loss: 0.6915 - val_accuracy: 0.5520\n",
      "Epoch 68/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.6903 - accuracy: 0.5364 - val_loss: 0.6897 - val_accuracy: 0.5611\n",
      "Epoch 69/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.6901 - accuracy: 0.5338 - val_loss: 0.6932 - val_accuracy: 0.5430\n",
      "Epoch 70/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6889 - accuracy: 0.5475 - val_loss: 0.6920 - val_accuracy: 0.5339\n",
      "Epoch 71/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6888 - accuracy: 0.5434 - val_loss: 0.6916 - val_accuracy: 0.5294\n",
      "Epoch 72/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 0.6878 - accuracy: 0.5313 - val_loss: 0.6994 - val_accuracy: 0.4480\n",
      "Epoch 73/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6885 - accuracy: 0.5384 - val_loss: 0.6917 - val_accuracy: 0.5204\n",
      "Epoch 74/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6881 - accuracy: 0.5434 - val_loss: 0.6914 - val_accuracy: 0.5294\n",
      "Epoch 75/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.6875 - accuracy: 0.5419 - val_loss: 0.6927 - val_accuracy: 0.5339\n",
      "Epoch 76/1000\n",
      "1980/1980 [==============================] - 1s 487us/step - loss: 0.6885 - accuracy: 0.5414 - val_loss: 0.6920 - val_accuracy: 0.5430\n",
      "Epoch 77/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.6876 - accuracy: 0.5490 - val_loss: 0.7049 - val_accuracy: 0.4570\n",
      "Epoch 78/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6897 - accuracy: 0.5273 - val_loss: 0.6916 - val_accuracy: 0.5294\n",
      "Epoch 79/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6897 - accuracy: 0.5263 - val_loss: 0.6932 - val_accuracy: 0.5113\n",
      "Epoch 80/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6868 - accuracy: 0.5419 - val_loss: 0.6906 - val_accuracy: 0.5656\n",
      "Epoch 81/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6877 - accuracy: 0.5359 - val_loss: 0.6953 - val_accuracy: 0.5294\n",
      "Epoch 82/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6853 - accuracy: 0.5606 - val_loss: 0.6942 - val_accuracy: 0.5566\n",
      "Epoch 83/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6864 - accuracy: 0.5490 - val_loss: 0.6938 - val_accuracy: 0.5475\n",
      "Epoch 84/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6855 - accuracy: 0.5556 - val_loss: 0.7017 - val_accuracy: 0.4796\n",
      "Epoch 85/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6839 - accuracy: 0.5535 - val_loss: 0.6888 - val_accuracy: 0.5701\n",
      "Epoch 86/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6836 - accuracy: 0.5540 - val_loss: 0.6951 - val_accuracy: 0.5475\n",
      "Epoch 87/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6844 - accuracy: 0.5535 - val_loss: 0.6944 - val_accuracy: 0.5520\n",
      "Epoch 88/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.6837 - accuracy: 0.5490 - val_loss: 0.6963 - val_accuracy: 0.5520\n",
      "Epoch 89/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6845 - accuracy: 0.5520 - val_loss: 0.6902 - val_accuracy: 0.5747\n",
      "Epoch 90/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6811 - accuracy: 0.5616 - val_loss: 0.6939 - val_accuracy: 0.5113\n",
      "Epoch 91/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.6808 - accuracy: 0.5606 - val_loss: 0.6905 - val_accuracy: 0.5339\n",
      "Epoch 92/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6869 - accuracy: 0.5333 - val_loss: 0.6864 - val_accuracy: 0.5430\n",
      "Epoch 93/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6828 - accuracy: 0.5540 - val_loss: 0.6906 - val_accuracy: 0.5385\n",
      "Epoch 94/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6814 - accuracy: 0.5556 - val_loss: 0.6952 - val_accuracy: 0.5294\n",
      "Epoch 95/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6835 - accuracy: 0.5535 - val_loss: 0.6956 - val_accuracy: 0.5611\n",
      "Epoch 96/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6861 - accuracy: 0.5414 - val_loss: 0.6877 - val_accuracy: 0.5204\n",
      "Epoch 97/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6846 - accuracy: 0.5505 - val_loss: 0.6905 - val_accuracy: 0.5204\n",
      "Epoch 98/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.6819 - accuracy: 0.5480 - val_loss: 0.6950 - val_accuracy: 0.5204\n",
      "Epoch 99/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6806 - accuracy: 0.5641 - val_loss: 0.7019 - val_accuracy: 0.5113\n",
      "Epoch 100/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6782 - accuracy: 0.5601 - val_loss: 0.6961 - val_accuracy: 0.5566\n",
      "Epoch 101/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6772 - accuracy: 0.5636 - val_loss: 0.7001 - val_accuracy: 0.5385\n",
      "Epoch 102/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.6751 - accuracy: 0.5697 - val_loss: 0.7030 - val_accuracy: 0.5611\n",
      "Epoch 103/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6766 - accuracy: 0.5641 - val_loss: 0.6973 - val_accuracy: 0.5566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.6829 - accuracy: 0.5697 - val_loss: 0.6901 - val_accuracy: 0.5430\n",
      "Epoch 105/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6803 - accuracy: 0.5697 - val_loss: 0.6989 - val_accuracy: 0.5068\n",
      "Epoch 106/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.6747 - accuracy: 0.5697 - val_loss: 0.7087 - val_accuracy: 0.5249\n",
      "Epoch 107/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.6784 - accuracy: 0.5707 - val_loss: 0.7000 - val_accuracy: 0.5701\n",
      "Epoch 108/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6745 - accuracy: 0.5707 - val_loss: 0.6912 - val_accuracy: 0.5339\n",
      "Epoch 109/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6730 - accuracy: 0.5707 - val_loss: 0.6972 - val_accuracy: 0.5611\n",
      "Epoch 110/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6713 - accuracy: 0.5747 - val_loss: 0.7008 - val_accuracy: 0.5339\n",
      "Epoch 111/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6714 - accuracy: 0.5753 - val_loss: 0.6975 - val_accuracy: 0.5113\n",
      "Epoch 112/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6696 - accuracy: 0.5753 - val_loss: 0.6997 - val_accuracy: 0.5113\n",
      "Epoch 113/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6652 - accuracy: 0.5843 - val_loss: 0.7100 - val_accuracy: 0.5792\n",
      "Epoch 114/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6623 - accuracy: 0.5874 - val_loss: 0.6944 - val_accuracy: 0.5747\n",
      "Epoch 115/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6636 - accuracy: 0.5955 - val_loss: 0.7023 - val_accuracy: 0.5204\n",
      "Epoch 116/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.6646 - accuracy: 0.5949 - val_loss: 0.7151 - val_accuracy: 0.5566\n",
      "Epoch 117/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6689 - accuracy: 0.5864 - val_loss: 0.6943 - val_accuracy: 0.5158\n",
      "Epoch 118/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.6658 - accuracy: 0.5879 - val_loss: 0.6997 - val_accuracy: 0.5204\n",
      "Epoch 119/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6647 - accuracy: 0.5747 - val_loss: 0.7044 - val_accuracy: 0.5792\n",
      "Epoch 120/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6597 - accuracy: 0.5899 - val_loss: 0.6925 - val_accuracy: 0.5520\n",
      "Epoch 121/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.6617 - accuracy: 0.5833 - val_loss: 0.6988 - val_accuracy: 0.5611\n",
      "Epoch 122/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6608 - accuracy: 0.5929 - val_loss: 0.7032 - val_accuracy: 0.5113\n",
      "Epoch 123/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.6606 - accuracy: 0.6010 - val_loss: 0.7042 - val_accuracy: 0.5430\n",
      "Epoch 124/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.6536 - accuracy: 0.6005 - val_loss: 0.7290 - val_accuracy: 0.5339\n",
      "Epoch 125/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.6600 - accuracy: 0.6015 - val_loss: 0.7031 - val_accuracy: 0.5339\n",
      "Epoch 126/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.6527 - accuracy: 0.6000 - val_loss: 0.7252 - val_accuracy: 0.5294\n",
      "Epoch 127/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.6546 - accuracy: 0.5919 - val_loss: 0.7007 - val_accuracy: 0.5339\n",
      "Epoch 128/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.6553 - accuracy: 0.6131 - val_loss: 0.6986 - val_accuracy: 0.5792\n",
      "Epoch 129/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.6514 - accuracy: 0.6035 - val_loss: 0.7444 - val_accuracy: 0.5023\n",
      "Epoch 130/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.6559 - accuracy: 0.6015 - val_loss: 0.7094 - val_accuracy: 0.5068\n",
      "Epoch 131/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.6494 - accuracy: 0.6101 - val_loss: 0.7114 - val_accuracy: 0.5204\n",
      "Epoch 132/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.6452 - accuracy: 0.6101 - val_loss: 0.7213 - val_accuracy: 0.4887\n",
      "Epoch 133/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.6531 - accuracy: 0.5884 - val_loss: 0.7101 - val_accuracy: 0.5475\n",
      "Epoch 134/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.6497 - accuracy: 0.6056 - val_loss: 0.7057 - val_accuracy: 0.5837\n",
      "Epoch 135/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6428 - accuracy: 0.6253 - val_loss: 0.7019 - val_accuracy: 0.5566\n",
      "Epoch 136/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.6402 - accuracy: 0.6141 - val_loss: 0.7225 - val_accuracy: 0.5475\n",
      "Epoch 137/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.6383 - accuracy: 0.6268 - val_loss: 0.7468 - val_accuracy: 0.5068\n",
      "Epoch 138/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6307 - accuracy: 0.6222 - val_loss: 0.7168 - val_accuracy: 0.5656\n",
      "Epoch 139/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6397 - accuracy: 0.6172 - val_loss: 0.7314 - val_accuracy: 0.5611\n",
      "Epoch 140/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.6365 - accuracy: 0.6222 - val_loss: 0.7364 - val_accuracy: 0.5204\n",
      "Epoch 141/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.6294 - accuracy: 0.6177 - val_loss: 0.7351 - val_accuracy: 0.5113\n",
      "Epoch 142/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.6375 - accuracy: 0.6136 - val_loss: 0.7300 - val_accuracy: 0.5023\n",
      "Epoch 143/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6371 - accuracy: 0.6172 - val_loss: 0.7266 - val_accuracy: 0.5430\n",
      "Epoch 144/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.6303 - accuracy: 0.6207 - val_loss: 0.7358 - val_accuracy: 0.5068\n",
      "Epoch 145/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6235 - accuracy: 0.6237 - val_loss: 0.7519 - val_accuracy: 0.5475\n",
      "Epoch 146/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6227 - accuracy: 0.6283 - val_loss: 0.7518 - val_accuracy: 0.5656\n",
      "Epoch 147/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.6316 - accuracy: 0.6283 - val_loss: 0.7428 - val_accuracy: 0.5068\n",
      "Epoch 148/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.6205 - accuracy: 0.6247 - val_loss: 0.7522 - val_accuracy: 0.5294\n",
      "Epoch 149/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6153 - accuracy: 0.6429 - val_loss: 0.7454 - val_accuracy: 0.4842\n",
      "Epoch 150/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.6319 - accuracy: 0.6192 - val_loss: 0.7132 - val_accuracy: 0.5023\n",
      "Epoch 151/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.6192 - accuracy: 0.6293 - val_loss: 0.7429 - val_accuracy: 0.5611\n",
      "Epoch 152/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.6131 - accuracy: 0.6566 - val_loss: 0.8148 - val_accuracy: 0.5385\n",
      "Epoch 153/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.6153 - accuracy: 0.6409 - val_loss: 0.7694 - val_accuracy: 0.5158\n",
      "Epoch 154/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.6084 - accuracy: 0.6480 - val_loss: 0.7851 - val_accuracy: 0.5520\n",
      "Epoch 155/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.6123 - accuracy: 0.6313 - val_loss: 0.7332 - val_accuracy: 0.5339\n",
      "Epoch 156/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.6095 - accuracy: 0.6444 - val_loss: 0.7992 - val_accuracy: 0.5023\n",
      "Epoch 157/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6083 - accuracy: 0.6475 - val_loss: 0.7727 - val_accuracy: 0.5475\n",
      "Epoch 158/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.6096 - accuracy: 0.6414 - val_loss: 0.8262 - val_accuracy: 0.5023\n",
      "Epoch 159/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.6040 - accuracy: 0.6475 - val_loss: 0.7697 - val_accuracy: 0.5294\n",
      "Epoch 160/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.5912 - accuracy: 0.6621 - val_loss: 0.8126 - val_accuracy: 0.5249\n",
      "Epoch 161/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.5980 - accuracy: 0.6556 - val_loss: 0.8180 - val_accuracy: 0.5385\n",
      "Epoch 162/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5977 - accuracy: 0.6520 - val_loss: 0.7706 - val_accuracy: 0.5249\n",
      "Epoch 163/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.5955 - accuracy: 0.6480 - val_loss: 0.8514 - val_accuracy: 0.5113\n",
      "Epoch 164/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.5941 - accuracy: 0.6591 - val_loss: 0.8725 - val_accuracy: 0.5023\n",
      "Epoch 165/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6106 - accuracy: 0.6667 - val_loss: 0.7658 - val_accuracy: 0.5385\n",
      "Epoch 166/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.5960 - accuracy: 0.6591 - val_loss: 0.7953 - val_accuracy: 0.5385\n",
      "Epoch 167/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.5952 - accuracy: 0.6535 - val_loss: 0.7724 - val_accuracy: 0.5249\n",
      "Epoch 168/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.5870 - accuracy: 0.6601 - val_loss: 0.7932 - val_accuracy: 0.5339\n",
      "Epoch 169/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.5875 - accuracy: 0.6500 - val_loss: 0.7700 - val_accuracy: 0.5475\n",
      "Epoch 170/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6072 - accuracy: 0.6434 - val_loss: 0.8134 - val_accuracy: 0.5204\n",
      "Epoch 171/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.5873 - accuracy: 0.6571 - val_loss: 0.7751 - val_accuracy: 0.5385\n",
      "Epoch 172/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.5850 - accuracy: 0.6697 - val_loss: 0.8215 - val_accuracy: 0.5475\n",
      "Epoch 173/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.5819 - accuracy: 0.6646 - val_loss: 0.7416 - val_accuracy: 0.5294\n",
      "Epoch 174/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.5922 - accuracy: 0.6596 - val_loss: 0.9158 - val_accuracy: 0.5339\n",
      "Epoch 175/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.5718 - accuracy: 0.6707 - val_loss: 0.8248 - val_accuracy: 0.5294\n",
      "Epoch 176/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.5688 - accuracy: 0.6737 - val_loss: 0.7794 - val_accuracy: 0.5339\n",
      "Epoch 177/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.5715 - accuracy: 0.6697 - val_loss: 0.8778 - val_accuracy: 0.5113\n",
      "Epoch 178/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.5668 - accuracy: 0.6783 - val_loss: 0.8035 - val_accuracy: 0.5339\n",
      "Epoch 179/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.5507 - accuracy: 0.6854 - val_loss: 0.9086 - val_accuracy: 0.5158\n",
      "Epoch 180/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.5485 - accuracy: 0.6944 - val_loss: 0.8688 - val_accuracy: 0.5068\n",
      "Epoch 181/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.5475 - accuracy: 0.6955 - val_loss: 0.9437 - val_accuracy: 0.5339\n",
      "Epoch 182/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.5377 - accuracy: 0.6970 - val_loss: 0.8518 - val_accuracy: 0.5204\n",
      "Epoch 183/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.5305 - accuracy: 0.7040 - val_loss: 0.8561 - val_accuracy: 0.5068\n",
      "Epoch 184/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.5493 - accuracy: 0.6949 - val_loss: 0.9243 - val_accuracy: 0.5204\n",
      "Epoch 185/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.5276 - accuracy: 0.7071 - val_loss: 1.0260 - val_accuracy: 0.5158\n",
      "Epoch 186/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.5509 - accuracy: 0.6949 - val_loss: 0.9562 - val_accuracy: 0.5023\n",
      "Epoch 187/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.5399 - accuracy: 0.6980 - val_loss: 0.9098 - val_accuracy: 0.5113\n",
      "Epoch 188/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.5340 - accuracy: 0.7045 - val_loss: 0.8061 - val_accuracy: 0.5430\n",
      "Epoch 189/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.5670 - accuracy: 0.6818 - val_loss: 0.9257 - val_accuracy: 0.5204\n",
      "Epoch 190/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.5372 - accuracy: 0.7035 - val_loss: 0.9879 - val_accuracy: 0.5204\n",
      "Epoch 191/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.5062 - accuracy: 0.7222 - val_loss: 0.9348 - val_accuracy: 0.4706\n",
      "Epoch 192/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.5295 - accuracy: 0.7106 - val_loss: 0.9393 - val_accuracy: 0.5023\n",
      "Epoch 193/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.5162 - accuracy: 0.7136 - val_loss: 0.9685 - val_accuracy: 0.4842\n",
      "Epoch 194/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.5035 - accuracy: 0.7303 - val_loss: 1.0864 - val_accuracy: 0.4932\n",
      "Epoch 195/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.5050 - accuracy: 0.7247 - val_loss: 1.1414 - val_accuracy: 0.5068\n",
      "Epoch 196/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.5037 - accuracy: 0.7237 - val_loss: 1.0139 - val_accuracy: 0.5475\n",
      "Epoch 197/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.4914 - accuracy: 0.7323 - val_loss: 1.0695 - val_accuracy: 0.5294\n",
      "Epoch 198/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.4928 - accuracy: 0.7338 - val_loss: 1.0568 - val_accuracy: 0.4796\n",
      "Epoch 199/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.4862 - accuracy: 0.7465 - val_loss: 1.0269 - val_accuracy: 0.5430\n",
      "Epoch 200/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.4758 - accuracy: 0.7359 - val_loss: 1.0553 - val_accuracy: 0.5023\n",
      "Epoch 201/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.4785 - accuracy: 0.7419 - val_loss: 1.1401 - val_accuracy: 0.4977\n",
      "Epoch 202/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.4680 - accuracy: 0.7525 - val_loss: 1.0798 - val_accuracy: 0.5158\n",
      "Epoch 203/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.4598 - accuracy: 0.7616 - val_loss: 1.1044 - val_accuracy: 0.5068\n",
      "Epoch 204/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.4680 - accuracy: 0.7500 - val_loss: 1.1465 - val_accuracy: 0.4706\n",
      "Epoch 205/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.4619 - accuracy: 0.7520 - val_loss: 1.1311 - val_accuracy: 0.4932\n",
      "Epoch 206/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.4412 - accuracy: 0.7662 - val_loss: 1.2479 - val_accuracy: 0.4887\n",
      "Epoch 207/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.4518 - accuracy: 0.7626 - val_loss: 1.2333 - val_accuracy: 0.4706\n",
      "Epoch 208/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.4357 - accuracy: 0.7818 - val_loss: 1.1794 - val_accuracy: 0.4661\n",
      "Epoch 209/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.4220 - accuracy: 0.7712 - val_loss: 1.1630 - val_accuracy: 0.4887\n",
      "Epoch 210/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.4139 - accuracy: 0.7843 - val_loss: 1.3479 - val_accuracy: 0.5023\n",
      "Epoch 211/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.4127 - accuracy: 0.7889 - val_loss: 1.3554 - val_accuracy: 0.5023\n",
      "Epoch 212/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.4334 - accuracy: 0.7692 - val_loss: 1.1807 - val_accuracy: 0.5068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.4349 - accuracy: 0.7753 - val_loss: 1.1791 - val_accuracy: 0.5068\n",
      "Epoch 214/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.4307 - accuracy: 0.7798 - val_loss: 1.1653 - val_accuracy: 0.4661\n",
      "Epoch 215/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.3978 - accuracy: 0.7949 - val_loss: 1.4560 - val_accuracy: 0.4796\n",
      "Epoch 216/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.3941 - accuracy: 0.8051 - val_loss: 1.1842 - val_accuracy: 0.4932\n",
      "Epoch 217/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.4289 - accuracy: 0.7833 - val_loss: 1.3587 - val_accuracy: 0.4842\n",
      "Epoch 218/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 0.4018 - accuracy: 0.7965 - val_loss: 1.4714 - val_accuracy: 0.5068\n",
      "Epoch 219/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.3932 - accuracy: 0.8056 - val_loss: 1.3058 - val_accuracy: 0.4706\n",
      "Epoch 220/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.3725 - accuracy: 0.8081 - val_loss: 1.3536 - val_accuracy: 0.5023\n",
      "Epoch 221/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.3879 - accuracy: 0.8051 - val_loss: 1.4079 - val_accuracy: 0.4932\n",
      "Epoch 222/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.3650 - accuracy: 0.8237 - val_loss: 1.4431 - val_accuracy: 0.5023\n",
      "Epoch 223/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.3684 - accuracy: 0.8172 - val_loss: 1.4240 - val_accuracy: 0.5385\n",
      "Epoch 224/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.3484 - accuracy: 0.8288 - val_loss: 1.4990 - val_accuracy: 0.5068\n",
      "Epoch 225/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3207 - accuracy: 0.8404 - val_loss: 1.6055 - val_accuracy: 0.4842\n",
      "Epoch 226/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3215 - accuracy: 0.8424 - val_loss: 1.5997 - val_accuracy: 0.4661\n",
      "Epoch 227/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.3012 - accuracy: 0.8591 - val_loss: 1.6757 - val_accuracy: 0.4932\n",
      "Epoch 228/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3293 - accuracy: 0.8429 - val_loss: 1.5415 - val_accuracy: 0.5113\n",
      "Epoch 229/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.3235 - accuracy: 0.8424 - val_loss: 1.5605 - val_accuracy: 0.5158\n",
      "Epoch 230/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.3039 - accuracy: 0.8551 - val_loss: 1.5661 - val_accuracy: 0.4661\n",
      "Epoch 231/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.3098 - accuracy: 0.8535 - val_loss: 1.6132 - val_accuracy: 0.5204\n",
      "Epoch 232/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.2802 - accuracy: 0.8742 - val_loss: 1.5962 - val_accuracy: 0.4977\n",
      "Epoch 233/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.2844 - accuracy: 0.8652 - val_loss: 1.6154 - val_accuracy: 0.4706\n",
      "Epoch 234/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.2628 - accuracy: 0.8788 - val_loss: 1.7689 - val_accuracy: 0.4842\n",
      "Epoch 235/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.2563 - accuracy: 0.8788 - val_loss: 1.7241 - val_accuracy: 0.5158\n",
      "Epoch 236/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.2877 - accuracy: 0.8646 - val_loss: 1.7852 - val_accuracy: 0.4842\n",
      "Epoch 237/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.2766 - accuracy: 0.8717 - val_loss: 1.9207 - val_accuracy: 0.4796\n",
      "Epoch 238/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.3201 - accuracy: 0.8485 - val_loss: 1.8651 - val_accuracy: 0.5249\n",
      "Epoch 239/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.2820 - accuracy: 0.8682 - val_loss: 1.8250 - val_accuracy: 0.4796\n",
      "Epoch 240/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 0.2558 - accuracy: 0.8818 - val_loss: 1.7709 - val_accuracy: 0.4570\n",
      "Epoch 241/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.2442 - accuracy: 0.8843 - val_loss: 1.8446 - val_accuracy: 0.5068\n",
      "Epoch 242/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.2497 - accuracy: 0.8889 - val_loss: 1.9171 - val_accuracy: 0.4570\n",
      "Epoch 243/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.2426 - accuracy: 0.8813 - val_loss: 1.8209 - val_accuracy: 0.4661\n",
      "Epoch 244/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.2414 - accuracy: 0.8874 - val_loss: 2.0298 - val_accuracy: 0.4932\n",
      "Epoch 245/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.2255 - accuracy: 0.9005 - val_loss: 2.0256 - val_accuracy: 0.4977\n",
      "Epoch 246/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.2060 - accuracy: 0.9101 - val_loss: 2.0060 - val_accuracy: 0.4706\n",
      "Epoch 247/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.2068 - accuracy: 0.9172 - val_loss: 2.0722 - val_accuracy: 0.4977\n",
      "Epoch 248/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.1832 - accuracy: 0.9232 - val_loss: 2.2849 - val_accuracy: 0.4842\n",
      "Epoch 249/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.1785 - accuracy: 0.9273 - val_loss: 2.2792 - val_accuracy: 0.5068\n",
      "Epoch 250/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1897 - accuracy: 0.9222 - val_loss: 2.1838 - val_accuracy: 0.4932\n",
      "Epoch 251/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.1736 - accuracy: 0.9328 - val_loss: 2.3409 - val_accuracy: 0.4480\n",
      "Epoch 252/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.1745 - accuracy: 0.9247 - val_loss: 2.3756 - val_accuracy: 0.4796\n",
      "Epoch 253/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.1626 - accuracy: 0.9323 - val_loss: 2.3362 - val_accuracy: 0.4842\n",
      "Epoch 254/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.1443 - accuracy: 0.9424 - val_loss: 2.4075 - val_accuracy: 0.4661\n",
      "Epoch 255/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.1455 - accuracy: 0.9515 - val_loss: 2.4165 - val_accuracy: 0.4570\n",
      "Epoch 256/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.1675 - accuracy: 0.9434 - val_loss: 2.4231 - val_accuracy: 0.4842\n",
      "Epoch 257/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.1517 - accuracy: 0.9465 - val_loss: 2.3081 - val_accuracy: 0.4977\n",
      "Epoch 258/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.1530 - accuracy: 0.9379 - val_loss: 2.4295 - val_accuracy: 0.4570\n",
      "Epoch 259/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1675 - accuracy: 0.9348 - val_loss: 2.2008 - val_accuracy: 0.4887\n",
      "Epoch 260/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.1765 - accuracy: 0.9308 - val_loss: 2.4498 - val_accuracy: 0.4661\n",
      "Epoch 261/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1268 - accuracy: 0.9515 - val_loss: 2.4247 - val_accuracy: 0.4887\n",
      "Epoch 262/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1098 - accuracy: 0.9586 - val_loss: 2.5812 - val_accuracy: 0.4751\n",
      "Epoch 263/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.1442 - accuracy: 0.9424 - val_loss: 2.5306 - val_accuracy: 0.4706\n",
      "Epoch 264/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.1371 - accuracy: 0.9455 - val_loss: 2.4742 - val_accuracy: 0.4887\n",
      "Epoch 265/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.1316 - accuracy: 0.9455 - val_loss: 2.6357 - val_accuracy: 0.4661\n",
      "Epoch 266/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.1078 - accuracy: 0.9530 - val_loss: 2.5635 - val_accuracy: 0.4751\n",
      "Epoch 267/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.1011 - accuracy: 0.9601 - val_loss: 2.7682 - val_accuracy: 0.4570\n",
      "Epoch 268/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0932 - accuracy: 0.9641 - val_loss: 2.7290 - val_accuracy: 0.4796\n",
      "Epoch 269/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0913 - accuracy: 0.9646 - val_loss: 2.6737 - val_accuracy: 0.4661\n",
      "Epoch 270/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0968 - accuracy: 0.9626 - val_loss: 2.7448 - val_accuracy: 0.4661\n",
      "Epoch 271/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0856 - accuracy: 0.9697 - val_loss: 2.7091 - val_accuracy: 0.4842\n",
      "Epoch 272/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0701 - accuracy: 0.9753 - val_loss: 2.6799 - val_accuracy: 0.4842\n",
      "Epoch 273/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0656 - accuracy: 0.9798 - val_loss: 2.8483 - val_accuracy: 0.4570\n",
      "Epoch 274/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0817 - accuracy: 0.9717 - val_loss: 2.8721 - val_accuracy: 0.4751\n",
      "Epoch 275/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0682 - accuracy: 0.9773 - val_loss: 2.7903 - val_accuracy: 0.5204\n",
      "Epoch 276/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0702 - accuracy: 0.9747 - val_loss: 2.8927 - val_accuracy: 0.4932\n",
      "Epoch 277/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0664 - accuracy: 0.9768 - val_loss: 2.9145 - val_accuracy: 0.4932\n",
      "Epoch 278/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.1018 - accuracy: 0.9616 - val_loss: 2.6536 - val_accuracy: 0.4706\n",
      "Epoch 279/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0957 - accuracy: 0.9692 - val_loss: 2.7512 - val_accuracy: 0.4751\n",
      "Epoch 280/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0951 - accuracy: 0.9606 - val_loss: 3.0834 - val_accuracy: 0.4525\n",
      "Epoch 281/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0982 - accuracy: 0.9652 - val_loss: 2.8369 - val_accuracy: 0.4706\n",
      "Epoch 282/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0717 - accuracy: 0.9773 - val_loss: 2.7504 - val_accuracy: 0.4751\n",
      "Epoch 283/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0569 - accuracy: 0.9803 - val_loss: 2.9530 - val_accuracy: 0.4751\n",
      "Epoch 284/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 0.0471 - accuracy: 0.9879 - val_loss: 3.0728 - val_accuracy: 0.4751\n",
      "Epoch 285/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0291 - accuracy: 0.9944 - val_loss: 3.1230 - val_accuracy: 0.4887\n",
      "Epoch 286/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0269 - accuracy: 0.9960 - val_loss: 3.2263 - val_accuracy: 0.4661\n",
      "Epoch 287/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0259 - accuracy: 0.9939 - val_loss: 3.2484 - val_accuracy: 0.4615\n",
      "Epoch 288/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0203 - accuracy: 0.9970 - val_loss: 3.2468 - val_accuracy: 0.4525\n",
      "Epoch 289/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0162 - accuracy: 0.9990 - val_loss: 3.3308 - val_accuracy: 0.4661\n",
      "Epoch 290/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 3.3433 - val_accuracy: 0.4706\n",
      "Epoch 291/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 3.3626 - val_accuracy: 0.4615\n",
      "Epoch 292/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0102 - accuracy: 0.9995 - val_loss: 3.3623 - val_accuracy: 0.4706\n",
      "Epoch 293/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 3.4123 - val_accuracy: 0.4570\n",
      "Epoch 294/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 3.4411 - val_accuracy: 0.4570\n",
      "Epoch 295/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 3.4239 - val_accuracy: 0.4570\n",
      "Epoch 296/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.4839 - val_accuracy: 0.4661\n",
      "Epoch 297/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.4709 - val_accuracy: 0.4661\n",
      "Epoch 298/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.4985 - val_accuracy: 0.4615\n",
      "Epoch 299/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.5216 - val_accuracy: 0.4796\n",
      "Epoch 300/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.5300 - val_accuracy: 0.4842\n",
      "Epoch 301/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.5706 - val_accuracy: 0.4615\n",
      "Epoch 302/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 3.5814 - val_accuracy: 0.4706\n",
      "Epoch 303/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.6196 - val_accuracy: 0.4615\n",
      "Epoch 304/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.6715 - val_accuracy: 0.4796\n",
      "Epoch 305/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.6693 - val_accuracy: 0.4706\n",
      "Epoch 306/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.7114 - val_accuracy: 0.4751\n",
      "Epoch 307/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.6978 - val_accuracy: 0.4796\n",
      "Epoch 308/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.7153 - val_accuracy: 0.4887\n",
      "Epoch 309/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 3.7284 - val_accuracy: 0.4796\n",
      "Epoch 310/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.7397 - val_accuracy: 0.4842\n",
      "Epoch 311/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.7557 - val_accuracy: 0.4706\n",
      "Epoch 312/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.7595 - val_accuracy: 0.4842\n",
      "Epoch 313/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.7680 - val_accuracy: 0.4706\n",
      "Epoch 314/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.7822 - val_accuracy: 0.4796\n",
      "Epoch 315/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.7927 - val_accuracy: 0.4796\n",
      "Epoch 316/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.8102 - val_accuracy: 0.4887\n",
      "Epoch 317/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.8249 - val_accuracy: 0.4842\n",
      "Epoch 318/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.8351 - val_accuracy: 0.4842\n",
      "Epoch 319/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.8525 - val_accuracy: 0.4796\n",
      "Epoch 320/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.8735 - val_accuracy: 0.4796\n",
      "Epoch 321/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.8647 - val_accuracy: 0.4887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.8708 - val_accuracy: 0.4796\n",
      "Epoch 323/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.9007 - val_accuracy: 0.4887\n",
      "Epoch 324/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9037 - val_accuracy: 0.4751\n",
      "Epoch 325/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.9148 - val_accuracy: 0.4796\n",
      "Epoch 326/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.9239 - val_accuracy: 0.4751\n",
      "Epoch 327/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9300 - val_accuracy: 0.4796\n",
      "Epoch 328/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.9555 - val_accuracy: 0.4932\n",
      "Epoch 329/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.9763 - val_accuracy: 0.4887\n",
      "Epoch 330/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9782 - val_accuracy: 0.4842\n",
      "Epoch 331/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9832 - val_accuracy: 0.4887\n",
      "Epoch 332/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.9883 - val_accuracy: 0.4887\n",
      "Epoch 333/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.9980 - val_accuracy: 0.4932\n",
      "Epoch 334/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.0158 - val_accuracy: 0.4842\n",
      "Epoch 335/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.0416 - val_accuracy: 0.4796\n",
      "Epoch 336/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.0374 - val_accuracy: 0.4796\n",
      "Epoch 337/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0412 - val_accuracy: 0.4887\n",
      "Epoch 338/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0558 - val_accuracy: 0.4887\n",
      "Epoch 339/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0592 - val_accuracy: 0.4842\n",
      "Epoch 340/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0775 - val_accuracy: 0.4932\n",
      "Epoch 341/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0770 - val_accuracy: 0.4796\n",
      "Epoch 342/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0863 - val_accuracy: 0.4887\n",
      "Epoch 343/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0934 - val_accuracy: 0.4887\n",
      "Epoch 344/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0962 - val_accuracy: 0.4842\n",
      "Epoch 345/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1058 - val_accuracy: 0.4842\n",
      "Epoch 346/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1062 - val_accuracy: 0.4796\n",
      "Epoch 347/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1196 - val_accuracy: 0.4706\n",
      "Epoch 348/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1104 - val_accuracy: 0.4842\n",
      "Epoch 349/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1377 - val_accuracy: 0.4706\n",
      "Epoch 350/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1439 - val_accuracy: 0.4751\n",
      "Epoch 351/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1412 - val_accuracy: 0.4751\n",
      "Epoch 352/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1557 - val_accuracy: 0.4751\n",
      "Epoch 353/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1626 - val_accuracy: 0.4796\n",
      "Epoch 354/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1880 - val_accuracy: 0.4796\n",
      "Epoch 355/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.1989 - val_accuracy: 0.4842\n",
      "Epoch 356/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.2086 - val_accuracy: 0.4842\n",
      "Epoch 357/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 9.3435e-04 - accuracy: 1.0000 - val_loss: 4.2074 - val_accuracy: 0.4842\n",
      "Epoch 358/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2144 - val_accuracy: 0.4842\n",
      "Epoch 359/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 9.4734e-04 - accuracy: 1.0000 - val_loss: 4.2276 - val_accuracy: 0.4796\n",
      "Epoch 360/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2318 - val_accuracy: 0.4796\n",
      "Epoch 361/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2370 - val_accuracy: 0.4796\n",
      "Epoch 362/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 9.3501e-04 - accuracy: 1.0000 - val_loss: 4.2563 - val_accuracy: 0.4796\n",
      "Epoch 363/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 9.7691e-04 - accuracy: 1.0000 - val_loss: 4.2730 - val_accuracy: 0.4796\n",
      "Epoch 364/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 9.5502e-04 - accuracy: 1.0000 - val_loss: 4.2798 - val_accuracy: 0.4842\n",
      "Epoch 365/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 9.8723e-04 - accuracy: 1.0000 - val_loss: 4.2814 - val_accuracy: 0.4796\n",
      "Epoch 366/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 9.7171e-04 - accuracy: 1.0000 - val_loss: 4.2810 - val_accuracy: 0.4796\n",
      "Epoch 367/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 7.9481e-04 - accuracy: 1.0000 - val_loss: 4.2977 - val_accuracy: 0.4842\n",
      "Epoch 368/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 9.3227e-04 - accuracy: 1.0000 - val_loss: 4.3072 - val_accuracy: 0.4842\n",
      "Epoch 369/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 8.4749e-04 - accuracy: 1.0000 - val_loss: 4.3195 - val_accuracy: 0.4887\n",
      "Epoch 370/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 8.5437e-04 - accuracy: 1.0000 - val_loss: 4.3144 - val_accuracy: 0.4887\n",
      "Epoch 371/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.2968 - val_accuracy: 0.4887\n",
      "Epoch 372/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.2920 - val_accuracy: 0.4751\n",
      "Epoch 373/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.1282 - accuracy: 0.9707 - val_loss: 3.6770 - val_accuracy: 0.4434\n",
      "Epoch 374/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.0400 - accuracy: 0.6051 - val_loss: 0.6874 - val_accuracy: 0.5701\n",
      "Epoch 375/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.7078 - accuracy: 0.5035 - val_loss: 0.6859 - val_accuracy: 0.5611\n",
      "Epoch 376/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.6976 - accuracy: 0.5263 - val_loss: 0.6856 - val_accuracy: 0.5792\n",
      "Epoch 377/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6927 - accuracy: 0.5384 - val_loss: 0.6898 - val_accuracy: 0.5611\n",
      "Epoch 378/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.6923 - accuracy: 0.5247 - val_loss: 0.6860 - val_accuracy: 0.5611\n",
      "Epoch 379/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.6936 - accuracy: 0.5283 - val_loss: 0.7080 - val_accuracy: 0.4615\n",
      "Epoch 380/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.6870 - accuracy: 0.5515 - val_loss: 0.6925 - val_accuracy: 0.5294\n",
      "Epoch 381/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6854 - accuracy: 0.5389 - val_loss: 0.6911 - val_accuracy: 0.5701\n",
      "Epoch 382/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.6830 - accuracy: 0.5510 - val_loss: 0.7036 - val_accuracy: 0.4751\n",
      "Epoch 383/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.6793 - accuracy: 0.5646 - val_loss: 0.7022 - val_accuracy: 0.5339\n",
      "Epoch 384/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.6749 - accuracy: 0.5712 - val_loss: 0.7046 - val_accuracy: 0.5249\n",
      "Epoch 385/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.6708 - accuracy: 0.5692 - val_loss: 0.7031 - val_accuracy: 0.5249\n",
      "Epoch 386/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.6619 - accuracy: 0.5914 - val_loss: 0.7359 - val_accuracy: 0.4751\n",
      "Epoch 387/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.6573 - accuracy: 0.6015 - val_loss: 0.7264 - val_accuracy: 0.5023\n",
      "Epoch 388/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.6513 - accuracy: 0.6192 - val_loss: 0.7390 - val_accuracy: 0.5204\n",
      "Epoch 389/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.6382 - accuracy: 0.6313 - val_loss: 0.7701 - val_accuracy: 0.4977\n",
      "Epoch 390/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.6343 - accuracy: 0.6258 - val_loss: 0.7539 - val_accuracy: 0.4977\n",
      "Epoch 391/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.6274 - accuracy: 0.6374 - val_loss: 0.8101 - val_accuracy: 0.5113\n",
      "Epoch 392/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.6213 - accuracy: 0.6409 - val_loss: 0.7914 - val_accuracy: 0.5068\n",
      "Epoch 393/1000\n",
      "1980/1980 [==============================] - 1s 441us/step - loss: 0.6016 - accuracy: 0.6530 - val_loss: 0.7806 - val_accuracy: 0.4932\n",
      "Epoch 394/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.5979 - accuracy: 0.6586 - val_loss: 0.8635 - val_accuracy: 0.5068\n",
      "Epoch 395/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.6033 - accuracy: 0.6510 - val_loss: 0.7534 - val_accuracy: 0.5566\n",
      "Epoch 396/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.5919 - accuracy: 0.6591 - val_loss: 0.8127 - val_accuracy: 0.5204\n",
      "Epoch 397/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.5755 - accuracy: 0.6758 - val_loss: 0.7930 - val_accuracy: 0.5068\n",
      "Epoch 398/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 0.5629 - accuracy: 0.6763 - val_loss: 0.8778 - val_accuracy: 0.5068\n",
      "Epoch 399/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.5642 - accuracy: 0.6788 - val_loss: 0.7733 - val_accuracy: 0.5068\n",
      "Epoch 400/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.5546 - accuracy: 0.6914 - val_loss: 0.8905 - val_accuracy: 0.4887\n",
      "Epoch 401/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.5516 - accuracy: 0.6904 - val_loss: 0.9124 - val_accuracy: 0.5294\n",
      "Epoch 402/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.5404 - accuracy: 0.7051 - val_loss: 0.8885 - val_accuracy: 0.5520\n",
      "Epoch 403/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.5127 - accuracy: 0.7187 - val_loss: 0.9915 - val_accuracy: 0.5113\n",
      "Epoch 404/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.4927 - accuracy: 0.7303 - val_loss: 1.0675 - val_accuracy: 0.4796\n",
      "Epoch 405/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.4890 - accuracy: 0.7278 - val_loss: 1.0866 - val_accuracy: 0.5249\n",
      "Epoch 406/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.4776 - accuracy: 0.7460 - val_loss: 1.0686 - val_accuracy: 0.4706\n",
      "Epoch 407/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.4711 - accuracy: 0.7374 - val_loss: 1.0387 - val_accuracy: 0.4932\n",
      "Epoch 408/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.4893 - accuracy: 0.7328 - val_loss: 1.0969 - val_accuracy: 0.4977\n",
      "Epoch 409/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.4603 - accuracy: 0.7591 - val_loss: 1.1018 - val_accuracy: 0.4887\n",
      "Epoch 410/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.4319 - accuracy: 0.7712 - val_loss: 1.1496 - val_accuracy: 0.5158\n",
      "Epoch 411/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.4327 - accuracy: 0.7682 - val_loss: 1.1220 - val_accuracy: 0.5204\n",
      "Epoch 412/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.4404 - accuracy: 0.7707 - val_loss: 1.2588 - val_accuracy: 0.5113\n",
      "Epoch 413/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.4106 - accuracy: 0.7854 - val_loss: 1.3428 - val_accuracy: 0.5023\n",
      "Epoch 414/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.3990 - accuracy: 0.7929 - val_loss: 1.3067 - val_accuracy: 0.5249\n",
      "Epoch 415/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.4093 - accuracy: 0.7939 - val_loss: 1.2103 - val_accuracy: 0.5158\n",
      "Epoch 416/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.4263 - accuracy: 0.7904 - val_loss: 1.2416 - val_accuracy: 0.5385\n",
      "Epoch 417/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.3896 - accuracy: 0.7995 - val_loss: 1.3747 - val_accuracy: 0.5294\n",
      "Epoch 418/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.3670 - accuracy: 0.8172 - val_loss: 1.2783 - val_accuracy: 0.4842\n",
      "Epoch 419/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.3655 - accuracy: 0.8177 - val_loss: 1.4161 - val_accuracy: 0.5113\n",
      "Epoch 420/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 0.3459 - accuracy: 0.8338 - val_loss: 1.4525 - val_accuracy: 0.5520\n",
      "Epoch 421/1000\n",
      "1980/1980 [==============================] - 1s 428us/step - loss: 0.3257 - accuracy: 0.8384 - val_loss: 1.4292 - val_accuracy: 0.5068\n",
      "Epoch 422/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.3104 - accuracy: 0.8399 - val_loss: 1.5200 - val_accuracy: 0.5204\n",
      "Epoch 423/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.3057 - accuracy: 0.8576 - val_loss: 1.5869 - val_accuracy: 0.5113\n",
      "Epoch 424/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.3377 - accuracy: 0.8343 - val_loss: 1.4176 - val_accuracy: 0.5158\n",
      "Epoch 425/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.3042 - accuracy: 0.8586 - val_loss: 1.6403 - val_accuracy: 0.5294\n",
      "Epoch 426/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.2674 - accuracy: 0.8798 - val_loss: 1.7270 - val_accuracy: 0.5294\n",
      "Epoch 427/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.2461 - accuracy: 0.8889 - val_loss: 1.6805 - val_accuracy: 0.5294\n",
      "Epoch 428/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.2440 - accuracy: 0.8833 - val_loss: 1.6763 - val_accuracy: 0.5385\n",
      "Epoch 429/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.2584 - accuracy: 0.8884 - val_loss: 1.7708 - val_accuracy: 0.5520\n",
      "Epoch 430/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.2285 - accuracy: 0.8919 - val_loss: 1.7861 - val_accuracy: 0.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 431/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.2262 - accuracy: 0.8965 - val_loss: 1.8393 - val_accuracy: 0.5385\n",
      "Epoch 432/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.2841 - accuracy: 0.8854 - val_loss: 1.7159 - val_accuracy: 0.5294\n",
      "Epoch 433/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.2550 - accuracy: 0.8818 - val_loss: 1.8005 - val_accuracy: 0.5385\n",
      "Epoch 434/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.2113 - accuracy: 0.9111 - val_loss: 1.6604 - val_accuracy: 0.5294\n",
      "Epoch 435/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.2215 - accuracy: 0.9040 - val_loss: 1.8719 - val_accuracy: 0.4887\n",
      "Epoch 436/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 0.2057 - accuracy: 0.9157 - val_loss: 1.8339 - val_accuracy: 0.5294\n",
      "Epoch 437/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.1659 - accuracy: 0.9268 - val_loss: 2.0721 - val_accuracy: 0.5385\n",
      "Epoch 438/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.1520 - accuracy: 0.9364 - val_loss: 1.9232 - val_accuracy: 0.5747\n",
      "Epoch 439/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.1636 - accuracy: 0.9313 - val_loss: 1.8913 - val_accuracy: 0.5566\n",
      "Epoch 440/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.1740 - accuracy: 0.9268 - val_loss: 2.0808 - val_accuracy: 0.5701\n",
      "Epoch 441/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.1899 - accuracy: 0.9182 - val_loss: 2.0017 - val_accuracy: 0.5339\n",
      "Epoch 442/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.1424 - accuracy: 0.9475 - val_loss: 2.0816 - val_accuracy: 0.5566\n",
      "Epoch 443/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.1330 - accuracy: 0.9465 - val_loss: 2.1603 - val_accuracy: 0.5701\n",
      "Epoch 444/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.1310 - accuracy: 0.9500 - val_loss: 2.1832 - val_accuracy: 0.5339\n",
      "Epoch 445/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.1270 - accuracy: 0.9500 - val_loss: 2.0888 - val_accuracy: 0.5430\n",
      "Epoch 446/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.1359 - accuracy: 0.9490 - val_loss: 2.2531 - val_accuracy: 0.5339\n",
      "Epoch 447/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.1337 - accuracy: 0.9490 - val_loss: 2.2140 - val_accuracy: 0.5520\n",
      "Epoch 448/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.1324 - accuracy: 0.9389 - val_loss: 2.0656 - val_accuracy: 0.5430\n",
      "Epoch 449/1000\n",
      "1980/1980 [==============================] - 1s 439us/step - loss: 0.1458 - accuracy: 0.9409 - val_loss: 2.2332 - val_accuracy: 0.5113\n",
      "Epoch 450/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.1312 - accuracy: 0.9465 - val_loss: 2.1783 - val_accuracy: 0.5475\n",
      "Epoch 451/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.1314 - accuracy: 0.9485 - val_loss: 2.3069 - val_accuracy: 0.5520\n",
      "Epoch 452/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.1026 - accuracy: 0.9621 - val_loss: 2.3196 - val_accuracy: 0.5113\n",
      "Epoch 453/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0754 - accuracy: 0.9758 - val_loss: 2.4212 - val_accuracy: 0.5430\n",
      "Epoch 454/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 0.0550 - accuracy: 0.9864 - val_loss: 2.3959 - val_accuracy: 0.5566\n",
      "Epoch 455/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0554 - accuracy: 0.9808 - val_loss: 2.5558 - val_accuracy: 0.5385\n",
      "Epoch 456/1000\n",
      "1980/1980 [==============================] - 1s 497us/step - loss: 0.0643 - accuracy: 0.9763 - val_loss: 2.4149 - val_accuracy: 0.5430\n",
      "Epoch 457/1000\n",
      "1980/1980 [==============================] - 1s 486us/step - loss: 0.0519 - accuracy: 0.9843 - val_loss: 2.5913 - val_accuracy: 0.5385\n",
      "Epoch 458/1000\n",
      "1980/1980 [==============================] - 1s 498us/step - loss: 0.0396 - accuracy: 0.9899 - val_loss: 2.6958 - val_accuracy: 0.5339\n",
      "Epoch 459/1000\n",
      "1980/1980 [==============================] - 1s 465us/step - loss: 0.0368 - accuracy: 0.9904 - val_loss: 2.6907 - val_accuracy: 0.5430\n",
      "Epoch 460/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0302 - accuracy: 0.9914 - val_loss: 2.8217 - val_accuracy: 0.5520\n",
      "Epoch 461/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 2.5597 - val_accuracy: 0.5520\n",
      "Epoch 462/1000\n",
      "1980/1980 [==============================] - 1s 458us/step - loss: 0.1023 - accuracy: 0.9611 - val_loss: 2.8182 - val_accuracy: 0.5068\n",
      "Epoch 463/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.1416 - accuracy: 0.9495 - val_loss: 2.4926 - val_accuracy: 0.5520\n",
      "Epoch 464/1000\n",
      "1980/1980 [==============================] - 1s 513us/step - loss: 0.1768 - accuracy: 0.9384 - val_loss: 2.6012 - val_accuracy: 0.5204\n",
      "Epoch 465/1000\n",
      "1980/1980 [==============================] - 1s 471us/step - loss: 0.1779 - accuracy: 0.9293 - val_loss: 2.3170 - val_accuracy: 0.5475\n",
      "Epoch 466/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.1334 - accuracy: 0.9520 - val_loss: 2.3373 - val_accuracy: 0.5339\n",
      "Epoch 467/1000\n",
      "1980/1980 [==============================] - 1s 432us/step - loss: 0.1048 - accuracy: 0.9682 - val_loss: 2.3431 - val_accuracy: 0.5792\n",
      "Epoch 468/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0556 - accuracy: 0.9823 - val_loss: 2.2662 - val_accuracy: 0.5747\n",
      "Epoch 469/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.0393 - accuracy: 0.9884 - val_loss: 2.5231 - val_accuracy: 0.5430\n",
      "Epoch 470/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 0.0210 - accuracy: 0.9975 - val_loss: 2.5927 - val_accuracy: 0.5475\n",
      "Epoch 471/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.0151 - accuracy: 0.9990 - val_loss: 2.6985 - val_accuracy: 0.5611\n",
      "Epoch 472/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0124 - accuracy: 0.9995 - val_loss: 2.7591 - val_accuracy: 0.5611\n",
      "Epoch 473/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.6983 - val_accuracy: 0.5520\n",
      "Epoch 474/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 2.8112 - val_accuracy: 0.5611\n",
      "Epoch 475/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 2.8469 - val_accuracy: 0.5611\n",
      "Epoch 476/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.8797 - val_accuracy: 0.5566\n",
      "Epoch 477/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.8811 - val_accuracy: 0.5566\n",
      "Epoch 478/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.9202 - val_accuracy: 0.5656\n",
      "Epoch 479/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.9195 - val_accuracy: 0.5701\n",
      "Epoch 480/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.9699 - val_accuracy: 0.5566\n",
      "Epoch 481/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.9753 - val_accuracy: 0.5611\n",
      "Epoch 482/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.0209 - val_accuracy: 0.5656\n",
      "Epoch 483/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.0156 - val_accuracy: 0.5656\n",
      "Epoch 484/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.0459 - val_accuracy: 0.5656\n",
      "Epoch 485/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.0523 - val_accuracy: 0.5656\n",
      "Epoch 486/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.0951 - val_accuracy: 0.5566\n",
      "Epoch 487/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.1046 - val_accuracy: 0.5701\n",
      "Epoch 488/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.1105 - val_accuracy: 0.5566\n",
      "Epoch 489/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.1295 - val_accuracy: 0.5520\n",
      "Epoch 490/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.1398 - val_accuracy: 0.5611\n",
      "Epoch 491/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 3.1743 - val_accuracy: 0.5566\n",
      "Epoch 492/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.1725 - val_accuracy: 0.5566\n",
      "Epoch 493/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.2012 - val_accuracy: 0.5701\n",
      "Epoch 494/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.2017 - val_accuracy: 0.5701\n",
      "Epoch 495/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.2063 - val_accuracy: 0.5701\n",
      "Epoch 496/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.2086 - val_accuracy: 0.5701\n",
      "Epoch 497/1000\n",
      "1980/1980 [==============================] - 1s 485us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.2211 - val_accuracy: 0.5701\n",
      "Epoch 498/1000\n",
      "1980/1980 [==============================] - 1s 486us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.2448 - val_accuracy: 0.5656\n",
      "Epoch 499/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.2481 - val_accuracy: 0.5520\n",
      "Epoch 500/1000\n",
      "1980/1980 [==============================] - 1s 464us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.2721 - val_accuracy: 0.5611\n",
      "Epoch 501/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.3002 - val_accuracy: 0.5566\n",
      "Epoch 502/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.2863 - val_accuracy: 0.5701\n",
      "Epoch 503/1000\n",
      "1980/1980 [==============================] - 1s 460us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.3273 - val_accuracy: 0.5566\n",
      "Epoch 504/1000\n",
      "1980/1980 [==============================] - 1s 504us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.3226 - val_accuracy: 0.5656\n",
      "Epoch 505/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.3143 - val_accuracy: 0.5701\n",
      "Epoch 506/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.3555 - val_accuracy: 0.5520\n",
      "Epoch 507/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.3675 - val_accuracy: 0.5475\n",
      "Epoch 508/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.3579 - val_accuracy: 0.5701\n",
      "Epoch 509/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.3864 - val_accuracy: 0.5611\n",
      "Epoch 510/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.3934 - val_accuracy: 0.5611\n",
      "Epoch 511/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.4010 - val_accuracy: 0.5656\n",
      "Epoch 512/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.4026 - val_accuracy: 0.5520\n",
      "Epoch 513/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.4162 - val_accuracy: 0.5566\n",
      "Epoch 514/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.4152 - val_accuracy: 0.5656\n",
      "Epoch 515/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.4376 - val_accuracy: 0.5566\n",
      "Epoch 516/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.4433 - val_accuracy: 0.5520\n",
      "Epoch 517/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.4389 - val_accuracy: 0.5566\n",
      "Epoch 518/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.4381 - val_accuracy: 0.5611\n",
      "Epoch 519/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.4617 - val_accuracy: 0.5566\n",
      "Epoch 520/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 9.6899e-04 - accuracy: 1.0000 - val_loss: 3.4638 - val_accuracy: 0.5566\n",
      "Epoch 521/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.4746 - val_accuracy: 0.5566\n",
      "Epoch 522/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 8.9129e-04 - accuracy: 1.0000 - val_loss: 3.4772 - val_accuracy: 0.5747\n",
      "Epoch 523/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 9.0472e-04 - accuracy: 1.0000 - val_loss: 3.4871 - val_accuracy: 0.5656\n",
      "Epoch 524/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 8.8350e-04 - accuracy: 1.0000 - val_loss: 3.5090 - val_accuracy: 0.5566\n",
      "Epoch 525/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 9.7238e-04 - accuracy: 1.0000 - val_loss: 3.5156 - val_accuracy: 0.5611\n",
      "Epoch 526/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 9.2359e-04 - accuracy: 1.0000 - val_loss: 3.5316 - val_accuracy: 0.5656\n",
      "Epoch 527/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.7929e-04 - accuracy: 1.0000 - val_loss: 3.5414 - val_accuracy: 0.5656\n",
      "Epoch 528/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 8.9534e-04 - accuracy: 1.0000 - val_loss: 3.5244 - val_accuracy: 0.5611\n",
      "Epoch 529/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.9900e-04 - accuracy: 1.0000 - val_loss: 3.5477 - val_accuracy: 0.5611\n",
      "Epoch 530/1000\n",
      "1980/1980 [==============================] - 1s 396us/step - loss: 8.4353e-04 - accuracy: 1.0000 - val_loss: 3.5476 - val_accuracy: 0.5656\n",
      "Epoch 531/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 9.0561e-04 - accuracy: 1.0000 - val_loss: 3.5350 - val_accuracy: 0.5611\n",
      "Epoch 532/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 8.0316e-04 - accuracy: 1.0000 - val_loss: 3.5641 - val_accuracy: 0.5656\n",
      "Epoch 533/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 7.4747e-04 - accuracy: 1.0000 - val_loss: 3.5830 - val_accuracy: 0.5611\n",
      "Epoch 534/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 7.1561e-04 - accuracy: 1.0000 - val_loss: 3.5875 - val_accuracy: 0.5475\n",
      "Epoch 535/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 7.1273e-04 - accuracy: 1.0000 - val_loss: 3.5944 - val_accuracy: 0.5475\n",
      "Epoch 536/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 7.3225e-04 - accuracy: 1.0000 - val_loss: 3.6027 - val_accuracy: 0.5701\n",
      "Epoch 537/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 8.4897e-04 - accuracy: 1.0000 - val_loss: 3.6305 - val_accuracy: 0.5566\n",
      "Epoch 538/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 7.2579e-04 - accuracy: 1.0000 - val_loss: 3.6507 - val_accuracy: 0.5656\n",
      "Epoch 539/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 372us/step - loss: 6.9389e-04 - accuracy: 1.0000 - val_loss: 3.6063 - val_accuracy: 0.5656\n",
      "Epoch 540/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 7.1348e-04 - accuracy: 1.0000 - val_loss: 3.6280 - val_accuracy: 0.5566\n",
      "Epoch 541/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 7.3702e-04 - accuracy: 1.0000 - val_loss: 3.6243 - val_accuracy: 0.5566\n",
      "Epoch 542/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 6.4612e-04 - accuracy: 1.0000 - val_loss: 3.6424 - val_accuracy: 0.5520\n",
      "Epoch 543/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 7.2025e-04 - accuracy: 1.0000 - val_loss: 3.6424 - val_accuracy: 0.5475\n",
      "Epoch 544/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 6.7337e-04 - accuracy: 1.0000 - val_loss: 3.6334 - val_accuracy: 0.5611\n",
      "Epoch 545/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 6.1391e-04 - accuracy: 1.0000 - val_loss: 3.6397 - val_accuracy: 0.5566\n",
      "Epoch 546/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 6.6662e-04 - accuracy: 1.0000 - val_loss: 3.6498 - val_accuracy: 0.5792\n",
      "Epoch 547/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 6.5030e-04 - accuracy: 1.0000 - val_loss: 3.6431 - val_accuracy: 0.5701\n",
      "Epoch 548/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 5.6724e-04 - accuracy: 1.0000 - val_loss: 3.6644 - val_accuracy: 0.5475\n",
      "Epoch 549/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 5.7349e-04 - accuracy: 1.0000 - val_loss: 3.6714 - val_accuracy: 0.5430\n",
      "Epoch 550/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 6.0620e-04 - accuracy: 1.0000 - val_loss: 3.6925 - val_accuracy: 0.5475\n",
      "Epoch 551/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 5.4876e-04 - accuracy: 1.0000 - val_loss: 3.6837 - val_accuracy: 0.5475\n",
      "Epoch 552/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 4.8904e-04 - accuracy: 1.0000 - val_loss: 3.6864 - val_accuracy: 0.5520\n",
      "Epoch 553/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.3021e-04 - accuracy: 1.0000 - val_loss: 3.6871 - val_accuracy: 0.5520\n",
      "Epoch 554/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 5.1688e-04 - accuracy: 1.0000 - val_loss: 3.6887 - val_accuracy: 0.5520\n",
      "Epoch 555/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 4.9361e-04 - accuracy: 1.0000 - val_loss: 3.7076 - val_accuracy: 0.5520\n",
      "Epoch 556/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 5.1398e-04 - accuracy: 1.0000 - val_loss: 3.7235 - val_accuracy: 0.5611\n",
      "Epoch 557/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 4.5972e-04 - accuracy: 1.0000 - val_loss: 3.7154 - val_accuracy: 0.5656\n",
      "Epoch 558/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 4.2474e-04 - accuracy: 1.0000 - val_loss: 3.7258 - val_accuracy: 0.5566\n",
      "Epoch 559/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 4.9834e-04 - accuracy: 1.0000 - val_loss: 3.7432 - val_accuracy: 0.5475\n",
      "Epoch 560/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 4.4751e-04 - accuracy: 1.0000 - val_loss: 3.7259 - val_accuracy: 0.5475\n",
      "Epoch 561/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 4.4149e-04 - accuracy: 1.0000 - val_loss: 3.7336 - val_accuracy: 0.5520\n",
      "Epoch 562/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 4.8053e-04 - accuracy: 1.0000 - val_loss: 3.7438 - val_accuracy: 0.5520\n",
      "Epoch 563/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 4.2179e-04 - accuracy: 1.0000 - val_loss: 3.7425 - val_accuracy: 0.5475\n",
      "Epoch 564/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 4.4718e-04 - accuracy: 1.0000 - val_loss: 3.7551 - val_accuracy: 0.5566\n",
      "Epoch 565/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 4.4735e-04 - accuracy: 1.0000 - val_loss: 3.7694 - val_accuracy: 0.5566\n",
      "Epoch 566/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 4.4327e-04 - accuracy: 1.0000 - val_loss: 3.7819 - val_accuracy: 0.5430\n",
      "Epoch 567/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 4.4524e-04 - accuracy: 1.0000 - val_loss: 3.7737 - val_accuracy: 0.5475\n",
      "Epoch 568/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 4.1878e-04 - accuracy: 1.0000 - val_loss: 3.7791 - val_accuracy: 0.5520\n",
      "Epoch 569/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 4.0771e-04 - accuracy: 1.0000 - val_loss: 3.7721 - val_accuracy: 0.5475\n",
      "Epoch 570/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 3.3780e-04 - accuracy: 1.0000 - val_loss: 3.7834 - val_accuracy: 0.5566\n",
      "Epoch 571/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 3.5565e-04 - accuracy: 1.0000 - val_loss: 3.7982 - val_accuracy: 0.5430\n",
      "Epoch 572/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 3.6610e-04 - accuracy: 1.0000 - val_loss: 3.8107 - val_accuracy: 0.5339\n",
      "Epoch 573/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 3.3125e-04 - accuracy: 1.0000 - val_loss: 3.8233 - val_accuracy: 0.5339\n",
      "Epoch 574/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 3.5955e-04 - accuracy: 1.0000 - val_loss: 3.8316 - val_accuracy: 0.5430\n",
      "Epoch 575/1000\n",
      "1980/1980 [==============================] - 1s 366us/step - loss: 3.1650e-04 - accuracy: 1.0000 - val_loss: 3.8343 - val_accuracy: 0.5385\n",
      "Epoch 576/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 3.7298e-04 - accuracy: 1.0000 - val_loss: 3.8489 - val_accuracy: 0.5339\n",
      "Epoch 577/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.7094e-04 - accuracy: 1.0000 - val_loss: 3.8403 - val_accuracy: 0.5339\n",
      "Epoch 578/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 3.0690e-04 - accuracy: 1.0000 - val_loss: 3.8233 - val_accuracy: 0.5385\n",
      "Epoch 579/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 3.4498e-04 - accuracy: 1.0000 - val_loss: 3.8223 - val_accuracy: 0.5566\n",
      "Epoch 580/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.2493e-04 - accuracy: 1.0000 - val_loss: 3.8614 - val_accuracy: 0.5385\n",
      "Epoch 581/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 3.0436e-04 - accuracy: 1.0000 - val_loss: 3.8721 - val_accuracy: 0.5294\n",
      "Epoch 582/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 2.6511e-04 - accuracy: 1.0000 - val_loss: 3.8592 - val_accuracy: 0.5430\n",
      "Epoch 583/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.8316e-04 - accuracy: 1.0000 - val_loss: 3.8639 - val_accuracy: 0.5430\n",
      "Epoch 584/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 2.7156e-04 - accuracy: 1.0000 - val_loss: 3.8741 - val_accuracy: 0.5385\n",
      "Epoch 585/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 3.0165e-04 - accuracy: 1.0000 - val_loss: 3.8781 - val_accuracy: 0.5385\n",
      "Epoch 586/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 2.7347e-04 - accuracy: 1.0000 - val_loss: 3.8876 - val_accuracy: 0.5430\n",
      "Epoch 587/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.7492e-04 - accuracy: 1.0000 - val_loss: 3.9042 - val_accuracy: 0.5385\n",
      "Epoch 588/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.7199e-04 - accuracy: 1.0000 - val_loss: 3.8988 - val_accuracy: 0.5385\n",
      "Epoch 589/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 2.3994e-04 - accuracy: 1.0000 - val_loss: 3.9023 - val_accuracy: 0.5475\n",
      "Epoch 590/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 2.7056e-04 - accuracy: 1.0000 - val_loss: 3.9100 - val_accuracy: 0.5430\n",
      "Epoch 591/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 3.1918e-04 - accuracy: 1.0000 - val_loss: 3.9103 - val_accuracy: 0.5385\n",
      "Epoch 592/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.9863e-04 - accuracy: 1.0000 - val_loss: 3.9116 - val_accuracy: 0.5430\n",
      "Epoch 593/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.6932e-04 - accuracy: 1.0000 - val_loss: 3.9241 - val_accuracy: 0.5339\n",
      "Epoch 594/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 2.5843e-04 - accuracy: 1.0000 - val_loss: 3.9182 - val_accuracy: 0.5339\n",
      "Epoch 595/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 2.4285e-04 - accuracy: 1.0000 - val_loss: 3.9244 - val_accuracy: 0.5430\n",
      "Epoch 596/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 2.4648e-04 - accuracy: 1.0000 - val_loss: 3.9365 - val_accuracy: 0.5249\n",
      "Epoch 597/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 2.5195e-04 - accuracy: 1.0000 - val_loss: 3.9557 - val_accuracy: 0.5339\n",
      "Epoch 598/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 2.6328e-04 - accuracy: 1.0000 - val_loss: 3.9223 - val_accuracy: 0.5430\n",
      "Epoch 599/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 2.6293e-04 - accuracy: 1.0000 - val_loss: 3.9850 - val_accuracy: 0.5294\n",
      "Epoch 600/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 2.6509e-04 - accuracy: 1.0000 - val_loss: 3.9585 - val_accuracy: 0.5339\n",
      "Epoch 601/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 2.4676e-04 - accuracy: 1.0000 - val_loss: 3.9580 - val_accuracy: 0.5430\n",
      "Epoch 602/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 2.5132e-04 - accuracy: 1.0000 - val_loss: 3.9963 - val_accuracy: 0.5339\n",
      "Epoch 603/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 1.9731e-04 - accuracy: 1.0000 - val_loss: 3.9870 - val_accuracy: 0.5294\n",
      "Epoch 604/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 2.4159e-04 - accuracy: 1.0000 - val_loss: 4.0143 - val_accuracy: 0.5339\n",
      "Epoch 605/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 2.3397e-04 - accuracy: 1.0000 - val_loss: 3.9990 - val_accuracy: 0.5339\n",
      "Epoch 606/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 2.3418e-04 - accuracy: 1.0000 - val_loss: 4.0075 - val_accuracy: 0.5294\n",
      "Epoch 607/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 2.6057e-04 - accuracy: 1.0000 - val_loss: 4.0009 - val_accuracy: 0.5204\n",
      "Epoch 608/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 1.9832e-04 - accuracy: 1.0000 - val_loss: 4.0115 - val_accuracy: 0.5249\n",
      "Epoch 609/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 2.0884e-04 - accuracy: 1.0000 - val_loss: 4.0274 - val_accuracy: 0.5294\n",
      "Epoch 610/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.5072e-04 - accuracy: 1.0000 - val_loss: 4.0317 - val_accuracy: 0.5339\n",
      "Epoch 611/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 1.9717e-04 - accuracy: 1.0000 - val_loss: 4.0278 - val_accuracy: 0.5339\n",
      "Epoch 612/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 1.8156e-04 - accuracy: 1.0000 - val_loss: 4.0110 - val_accuracy: 0.5339\n",
      "Epoch 613/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 1.9008e-04 - accuracy: 1.0000 - val_loss: 4.0168 - val_accuracy: 0.5294\n",
      "Epoch 614/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 1.8368e-04 - accuracy: 1.0000 - val_loss: 4.0242 - val_accuracy: 0.5294\n",
      "Epoch 615/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 1.6906e-04 - accuracy: 1.0000 - val_loss: 4.0320 - val_accuracy: 0.5385\n",
      "Epoch 616/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 1.9014e-04 - accuracy: 1.0000 - val_loss: 4.0455 - val_accuracy: 0.5339\n",
      "Epoch 617/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 1.9429e-04 - accuracy: 1.0000 - val_loss: 4.0459 - val_accuracy: 0.5294\n",
      "Epoch 618/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 1.9125e-04 - accuracy: 1.0000 - val_loss: 4.0368 - val_accuracy: 0.5249\n",
      "Epoch 619/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 1.7274e-04 - accuracy: 1.0000 - val_loss: 4.0520 - val_accuracy: 0.5204\n",
      "Epoch 620/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.6781e-04 - accuracy: 1.0000 - val_loss: 4.0574 - val_accuracy: 0.5249\n",
      "Epoch 621/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.6989e-04 - accuracy: 1.0000 - val_loss: 4.0502 - val_accuracy: 0.5294\n",
      "Epoch 622/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 1.7653e-04 - accuracy: 1.0000 - val_loss: 4.0599 - val_accuracy: 0.5339\n",
      "Epoch 623/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 1.6005e-04 - accuracy: 1.0000 - val_loss: 4.0609 - val_accuracy: 0.5294\n",
      "Epoch 624/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 1.6277e-04 - accuracy: 1.0000 - val_loss: 4.0628 - val_accuracy: 0.5294\n",
      "Epoch 625/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 1.7294e-04 - accuracy: 1.0000 - val_loss: 4.0602 - val_accuracy: 0.5339\n",
      "Epoch 626/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 1.5195e-04 - accuracy: 1.0000 - val_loss: 4.0680 - val_accuracy: 0.5294\n",
      "Epoch 627/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 1.7644e-04 - accuracy: 1.0000 - val_loss: 4.0872 - val_accuracy: 0.5249\n",
      "Epoch 628/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 1.9067e-04 - accuracy: 1.0000 - val_loss: 4.0692 - val_accuracy: 0.5249\n",
      "Epoch 629/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 1.7182e-04 - accuracy: 1.0000 - val_loss: 4.0833 - val_accuracy: 0.5294\n",
      "Epoch 630/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 1.6291e-04 - accuracy: 1.0000 - val_loss: 4.0919 - val_accuracy: 0.5249\n",
      "Epoch 631/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 1.4517e-04 - accuracy: 1.0000 - val_loss: 4.0861 - val_accuracy: 0.5294\n",
      "Epoch 632/1000\n",
      "1980/1980 [==============================] - 1s 502us/step - loss: 1.4891e-04 - accuracy: 1.0000 - val_loss: 4.0854 - val_accuracy: 0.5385\n",
      "Epoch 633/1000\n",
      "1980/1980 [==============================] - 1s 437us/step - loss: 1.3633e-04 - accuracy: 1.0000 - val_loss: 4.0943 - val_accuracy: 0.5249\n",
      "Epoch 634/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 1.5431e-04 - accuracy: 1.0000 - val_loss: 4.1109 - val_accuracy: 0.5204\n",
      "Epoch 635/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 1.4456e-04 - accuracy: 1.0000 - val_loss: 4.0706 - val_accuracy: 0.5339\n",
      "Epoch 636/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.5757e-04 - accuracy: 1.0000 - val_loss: 4.1033 - val_accuracy: 0.5385\n",
      "Epoch 637/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 2.0615e-04 - accuracy: 1.0000 - val_loss: 4.0975 - val_accuracy: 0.5294\n",
      "Epoch 638/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 1.6667e-04 - accuracy: 1.0000 - val_loss: 4.0966 - val_accuracy: 0.5339\n",
      "Epoch 639/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 1.5992e-04 - accuracy: 1.0000 - val_loss: 4.0968 - val_accuracy: 0.5385\n",
      "Epoch 640/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 1.2760e-04 - accuracy: 1.0000 - val_loss: 4.1207 - val_accuracy: 0.5249\n",
      "Epoch 641/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 1.5414e-04 - accuracy: 1.0000 - val_loss: 4.1319 - val_accuracy: 0.5249\n",
      "Epoch 642/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 1.4715e-04 - accuracy: 1.0000 - val_loss: 4.1193 - val_accuracy: 0.5294\n",
      "Epoch 643/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.3302e-04 - accuracy: 1.0000 - val_loss: 4.1418 - val_accuracy: 0.5294\n",
      "Epoch 644/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 1.3838e-04 - accuracy: 1.0000 - val_loss: 4.1531 - val_accuracy: 0.5385\n",
      "Epoch 645/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 387us/step - loss: 1.2032e-04 - accuracy: 1.0000 - val_loss: 4.1516 - val_accuracy: 0.5249\n",
      "Epoch 646/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 1.2733e-04 - accuracy: 1.0000 - val_loss: 4.1632 - val_accuracy: 0.5249\n",
      "Epoch 647/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.1564e-04 - accuracy: 1.0000 - val_loss: 4.1720 - val_accuracy: 0.5204\n",
      "Epoch 648/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.4881e-04 - accuracy: 1.0000 - val_loss: 4.1583 - val_accuracy: 0.5385\n",
      "Epoch 649/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 1.2439e-04 - accuracy: 1.0000 - val_loss: 4.1654 - val_accuracy: 0.5294\n",
      "Epoch 650/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 1.1717e-04 - accuracy: 1.0000 - val_loss: 4.1900 - val_accuracy: 0.5294\n",
      "Epoch 651/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.1018e-04 - accuracy: 1.0000 - val_loss: 4.1914 - val_accuracy: 0.5249\n",
      "Epoch 652/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 1.1005e-04 - accuracy: 1.0000 - val_loss: 4.1998 - val_accuracy: 0.5339\n",
      "Epoch 653/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 1.4469e-04 - accuracy: 1.0000 - val_loss: 4.2015 - val_accuracy: 0.5249\n",
      "Epoch 654/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 1.0852e-04 - accuracy: 1.0000 - val_loss: 4.2279 - val_accuracy: 0.5204\n",
      "Epoch 655/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 1.0377e-04 - accuracy: 1.0000 - val_loss: 4.2317 - val_accuracy: 0.5339\n",
      "Epoch 656/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 1.0840e-04 - accuracy: 1.0000 - val_loss: 4.2212 - val_accuracy: 0.5385\n",
      "Epoch 657/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 1.1063e-04 - accuracy: 1.0000 - val_loss: 4.2150 - val_accuracy: 0.5339\n",
      "Epoch 658/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 1.0636e-04 - accuracy: 1.0000 - val_loss: 4.2352 - val_accuracy: 0.5294\n",
      "Epoch 659/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 1.3063e-04 - accuracy: 1.0000 - val_loss: 4.2515 - val_accuracy: 0.5294\n",
      "Epoch 660/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 1.0913e-04 - accuracy: 1.0000 - val_loss: 4.2540 - val_accuracy: 0.5249\n",
      "Epoch 661/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 1.2527e-04 - accuracy: 1.0000 - val_loss: 4.2582 - val_accuracy: 0.5294\n",
      "Epoch 662/1000\n",
      "1980/1980 [==============================] - 1s 468us/step - loss: 1.1115e-04 - accuracy: 1.0000 - val_loss: 4.2375 - val_accuracy: 0.5249\n",
      "Epoch 663/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 1.1080e-04 - accuracy: 1.0000 - val_loss: 4.2377 - val_accuracy: 0.5249\n",
      "Epoch 664/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 1.2929e-04 - accuracy: 1.0000 - val_loss: 4.2648 - val_accuracy: 0.5294\n",
      "Epoch 665/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 1.0249e-04 - accuracy: 1.0000 - val_loss: 4.2620 - val_accuracy: 0.5294\n",
      "Epoch 666/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 9.9527e-05 - accuracy: 1.0000 - val_loss: 4.2615 - val_accuracy: 0.5249\n",
      "Epoch 667/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 1.0698e-04 - accuracy: 1.0000 - val_loss: 4.2689 - val_accuracy: 0.5294\n",
      "Epoch 668/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.0077e-04 - accuracy: 1.0000 - val_loss: 4.2764 - val_accuracy: 0.5294\n",
      "Epoch 669/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 9.6588e-05 - accuracy: 1.0000 - val_loss: 4.2824 - val_accuracy: 0.5249\n",
      "Epoch 670/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 1.0839e-04 - accuracy: 1.0000 - val_loss: 4.2921 - val_accuracy: 0.5249\n",
      "Epoch 671/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 9.0397e-05 - accuracy: 1.0000 - val_loss: 4.2954 - val_accuracy: 0.5294\n",
      "Epoch 672/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.0118e-04 - accuracy: 1.0000 - val_loss: 4.2884 - val_accuracy: 0.5294\n",
      "Epoch 673/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 9.1746e-05 - accuracy: 1.0000 - val_loss: 4.2991 - val_accuracy: 0.5294\n",
      "Epoch 674/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 8.6918e-05 - accuracy: 1.0000 - val_loss: 4.3022 - val_accuracy: 0.5294\n",
      "Epoch 675/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 1.1351e-04 - accuracy: 1.0000 - val_loss: 4.2843 - val_accuracy: 0.5294\n",
      "Epoch 676/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 9.2011e-05 - accuracy: 1.0000 - val_loss: 4.3115 - val_accuracy: 0.5294\n",
      "Epoch 677/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 9.0940e-05 - accuracy: 1.0000 - val_loss: 4.3425 - val_accuracy: 0.5339\n",
      "Epoch 678/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 7.7424e-05 - accuracy: 1.0000 - val_loss: 4.3146 - val_accuracy: 0.5294\n",
      "Epoch 679/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 8.5612e-05 - accuracy: 1.0000 - val_loss: 4.3201 - val_accuracy: 0.5294\n",
      "Epoch 680/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 7.5361e-05 - accuracy: 1.0000 - val_loss: 4.3226 - val_accuracy: 0.5294\n",
      "Epoch 681/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 1.0734e-04 - accuracy: 1.0000 - val_loss: 4.2946 - val_accuracy: 0.5385\n",
      "Epoch 682/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 9.5653e-05 - accuracy: 1.0000 - val_loss: 4.3121 - val_accuracy: 0.5339\n",
      "Epoch 683/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 8.3726e-05 - accuracy: 1.0000 - val_loss: 4.3503 - val_accuracy: 0.5339\n",
      "Epoch 684/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 7.6963e-05 - accuracy: 1.0000 - val_loss: 4.3783 - val_accuracy: 0.5249\n",
      "Epoch 685/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 1.1594e-04 - accuracy: 1.0000 - val_loss: 4.3305 - val_accuracy: 0.5204\n",
      "Epoch 686/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 7.2935e-05 - accuracy: 1.0000 - val_loss: 4.3228 - val_accuracy: 0.5294\n",
      "Epoch 687/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 7.8988e-05 - accuracy: 1.0000 - val_loss: 4.3648 - val_accuracy: 0.5339\n",
      "Epoch 688/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 8.1214e-05 - accuracy: 1.0000 - val_loss: 4.3599 - val_accuracy: 0.5339\n",
      "Epoch 689/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 8.1798e-05 - accuracy: 1.0000 - val_loss: 4.3575 - val_accuracy: 0.5339\n",
      "Epoch 690/1000\n",
      "1980/1980 [==============================] - 1s 429us/step - loss: 7.7444e-05 - accuracy: 1.0000 - val_loss: 4.3613 - val_accuracy: 0.5339\n",
      "Epoch 691/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 7.4935e-05 - accuracy: 1.0000 - val_loss: 4.3601 - val_accuracy: 0.5294\n",
      "Epoch 692/1000\n",
      "1980/1980 [==============================] - 1s 500us/step - loss: 7.8239e-05 - accuracy: 1.0000 - val_loss: 4.3512 - val_accuracy: 0.5339\n",
      "Epoch 693/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 7.0077e-05 - accuracy: 1.0000 - val_loss: 4.3744 - val_accuracy: 0.5339\n",
      "Epoch 694/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 8.0339e-05 - accuracy: 1.0000 - val_loss: 4.3791 - val_accuracy: 0.5339\n",
      "Epoch 695/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 6.6797e-05 - accuracy: 1.0000 - val_loss: 4.3811 - val_accuracy: 0.5294\n",
      "Epoch 696/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 7.2432e-05 - accuracy: 1.0000 - val_loss: 4.3990 - val_accuracy: 0.5339\n",
      "Epoch 697/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 7.7917e-05 - accuracy: 1.0000 - val_loss: 4.4114 - val_accuracy: 0.5294\n",
      "Epoch 698/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 7.2916e-05 - accuracy: 1.0000 - val_loss: 4.4097 - val_accuracy: 0.5294\n",
      "Epoch 699/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 7.9618e-05 - accuracy: 1.0000 - val_loss: 4.4068 - val_accuracy: 0.5339\n",
      "Epoch 700/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 7.2259e-05 - accuracy: 1.0000 - val_loss: 4.4039 - val_accuracy: 0.5385\n",
      "Epoch 701/1000\n",
      "1980/1980 [==============================] - 1s 456us/step - loss: 7.3473e-05 - accuracy: 1.0000 - val_loss: 4.3960 - val_accuracy: 0.5294\n",
      "Epoch 702/1000\n",
      "1980/1980 [==============================] - 1s 476us/step - loss: 6.6781e-05 - accuracy: 1.0000 - val_loss: 4.3959 - val_accuracy: 0.5294\n",
      "Epoch 703/1000\n",
      "1980/1980 [==============================] - 1s 451us/step - loss: 6.8565e-05 - accuracy: 1.0000 - val_loss: 4.4072 - val_accuracy: 0.5339\n",
      "Epoch 704/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 7.1495e-05 - accuracy: 1.0000 - val_loss: 4.3911 - val_accuracy: 0.5249\n",
      "Epoch 705/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 6.5365e-05 - accuracy: 1.0000 - val_loss: 4.4015 - val_accuracy: 0.5339\n",
      "Epoch 706/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 7.2037e-05 - accuracy: 1.0000 - val_loss: 4.4241 - val_accuracy: 0.5294\n",
      "Epoch 707/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 6.8844e-05 - accuracy: 1.0000 - val_loss: 4.4266 - val_accuracy: 0.5339\n",
      "Epoch 708/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 6.1149e-05 - accuracy: 1.0000 - val_loss: 4.4287 - val_accuracy: 0.5339\n",
      "Epoch 709/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 6.6521e-05 - accuracy: 1.0000 - val_loss: 4.4326 - val_accuracy: 0.5339\n",
      "Epoch 710/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 6.5666e-05 - accuracy: 1.0000 - val_loss: 4.4399 - val_accuracy: 0.5249\n",
      "Epoch 711/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 7.9079e-05 - accuracy: 1.0000 - val_loss: 4.4534 - val_accuracy: 0.5385\n",
      "Epoch 712/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 6.7461e-05 - accuracy: 1.0000 - val_loss: 4.4497 - val_accuracy: 0.5294\n",
      "Epoch 713/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 6.7397e-05 - accuracy: 1.0000 - val_loss: 4.4741 - val_accuracy: 0.5339\n",
      "Epoch 714/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 5.4150e-05 - accuracy: 1.0000 - val_loss: 4.4796 - val_accuracy: 0.5294\n",
      "Epoch 715/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 5.3513e-05 - accuracy: 1.0000 - val_loss: 4.4738 - val_accuracy: 0.5249\n",
      "Epoch 716/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.0679e-05 - accuracy: 1.0000 - val_loss: 4.4794 - val_accuracy: 0.5294\n",
      "Epoch 717/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 5.8437e-05 - accuracy: 1.0000 - val_loss: 4.4902 - val_accuracy: 0.5249\n",
      "Epoch 718/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 6.0094e-05 - accuracy: 1.0000 - val_loss: 4.4809 - val_accuracy: 0.5294\n",
      "Epoch 719/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 7.3260e-05 - accuracy: 1.0000 - val_loss: 4.4885 - val_accuracy: 0.5249\n",
      "Epoch 720/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 8.4745e-05 - accuracy: 1.0000 - val_loss: 4.4860 - val_accuracy: 0.5385\n",
      "Epoch 721/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 8.0320e-05 - accuracy: 1.0000 - val_loss: 4.4992 - val_accuracy: 0.5430\n",
      "Epoch 722/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 7.0336e-05 - accuracy: 1.0000 - val_loss: 4.4752 - val_accuracy: 0.5339\n",
      "Epoch 723/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 5.9607e-05 - accuracy: 1.0000 - val_loss: 4.4499 - val_accuracy: 0.5339\n",
      "Epoch 724/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 5.7447e-05 - accuracy: 1.0000 - val_loss: 4.4626 - val_accuracy: 0.5294\n",
      "Epoch 725/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 5.6825e-05 - accuracy: 1.0000 - val_loss: 4.4808 - val_accuracy: 0.5385\n",
      "Epoch 726/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 8.2461e-05 - accuracy: 1.0000 - val_loss: 4.4885 - val_accuracy: 0.5294\n",
      "Epoch 727/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.8967e-05 - accuracy: 1.0000 - val_loss: 4.5128 - val_accuracy: 0.5294\n",
      "Epoch 728/1000\n",
      "1980/1980 [==============================] - 1s 414us/step - loss: 5.8699e-05 - accuracy: 1.0000 - val_loss: 4.5219 - val_accuracy: 0.5339\n",
      "Epoch 729/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 5.3315e-05 - accuracy: 1.0000 - val_loss: 4.5329 - val_accuracy: 0.5294\n",
      "Epoch 730/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 5.3697e-05 - accuracy: 1.0000 - val_loss: 4.5368 - val_accuracy: 0.5339\n",
      "Epoch 731/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 4.7836e-05 - accuracy: 1.0000 - val_loss: 4.5435 - val_accuracy: 0.5339\n",
      "Epoch 732/1000\n",
      "1980/1980 [==============================] - 1s 409us/step - loss: 5.2497e-05 - accuracy: 1.0000 - val_loss: 4.5477 - val_accuracy: 0.5430\n",
      "Epoch 733/1000\n",
      "1980/1980 [==============================] - 1s 477us/step - loss: 5.2411e-05 - accuracy: 1.0000 - val_loss: 4.5647 - val_accuracy: 0.5385\n",
      "Epoch 734/1000\n",
      "1980/1980 [==============================] - 1s 436us/step - loss: 5.0986e-05 - accuracy: 1.0000 - val_loss: 4.5589 - val_accuracy: 0.5385\n",
      "Epoch 735/1000\n",
      "1980/1980 [==============================] - 1s 499us/step - loss: 5.4484e-05 - accuracy: 1.0000 - val_loss: 4.5588 - val_accuracy: 0.5385\n",
      "Epoch 736/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 5.2269e-05 - accuracy: 1.0000 - val_loss: 4.5540 - val_accuracy: 0.5339\n",
      "Epoch 737/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 6.1017e-05 - accuracy: 1.0000 - val_loss: 4.5246 - val_accuracy: 0.5385\n",
      "Epoch 738/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 5.0578e-05 - accuracy: 1.0000 - val_loss: 4.5690 - val_accuracy: 0.5430\n",
      "Epoch 739/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 5.0395e-05 - accuracy: 1.0000 - val_loss: 4.5753 - val_accuracy: 0.5430\n",
      "Epoch 740/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 4.8655e-05 - accuracy: 1.0000 - val_loss: 4.5708 - val_accuracy: 0.5430\n",
      "Epoch 741/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 6.1743e-05 - accuracy: 1.0000 - val_loss: 4.5599 - val_accuracy: 0.5385\n",
      "Epoch 742/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 4.8795e-05 - accuracy: 1.0000 - val_loss: 4.5596 - val_accuracy: 0.5430\n",
      "Epoch 743/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.5650e-05 - accuracy: 1.0000 - val_loss: 4.5649 - val_accuracy: 0.5430\n",
      "Epoch 744/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 5.1671e-05 - accuracy: 1.0000 - val_loss: 4.5851 - val_accuracy: 0.5475\n",
      "Epoch 745/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 4.1053e-05 - accuracy: 1.0000 - val_loss: 4.5877 - val_accuracy: 0.5430\n",
      "Epoch 746/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 6.4373e-05 - accuracy: 1.0000 - val_loss: 4.5904 - val_accuracy: 0.5294\n",
      "Epoch 747/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 4.2629e-05 - accuracy: 1.0000 - val_loss: 4.5788 - val_accuracy: 0.5294\n",
      "Epoch 748/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 4.6736e-05 - accuracy: 1.0000 - val_loss: 4.6065 - val_accuracy: 0.5339\n",
      "Epoch 749/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 4.5479e-05 - accuracy: 1.0000 - val_loss: 4.6144 - val_accuracy: 0.5385\n",
      "Epoch 750/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 4.3249e-05 - accuracy: 1.0000 - val_loss: 4.6150 - val_accuracy: 0.5385\n",
      "Epoch 751/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 360us/step - loss: 4.4114e-05 - accuracy: 1.0000 - val_loss: 4.6373 - val_accuracy: 0.5385\n",
      "Epoch 752/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 4.3369e-05 - accuracy: 1.0000 - val_loss: 4.6661 - val_accuracy: 0.5339\n",
      "Epoch 753/1000\n",
      "1980/1980 [==============================] - 1s 358us/step - loss: 4.2991e-05 - accuracy: 1.0000 - val_loss: 4.6798 - val_accuracy: 0.5385\n",
      "Epoch 754/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 4.3813e-05 - accuracy: 1.0000 - val_loss: 4.6361 - val_accuracy: 0.5385\n",
      "Epoch 755/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 4.2633e-05 - accuracy: 1.0000 - val_loss: 4.6379 - val_accuracy: 0.5385\n",
      "Epoch 756/1000\n",
      "1980/1980 [==============================] - 1s 362us/step - loss: 5.2006e-05 - accuracy: 1.0000 - val_loss: 4.6652 - val_accuracy: 0.5385\n",
      "Epoch 757/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 4.1939e-05 - accuracy: 1.0000 - val_loss: 4.6752 - val_accuracy: 0.5385\n",
      "Epoch 758/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 4.1424e-05 - accuracy: 1.0000 - val_loss: 4.6432 - val_accuracy: 0.5430\n",
      "Epoch 759/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 4.4956e-05 - accuracy: 1.0000 - val_loss: 4.6666 - val_accuracy: 0.5430\n",
      "Epoch 760/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 4.4693e-05 - accuracy: 1.0000 - val_loss: 4.6607 - val_accuracy: 0.5430\n",
      "Epoch 761/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 4.0211e-05 - accuracy: 1.0000 - val_loss: 4.6380 - val_accuracy: 0.5430\n",
      "Epoch 762/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 4.3408e-05 - accuracy: 1.0000 - val_loss: 4.6509 - val_accuracy: 0.5339\n",
      "Epoch 763/1000\n",
      "1980/1980 [==============================] - 1s 554us/step - loss: 3.9613e-05 - accuracy: 1.0000 - val_loss: 4.6737 - val_accuracy: 0.5294\n",
      "Epoch 764/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 4.6457e-05 - accuracy: 1.0000 - val_loss: 4.6872 - val_accuracy: 0.5475\n",
      "Epoch 765/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 3.4243e-05 - accuracy: 1.0000 - val_loss: 4.6810 - val_accuracy: 0.5475\n",
      "Epoch 766/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 3.9901e-05 - accuracy: 1.0000 - val_loss: 4.6901 - val_accuracy: 0.5475\n",
      "Epoch 767/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 4.3739e-05 - accuracy: 1.0000 - val_loss: 4.6907 - val_accuracy: 0.5385\n",
      "Epoch 768/1000\n",
      "1980/1980 [==============================] - 1s 390us/step - loss: 4.1440e-05 - accuracy: 1.0000 - val_loss: 4.6954 - val_accuracy: 0.5430\n",
      "Epoch 769/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 3.1780e-05 - accuracy: 1.0000 - val_loss: 4.6923 - val_accuracy: 0.5430\n",
      "Epoch 770/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 3.7611e-05 - accuracy: 1.0000 - val_loss: 4.7135 - val_accuracy: 0.5430\n",
      "Epoch 771/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 3.5251e-05 - accuracy: 1.0000 - val_loss: 4.7134 - val_accuracy: 0.5475\n",
      "Epoch 772/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 4.5145e-05 - accuracy: 1.0000 - val_loss: 4.7134 - val_accuracy: 0.5385\n",
      "Epoch 773/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 3.4643e-05 - accuracy: 1.0000 - val_loss: 4.7382 - val_accuracy: 0.5339\n",
      "Epoch 774/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 3.5495e-05 - accuracy: 1.0000 - val_loss: 4.7376 - val_accuracy: 0.5339\n",
      "Epoch 775/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 3.2446e-05 - accuracy: 1.0000 - val_loss: 4.7395 - val_accuracy: 0.5430\n",
      "Epoch 776/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 3.2899e-05 - accuracy: 1.0000 - val_loss: 4.7361 - val_accuracy: 0.5385\n",
      "Epoch 777/1000\n",
      "1980/1980 [==============================] - 1s 364us/step - loss: 3.9845e-05 - accuracy: 1.0000 - val_loss: 4.7243 - val_accuracy: 0.5475\n",
      "Epoch 778/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 3.3915e-05 - accuracy: 1.0000 - val_loss: 4.7165 - val_accuracy: 0.5475\n",
      "Epoch 779/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 3.4354e-05 - accuracy: 1.0000 - val_loss: 4.7397 - val_accuracy: 0.5520\n",
      "Epoch 780/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 3.6493e-05 - accuracy: 1.0000 - val_loss: 4.7417 - val_accuracy: 0.5430\n",
      "Epoch 781/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 3.7911e-05 - accuracy: 1.0000 - val_loss: 4.7091 - val_accuracy: 0.5475\n",
      "Epoch 782/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 3.3314e-05 - accuracy: 1.0000 - val_loss: 4.7125 - val_accuracy: 0.5475\n",
      "Epoch 783/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 3.2532e-05 - accuracy: 1.0000 - val_loss: 4.7322 - val_accuracy: 0.5475\n",
      "Epoch 784/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 3.0912e-05 - accuracy: 1.0000 - val_loss: 4.7182 - val_accuracy: 0.5475\n",
      "Epoch 785/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 3.1459e-05 - accuracy: 1.0000 - val_loss: 4.7256 - val_accuracy: 0.5566\n",
      "Epoch 786/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 8.3760e-05 - accuracy: 1.0000 - val_loss: 4.6714 - val_accuracy: 0.5566\n",
      "Epoch 787/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 0.2204 - accuracy: 0.9601 - val_loss: 2.8763 - val_accuracy: 0.5068\n",
      "Epoch 788/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 1.2892 - accuracy: 0.5394 - val_loss: 0.7974 - val_accuracy: 0.4615\n",
      "Epoch 789/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.8148 - accuracy: 0.5232 - val_loss: 0.7125 - val_accuracy: 0.4299\n",
      "Epoch 790/1000\n",
      "1980/1980 [==============================] - 1s 413us/step - loss: 0.7603 - accuracy: 0.4874 - val_loss: 0.7108 - val_accuracy: 0.5520\n",
      "Epoch 791/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.7328 - accuracy: 0.5111 - val_loss: 0.6918 - val_accuracy: 0.5249\n",
      "Epoch 792/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.6990 - accuracy: 0.5359 - val_loss: 0.7014 - val_accuracy: 0.4796\n",
      "Epoch 793/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.6926 - accuracy: 0.5298 - val_loss: 0.7030 - val_accuracy: 0.4751\n",
      "Epoch 794/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.6900 - accuracy: 0.5338 - val_loss: 0.6926 - val_accuracy: 0.5520\n",
      "Epoch 795/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.6883 - accuracy: 0.5369 - val_loss: 0.6970 - val_accuracy: 0.5113\n",
      "Epoch 796/1000\n",
      "1980/1980 [==============================] - 1s 467us/step - loss: 0.6826 - accuracy: 0.5581 - val_loss: 0.7025 - val_accuracy: 0.5339\n",
      "Epoch 797/1000\n",
      "1980/1980 [==============================] - 1s 449us/step - loss: 0.6765 - accuracy: 0.5677 - val_loss: 0.7533 - val_accuracy: 0.4434\n",
      "Epoch 798/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.6846 - accuracy: 0.5510 - val_loss: 0.7047 - val_accuracy: 0.4932\n",
      "Epoch 799/1000\n",
      "1980/1980 [==============================] - 1s 419us/step - loss: 0.6855 - accuracy: 0.5379 - val_loss: 0.6981 - val_accuracy: 0.5520\n",
      "Epoch 800/1000\n",
      "1980/1980 [==============================] - 1s 510us/step - loss: 0.6831 - accuracy: 0.5535 - val_loss: 0.7184 - val_accuracy: 0.4977\n",
      "Epoch 801/1000\n",
      "1980/1980 [==============================] - 1s 501us/step - loss: 0.6769 - accuracy: 0.5616 - val_loss: 0.7141 - val_accuracy: 0.5023\n",
      "Epoch 802/1000\n",
      "1980/1980 [==============================] - 1s 466us/step - loss: 0.6707 - accuracy: 0.5808 - val_loss: 0.7232 - val_accuracy: 0.4796\n",
      "Epoch 803/1000\n",
      "1980/1980 [==============================] - 1s 459us/step - loss: 0.6685 - accuracy: 0.5854 - val_loss: 0.7151 - val_accuracy: 0.5113\n",
      "Epoch 804/1000\n",
      "1980/1980 [==============================] - 1s 495us/step - loss: 0.6649 - accuracy: 0.5985 - val_loss: 0.7110 - val_accuracy: 0.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 805/1000\n",
      "1980/1980 [==============================] - 1s 488us/step - loss: 0.6579 - accuracy: 0.6061 - val_loss: 0.7247 - val_accuracy: 0.5204\n",
      "Epoch 806/1000\n",
      "1980/1980 [==============================] - 1s 461us/step - loss: 0.6553 - accuracy: 0.6076 - val_loss: 0.7175 - val_accuracy: 0.5294\n",
      "Epoch 807/1000\n",
      "1980/1980 [==============================] - 1s 438us/step - loss: 0.6544 - accuracy: 0.5955 - val_loss: 0.7186 - val_accuracy: 0.5475\n",
      "Epoch 808/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.6434 - accuracy: 0.6106 - val_loss: 0.7471 - val_accuracy: 0.5113\n",
      "Epoch 809/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.6463 - accuracy: 0.6253 - val_loss: 0.7167 - val_accuracy: 0.5249\n",
      "Epoch 810/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.6424 - accuracy: 0.6197 - val_loss: 0.7209 - val_accuracy: 0.5204\n",
      "Epoch 811/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.6367 - accuracy: 0.6338 - val_loss: 0.7096 - val_accuracy: 0.5566\n",
      "Epoch 812/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.6242 - accuracy: 0.6399 - val_loss: 0.7447 - val_accuracy: 0.5611\n",
      "Epoch 813/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.6342 - accuracy: 0.6338 - val_loss: 0.7025 - val_accuracy: 0.5294\n",
      "Epoch 814/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 0.6073 - accuracy: 0.6747 - val_loss: 0.7598 - val_accuracy: 0.5068\n",
      "Epoch 815/1000\n",
      "1980/1980 [==============================] - 1s 480us/step - loss: 0.5911 - accuracy: 0.6737 - val_loss: 0.7705 - val_accuracy: 0.5475\n",
      "Epoch 816/1000\n",
      "1980/1980 [==============================] - 1s 499us/step - loss: 0.5987 - accuracy: 0.6636 - val_loss: 0.7578 - val_accuracy: 0.5113\n",
      "Epoch 817/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.5864 - accuracy: 0.6843 - val_loss: 0.7757 - val_accuracy: 0.5475\n",
      "Epoch 818/1000\n",
      "1980/1980 [==============================] - 1s 421us/step - loss: 0.5682 - accuracy: 0.6985 - val_loss: 0.8052 - val_accuracy: 0.5158\n",
      "Epoch 819/1000\n",
      "1980/1980 [==============================] - 1s 425us/step - loss: 0.5572 - accuracy: 0.7030 - val_loss: 0.8298 - val_accuracy: 0.5023\n",
      "Epoch 820/1000\n",
      "1980/1980 [==============================] - 1s 422us/step - loss: 0.5416 - accuracy: 0.7268 - val_loss: 0.8208 - val_accuracy: 0.5294\n",
      "Epoch 821/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.5178 - accuracy: 0.7348 - val_loss: 0.8981 - val_accuracy: 0.4977\n",
      "Epoch 822/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.5143 - accuracy: 0.7364 - val_loss: 0.9092 - val_accuracy: 0.5023\n",
      "Epoch 823/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.5028 - accuracy: 0.7449 - val_loss: 0.9214 - val_accuracy: 0.4932\n",
      "Epoch 824/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 0.4855 - accuracy: 0.7702 - val_loss: 0.9315 - val_accuracy: 0.4796\n",
      "Epoch 825/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.4425 - accuracy: 0.7843 - val_loss: 0.9684 - val_accuracy: 0.4977\n",
      "Epoch 826/1000\n",
      "1980/1980 [==============================] - 1s 423us/step - loss: 0.4550 - accuracy: 0.7874 - val_loss: 1.0089 - val_accuracy: 0.4977\n",
      "Epoch 827/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.4326 - accuracy: 0.7970 - val_loss: 0.9925 - val_accuracy: 0.4932\n",
      "Epoch 828/1000\n",
      "1980/1980 [==============================] - 1s 426us/step - loss: 0.4196 - accuracy: 0.8051 - val_loss: 1.0084 - val_accuracy: 0.4796\n",
      "Epoch 829/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.4101 - accuracy: 0.8076 - val_loss: 1.0617 - val_accuracy: 0.4615\n",
      "Epoch 830/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 0.4030 - accuracy: 0.8177 - val_loss: 1.0064 - val_accuracy: 0.4887\n",
      "Epoch 831/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.3819 - accuracy: 0.8268 - val_loss: 1.2005 - val_accuracy: 0.4887\n",
      "Epoch 832/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 0.3436 - accuracy: 0.8515 - val_loss: 1.1785 - val_accuracy: 0.4932\n",
      "Epoch 833/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.3315 - accuracy: 0.8571 - val_loss: 1.2315 - val_accuracy: 0.4253\n",
      "Epoch 834/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 0.3157 - accuracy: 0.8677 - val_loss: 1.2202 - val_accuracy: 0.5023\n",
      "Epoch 835/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.3043 - accuracy: 0.8652 - val_loss: 1.3374 - val_accuracy: 0.4615\n",
      "Epoch 836/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.2951 - accuracy: 0.8727 - val_loss: 1.2707 - val_accuracy: 0.4751\n",
      "Epoch 837/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.2584 - accuracy: 0.8894 - val_loss: 1.2863 - val_accuracy: 0.5023\n",
      "Epoch 838/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.2505 - accuracy: 0.8914 - val_loss: 1.4410 - val_accuracy: 0.5068\n",
      "Epoch 839/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.2301 - accuracy: 0.9131 - val_loss: 1.3752 - val_accuracy: 0.5113\n",
      "Epoch 840/1000\n",
      "1980/1980 [==============================] - 1s 373us/step - loss: 0.2341 - accuracy: 0.9010 - val_loss: 1.5505 - val_accuracy: 0.5249\n",
      "Epoch 841/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.2077 - accuracy: 0.9192 - val_loss: 1.5661 - val_accuracy: 0.5113\n",
      "Epoch 842/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.1805 - accuracy: 0.9278 - val_loss: 1.6538 - val_accuracy: 0.4842\n",
      "Epoch 843/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 0.1650 - accuracy: 0.9374 - val_loss: 1.6247 - val_accuracy: 0.5249\n",
      "Epoch 844/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.1347 - accuracy: 0.9530 - val_loss: 1.8296 - val_accuracy: 0.5158\n",
      "Epoch 845/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.1440 - accuracy: 0.9505 - val_loss: 1.7007 - val_accuracy: 0.4887\n",
      "Epoch 846/1000\n",
      "1980/1980 [==============================] - 1s 374us/step - loss: 0.1318 - accuracy: 0.9520 - val_loss: 1.7276 - val_accuracy: 0.5023\n",
      "Epoch 847/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 0.1231 - accuracy: 0.9515 - val_loss: 1.8204 - val_accuracy: 0.5520\n",
      "Epoch 848/1000\n",
      "1980/1980 [==============================] - 1s 368us/step - loss: 0.1067 - accuracy: 0.9646 - val_loss: 1.7690 - val_accuracy: 0.5204\n",
      "Epoch 849/1000\n",
      "1980/1980 [==============================] - 1s 367us/step - loss: 0.0835 - accuracy: 0.9707 - val_loss: 1.9570 - val_accuracy: 0.4706\n",
      "Epoch 850/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0814 - accuracy: 0.9712 - val_loss: 1.9601 - val_accuracy: 0.5158\n",
      "Epoch 851/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.1192 - accuracy: 0.9545 - val_loss: 1.9170 - val_accuracy: 0.4887\n",
      "Epoch 852/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.1077 - accuracy: 0.9652 - val_loss: 1.9322 - val_accuracy: 0.4842\n",
      "Epoch 853/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0722 - accuracy: 0.9788 - val_loss: 1.9666 - val_accuracy: 0.5023\n",
      "Epoch 854/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 0.0778 - accuracy: 0.9747 - val_loss: 2.1883 - val_accuracy: 0.4932\n",
      "Epoch 855/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0643 - accuracy: 0.9768 - val_loss: 2.0933 - val_accuracy: 0.4887\n",
      "Epoch 856/1000\n",
      "1980/1980 [==============================] - 1s 370us/step - loss: 0.0520 - accuracy: 0.9854 - val_loss: 2.1390 - val_accuracy: 0.5113\n",
      "Epoch 857/1000\n",
      "1980/1980 [==============================] - 1s 363us/step - loss: 0.0633 - accuracy: 0.9798 - val_loss: 2.1145 - val_accuracy: 0.5339\n",
      "Epoch 858/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0663 - accuracy: 0.9773 - val_loss: 2.1992 - val_accuracy: 0.5023\n",
      "Epoch 859/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 0.0332 - accuracy: 0.9924 - val_loss: 2.2204 - val_accuracy: 0.4706\n",
      "Epoch 860/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0192 - accuracy: 0.9960 - val_loss: 2.3596 - val_accuracy: 0.4977\n",
      "Epoch 861/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0145 - accuracy: 0.9985 - val_loss: 2.3510 - val_accuracy: 0.4706\n",
      "Epoch 862/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 2.4582 - val_accuracy: 0.4887\n",
      "Epoch 863/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0100 - accuracy: 0.9990 - val_loss: 2.4754 - val_accuracy: 0.4887\n",
      "Epoch 864/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.5341 - val_accuracy: 0.4932\n",
      "Epoch 865/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 2.5813 - val_accuracy: 0.4796\n",
      "Epoch 866/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 2.5691 - val_accuracy: 0.4796\n",
      "Epoch 867/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.5835 - val_accuracy: 0.4932\n",
      "Epoch 868/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.6241 - val_accuracy: 0.4887\n",
      "Epoch 869/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.6429 - val_accuracy: 0.5023\n",
      "Epoch 870/1000\n",
      "1980/1980 [==============================] - 1s 411us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.6424 - val_accuracy: 0.4977\n",
      "Epoch 871/1000\n",
      "1980/1980 [==============================] - 1s 453us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.6783 - val_accuracy: 0.4977\n",
      "Epoch 872/1000\n",
      "1980/1980 [==============================] - 1s 484us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.6989 - val_accuracy: 0.5023\n",
      "Epoch 873/1000\n",
      "1980/1980 [==============================] - 1s 454us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.7159 - val_accuracy: 0.4842\n",
      "Epoch 874/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.7088 - val_accuracy: 0.4932\n",
      "Epoch 875/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.7207 - val_accuracy: 0.4932\n",
      "Epoch 876/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.7451 - val_accuracy: 0.5023\n",
      "Epoch 877/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.7651 - val_accuracy: 0.5023\n",
      "Epoch 878/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.7736 - val_accuracy: 0.4932\n",
      "Epoch 879/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.7886 - val_accuracy: 0.5113\n",
      "Epoch 880/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.7932 - val_accuracy: 0.5023\n",
      "Epoch 881/1000\n",
      "1980/1980 [==============================] - 1s 398us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.8016 - val_accuracy: 0.5158\n",
      "Epoch 882/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.7968 - val_accuracy: 0.5113\n",
      "Epoch 883/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.8175 - val_accuracy: 0.4932\n",
      "Epoch 884/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.8283 - val_accuracy: 0.5068\n",
      "Epoch 885/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.8151 - val_accuracy: 0.5113\n",
      "Epoch 886/1000\n",
      "1980/1980 [==============================] - 1s 395us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.8487 - val_accuracy: 0.4977\n",
      "Epoch 887/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.8612 - val_accuracy: 0.4932\n",
      "Epoch 888/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.8635 - val_accuracy: 0.5158\n",
      "Epoch 889/1000\n",
      "1980/1980 [==============================] - 1s 416us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.8680 - val_accuracy: 0.5113\n",
      "Epoch 890/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.8817 - val_accuracy: 0.5113\n",
      "Epoch 891/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9016 - val_accuracy: 0.4977\n",
      "Epoch 892/1000\n",
      "1980/1980 [==============================] - 1s 391us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9190 - val_accuracy: 0.5068\n",
      "Epoch 893/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.9277 - val_accuracy: 0.5158\n",
      "Epoch 894/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9352 - val_accuracy: 0.5068\n",
      "Epoch 895/1000\n",
      "1980/1980 [==============================] - 1s 400us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9372 - val_accuracy: 0.5113\n",
      "Epoch 896/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.9556 - val_accuracy: 0.5068\n",
      "Epoch 897/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.9662 - val_accuracy: 0.5068\n",
      "Epoch 898/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.9710 - val_accuracy: 0.5113\n",
      "Epoch 899/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.9717 - val_accuracy: 0.5113\n",
      "Epoch 900/1000\n",
      "1980/1980 [==============================] - 1s 371us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.9972 - val_accuracy: 0.5113\n",
      "Epoch 901/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.0118 - val_accuracy: 0.5068\n",
      "Epoch 902/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 9.0365e-04 - accuracy: 1.0000 - val_loss: 3.0127 - val_accuracy: 0.5023\n",
      "Epoch 903/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 9.3030e-04 - accuracy: 1.0000 - val_loss: 3.0173 - val_accuracy: 0.5023\n",
      "Epoch 904/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.0250 - val_accuracy: 0.5158\n",
      "Epoch 905/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.0477 - val_accuracy: 0.5113\n",
      "Epoch 906/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 8.9298e-04 - accuracy: 1.0000 - val_loss: 3.0553 - val_accuracy: 0.5158\n",
      "Epoch 907/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 8.8010e-04 - accuracy: 1.0000 - val_loss: 3.0592 - val_accuracy: 0.5158\n",
      "Epoch 908/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 8.2235e-04 - accuracy: 1.0000 - val_loss: 3.0656 - val_accuracy: 0.5158\n",
      "Epoch 909/1000\n",
      "1980/1980 [==============================] - 1s 443us/step - loss: 9.6920e-04 - accuracy: 1.0000 - val_loss: 3.0708 - val_accuracy: 0.5158\n",
      "Epoch 910/1000\n",
      "1980/1980 [==============================] - 1s 405us/step - loss: 7.9616e-04 - accuracy: 1.0000 - val_loss: 3.0682 - val_accuracy: 0.5158\n",
      "Epoch 911/1000\n",
      "1980/1980 [==============================] - 1s 433us/step - loss: 7.7685e-04 - accuracy: 1.0000 - val_loss: 3.0724 - val_accuracy: 0.5113\n",
      "Epoch 912/1000\n",
      "1980/1980 [==============================] - 1s 406us/step - loss: 7.7789e-04 - accuracy: 1.0000 - val_loss: 3.0658 - val_accuracy: 0.5023\n",
      "Epoch 913/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 1s 388us/step - loss: 7.4558e-04 - accuracy: 1.0000 - val_loss: 3.0855 - val_accuracy: 0.5068\n",
      "Epoch 914/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 8.3515e-04 - accuracy: 1.0000 - val_loss: 3.0939 - val_accuracy: 0.5113\n",
      "Epoch 915/1000\n",
      "1980/1980 [==============================] - 1s 397us/step - loss: 7.8892e-04 - accuracy: 1.0000 - val_loss: 3.1026 - val_accuracy: 0.5068\n",
      "Epoch 916/1000\n",
      "1980/1980 [==============================] - 1s 402us/step - loss: 8.0538e-04 - accuracy: 1.0000 - val_loss: 3.1081 - val_accuracy: 0.4977\n",
      "Epoch 917/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 7.5532e-04 - accuracy: 1.0000 - val_loss: 3.1131 - val_accuracy: 0.5023\n",
      "Epoch 918/1000\n",
      "1980/1980 [==============================] - 1s 388us/step - loss: 7.0132e-04 - accuracy: 1.0000 - val_loss: 3.1254 - val_accuracy: 0.5068\n",
      "Epoch 919/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 7.3722e-04 - accuracy: 1.0000 - val_loss: 3.1375 - val_accuracy: 0.5023\n",
      "Epoch 920/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 7.6054e-04 - accuracy: 1.0000 - val_loss: 3.1356 - val_accuracy: 0.5023\n",
      "Epoch 921/1000\n",
      "1980/1980 [==============================] - 1s 369us/step - loss: 7.3447e-04 - accuracy: 1.0000 - val_loss: 3.1364 - val_accuracy: 0.5158\n",
      "Epoch 922/1000\n",
      "1980/1980 [==============================] - 1s 365us/step - loss: 6.6807e-04 - accuracy: 1.0000 - val_loss: 3.1635 - val_accuracy: 0.5068\n",
      "Epoch 923/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 7.3043e-04 - accuracy: 1.0000 - val_loss: 3.1642 - val_accuracy: 0.5068\n",
      "Epoch 924/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 6.4784e-04 - accuracy: 1.0000 - val_loss: 3.1694 - val_accuracy: 0.5158\n",
      "Epoch 925/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 6.0644e-04 - accuracy: 1.0000 - val_loss: 3.1826 - val_accuracy: 0.5113\n",
      "Epoch 926/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 5.4791e-04 - accuracy: 1.0000 - val_loss: 3.1990 - val_accuracy: 0.5068\n",
      "Epoch 927/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 5.3892e-04 - accuracy: 1.0000 - val_loss: 3.2023 - val_accuracy: 0.5068\n",
      "Epoch 928/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 5.7388e-04 - accuracy: 1.0000 - val_loss: 3.2138 - val_accuracy: 0.5113\n",
      "Epoch 929/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 5.1503e-04 - accuracy: 1.0000 - val_loss: 3.2348 - val_accuracy: 0.5068\n",
      "Epoch 930/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.3948e-04 - accuracy: 1.0000 - val_loss: 3.2596 - val_accuracy: 0.5068\n",
      "Epoch 931/1000\n",
      "1980/1980 [==============================] - 1s 392us/step - loss: 5.2062e-04 - accuracy: 1.0000 - val_loss: 3.2430 - val_accuracy: 0.5023\n",
      "Epoch 932/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 5.1015e-04 - accuracy: 1.0000 - val_loss: 3.2445 - val_accuracy: 0.4977\n",
      "Epoch 933/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 5.3148e-04 - accuracy: 1.0000 - val_loss: 3.2589 - val_accuracy: 0.5068\n",
      "Epoch 934/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 5.8920e-04 - accuracy: 1.0000 - val_loss: 3.2637 - val_accuracy: 0.5068\n",
      "Epoch 935/1000\n",
      "1980/1980 [==============================] - 1s 463us/step - loss: 4.9202e-04 - accuracy: 1.0000 - val_loss: 3.2609 - val_accuracy: 0.5023\n",
      "Epoch 936/1000\n",
      "1980/1980 [==============================] - 1s 472us/step - loss: 4.4524e-04 - accuracy: 1.0000 - val_loss: 3.2588 - val_accuracy: 0.5068\n",
      "Epoch 937/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 4.6905e-04 - accuracy: 1.0000 - val_loss: 3.2750 - val_accuracy: 0.5023\n",
      "Epoch 938/1000\n",
      "1980/1980 [==============================] - 1s 489us/step - loss: 4.7236e-04 - accuracy: 1.0000 - val_loss: 3.2835 - val_accuracy: 0.5068\n",
      "Epoch 939/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 4.3136e-04 - accuracy: 1.0000 - val_loss: 3.2881 - val_accuracy: 0.5068\n",
      "Epoch 940/1000\n",
      "1980/1980 [==============================] - 1s 462us/step - loss: 4.2255e-04 - accuracy: 1.0000 - val_loss: 3.2875 - val_accuracy: 0.5113\n",
      "Epoch 941/1000\n",
      "1980/1980 [==============================] - 1s 440us/step - loss: 4.9840e-04 - accuracy: 1.0000 - val_loss: 3.2941 - val_accuracy: 0.5113\n",
      "Epoch 942/1000\n",
      "1980/1980 [==============================] - 1s 435us/step - loss: 4.2436e-04 - accuracy: 1.0000 - val_loss: 3.2957 - val_accuracy: 0.5068\n",
      "Epoch 943/1000\n",
      "1980/1980 [==============================] - 1s 444us/step - loss: 4.2289e-04 - accuracy: 1.0000 - val_loss: 3.3122 - val_accuracy: 0.5023\n",
      "Epoch 944/1000\n",
      "1980/1980 [==============================] - 1s 430us/step - loss: 4.2854e-04 - accuracy: 1.0000 - val_loss: 3.3292 - val_accuracy: 0.5068\n",
      "Epoch 945/1000\n",
      "1980/1980 [==============================] - 1s 417us/step - loss: 4.2641e-04 - accuracy: 1.0000 - val_loss: 3.3437 - val_accuracy: 0.5113\n",
      "Epoch 946/1000\n",
      "1980/1980 [==============================] - 1s 415us/step - loss: 4.0657e-04 - accuracy: 1.0000 - val_loss: 3.3529 - val_accuracy: 0.5113\n",
      "Epoch 947/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 4.5749e-04 - accuracy: 1.0000 - val_loss: 3.3694 - val_accuracy: 0.5113\n",
      "Epoch 948/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 3.7566e-04 - accuracy: 1.0000 - val_loss: 3.3686 - val_accuracy: 0.5113\n",
      "Epoch 949/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 4.1126e-04 - accuracy: 1.0000 - val_loss: 3.3620 - val_accuracy: 0.5158\n",
      "Epoch 950/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 3.8953e-04 - accuracy: 1.0000 - val_loss: 3.3656 - val_accuracy: 0.5068\n",
      "Epoch 951/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 3.7500e-04 - accuracy: 1.0000 - val_loss: 3.3671 - val_accuracy: 0.5113\n",
      "Epoch 952/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 3.8774e-04 - accuracy: 1.0000 - val_loss: 3.3773 - val_accuracy: 0.5068\n",
      "Epoch 953/1000\n",
      "1980/1980 [==============================] - 1s 372us/step - loss: 4.2568e-04 - accuracy: 1.0000 - val_loss: 3.3804 - val_accuracy: 0.5113\n",
      "Epoch 954/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 4.0414e-04 - accuracy: 1.0000 - val_loss: 3.4049 - val_accuracy: 0.5023\n",
      "Epoch 955/1000\n",
      "1980/1980 [==============================] - 1s 385us/step - loss: 3.4638e-04 - accuracy: 1.0000 - val_loss: 3.4091 - val_accuracy: 0.5158\n",
      "Epoch 956/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 3.2326e-04 - accuracy: 1.0000 - val_loss: 3.4064 - val_accuracy: 0.5068\n",
      "Epoch 957/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.5578e-04 - accuracy: 1.0000 - val_loss: 3.4121 - val_accuracy: 0.5204\n",
      "Epoch 958/1000\n",
      "1980/1980 [==============================] - 1s 384us/step - loss: 3.6187e-04 - accuracy: 1.0000 - val_loss: 3.4020 - val_accuracy: 0.5068\n",
      "Epoch 959/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.8485e-04 - accuracy: 1.0000 - val_loss: 3.4091 - val_accuracy: 0.5113\n",
      "Epoch 960/1000\n",
      "1980/1980 [==============================] - 1s 381us/step - loss: 3.1397e-04 - accuracy: 1.0000 - val_loss: 3.4203 - val_accuracy: 0.5113\n",
      "Epoch 961/1000\n",
      "1980/1980 [==============================] - 1s 377us/step - loss: 3.1910e-04 - accuracy: 1.0000 - val_loss: 3.4330 - val_accuracy: 0.5204\n",
      "Epoch 962/1000\n",
      "1980/1980 [==============================] - 1s 386us/step - loss: 3.6544e-04 - accuracy: 1.0000 - val_loss: 3.4394 - val_accuracy: 0.5068\n",
      "Epoch 963/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 3.3775e-04 - accuracy: 1.0000 - val_loss: 3.4409 - val_accuracy: 0.5158\n",
      "Epoch 964/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 2.9551e-04 - accuracy: 1.0000 - val_loss: 3.4468 - val_accuracy: 0.5158\n",
      "Epoch 965/1000\n",
      "1980/1980 [==============================] - 1s 389us/step - loss: 2.9903e-04 - accuracy: 1.0000 - val_loss: 3.4490 - val_accuracy: 0.5204\n",
      "Epoch 966/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 3.0186e-04 - accuracy: 1.0000 - val_loss: 3.4512 - val_accuracy: 0.5068\n",
      "Epoch 967/1000\n",
      "1980/1980 [==============================] - 1s 393us/step - loss: 3.2495e-04 - accuracy: 1.0000 - val_loss: 3.4672 - val_accuracy: 0.5294\n",
      "Epoch 968/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 2.9743e-04 - accuracy: 1.0000 - val_loss: 3.4861 - val_accuracy: 0.5249\n",
      "Epoch 969/1000\n",
      "1980/1980 [==============================] - 1s 420us/step - loss: 3.0877e-04 - accuracy: 1.0000 - val_loss: 3.4839 - val_accuracy: 0.5249\n",
      "Epoch 970/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 2.9910e-04 - accuracy: 1.0000 - val_loss: 3.4960 - val_accuracy: 0.5249\n",
      "Epoch 971/1000\n",
      "1980/1980 [==============================] - 1s 408us/step - loss: 2.8428e-04 - accuracy: 1.0000 - val_loss: 3.5010 - val_accuracy: 0.5294\n",
      "Epoch 972/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 2.4530e-04 - accuracy: 1.0000 - val_loss: 3.5118 - val_accuracy: 0.5249\n",
      "Epoch 973/1000\n",
      "1980/1980 [==============================] - 1s 410us/step - loss: 2.7523e-04 - accuracy: 1.0000 - val_loss: 3.5218 - val_accuracy: 0.5204\n",
      "Epoch 974/1000\n",
      "1980/1980 [==============================] - 1s 404us/step - loss: 3.2784e-04 - accuracy: 1.0000 - val_loss: 3.5148 - val_accuracy: 0.5113\n",
      "Epoch 975/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 2.9739e-04 - accuracy: 1.0000 - val_loss: 3.5181 - val_accuracy: 0.5068\n",
      "Epoch 976/1000\n",
      "1980/1980 [==============================] - 1s 394us/step - loss: 2.5520e-04 - accuracy: 1.0000 - val_loss: 3.5251 - val_accuracy: 0.5158\n",
      "Epoch 977/1000\n",
      "1980/1980 [==============================] - 1s 383us/step - loss: 2.6538e-04 - accuracy: 1.0000 - val_loss: 3.5270 - val_accuracy: 0.5249\n",
      "Epoch 978/1000\n",
      "1980/1980 [==============================] - 1s 387us/step - loss: 2.3795e-04 - accuracy: 1.0000 - val_loss: 3.5299 - val_accuracy: 0.5249\n",
      "Epoch 979/1000\n",
      "1980/1980 [==============================] - 1s 376us/step - loss: 2.1640e-04 - accuracy: 1.0000 - val_loss: 3.5384 - val_accuracy: 0.5204\n",
      "Epoch 980/1000\n",
      "1980/1980 [==============================] - 1s 375us/step - loss: 2.2730e-04 - accuracy: 1.0000 - val_loss: 3.5369 - val_accuracy: 0.5158\n",
      "Epoch 981/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 2.1804e-04 - accuracy: 1.0000 - val_loss: 3.5474 - val_accuracy: 0.5158\n",
      "Epoch 982/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 2.3688e-04 - accuracy: 1.0000 - val_loss: 3.5468 - val_accuracy: 0.5158\n",
      "Epoch 983/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 2.2824e-04 - accuracy: 1.0000 - val_loss: 3.5394 - val_accuracy: 0.5158\n",
      "Epoch 984/1000\n",
      "1980/1980 [==============================] - 1s 380us/step - loss: 2.3989e-04 - accuracy: 1.0000 - val_loss: 3.5379 - val_accuracy: 0.5204\n",
      "Epoch 985/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 2.0062e-04 - accuracy: 1.0000 - val_loss: 3.5501 - val_accuracy: 0.5204\n",
      "Epoch 986/1000\n",
      "1980/1980 [==============================] - 1s 378us/step - loss: 2.1452e-04 - accuracy: 1.0000 - val_loss: 3.5678 - val_accuracy: 0.5249\n",
      "Epoch 987/1000\n",
      "1980/1980 [==============================] - 1s 382us/step - loss: 2.7527e-04 - accuracy: 1.0000 - val_loss: 3.5479 - val_accuracy: 0.5249\n",
      "Epoch 988/1000\n",
      "1980/1980 [==============================] - 1s 379us/step - loss: 2.3466e-04 - accuracy: 1.0000 - val_loss: 3.5511 - val_accuracy: 0.5249\n",
      "Epoch 989/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 2.0440e-04 - accuracy: 1.0000 - val_loss: 3.5677 - val_accuracy: 0.5249\n",
      "Epoch 990/1000\n",
      "1980/1980 [==============================] - 1s 399us/step - loss: 1.8792e-04 - accuracy: 1.0000 - val_loss: 3.5769 - val_accuracy: 0.5113\n",
      "Epoch 991/1000\n",
      "1980/1980 [==============================] - 1s 418us/step - loss: 1.9570e-04 - accuracy: 1.0000 - val_loss: 3.5893 - val_accuracy: 0.5204\n",
      "Epoch 992/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 2.0352e-04 - accuracy: 1.0000 - val_loss: 3.5920 - val_accuracy: 0.5249\n",
      "Epoch 993/1000\n",
      "1980/1980 [==============================] - 1s 457us/step - loss: 2.0617e-04 - accuracy: 1.0000 - val_loss: 3.6004 - val_accuracy: 0.5204\n",
      "Epoch 994/1000\n",
      "1980/1980 [==============================] - 1s 424us/step - loss: 2.0289e-04 - accuracy: 1.0000 - val_loss: 3.6056 - val_accuracy: 0.5249\n",
      "Epoch 995/1000\n",
      "1980/1980 [==============================] - 1s 441us/step - loss: 1.9796e-04 - accuracy: 1.0000 - val_loss: 3.6152 - val_accuracy: 0.5204\n",
      "Epoch 996/1000\n",
      "1980/1980 [==============================] - 1s 403us/step - loss: 2.0541e-04 - accuracy: 1.0000 - val_loss: 3.6233 - val_accuracy: 0.5249\n",
      "Epoch 997/1000\n",
      "1980/1980 [==============================] - 1s 401us/step - loss: 1.9938e-04 - accuracy: 1.0000 - val_loss: 3.6256 - val_accuracy: 0.5249\n",
      "Epoch 998/1000\n",
      "1980/1980 [==============================] - 1s 431us/step - loss: 1.9850e-04 - accuracy: 1.0000 - val_loss: 3.6199 - val_accuracy: 0.5249\n",
      "Epoch 999/1000\n",
      "1980/1980 [==============================] - 1s 412us/step - loss: 1.9716e-04 - accuracy: 1.0000 - val_loss: 3.6285 - val_accuracy: 0.5249\n",
      "Epoch 1000/1000\n",
      "1980/1980 [==============================] - 1s 407us/step - loss: 1.7939e-04 - accuracy: 1.0000 - val_loss: 3.6374 - val_accuracy: 0.5249\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "245/245 [==============================] - 0s 140us/step\n",
      "test loss, test acc: [3.57752053202415, 0.5061224699020386]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (245, 1)\n",
      "rmse: 0.6877859121440838\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 15\n",
    "X_train_batches, y_train_batches = build_batch(stock_without_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=500, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(2, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_73 (LSTM)               (None, 2, 128)            113152    \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 2, 64)             49408     \n",
      "_________________________________________________________________\n",
      "lstm_75 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 175,009\n",
      "Trainable params: 175,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1991 samples, validate on 222 samples\n",
      "Epoch 1/1000\n",
      "1991/1991 [==============================] - 1s 649us/step - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6808 - val_accuracy: 0.6036\n",
      "Epoch 2/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6926 - accuracy: 0.5229 - val_loss: 0.6890 - val_accuracy: 0.6036\n",
      "Epoch 3/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6920 - accuracy: 0.5229 - val_loss: 0.6847 - val_accuracy: 0.6036\n",
      "Epoch 4/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6914 - accuracy: 0.5229 - val_loss: 0.6841 - val_accuracy: 0.6036\n",
      "Epoch 5/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6915 - accuracy: 0.5229 - val_loss: 0.6855 - val_accuracy: 0.6036\n",
      "Epoch 6/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6914 - accuracy: 0.5229 - val_loss: 0.6853 - val_accuracy: 0.5901\n",
      "Epoch 7/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6913 - accuracy: 0.5239 - val_loss: 0.6925 - val_accuracy: 0.5360\n",
      "Epoch 8/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.6913 - accuracy: 0.5299 - val_loss: 0.6867 - val_accuracy: 0.5766\n",
      "Epoch 9/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6917 - accuracy: 0.5158 - val_loss: 0.6876 - val_accuracy: 0.5766\n",
      "Epoch 10/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6904 - accuracy: 0.5304 - val_loss: 0.6882 - val_accuracy: 0.5586\n",
      "Epoch 11/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6908 - accuracy: 0.5208 - val_loss: 0.6899 - val_accuracy: 0.5360\n",
      "Epoch 12/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6905 - accuracy: 0.5279 - val_loss: 0.6981 - val_accuracy: 0.4550\n",
      "Epoch 13/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6911 - accuracy: 0.5349 - val_loss: 0.6875 - val_accuracy: 0.5676\n",
      "Epoch 14/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6899 - accuracy: 0.5349 - val_loss: 0.6885 - val_accuracy: 0.5541\n",
      "Epoch 15/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6898 - accuracy: 0.5289 - val_loss: 0.6915 - val_accuracy: 0.5225\n",
      "Epoch 16/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6898 - accuracy: 0.5369 - val_loss: 0.6888 - val_accuracy: 0.5450\n",
      "Epoch 17/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6894 - accuracy: 0.5374 - val_loss: 0.6949 - val_accuracy: 0.5180\n",
      "Epoch 18/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6905 - accuracy: 0.5419 - val_loss: 0.6841 - val_accuracy: 0.5721\n",
      "Epoch 19/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6897 - accuracy: 0.5450 - val_loss: 0.6900 - val_accuracy: 0.5495\n",
      "Epoch 20/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6889 - accuracy: 0.5409 - val_loss: 0.7005 - val_accuracy: 0.4865\n",
      "Epoch 21/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6873 - accuracy: 0.5455 - val_loss: 0.6920 - val_accuracy: 0.5315\n",
      "Epoch 22/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6888 - accuracy: 0.5334 - val_loss: 0.6893 - val_accuracy: 0.5360\n",
      "Epoch 23/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6867 - accuracy: 0.5455 - val_loss: 0.6932 - val_accuracy: 0.5270\n",
      "Epoch 24/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6870 - accuracy: 0.5445 - val_loss: 0.7109 - val_accuracy: 0.4595\n",
      "Epoch 25/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6865 - accuracy: 0.5309 - val_loss: 0.6821 - val_accuracy: 0.5946\n",
      "Epoch 26/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6863 - accuracy: 0.5374 - val_loss: 0.7085 - val_accuracy: 0.4189\n",
      "Epoch 27/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6904 - accuracy: 0.5324 - val_loss: 0.6972 - val_accuracy: 0.5270\n",
      "Epoch 28/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6874 - accuracy: 0.5565 - val_loss: 0.6898 - val_accuracy: 0.5450\n",
      "Epoch 29/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6843 - accuracy: 0.5510 - val_loss: 0.7095 - val_accuracy: 0.5045\n",
      "Epoch 30/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6867 - accuracy: 0.5460 - val_loss: 0.6985 - val_accuracy: 0.5090\n",
      "Epoch 31/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 0.6849 - accuracy: 0.5505 - val_loss: 0.6996 - val_accuracy: 0.5225\n",
      "Epoch 32/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6836 - accuracy: 0.5655 - val_loss: 0.7032 - val_accuracy: 0.5135\n",
      "Epoch 33/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6870 - accuracy: 0.5460 - val_loss: 0.7033 - val_accuracy: 0.4910\n",
      "Epoch 34/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6859 - accuracy: 0.5450 - val_loss: 0.7012 - val_accuracy: 0.5045\n",
      "Epoch 35/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.6847 - accuracy: 0.5555 - val_loss: 0.6924 - val_accuracy: 0.5450\n",
      "Epoch 36/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6840 - accuracy: 0.5535 - val_loss: 0.6856 - val_accuracy: 0.5721\n",
      "Epoch 37/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.6872 - accuracy: 0.5434 - val_loss: 0.7186 - val_accuracy: 0.4459\n",
      "Epoch 38/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6875 - accuracy: 0.5434 - val_loss: 0.6995 - val_accuracy: 0.4955\n",
      "Epoch 39/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6832 - accuracy: 0.5595 - val_loss: 0.7107 - val_accuracy: 0.4820\n",
      "Epoch 40/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 0.6821 - accuracy: 0.5645 - val_loss: 0.7083 - val_accuracy: 0.5000\n",
      "Epoch 41/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6831 - accuracy: 0.5515 - val_loss: 0.7206 - val_accuracy: 0.4369\n",
      "Epoch 42/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6852 - accuracy: 0.5555 - val_loss: 0.7057 - val_accuracy: 0.4865\n",
      "Epoch 43/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6817 - accuracy: 0.5605 - val_loss: 0.7089 - val_accuracy: 0.5000\n",
      "Epoch 44/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6824 - accuracy: 0.5580 - val_loss: 0.6968 - val_accuracy: 0.5180\n",
      "Epoch 45/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6816 - accuracy: 0.5671 - val_loss: 0.7041 - val_accuracy: 0.5000\n",
      "Epoch 46/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6784 - accuracy: 0.5620 - val_loss: 0.7056 - val_accuracy: 0.5405\n",
      "Epoch 47/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6799 - accuracy: 0.5655 - val_loss: 0.7092 - val_accuracy: 0.5090\n",
      "Epoch 48/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6816 - accuracy: 0.5550 - val_loss: 0.6952 - val_accuracy: 0.5450\n",
      "Epoch 49/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6789 - accuracy: 0.5605 - val_loss: 0.6971 - val_accuracy: 0.5270\n",
      "Epoch 50/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6768 - accuracy: 0.5761 - val_loss: 0.7278 - val_accuracy: 0.5135\n",
      "Epoch 51/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6752 - accuracy: 0.5610 - val_loss: 0.7269 - val_accuracy: 0.4685\n",
      "Epoch 52/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6753 - accuracy: 0.5681 - val_loss: 0.7396 - val_accuracy: 0.4685\n",
      "Epoch 53/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6748 - accuracy: 0.5751 - val_loss: 0.7335 - val_accuracy: 0.4865\n",
      "Epoch 54/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6819 - accuracy: 0.5660 - val_loss: 0.7047 - val_accuracy: 0.4955\n",
      "Epoch 55/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6808 - accuracy: 0.5665 - val_loss: 0.7142 - val_accuracy: 0.4820\n",
      "Epoch 56/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6803 - accuracy: 0.5660 - val_loss: 0.7243 - val_accuracy: 0.4640\n",
      "Epoch 57/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6814 - accuracy: 0.5480 - val_loss: 0.6841 - val_accuracy: 0.5721\n",
      "Epoch 58/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6770 - accuracy: 0.5691 - val_loss: 0.7072 - val_accuracy: 0.5135\n",
      "Epoch 59/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6752 - accuracy: 0.5741 - val_loss: 0.7195 - val_accuracy: 0.4955\n",
      "Epoch 60/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6712 - accuracy: 0.5726 - val_loss: 0.7293 - val_accuracy: 0.5090\n",
      "Epoch 61/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6763 - accuracy: 0.5681 - val_loss: 0.7440 - val_accuracy: 0.4324\n",
      "Epoch 62/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6785 - accuracy: 0.5701 - val_loss: 0.7308 - val_accuracy: 0.4640\n",
      "Epoch 63/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 0.6753 - accuracy: 0.5721 - val_loss: 0.7160 - val_accuracy: 0.4685\n",
      "Epoch 64/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6693 - accuracy: 0.5846 - val_loss: 0.7040 - val_accuracy: 0.5541\n",
      "Epoch 65/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6726 - accuracy: 0.5686 - val_loss: 0.7233 - val_accuracy: 0.4955\n",
      "Epoch 66/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6752 - accuracy: 0.5756 - val_loss: 0.7012 - val_accuracy: 0.5270\n",
      "Epoch 67/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6734 - accuracy: 0.5595 - val_loss: 0.6976 - val_accuracy: 0.5360\n",
      "Epoch 68/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6726 - accuracy: 0.5711 - val_loss: 0.7251 - val_accuracy: 0.5045\n",
      "Epoch 69/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6667 - accuracy: 0.5816 - val_loss: 0.7214 - val_accuracy: 0.4910\n",
      "Epoch 70/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6676 - accuracy: 0.5816 - val_loss: 0.7308 - val_accuracy: 0.4955\n",
      "Epoch 71/1000\n",
      "1991/1991 [==============================] - 0s 84us/step - loss: 0.6635 - accuracy: 0.5922 - val_loss: 0.7066 - val_accuracy: 0.5450\n",
      "Epoch 72/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.6659 - accuracy: 0.5786 - val_loss: 0.7154 - val_accuracy: 0.5225\n",
      "Epoch 73/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.6664 - accuracy: 0.5841 - val_loss: 0.6975 - val_accuracy: 0.5631\n",
      "Epoch 74/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6636 - accuracy: 0.5932 - val_loss: 0.7322 - val_accuracy: 0.4775\n",
      "Epoch 75/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.6621 - accuracy: 0.5897 - val_loss: 0.7346 - val_accuracy: 0.5045\n",
      "Epoch 76/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6593 - accuracy: 0.5897 - val_loss: 0.7278 - val_accuracy: 0.4955\n",
      "Epoch 77/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.6563 - accuracy: 0.5922 - val_loss: 0.7472 - val_accuracy: 0.4865\n",
      "Epoch 78/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.6560 - accuracy: 0.6002 - val_loss: 0.7661 - val_accuracy: 0.4595\n",
      "Epoch 79/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6575 - accuracy: 0.6007 - val_loss: 0.7157 - val_accuracy: 0.5000\n",
      "Epoch 80/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6618 - accuracy: 0.6047 - val_loss: 0.7329 - val_accuracy: 0.5090\n",
      "Epoch 81/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6570 - accuracy: 0.5897 - val_loss: 0.7285 - val_accuracy: 0.4685\n",
      "Epoch 82/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.6541 - accuracy: 0.6017 - val_loss: 0.7495 - val_accuracy: 0.5270\n",
      "Epoch 83/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6521 - accuracy: 0.5922 - val_loss: 0.7086 - val_accuracy: 0.5541\n",
      "Epoch 84/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6547 - accuracy: 0.5967 - val_loss: 0.7428 - val_accuracy: 0.4640\n",
      "Epoch 85/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6498 - accuracy: 0.6057 - val_loss: 0.7587 - val_accuracy: 0.4955\n",
      "Epoch 86/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.6528 - accuracy: 0.6012 - val_loss: 0.7321 - val_accuracy: 0.5000\n",
      "Epoch 87/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.6498 - accuracy: 0.6067 - val_loss: 0.7503 - val_accuracy: 0.4865\n",
      "Epoch 88/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6457 - accuracy: 0.6077 - val_loss: 0.7262 - val_accuracy: 0.5090\n",
      "Epoch 89/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6469 - accuracy: 0.6148 - val_loss: 0.7658 - val_accuracy: 0.4955\n",
      "Epoch 90/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6435 - accuracy: 0.6092 - val_loss: 0.7181 - val_accuracy: 0.5541\n",
      "Epoch 91/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6493 - accuracy: 0.5967 - val_loss: 0.7360 - val_accuracy: 0.5495\n",
      "Epoch 92/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6498 - accuracy: 0.6022 - val_loss: 0.7389 - val_accuracy: 0.5360\n",
      "Epoch 93/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6512 - accuracy: 0.5987 - val_loss: 0.7211 - val_accuracy: 0.5225\n",
      "Epoch 94/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6486 - accuracy: 0.6087 - val_loss: 0.7355 - val_accuracy: 0.5315\n",
      "Epoch 95/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6411 - accuracy: 0.6072 - val_loss: 0.7743 - val_accuracy: 0.5090\n",
      "Epoch 96/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.6422 - accuracy: 0.6123 - val_loss: 0.7802 - val_accuracy: 0.4910\n",
      "Epoch 97/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6288 - accuracy: 0.6298 - val_loss: 0.8008 - val_accuracy: 0.5180\n",
      "Epoch 98/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6324 - accuracy: 0.6268 - val_loss: 0.7652 - val_accuracy: 0.5000\n",
      "Epoch 99/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6319 - accuracy: 0.6354 - val_loss: 0.7997 - val_accuracy: 0.5090\n",
      "Epoch 100/1000\n",
      "1991/1991 [==============================] - 0s 85us/step - loss: 0.6336 - accuracy: 0.6183 - val_loss: 0.8150 - val_accuracy: 0.5045\n",
      "Epoch 101/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.6343 - accuracy: 0.6288 - val_loss: 0.7392 - val_accuracy: 0.5045\n",
      "Epoch 102/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6290 - accuracy: 0.6228 - val_loss: 0.7704 - val_accuracy: 0.5090\n",
      "Epoch 103/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.6292 - accuracy: 0.6258 - val_loss: 0.8272 - val_accuracy: 0.5045\n",
      "Epoch 104/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.6196 - accuracy: 0.6459 - val_loss: 0.8167 - val_accuracy: 0.5360\n",
      "Epoch 105/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.6248 - accuracy: 0.6318 - val_loss: 0.7575 - val_accuracy: 0.5270\n",
      "Epoch 106/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.6279 - accuracy: 0.6138 - val_loss: 0.7900 - val_accuracy: 0.5135\n",
      "Epoch 107/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.6168 - accuracy: 0.6479 - val_loss: 0.7818 - val_accuracy: 0.5225\n",
      "Epoch 108/1000\n",
      "1991/1991 [==============================] - 0s 86us/step - loss: 0.6162 - accuracy: 0.6374 - val_loss: 0.8623 - val_accuracy: 0.5180\n",
      "Epoch 109/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6078 - accuracy: 0.6444 - val_loss: 0.8157 - val_accuracy: 0.5045\n",
      "Epoch 110/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.6079 - accuracy: 0.6404 - val_loss: 0.8866 - val_accuracy: 0.5180\n",
      "Epoch 111/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6068 - accuracy: 0.6434 - val_loss: 0.8790 - val_accuracy: 0.4955\n",
      "Epoch 112/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.6056 - accuracy: 0.6444 - val_loss: 0.8513 - val_accuracy: 0.5045\n",
      "Epoch 113/1000\n",
      "1991/1991 [==============================] - 0s 87us/step - loss: 0.6070 - accuracy: 0.6459 - val_loss: 0.8513 - val_accuracy: 0.5315\n",
      "Epoch 114/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.6046 - accuracy: 0.6539 - val_loss: 0.8784 - val_accuracy: 0.4730\n",
      "Epoch 115/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.6068 - accuracy: 0.6519 - val_loss: 0.8078 - val_accuracy: 0.5315\n",
      "Epoch 116/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5935 - accuracy: 0.6620 - val_loss: 0.8505 - val_accuracy: 0.5135\n",
      "Epoch 117/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.5921 - accuracy: 0.6489 - val_loss: 0.8919 - val_accuracy: 0.5090\n",
      "Epoch 118/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5935 - accuracy: 0.6539 - val_loss: 0.8762 - val_accuracy: 0.5360\n",
      "Epoch 119/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5956 - accuracy: 0.6560 - val_loss: 0.8338 - val_accuracy: 0.4910\n",
      "Epoch 120/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.5825 - accuracy: 0.6700 - val_loss: 0.8724 - val_accuracy: 0.5270\n",
      "Epoch 121/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5730 - accuracy: 0.6765 - val_loss: 0.9802 - val_accuracy: 0.4820\n",
      "Epoch 122/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5862 - accuracy: 0.6544 - val_loss: 0.7746 - val_accuracy: 0.5721\n",
      "Epoch 123/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5890 - accuracy: 0.6655 - val_loss: 0.8821 - val_accuracy: 0.5135\n",
      "Epoch 124/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5806 - accuracy: 0.6690 - val_loss: 0.8962 - val_accuracy: 0.5315\n",
      "Epoch 125/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5712 - accuracy: 0.6715 - val_loss: 0.9990 - val_accuracy: 0.4865\n",
      "Epoch 126/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5672 - accuracy: 0.6781 - val_loss: 0.8713 - val_accuracy: 0.5090\n",
      "Epoch 127/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.5652 - accuracy: 0.6750 - val_loss: 0.9245 - val_accuracy: 0.5045\n",
      "Epoch 128/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.5638 - accuracy: 0.6816 - val_loss: 0.9005 - val_accuracy: 0.4820\n",
      "Epoch 129/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.5572 - accuracy: 0.6891 - val_loss: 1.0167 - val_accuracy: 0.5270\n",
      "Epoch 130/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5578 - accuracy: 0.6851 - val_loss: 0.8367 - val_accuracy: 0.5405\n",
      "Epoch 131/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.5642 - accuracy: 0.6806 - val_loss: 1.1225 - val_accuracy: 0.4640\n",
      "Epoch 132/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.5599 - accuracy: 0.6891 - val_loss: 1.0350 - val_accuracy: 0.4775\n",
      "Epoch 133/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5518 - accuracy: 0.6791 - val_loss: 0.9967 - val_accuracy: 0.5000\n",
      "Epoch 134/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5355 - accuracy: 0.7067 - val_loss: 1.0473 - val_accuracy: 0.4685\n",
      "Epoch 135/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.5518 - accuracy: 0.6926 - val_loss: 0.9731 - val_accuracy: 0.4910\n",
      "Epoch 136/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5392 - accuracy: 0.7027 - val_loss: 1.0436 - val_accuracy: 0.4640\n",
      "Epoch 137/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5229 - accuracy: 0.7097 - val_loss: 1.1064 - val_accuracy: 0.4685\n",
      "Epoch 138/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.5244 - accuracy: 0.7182 - val_loss: 1.0478 - val_accuracy: 0.4865\n",
      "Epoch 139/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5375 - accuracy: 0.7067 - val_loss: 0.9570 - val_accuracy: 0.5135\n",
      "Epoch 140/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.5302 - accuracy: 0.7087 - val_loss: 1.0493 - val_accuracy: 0.5180\n",
      "Epoch 141/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.5209 - accuracy: 0.7042 - val_loss: 1.1489 - val_accuracy: 0.4595\n",
      "Epoch 142/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5382 - accuracy: 0.7082 - val_loss: 0.9750 - val_accuracy: 0.4910\n",
      "Epoch 143/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.5224 - accuracy: 0.7202 - val_loss: 1.1524 - val_accuracy: 0.5000\n",
      "Epoch 144/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5046 - accuracy: 0.7243 - val_loss: 1.0260 - val_accuracy: 0.5405\n",
      "Epoch 145/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4945 - accuracy: 0.7293 - val_loss: 1.0310 - val_accuracy: 0.5135\n",
      "Epoch 146/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.5009 - accuracy: 0.7243 - val_loss: 1.0762 - val_accuracy: 0.5135\n",
      "Epoch 147/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.4922 - accuracy: 0.7353 - val_loss: 1.1486 - val_accuracy: 0.4775\n",
      "Epoch 148/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.4809 - accuracy: 0.7343 - val_loss: 1.1298 - val_accuracy: 0.4910\n",
      "Epoch 149/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.4722 - accuracy: 0.7519 - val_loss: 1.0957 - val_accuracy: 0.5450\n",
      "Epoch 150/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.4901 - accuracy: 0.7363 - val_loss: 1.1952 - val_accuracy: 0.4955\n",
      "Epoch 151/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.4692 - accuracy: 0.7438 - val_loss: 1.1345 - val_accuracy: 0.5541\n",
      "Epoch 152/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.4876 - accuracy: 0.7408 - val_loss: 1.1751 - val_accuracy: 0.5135\n",
      "Epoch 153/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4739 - accuracy: 0.7388 - val_loss: 1.3066 - val_accuracy: 0.4910\n",
      "Epoch 154/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.4507 - accuracy: 0.7509 - val_loss: 1.2575 - val_accuracy: 0.4865\n",
      "Epoch 155/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.4475 - accuracy: 0.7619 - val_loss: 1.1957 - val_accuracy: 0.5045\n",
      "Epoch 156/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.4575 - accuracy: 0.7599 - val_loss: 1.1355 - val_accuracy: 0.5270\n",
      "Epoch 157/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4486 - accuracy: 0.7484 - val_loss: 1.3636 - val_accuracy: 0.4685\n",
      "Epoch 158/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4329 - accuracy: 0.7800 - val_loss: 1.0923 - val_accuracy: 0.5631\n",
      "Epoch 159/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4486 - accuracy: 0.7584 - val_loss: 1.4422 - val_accuracy: 0.4730\n",
      "Epoch 160/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.4467 - accuracy: 0.7644 - val_loss: 1.2449 - val_accuracy: 0.4820\n",
      "Epoch 161/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.4294 - accuracy: 0.7624 - val_loss: 1.2580 - val_accuracy: 0.5090\n",
      "Epoch 162/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.4126 - accuracy: 0.7835 - val_loss: 1.4442 - val_accuracy: 0.5045\n",
      "Epoch 163/1000\n",
      "1991/1991 [==============================] - 0s 91us/step - loss: 0.4182 - accuracy: 0.7740 - val_loss: 1.4445 - val_accuracy: 0.5000\n",
      "Epoch 164/1000\n",
      "1991/1991 [==============================] - 0s 93us/step - loss: 0.4327 - accuracy: 0.7715 - val_loss: 1.3832 - val_accuracy: 0.4955\n",
      "Epoch 165/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.4043 - accuracy: 0.7906 - val_loss: 1.3514 - val_accuracy: 0.5000\n",
      "Epoch 166/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.3968 - accuracy: 0.7951 - val_loss: 1.4267 - val_accuracy: 0.4775\n",
      "Epoch 167/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 0.3872 - accuracy: 0.7996 - val_loss: 1.5166 - val_accuracy: 0.4775\n",
      "Epoch 168/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.3869 - accuracy: 0.8011 - val_loss: 1.4841 - val_accuracy: 0.4775\n",
      "Epoch 169/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.3919 - accuracy: 0.7916 - val_loss: 1.4624 - val_accuracy: 0.5180\n",
      "Epoch 170/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.3927 - accuracy: 0.7891 - val_loss: 1.5455 - val_accuracy: 0.4910\n",
      "Epoch 171/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.3627 - accuracy: 0.8071 - val_loss: 1.5244 - val_accuracy: 0.4865\n",
      "Epoch 172/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.3663 - accuracy: 0.8011 - val_loss: 1.5125 - val_accuracy: 0.5000\n",
      "Epoch 173/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.3625 - accuracy: 0.8086 - val_loss: 1.4251 - val_accuracy: 0.4955\n",
      "Epoch 174/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.3669 - accuracy: 0.8137 - val_loss: 1.5586 - val_accuracy: 0.4775\n",
      "Epoch 175/1000\n",
      "1991/1991 [==============================] - 0s 87us/step - loss: 0.3939 - accuracy: 0.8041 - val_loss: 1.3618 - val_accuracy: 0.4775\n",
      "Epoch 176/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.3892 - accuracy: 0.8011 - val_loss: 1.5594 - val_accuracy: 0.4730\n",
      "Epoch 177/1000\n",
      "1991/1991 [==============================] - 0s 98us/step - loss: 0.3601 - accuracy: 0.8006 - val_loss: 1.5330 - val_accuracy: 0.5000\n",
      "Epoch 178/1000\n",
      "1991/1991 [==============================] - 0s 92us/step - loss: 0.3434 - accuracy: 0.8282 - val_loss: 1.6033 - val_accuracy: 0.5180\n",
      "Epoch 179/1000\n",
      "1991/1991 [==============================] - 0s 84us/step - loss: 0.3447 - accuracy: 0.8162 - val_loss: 1.4773 - val_accuracy: 0.5180\n",
      "Epoch 180/1000\n",
      "1991/1991 [==============================] - 0s 85us/step - loss: 0.3437 - accuracy: 0.8207 - val_loss: 1.7838 - val_accuracy: 0.5000\n",
      "Epoch 181/1000\n",
      "1991/1991 [==============================] - 0s 92us/step - loss: 0.3473 - accuracy: 0.8282 - val_loss: 1.6463 - val_accuracy: 0.5045\n",
      "Epoch 182/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.3397 - accuracy: 0.8363 - val_loss: 1.5133 - val_accuracy: 0.5000\n",
      "Epoch 183/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 0.3230 - accuracy: 0.8378 - val_loss: 1.6295 - val_accuracy: 0.4865\n",
      "Epoch 184/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.3236 - accuracy: 0.8332 - val_loss: 1.9671 - val_accuracy: 0.4775\n",
      "Epoch 185/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.3411 - accuracy: 0.8322 - val_loss: 1.5970 - val_accuracy: 0.5135\n",
      "Epoch 186/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.3611 - accuracy: 0.8167 - val_loss: 1.5864 - val_accuracy: 0.4910\n",
      "Epoch 187/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.3259 - accuracy: 0.8378 - val_loss: 1.8988 - val_accuracy: 0.4505\n",
      "Epoch 188/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.3000 - accuracy: 0.8463 - val_loss: 1.7125 - val_accuracy: 0.4865\n",
      "Epoch 189/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.2915 - accuracy: 0.8564 - val_loss: 1.7882 - val_accuracy: 0.5000\n",
      "Epoch 190/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.2868 - accuracy: 0.8599 - val_loss: 2.1914 - val_accuracy: 0.4910\n",
      "Epoch 191/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.2837 - accuracy: 0.8594 - val_loss: 1.9092 - val_accuracy: 0.5405\n",
      "Epoch 192/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.2894 - accuracy: 0.8559 - val_loss: 1.8336 - val_accuracy: 0.5090\n",
      "Epoch 193/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.2767 - accuracy: 0.8649 - val_loss: 1.9686 - val_accuracy: 0.5000\n",
      "Epoch 194/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.2626 - accuracy: 0.8689 - val_loss: 2.0268 - val_accuracy: 0.4775\n",
      "Epoch 195/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.2712 - accuracy: 0.8699 - val_loss: 1.9646 - val_accuracy: 0.4820\n",
      "Epoch 196/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2557 - accuracy: 0.8774 - val_loss: 2.0046 - val_accuracy: 0.4820\n",
      "Epoch 197/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2480 - accuracy: 0.8815 - val_loss: 1.8606 - val_accuracy: 0.5000\n",
      "Epoch 198/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2468 - accuracy: 0.8719 - val_loss: 1.8491 - val_accuracy: 0.5045\n",
      "Epoch 199/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2479 - accuracy: 0.8830 - val_loss: 2.0877 - val_accuracy: 0.5045\n",
      "Epoch 200/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2279 - accuracy: 0.8935 - val_loss: 2.0168 - val_accuracy: 0.4910\n",
      "Epoch 201/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2154 - accuracy: 0.8995 - val_loss: 2.1563 - val_accuracy: 0.4685\n",
      "Epoch 202/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.2102 - accuracy: 0.9016 - val_loss: 2.3479 - val_accuracy: 0.4955\n",
      "Epoch 203/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2118 - accuracy: 0.8985 - val_loss: 2.2331 - val_accuracy: 0.5135\n",
      "Epoch 204/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2112 - accuracy: 0.9021 - val_loss: 2.0577 - val_accuracy: 0.5000\n",
      "Epoch 205/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2252 - accuracy: 0.8935 - val_loss: 2.3679 - val_accuracy: 0.4820\n",
      "Epoch 206/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2118 - accuracy: 0.8985 - val_loss: 2.2374 - val_accuracy: 0.4685\n",
      "Epoch 207/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.1998 - accuracy: 0.9046 - val_loss: 2.4144 - val_accuracy: 0.4955\n",
      "Epoch 208/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2170 - accuracy: 0.9016 - val_loss: 2.2319 - val_accuracy: 0.4775\n",
      "Epoch 209/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1943 - accuracy: 0.9116 - val_loss: 2.3942 - val_accuracy: 0.4685\n",
      "Epoch 210/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.1988 - accuracy: 0.9051 - val_loss: 2.2616 - val_accuracy: 0.5000\n",
      "Epoch 211/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.1754 - accuracy: 0.9221 - val_loss: 2.4007 - val_accuracy: 0.5045\n",
      "Epoch 212/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1703 - accuracy: 0.9252 - val_loss: 2.3370 - val_accuracy: 0.5090\n",
      "Epoch 213/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1684 - accuracy: 0.9216 - val_loss: 2.3609 - val_accuracy: 0.4865\n",
      "Epoch 214/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1896 - accuracy: 0.9121 - val_loss: 2.3847 - val_accuracy: 0.5180\n",
      "Epoch 215/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1960 - accuracy: 0.9101 - val_loss: 2.7424 - val_accuracy: 0.4820\n",
      "Epoch 216/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1859 - accuracy: 0.9146 - val_loss: 2.3810 - val_accuracy: 0.5000\n",
      "Epoch 217/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.3040 - accuracy: 0.8805 - val_loss: 2.3664 - val_accuracy: 0.4910\n",
      "Epoch 218/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2514 - accuracy: 0.8825 - val_loss: 2.2155 - val_accuracy: 0.4910\n",
      "Epoch 219/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.2714 - accuracy: 0.8724 - val_loss: 2.5505 - val_accuracy: 0.4640\n",
      "Epoch 220/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.2032 - accuracy: 0.9071 - val_loss: 2.4113 - val_accuracy: 0.4550\n",
      "Epoch 221/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1752 - accuracy: 0.9191 - val_loss: 2.3828 - val_accuracy: 0.4685\n",
      "Epoch 222/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1541 - accuracy: 0.9327 - val_loss: 2.3953 - val_accuracy: 0.4730\n",
      "Epoch 223/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1633 - accuracy: 0.9267 - val_loss: 2.4023 - val_accuracy: 0.4820\n",
      "Epoch 224/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1490 - accuracy: 0.9302 - val_loss: 2.3291 - val_accuracy: 0.4550\n",
      "Epoch 225/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1396 - accuracy: 0.9392 - val_loss: 2.6581 - val_accuracy: 0.4775\n",
      "Epoch 226/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1384 - accuracy: 0.9347 - val_loss: 2.6431 - val_accuracy: 0.5180\n",
      "Epoch 227/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.1259 - accuracy: 0.9473 - val_loss: 2.4259 - val_accuracy: 0.5090\n",
      "Epoch 228/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.1254 - accuracy: 0.9432 - val_loss: 2.5703 - val_accuracy: 0.4910\n",
      "Epoch 229/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.1257 - accuracy: 0.9437 - val_loss: 2.5908 - val_accuracy: 0.4865\n",
      "Epoch 230/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1169 - accuracy: 0.9473 - val_loss: 2.5358 - val_accuracy: 0.5000\n",
      "Epoch 231/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1261 - accuracy: 0.9442 - val_loss: 2.5007 - val_accuracy: 0.4595\n",
      "Epoch 232/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.1265 - accuracy: 0.9412 - val_loss: 2.8292 - val_accuracy: 0.4459\n",
      "Epoch 233/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.1218 - accuracy: 0.9448 - val_loss: 2.6218 - val_accuracy: 0.4865\n",
      "Epoch 234/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.1078 - accuracy: 0.9553 - val_loss: 2.9229 - val_accuracy: 0.4505\n",
      "Epoch 235/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.1058 - accuracy: 0.9558 - val_loss: 2.6570 - val_accuracy: 0.4685\n",
      "Epoch 236/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.1007 - accuracy: 0.9623 - val_loss: 2.9096 - val_accuracy: 0.4595\n",
      "Epoch 237/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0909 - accuracy: 0.9628 - val_loss: 3.0406 - val_accuracy: 0.4459\n",
      "Epoch 238/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0991 - accuracy: 0.9613 - val_loss: 2.9507 - val_accuracy: 0.4595\n",
      "Epoch 239/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0963 - accuracy: 0.9618 - val_loss: 2.9097 - val_accuracy: 0.4595\n",
      "Epoch 240/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0960 - accuracy: 0.9613 - val_loss: 2.7658 - val_accuracy: 0.4820\n",
      "Epoch 241/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0925 - accuracy: 0.9618 - val_loss: 2.8592 - val_accuracy: 0.4369\n",
      "Epoch 242/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.1249 - accuracy: 0.9463 - val_loss: 2.8395 - val_accuracy: 0.4685\n",
      "Epoch 243/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1557 - accuracy: 0.9337 - val_loss: 3.0727 - val_accuracy: 0.4414\n",
      "Epoch 244/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1423 - accuracy: 0.9397 - val_loss: 2.9412 - val_accuracy: 0.4414\n",
      "Epoch 245/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1811 - accuracy: 0.9242 - val_loss: 3.1041 - val_accuracy: 0.4640\n",
      "Epoch 246/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1481 - accuracy: 0.9367 - val_loss: 2.9614 - val_accuracy: 0.4820\n",
      "Epoch 247/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1333 - accuracy: 0.9458 - val_loss: 2.7609 - val_accuracy: 0.4640\n",
      "Epoch 248/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.1162 - accuracy: 0.9558 - val_loss: 2.9329 - val_accuracy: 0.4955\n",
      "Epoch 249/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.1536 - accuracy: 0.9297 - val_loss: 2.9442 - val_accuracy: 0.4505\n",
      "Epoch 250/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.1014 - accuracy: 0.9598 - val_loss: 2.8709 - val_accuracy: 0.4550\n",
      "Epoch 251/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0802 - accuracy: 0.9714 - val_loss: 2.8916 - val_accuracy: 0.4640\n",
      "Epoch 252/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0717 - accuracy: 0.9734 - val_loss: 2.9432 - val_accuracy: 0.4730\n",
      "Epoch 253/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0671 - accuracy: 0.9729 - val_loss: 3.1571 - val_accuracy: 0.4595\n",
      "Epoch 254/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0686 - accuracy: 0.9729 - val_loss: 2.9323 - val_accuracy: 0.4640\n",
      "Epoch 255/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0658 - accuracy: 0.9779 - val_loss: 3.0132 - val_accuracy: 0.4775\n",
      "Epoch 256/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0682 - accuracy: 0.9729 - val_loss: 3.1058 - val_accuracy: 0.4685\n",
      "Epoch 257/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0589 - accuracy: 0.9789 - val_loss: 3.2389 - val_accuracy: 0.4820\n",
      "Epoch 258/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0688 - accuracy: 0.9709 - val_loss: 3.2666 - val_accuracy: 0.4550\n",
      "Epoch 259/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0634 - accuracy: 0.9759 - val_loss: 2.8962 - val_accuracy: 0.4910\n",
      "Epoch 260/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0643 - accuracy: 0.9774 - val_loss: 3.0496 - val_accuracy: 0.4685\n",
      "Epoch 261/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0656 - accuracy: 0.9764 - val_loss: 3.1212 - val_accuracy: 0.4820\n",
      "Epoch 262/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0607 - accuracy: 0.9789 - val_loss: 3.2682 - val_accuracy: 0.4550\n",
      "Epoch 263/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0578 - accuracy: 0.9789 - val_loss: 3.3061 - val_accuracy: 0.4505\n",
      "Epoch 264/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0515 - accuracy: 0.9839 - val_loss: 3.2566 - val_accuracy: 0.4595\n",
      "Epoch 265/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0454 - accuracy: 0.9834 - val_loss: 3.2222 - val_accuracy: 0.4820\n",
      "Epoch 266/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0456 - accuracy: 0.9844 - val_loss: 3.4036 - val_accuracy: 0.4775\n",
      "Epoch 267/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 3.4670 - val_accuracy: 0.4640\n",
      "Epoch 268/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0463 - accuracy: 0.9839 - val_loss: 3.3354 - val_accuracy: 0.4730\n",
      "Epoch 269/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0521 - accuracy: 0.9819 - val_loss: 3.3372 - val_accuracy: 0.4730\n",
      "Epoch 270/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0465 - accuracy: 0.9844 - val_loss: 3.1580 - val_accuracy: 0.4955\n",
      "Epoch 271/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0572 - accuracy: 0.9829 - val_loss: 3.5745 - val_accuracy: 0.4550\n",
      "Epoch 272/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0407 - accuracy: 0.9884 - val_loss: 3.5753 - val_accuracy: 0.4640\n",
      "Epoch 273/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0421 - accuracy: 0.9874 - val_loss: 3.4361 - val_accuracy: 0.4775\n",
      "Epoch 274/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0773 - accuracy: 0.9744 - val_loss: 3.4450 - val_accuracy: 0.4775\n",
      "Epoch 275/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1818 - accuracy: 0.9277 - val_loss: 3.2149 - val_accuracy: 0.4865\n",
      "Epoch 276/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.2391 - accuracy: 0.9101 - val_loss: 3.3608 - val_accuracy: 0.4369\n",
      "Epoch 277/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.2924 - accuracy: 0.9021 - val_loss: 3.0207 - val_accuracy: 0.5090\n",
      "Epoch 278/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.2022 - accuracy: 0.9216 - val_loss: 2.8092 - val_accuracy: 0.4685\n",
      "Epoch 279/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.1143 - accuracy: 0.9493 - val_loss: 2.7220 - val_accuracy: 0.5180\n",
      "Epoch 280/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0858 - accuracy: 0.9674 - val_loss: 3.0443 - val_accuracy: 0.4595\n",
      "Epoch 281/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0727 - accuracy: 0.9724 - val_loss: 3.3188 - val_accuracy: 0.4459\n",
      "Epoch 282/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0549 - accuracy: 0.9819 - val_loss: 3.0259 - val_accuracy: 0.4595\n",
      "Epoch 283/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0531 - accuracy: 0.9824 - val_loss: 3.2419 - val_accuracy: 0.4369\n",
      "Epoch 284/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.0489 - accuracy: 0.9839 - val_loss: 3.1858 - val_accuracy: 0.4595\n",
      "Epoch 285/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0491 - accuracy: 0.9854 - val_loss: 3.3575 - val_accuracy: 0.4505\n",
      "Epoch 286/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.0409 - accuracy: 0.9884 - val_loss: 3.3464 - val_accuracy: 0.4414\n",
      "Epoch 287/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.0335 - accuracy: 0.9910 - val_loss: 3.2874 - val_accuracy: 0.4685\n",
      "Epoch 288/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0307 - accuracy: 0.9930 - val_loss: 3.4231 - val_accuracy: 0.4730\n",
      "Epoch 289/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0347 - accuracy: 0.9879 - val_loss: 3.5325 - val_accuracy: 0.4910\n",
      "Epoch 290/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0268 - accuracy: 0.9915 - val_loss: 3.6094 - val_accuracy: 0.4640\n",
      "Epoch 291/1000\n",
      "1991/1991 [==============================] - 0s 83us/step - loss: 0.0223 - accuracy: 0.9960 - val_loss: 3.5354 - val_accuracy: 0.4685\n",
      "Epoch 292/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0235 - accuracy: 0.9945 - val_loss: 3.6401 - val_accuracy: 0.4595\n",
      "Epoch 293/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.0251 - accuracy: 0.9930 - val_loss: 3.5318 - val_accuracy: 0.4685\n",
      "Epoch 294/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0200 - accuracy: 0.9950 - val_loss: 3.5660 - val_accuracy: 0.4775\n",
      "Epoch 295/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 3.4728 - val_accuracy: 0.4910\n",
      "Epoch 296/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0195 - accuracy: 0.9945 - val_loss: 3.6987 - val_accuracy: 0.4685\n",
      "Epoch 297/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.0225 - accuracy: 0.9925 - val_loss: 3.7773 - val_accuracy: 0.4685\n",
      "Epoch 298/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0269 - accuracy: 0.9905 - val_loss: 3.7102 - val_accuracy: 0.4459\n",
      "Epoch 299/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0275 - accuracy: 0.9930 - val_loss: 3.7818 - val_accuracy: 0.4730\n",
      "Epoch 300/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0282 - accuracy: 0.9920 - val_loss: 3.5984 - val_accuracy: 0.4730\n",
      "Epoch 301/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0334 - accuracy: 0.9890 - val_loss: 3.6033 - val_accuracy: 0.5000\n",
      "Epoch 302/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0472 - accuracy: 0.9824 - val_loss: 3.9747 - val_accuracy: 0.4414\n",
      "Epoch 303/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0437 - accuracy: 0.9859 - val_loss: 3.5724 - val_accuracy: 0.4820\n",
      "Epoch 304/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0324 - accuracy: 0.9864 - val_loss: 3.7181 - val_accuracy: 0.4685\n",
      "Epoch 305/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0311 - accuracy: 0.9915 - val_loss: 3.8947 - val_accuracy: 0.4730\n",
      "Epoch 306/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0224 - accuracy: 0.9935 - val_loss: 3.9662 - val_accuracy: 0.4775\n",
      "Epoch 307/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0254 - accuracy: 0.9930 - val_loss: 4.0038 - val_accuracy: 0.4685\n",
      "Epoch 308/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0259 - accuracy: 0.9925 - val_loss: 3.9059 - val_accuracy: 0.4775\n",
      "Epoch 309/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 0.0162 - accuracy: 0.9965 - val_loss: 4.1274 - val_accuracy: 0.4369\n",
      "Epoch 310/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0201 - accuracy: 0.9940 - val_loss: 3.8639 - val_accuracy: 0.4865\n",
      "Epoch 311/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0172 - accuracy: 0.9950 - val_loss: 3.9880 - val_accuracy: 0.4730\n",
      "Epoch 312/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0203 - accuracy: 0.9930 - val_loss: 4.2837 - val_accuracy: 0.4550\n",
      "Epoch 313/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0196 - accuracy: 0.9940 - val_loss: 3.8785 - val_accuracy: 0.4595\n",
      "Epoch 314/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0136 - accuracy: 0.9975 - val_loss: 3.8958 - val_accuracy: 0.4640\n",
      "Epoch 315/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0111 - accuracy: 0.9980 - val_loss: 4.2061 - val_accuracy: 0.4595\n",
      "Epoch 316/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0098 - accuracy: 0.9980 - val_loss: 4.1377 - val_accuracy: 0.4595\n",
      "Epoch 317/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0088 - accuracy: 0.9990 - val_loss: 4.1454 - val_accuracy: 0.4730\n",
      "Epoch 318/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 4.2496 - val_accuracy: 0.4775\n",
      "Epoch 319/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0079 - accuracy: 0.9990 - val_loss: 4.1343 - val_accuracy: 0.4685\n",
      "Epoch 320/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0070 - accuracy: 0.9990 - val_loss: 4.1833 - val_accuracy: 0.4685\n",
      "Epoch 321/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 4.2020 - val_accuracy: 0.4595\n",
      "Epoch 322/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 4.2202 - val_accuracy: 0.4730\n",
      "Epoch 323/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 4.2827 - val_accuracy: 0.4640\n",
      "Epoch 324/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 4.2115 - val_accuracy: 0.4595\n",
      "Epoch 325/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 4.2326 - val_accuracy: 0.4820\n",
      "Epoch 326/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 4.3583 - val_accuracy: 0.4685\n",
      "Epoch 327/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 4.3657 - val_accuracy: 0.4730\n",
      "Epoch 328/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 4.2481 - val_accuracy: 0.4550\n",
      "Epoch 329/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 4.3161 - val_accuracy: 0.4730\n",
      "Epoch 330/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 4.3974 - val_accuracy: 0.4640\n",
      "Epoch 331/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 4.3662 - val_accuracy: 0.4775\n",
      "Epoch 332/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 4.2996 - val_accuracy: 0.4685\n",
      "Epoch 333/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 4.3798 - val_accuracy: 0.4685\n",
      "Epoch 334/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 4.2659 - val_accuracy: 0.4505\n",
      "Epoch 335/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 4.3653 - val_accuracy: 0.4640\n",
      "Epoch 336/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 4.4885 - val_accuracy: 0.4595\n",
      "Epoch 337/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 4.2806 - val_accuracy: 0.4550\n",
      "Epoch 338/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 4.4012 - val_accuracy: 0.4550\n",
      "Epoch 339/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 4.2792 - val_accuracy: 0.4550\n",
      "Epoch 340/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0088 - accuracy: 0.9980 - val_loss: 4.2643 - val_accuracy: 0.4595\n",
      "Epoch 341/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 4.1492 - val_accuracy: 0.4550\n",
      "Epoch 342/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0289 - accuracy: 0.9910 - val_loss: 4.4506 - val_accuracy: 0.4820\n",
      "Epoch 343/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0398 - accuracy: 0.9834 - val_loss: 4.3439 - val_accuracy: 0.4505\n",
      "Epoch 344/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.1183 - accuracy: 0.9658 - val_loss: 3.8214 - val_accuracy: 0.4685\n",
      "Epoch 345/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.3076 - accuracy: 0.9121 - val_loss: 3.9643 - val_accuracy: 0.4730\n",
      "Epoch 346/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.3965 - accuracy: 0.8704 - val_loss: 2.6574 - val_accuracy: 0.5135\n",
      "Epoch 347/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.2365 - accuracy: 0.9091 - val_loss: 3.5678 - val_accuracy: 0.4414\n",
      "Epoch 348/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.1687 - accuracy: 0.9453 - val_loss: 3.4551 - val_accuracy: 0.4279\n",
      "Epoch 349/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0777 - accuracy: 0.9749 - val_loss: 3.4205 - val_accuracy: 0.4414\n",
      "Epoch 350/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0447 - accuracy: 0.9879 - val_loss: 3.6250 - val_accuracy: 0.4369\n",
      "Epoch 351/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0289 - accuracy: 0.9945 - val_loss: 3.7655 - val_accuracy: 0.4550\n",
      "Epoch 352/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0192 - accuracy: 0.9970 - val_loss: 3.7618 - val_accuracy: 0.4369\n",
      "Epoch 353/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 3.7720 - val_accuracy: 0.4369\n",
      "Epoch 354/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0092 - accuracy: 0.9995 - val_loss: 3.8363 - val_accuracy: 0.4414\n",
      "Epoch 355/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0077 - accuracy: 0.9995 - val_loss: 3.9151 - val_accuracy: 0.4640\n",
      "Epoch 356/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 3.9469 - val_accuracy: 0.4505\n",
      "Epoch 357/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 3.9744 - val_accuracy: 0.4595\n",
      "Epoch 358/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 4.0100 - val_accuracy: 0.4640\n",
      "Epoch 359/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0052 - accuracy: 0.9995 - val_loss: 4.0552 - val_accuracy: 0.4640\n",
      "Epoch 360/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 4.0252 - val_accuracy: 0.4595\n",
      "Epoch 361/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 4.0860 - val_accuracy: 0.4640\n",
      "Epoch 362/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 4.0701 - val_accuracy: 0.4685\n",
      "Epoch 363/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 4.0923 - val_accuracy: 0.4685\n",
      "Epoch 364/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 4.1175 - val_accuracy: 0.4685\n",
      "Epoch 365/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 4.1293 - val_accuracy: 0.4640\n",
      "Epoch 366/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.1602 - val_accuracy: 0.4685\n",
      "Epoch 367/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 4.1680 - val_accuracy: 0.4550\n",
      "Epoch 368/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 4.1645 - val_accuracy: 0.4595\n",
      "Epoch 369/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.2192 - val_accuracy: 0.4640\n",
      "Epoch 370/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 4.2507 - val_accuracy: 0.4550\n",
      "Epoch 371/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 4.2567 - val_accuracy: 0.4640\n",
      "Epoch 372/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.2897 - val_accuracy: 0.4685\n",
      "Epoch 373/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.2946 - val_accuracy: 0.4640\n",
      "Epoch 374/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.3033 - val_accuracy: 0.4595\n",
      "Epoch 375/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.3337 - val_accuracy: 0.4685\n",
      "Epoch 376/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.3346 - val_accuracy: 0.4595\n",
      "Epoch 377/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.3328 - val_accuracy: 0.4505\n",
      "Epoch 378/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.3622 - val_accuracy: 0.4730\n",
      "Epoch 379/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.3835 - val_accuracy: 0.4730\n",
      "Epoch 380/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.3704 - val_accuracy: 0.4640\n",
      "Epoch 381/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.3998 - val_accuracy: 0.4640\n",
      "Epoch 382/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.4212 - val_accuracy: 0.4595\n",
      "Epoch 383/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.4328 - val_accuracy: 0.4640\n",
      "Epoch 384/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.3632 - val_accuracy: 0.4640\n",
      "Epoch 385/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.4513 - val_accuracy: 0.4685\n",
      "Epoch 386/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.4530 - val_accuracy: 0.4640\n",
      "Epoch 387/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.4757 - val_accuracy: 0.4595\n",
      "Epoch 388/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.4890 - val_accuracy: 0.4640\n",
      "Epoch 389/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.4723 - val_accuracy: 0.4640\n",
      "Epoch 390/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.5038 - val_accuracy: 0.4595\n",
      "Epoch 391/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.4936 - val_accuracy: 0.4685\n",
      "Epoch 392/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.5162 - val_accuracy: 0.4640\n",
      "Epoch 393/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.5331 - val_accuracy: 0.4685\n",
      "Epoch 394/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.5375 - val_accuracy: 0.4550\n",
      "Epoch 395/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.5443 - val_accuracy: 0.4550\n",
      "Epoch 396/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.5664 - val_accuracy: 0.4550\n",
      "Epoch 397/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.5581 - val_accuracy: 0.4550\n",
      "Epoch 398/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.6116 - val_accuracy: 0.4685\n",
      "Epoch 399/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.5452 - val_accuracy: 0.4505\n",
      "Epoch 400/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.5953 - val_accuracy: 0.4640\n",
      "Epoch 401/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.5889 - val_accuracy: 0.4550\n",
      "Epoch 402/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.5960 - val_accuracy: 0.4550\n",
      "Epoch 403/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.6199 - val_accuracy: 0.4730\n",
      "Epoch 404/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.5922 - val_accuracy: 0.4595\n",
      "Epoch 405/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.6425 - val_accuracy: 0.4685\n",
      "Epoch 406/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.6473 - val_accuracy: 0.4595\n",
      "Epoch 407/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6415 - val_accuracy: 0.4595\n",
      "Epoch 408/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6772 - val_accuracy: 0.4640\n",
      "Epoch 409/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.5304e-04 - accuracy: 1.0000 - val_loss: 4.6666 - val_accuracy: 0.4595\n",
      "Epoch 410/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.9247e-04 - accuracy: 1.0000 - val_loss: 4.6826 - val_accuracy: 0.4640\n",
      "Epoch 411/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.4449e-04 - accuracy: 1.0000 - val_loss: 4.6937 - val_accuracy: 0.4550\n",
      "Epoch 412/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 9.5975e-04 - accuracy: 1.0000 - val_loss: 4.7091 - val_accuracy: 0.4640\n",
      "Epoch 413/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 9.0070e-04 - accuracy: 1.0000 - val_loss: 4.7089 - val_accuracy: 0.4550\n",
      "Epoch 414/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 9.6735e-04 - accuracy: 1.0000 - val_loss: 4.7155 - val_accuracy: 0.4550\n",
      "Epoch 415/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.6182e-04 - accuracy: 1.0000 - val_loss: 4.7320 - val_accuracy: 0.4550\n",
      "Epoch 416/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.4449e-04 - accuracy: 1.0000 - val_loss: 4.7316 - val_accuracy: 0.4595\n",
      "Epoch 417/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.2456e-04 - accuracy: 1.0000 - val_loss: 4.7377 - val_accuracy: 0.4595\n",
      "Epoch 418/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 8.2514e-04 - accuracy: 1.0000 - val_loss: 4.7501 - val_accuracy: 0.4640\n",
      "Epoch 419/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.2305e-04 - accuracy: 1.0000 - val_loss: 4.7584 - val_accuracy: 0.4550\n",
      "Epoch 420/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 7.6424e-04 - accuracy: 1.0000 - val_loss: 4.7695 - val_accuracy: 0.4595\n",
      "Epoch 421/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 8.6323e-04 - accuracy: 1.0000 - val_loss: 4.7710 - val_accuracy: 0.4595\n",
      "Epoch 422/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 7.6377e-04 - accuracy: 1.0000 - val_loss: 4.7818 - val_accuracy: 0.4640\n",
      "Epoch 423/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.7084e-04 - accuracy: 1.0000 - val_loss: 4.8107 - val_accuracy: 0.4640\n",
      "Epoch 424/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.1213e-04 - accuracy: 1.0000 - val_loss: 4.7988 - val_accuracy: 0.4640\n",
      "Epoch 425/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 7.7380e-04 - accuracy: 1.0000 - val_loss: 4.8086 - val_accuracy: 0.4595\n",
      "Epoch 426/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 6.8453e-04 - accuracy: 1.0000 - val_loss: 4.8170 - val_accuracy: 0.4640\n",
      "Epoch 427/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.6371e-04 - accuracy: 1.0000 - val_loss: 4.8219 - val_accuracy: 0.4640\n",
      "Epoch 428/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 6.6048e-04 - accuracy: 1.0000 - val_loss: 4.8273 - val_accuracy: 0.4595\n",
      "Epoch 429/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 7.3277e-04 - accuracy: 1.0000 - val_loss: 4.8353 - val_accuracy: 0.4640\n",
      "Epoch 430/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 7.8540e-04 - accuracy: 1.0000 - val_loss: 4.8260 - val_accuracy: 0.4640\n",
      "Epoch 431/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.5989e-04 - accuracy: 1.0000 - val_loss: 4.8420 - val_accuracy: 0.4730\n",
      "Epoch 432/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 7.5087e-04 - accuracy: 1.0000 - val_loss: 4.8378 - val_accuracy: 0.4640\n",
      "Epoch 433/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.6046e-04 - accuracy: 1.0000 - val_loss: 4.8487 - val_accuracy: 0.4640\n",
      "Epoch 434/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.1101e-04 - accuracy: 1.0000 - val_loss: 4.8574 - val_accuracy: 0.4640\n",
      "Epoch 435/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 5.9923e-04 - accuracy: 1.0000 - val_loss: 4.8703 - val_accuracy: 0.4685\n",
      "Epoch 436/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 6.3060e-04 - accuracy: 1.0000 - val_loss: 4.8816 - val_accuracy: 0.4640\n",
      "Epoch 437/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.8097e-04 - accuracy: 1.0000 - val_loss: 4.8890 - val_accuracy: 0.4640\n",
      "Epoch 438/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.0771e-04 - accuracy: 1.0000 - val_loss: 4.8975 - val_accuracy: 0.4640\n",
      "Epoch 439/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.6760e-04 - accuracy: 1.0000 - val_loss: 4.9106 - val_accuracy: 0.4595\n",
      "Epoch 440/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.0776e-04 - accuracy: 1.0000 - val_loss: 4.9264 - val_accuracy: 0.4595\n",
      "Epoch 441/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.3980e-04 - accuracy: 1.0000 - val_loss: 4.9215 - val_accuracy: 0.4640\n",
      "Epoch 442/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.3859e-04 - accuracy: 1.0000 - val_loss: 4.9168 - val_accuracy: 0.4595\n",
      "Epoch 443/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 5.7444e-04 - accuracy: 1.0000 - val_loss: 4.9330 - val_accuracy: 0.4640\n",
      "Epoch 444/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.0314e-04 - accuracy: 1.0000 - val_loss: 4.9412 - val_accuracy: 0.4640\n",
      "Epoch 445/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.2658e-04 - accuracy: 1.0000 - val_loss: 4.9504 - val_accuracy: 0.4595\n",
      "Epoch 446/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.1838e-04 - accuracy: 1.0000 - val_loss: 4.9248 - val_accuracy: 0.4595\n",
      "Epoch 447/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 5.2462e-04 - accuracy: 1.0000 - val_loss: 4.9434 - val_accuracy: 0.4640\n",
      "Epoch 448/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 5.2809e-04 - accuracy: 1.0000 - val_loss: 4.9455 - val_accuracy: 0.4685\n",
      "Epoch 449/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 4.6537e-04 - accuracy: 1.0000 - val_loss: 4.9605 - val_accuracy: 0.4640\n",
      "Epoch 450/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 5.6808e-04 - accuracy: 1.0000 - val_loss: 4.9694 - val_accuracy: 0.4640\n",
      "Epoch 451/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 5.6059e-04 - accuracy: 1.0000 - val_loss: 4.9631 - val_accuracy: 0.4640\n",
      "Epoch 452/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 4.6875e-04 - accuracy: 1.0000 - val_loss: 4.9941 - val_accuracy: 0.4640\n",
      "Epoch 453/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 4.8128e-04 - accuracy: 1.0000 - val_loss: 4.9990 - val_accuracy: 0.4685\n",
      "Epoch 454/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 4.6628e-04 - accuracy: 1.0000 - val_loss: 4.9980 - val_accuracy: 0.4595\n",
      "Epoch 455/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 4.4619e-04 - accuracy: 1.0000 - val_loss: 5.0099 - val_accuracy: 0.4640\n",
      "Epoch 456/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 5.2978e-04 - accuracy: 1.0000 - val_loss: 5.0138 - val_accuracy: 0.4685\n",
      "Epoch 457/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 4.3713e-04 - accuracy: 1.0000 - val_loss: 4.9973 - val_accuracy: 0.4640\n",
      "Epoch 458/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 4.2352e-04 - accuracy: 1.0000 - val_loss: 5.0237 - val_accuracy: 0.4640\n",
      "Epoch 459/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.9705e-04 - accuracy: 1.0000 - val_loss: 5.0083 - val_accuracy: 0.4550\n",
      "Epoch 460/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 4.4802e-04 - accuracy: 1.0000 - val_loss: 5.0286 - val_accuracy: 0.4685\n",
      "Epoch 461/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 5.8355e-04 - accuracy: 1.0000 - val_loss: 5.0480 - val_accuracy: 0.4595\n",
      "Epoch 462/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 5.2886e-04 - accuracy: 1.0000 - val_loss: 5.0749 - val_accuracy: 0.4595\n",
      "Epoch 463/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 4.4288e-04 - accuracy: 1.0000 - val_loss: 5.0752 - val_accuracy: 0.4550\n",
      "Epoch 464/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 3.8879e-04 - accuracy: 1.0000 - val_loss: 5.0859 - val_accuracy: 0.4595\n",
      "Epoch 465/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 3.7291e-04 - accuracy: 1.0000 - val_loss: 5.0787 - val_accuracy: 0.4640\n",
      "Epoch 466/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 4.0916e-04 - accuracy: 1.0000 - val_loss: 5.0786 - val_accuracy: 0.4640\n",
      "Epoch 467/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 3.9835e-04 - accuracy: 1.0000 - val_loss: 5.0838 - val_accuracy: 0.4640\n",
      "Epoch 468/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.5797e-04 - accuracy: 1.0000 - val_loss: 5.1039 - val_accuracy: 0.4640\n",
      "Epoch 469/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.6173e-04 - accuracy: 1.0000 - val_loss: 5.1117 - val_accuracy: 0.4640\n",
      "Epoch 470/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.5449e-04 - accuracy: 1.0000 - val_loss: 5.1062 - val_accuracy: 0.4595\n",
      "Epoch 471/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.1887e-04 - accuracy: 1.0000 - val_loss: 5.1134 - val_accuracy: 0.4640\n",
      "Epoch 472/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 3.3644e-04 - accuracy: 1.0000 - val_loss: 5.1068 - val_accuracy: 0.4640\n",
      "Epoch 473/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.5044e-04 - accuracy: 1.0000 - val_loss: 5.1074 - val_accuracy: 0.4685\n",
      "Epoch 474/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.5320e-04 - accuracy: 1.0000 - val_loss: 5.1178 - val_accuracy: 0.4685\n",
      "Epoch 475/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 3.2760e-04 - accuracy: 1.0000 - val_loss: 5.1419 - val_accuracy: 0.4685\n",
      "Epoch 476/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.5672e-04 - accuracy: 1.0000 - val_loss: 5.1350 - val_accuracy: 0.4640\n",
      "Epoch 477/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.3138e-04 - accuracy: 1.0000 - val_loss: 5.1336 - val_accuracy: 0.4595\n",
      "Epoch 478/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.8509e-04 - accuracy: 1.0000 - val_loss: 5.1518 - val_accuracy: 0.4595\n",
      "Epoch 479/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.3334e-04 - accuracy: 1.0000 - val_loss: 5.1793 - val_accuracy: 0.4640\n",
      "Epoch 480/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.6307e-04 - accuracy: 1.0000 - val_loss: 5.1604 - val_accuracy: 0.4550\n",
      "Epoch 481/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.1248e-04 - accuracy: 1.0000 - val_loss: 5.1749 - val_accuracy: 0.4595\n",
      "Epoch 482/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.0102e-04 - accuracy: 1.0000 - val_loss: 5.1852 - val_accuracy: 0.4640\n",
      "Epoch 483/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.7721e-04 - accuracy: 1.0000 - val_loss: 5.1892 - val_accuracy: 0.4640\n",
      "Epoch 484/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.1704e-04 - accuracy: 1.0000 - val_loss: 5.1775 - val_accuracy: 0.4595\n",
      "Epoch 485/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.8765e-04 - accuracy: 1.0000 - val_loss: 5.2047 - val_accuracy: 0.4685\n",
      "Epoch 486/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.0150e-04 - accuracy: 1.0000 - val_loss: 5.1967 - val_accuracy: 0.4595\n",
      "Epoch 487/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 3.8254e-04 - accuracy: 1.0000 - val_loss: 5.1947 - val_accuracy: 0.4595\n",
      "Epoch 488/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 3.1260e-04 - accuracy: 1.0000 - val_loss: 5.2039 - val_accuracy: 0.4640\n",
      "Epoch 489/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.2755e-04 - accuracy: 1.0000 - val_loss: 5.2308 - val_accuracy: 0.4640\n",
      "Epoch 490/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.6527e-04 - accuracy: 1.0000 - val_loss: 5.1938 - val_accuracy: 0.4640\n",
      "Epoch 491/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.8291e-04 - accuracy: 1.0000 - val_loss: 5.2170 - val_accuracy: 0.4640\n",
      "Epoch 492/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.6366e-04 - accuracy: 1.0000 - val_loss: 5.2250 - val_accuracy: 0.4640\n",
      "Epoch 493/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.9011e-04 - accuracy: 1.0000 - val_loss: 5.2419 - val_accuracy: 0.4640\n",
      "Epoch 494/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.6010e-04 - accuracy: 1.0000 - val_loss: 5.2450 - val_accuracy: 0.4640\n",
      "Epoch 495/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.7483e-04 - accuracy: 1.0000 - val_loss: 5.2456 - val_accuracy: 0.4640\n",
      "Epoch 496/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.3032e-04 - accuracy: 1.0000 - val_loss: 5.2565 - val_accuracy: 0.4595\n",
      "Epoch 497/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.7979e-04 - accuracy: 1.0000 - val_loss: 5.2689 - val_accuracy: 0.4640\n",
      "Epoch 498/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.4777e-04 - accuracy: 1.0000 - val_loss: 5.2548 - val_accuracy: 0.4640\n",
      "Epoch 499/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.9618e-04 - accuracy: 1.0000 - val_loss: 5.2650 - val_accuracy: 0.4640\n",
      "Epoch 500/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.5010e-04 - accuracy: 1.0000 - val_loss: 5.2719 - val_accuracy: 0.4595\n",
      "Epoch 501/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.5184e-04 - accuracy: 1.0000 - val_loss: 5.2874 - val_accuracy: 0.4640\n",
      "Epoch 502/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.3311e-04 - accuracy: 1.0000 - val_loss: 5.2942 - val_accuracy: 0.4640\n",
      "Epoch 503/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.3069e-04 - accuracy: 1.0000 - val_loss: 5.3001 - val_accuracy: 0.4640\n",
      "Epoch 504/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.5587e-04 - accuracy: 1.0000 - val_loss: 5.3106 - val_accuracy: 0.4640\n",
      "Epoch 505/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.5361e-04 - accuracy: 1.0000 - val_loss: 5.3100 - val_accuracy: 0.4595\n",
      "Epoch 506/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.2486e-04 - accuracy: 1.0000 - val_loss: 5.3046 - val_accuracy: 0.4595\n",
      "Epoch 507/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.6411e-04 - accuracy: 1.0000 - val_loss: 5.3218 - val_accuracy: 0.4595\n",
      "Epoch 508/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.2315e-04 - accuracy: 1.0000 - val_loss: 5.3241 - val_accuracy: 0.4595\n",
      "Epoch 509/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.0149e-04 - accuracy: 1.0000 - val_loss: 5.3199 - val_accuracy: 0.4640\n",
      "Epoch 510/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.4146e-04 - accuracy: 1.0000 - val_loss: 5.3272 - val_accuracy: 0.4685\n",
      "Epoch 511/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.3700e-04 - accuracy: 1.0000 - val_loss: 5.3422 - val_accuracy: 0.4685\n",
      "Epoch 512/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.0140e-04 - accuracy: 1.0000 - val_loss: 5.3543 - val_accuracy: 0.4640\n",
      "Epoch 513/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.5988e-04 - accuracy: 1.0000 - val_loss: 5.3586 - val_accuracy: 0.4640\n",
      "Epoch 514/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.0362e-04 - accuracy: 1.0000 - val_loss: 5.3600 - val_accuracy: 0.4595\n",
      "Epoch 515/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1154e-04 - accuracy: 1.0000 - val_loss: 5.3545 - val_accuracy: 0.4595\n",
      "Epoch 516/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.0873e-04 - accuracy: 1.0000 - val_loss: 5.3833 - val_accuracy: 0.4595\n",
      "Epoch 517/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 2.1066e-04 - accuracy: 1.0000 - val_loss: 5.3866 - val_accuracy: 0.4595\n",
      "Epoch 518/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8265e-04 - accuracy: 1.0000 - val_loss: 5.4047 - val_accuracy: 0.4640\n",
      "Epoch 519/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.8512e-04 - accuracy: 1.0000 - val_loss: 5.4096 - val_accuracy: 0.4595\n",
      "Epoch 520/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.2100e-04 - accuracy: 1.0000 - val_loss: 5.4033 - val_accuracy: 0.4595\n",
      "Epoch 521/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9821e-04 - accuracy: 1.0000 - val_loss: 5.4091 - val_accuracy: 0.4595\n",
      "Epoch 522/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.8781e-04 - accuracy: 1.0000 - val_loss: 5.4053 - val_accuracy: 0.4595\n",
      "Epoch 523/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 2.3571e-04 - accuracy: 1.0000 - val_loss: 5.3799 - val_accuracy: 0.4640\n",
      "Epoch 524/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 2.1214e-04 - accuracy: 1.0000 - val_loss: 5.3955 - val_accuracy: 0.4595\n",
      "Epoch 525/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8394e-04 - accuracy: 1.0000 - val_loss: 5.4339 - val_accuracy: 0.4640\n",
      "Epoch 526/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.9694e-04 - accuracy: 1.0000 - val_loss: 5.4176 - val_accuracy: 0.4595\n",
      "Epoch 527/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8661e-04 - accuracy: 1.0000 - val_loss: 5.4427 - val_accuracy: 0.4595\n",
      "Epoch 528/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.8598e-04 - accuracy: 1.0000 - val_loss: 5.4390 - val_accuracy: 0.4595\n",
      "Epoch 529/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 2.1331e-04 - accuracy: 1.0000 - val_loss: 5.4189 - val_accuracy: 0.4595\n",
      "Epoch 530/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.9670e-04 - accuracy: 1.0000 - val_loss: 5.4563 - val_accuracy: 0.4640\n",
      "Epoch 531/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8096e-04 - accuracy: 1.0000 - val_loss: 5.4540 - val_accuracy: 0.4595\n",
      "Epoch 532/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.7594e-04 - accuracy: 1.0000 - val_loss: 5.4657 - val_accuracy: 0.4595\n",
      "Epoch 533/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9877e-04 - accuracy: 1.0000 - val_loss: 5.4529 - val_accuracy: 0.4640\n",
      "Epoch 534/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.7239e-04 - accuracy: 1.0000 - val_loss: 5.4633 - val_accuracy: 0.4640\n",
      "Epoch 535/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8017e-04 - accuracy: 1.0000 - val_loss: 5.4650 - val_accuracy: 0.4595\n",
      "Epoch 536/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.4619e-04 - accuracy: 1.0000 - val_loss: 5.4732 - val_accuracy: 0.4640\n",
      "Epoch 537/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.6162e-04 - accuracy: 1.0000 - val_loss: 5.4799 - val_accuracy: 0.4595\n",
      "Epoch 538/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.5350e-04 - accuracy: 1.0000 - val_loss: 5.4849 - val_accuracy: 0.4640\n",
      "Epoch 539/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.6586e-04 - accuracy: 1.0000 - val_loss: 5.5005 - val_accuracy: 0.4640\n",
      "Epoch 540/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 1.8604e-04 - accuracy: 1.0000 - val_loss: 5.5116 - val_accuracy: 0.4595\n",
      "Epoch 541/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.4464e-04 - accuracy: 1.0000 - val_loss: 5.5074 - val_accuracy: 0.4595\n",
      "Epoch 542/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.4462e-04 - accuracy: 1.0000 - val_loss: 5.5000 - val_accuracy: 0.4595\n",
      "Epoch 543/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.6071e-04 - accuracy: 1.0000 - val_loss: 5.4977 - val_accuracy: 0.4595\n",
      "Epoch 544/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.4736e-04 - accuracy: 1.0000 - val_loss: 5.5008 - val_accuracy: 0.4595\n",
      "Epoch 545/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.7103e-04 - accuracy: 1.0000 - val_loss: 5.5092 - val_accuracy: 0.4595\n",
      "Epoch 546/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.3931e-04 - accuracy: 1.0000 - val_loss: 5.5292 - val_accuracy: 0.4595\n",
      "Epoch 547/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.2999e-04 - accuracy: 1.0000 - val_loss: 5.5221 - val_accuracy: 0.4640\n",
      "Epoch 548/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.3618e-04 - accuracy: 1.0000 - val_loss: 5.5341 - val_accuracy: 0.4640\n",
      "Epoch 549/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.4160e-04 - accuracy: 1.0000 - val_loss: 5.5421 - val_accuracy: 0.4640\n",
      "Epoch 550/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.5462e-04 - accuracy: 1.0000 - val_loss: 5.5425 - val_accuracy: 0.4640\n",
      "Epoch 551/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.4345e-04 - accuracy: 1.0000 - val_loss: 5.5516 - val_accuracy: 0.4640\n",
      "Epoch 552/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.3991e-04 - accuracy: 1.0000 - val_loss: 5.5431 - val_accuracy: 0.4595\n",
      "Epoch 553/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.4704e-04 - accuracy: 1.0000 - val_loss: 5.5507 - val_accuracy: 0.4595\n",
      "Epoch 554/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 1.4741e-04 - accuracy: 1.0000 - val_loss: 5.5574 - val_accuracy: 0.4595\n",
      "Epoch 555/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.2888e-04 - accuracy: 1.0000 - val_loss: 5.5652 - val_accuracy: 0.4595\n",
      "Epoch 556/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2269e-04 - accuracy: 1.0000 - val_loss: 5.5769 - val_accuracy: 0.4595\n",
      "Epoch 557/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.2981e-04 - accuracy: 1.0000 - val_loss: 5.5832 - val_accuracy: 0.4595\n",
      "Epoch 558/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.2415e-04 - accuracy: 1.0000 - val_loss: 5.5901 - val_accuracy: 0.4595\n",
      "Epoch 559/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.2555e-04 - accuracy: 1.0000 - val_loss: 5.5849 - val_accuracy: 0.4640\n",
      "Epoch 560/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.1376e-04 - accuracy: 1.0000 - val_loss: 5.5899 - val_accuracy: 0.4595\n",
      "Epoch 561/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.3075e-04 - accuracy: 1.0000 - val_loss: 5.6031 - val_accuracy: 0.4640\n",
      "Epoch 562/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 1.2768e-04 - accuracy: 1.0000 - val_loss: 5.5942 - val_accuracy: 0.4595\n",
      "Epoch 563/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 1.2178e-04 - accuracy: 1.0000 - val_loss: 5.5997 - val_accuracy: 0.4595\n",
      "Epoch 564/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.1531e-04 - accuracy: 1.0000 - val_loss: 5.6182 - val_accuracy: 0.4595\n",
      "Epoch 565/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 1.0563e-04 - accuracy: 1.0000 - val_loss: 5.6268 - val_accuracy: 0.4595\n",
      "Epoch 566/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.0387e-04 - accuracy: 1.0000 - val_loss: 5.6334 - val_accuracy: 0.4640\n",
      "Epoch 567/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.1760e-04 - accuracy: 1.0000 - val_loss: 5.6409 - val_accuracy: 0.4595\n",
      "Epoch 568/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.1742e-04 - accuracy: 1.0000 - val_loss: 5.6429 - val_accuracy: 0.4640\n",
      "Epoch 569/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0475e-04 - accuracy: 1.0000 - val_loss: 5.6410 - val_accuracy: 0.4640\n",
      "Epoch 570/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2902e-04 - accuracy: 1.0000 - val_loss: 5.6353 - val_accuracy: 0.4595\n",
      "Epoch 571/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0967e-04 - accuracy: 1.0000 - val_loss: 5.6378 - val_accuracy: 0.4595\n",
      "Epoch 572/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.2242e-04 - accuracy: 1.0000 - val_loss: 5.6579 - val_accuracy: 0.4640\n",
      "Epoch 573/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.0534e-04 - accuracy: 1.0000 - val_loss: 5.6598 - val_accuracy: 0.4595\n",
      "Epoch 574/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 1.3814e-04 - accuracy: 1.0000 - val_loss: 5.6434 - val_accuracy: 0.4595\n",
      "Epoch 575/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.6395e-05 - accuracy: 1.0000 - val_loss: 5.6455 - val_accuracy: 0.4640\n",
      "Epoch 576/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.0875e-04 - accuracy: 1.0000 - val_loss: 5.6463 - val_accuracy: 0.4595\n",
      "Epoch 577/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.1367e-04 - accuracy: 1.0000 - val_loss: 5.6702 - val_accuracy: 0.4595\n",
      "Epoch 578/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.1010e-04 - accuracy: 1.0000 - val_loss: 5.6929 - val_accuracy: 0.4640\n",
      "Epoch 579/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.0509e-04 - accuracy: 1.0000 - val_loss: 5.6799 - val_accuracy: 0.4595\n",
      "Epoch 580/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0855e-04 - accuracy: 1.0000 - val_loss: 5.6828 - val_accuracy: 0.4640\n",
      "Epoch 581/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.2380e-04 - accuracy: 1.0000 - val_loss: 5.6876 - val_accuracy: 0.4640\n",
      "Epoch 582/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 9.9839e-05 - accuracy: 1.0000 - val_loss: 5.6851 - val_accuracy: 0.4595\n",
      "Epoch 583/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.0056e-04 - accuracy: 1.0000 - val_loss: 5.7108 - val_accuracy: 0.4595\n",
      "Epoch 584/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.8432e-05 - accuracy: 1.0000 - val_loss: 5.7069 - val_accuracy: 0.4595\n",
      "Epoch 585/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.0718e-05 - accuracy: 1.0000 - val_loss: 5.7134 - val_accuracy: 0.4640\n",
      "Epoch 586/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.9348e-05 - accuracy: 1.0000 - val_loss: 5.7128 - val_accuracy: 0.4595\n",
      "Epoch 587/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.0120e-05 - accuracy: 1.0000 - val_loss: 5.7047 - val_accuracy: 0.4595\n",
      "Epoch 588/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.3358e-05 - accuracy: 1.0000 - val_loss: 5.7297 - val_accuracy: 0.4640\n",
      "Epoch 589/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 7.7643e-05 - accuracy: 1.0000 - val_loss: 5.7387 - val_accuracy: 0.4640\n",
      "Epoch 590/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.6235e-05 - accuracy: 1.0000 - val_loss: 5.7358 - val_accuracy: 0.4640\n",
      "Epoch 591/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.1837e-05 - accuracy: 1.0000 - val_loss: 5.7372 - val_accuracy: 0.4640\n",
      "Epoch 592/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.5988e-05 - accuracy: 1.0000 - val_loss: 5.7595 - val_accuracy: 0.4640\n",
      "Epoch 593/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0706e-04 - accuracy: 1.0000 - val_loss: 5.7637 - val_accuracy: 0.4640\n",
      "Epoch 594/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 9.6225e-05 - accuracy: 1.0000 - val_loss: 5.7814 - val_accuracy: 0.4640\n",
      "Epoch 595/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.0457e-05 - accuracy: 1.0000 - val_loss: 5.7861 - val_accuracy: 0.4640\n",
      "Epoch 596/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.9569e-05 - accuracy: 1.0000 - val_loss: 5.7850 - val_accuracy: 0.4640\n",
      "Epoch 597/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.7900e-05 - accuracy: 1.0000 - val_loss: 5.7979 - val_accuracy: 0.4640\n",
      "Epoch 598/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 9.2221e-05 - accuracy: 1.0000 - val_loss: 5.7840 - val_accuracy: 0.4640\n",
      "Epoch 599/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.5962e-05 - accuracy: 1.0000 - val_loss: 5.7736 - val_accuracy: 0.4595\n",
      "Epoch 600/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 7.7769e-05 - accuracy: 1.0000 - val_loss: 5.7855 - val_accuracy: 0.4595\n",
      "Epoch 601/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 7.2430e-05 - accuracy: 1.0000 - val_loss: 5.7851 - val_accuracy: 0.4640\n",
      "Epoch 602/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.2458e-05 - accuracy: 1.0000 - val_loss: 5.7868 - val_accuracy: 0.4595\n",
      "Epoch 603/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.5777e-05 - accuracy: 1.0000 - val_loss: 5.8077 - val_accuracy: 0.4640\n",
      "Epoch 604/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 7.6864e-05 - accuracy: 1.0000 - val_loss: 5.8232 - val_accuracy: 0.4640\n",
      "Epoch 605/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.8502e-05 - accuracy: 1.0000 - val_loss: 5.8046 - val_accuracy: 0.4595\n",
      "Epoch 606/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 7.5325e-05 - accuracy: 1.0000 - val_loss: 5.7954 - val_accuracy: 0.4640\n",
      "Epoch 607/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 7.7989e-05 - accuracy: 1.0000 - val_loss: 5.8185 - val_accuracy: 0.4685\n",
      "Epoch 608/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.2597e-05 - accuracy: 1.0000 - val_loss: 5.8129 - val_accuracy: 0.4640\n",
      "Epoch 609/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 6.8389e-05 - accuracy: 1.0000 - val_loss: 5.8307 - val_accuracy: 0.4595\n",
      "Epoch 610/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 7.9735e-05 - accuracy: 1.0000 - val_loss: 5.8430 - val_accuracy: 0.4595\n",
      "Epoch 611/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.2257e-05 - accuracy: 1.0000 - val_loss: 5.8486 - val_accuracy: 0.4595\n",
      "Epoch 612/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 6.4424e-05 - accuracy: 1.0000 - val_loss: 5.8504 - val_accuracy: 0.4640\n",
      "Epoch 613/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 7.5788e-05 - accuracy: 1.0000 - val_loss: 5.8491 - val_accuracy: 0.4640\n",
      "Epoch 614/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 8.6840e-05 - accuracy: 1.0000 - val_loss: 5.8613 - val_accuracy: 0.4640\n",
      "Epoch 615/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 8.0454e-05 - accuracy: 1.0000 - val_loss: 5.8645 - val_accuracy: 0.4595\n",
      "Epoch 616/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 7.9784e-05 - accuracy: 1.0000 - val_loss: 5.8527 - val_accuracy: 0.4595\n",
      "Epoch 617/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 6.5687e-05 - accuracy: 1.0000 - val_loss: 5.8579 - val_accuracy: 0.4595\n",
      "Epoch 618/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 5.3311e-05 - accuracy: 1.0000 - val_loss: 5.8780 - val_accuracy: 0.4640\n",
      "Epoch 619/1000\n",
      "1991/1991 [==============================] - 0s 86us/step - loss: 5.9733e-05 - accuracy: 1.0000 - val_loss: 5.8777 - val_accuracy: 0.4640\n",
      "Epoch 620/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 6.8209e-05 - accuracy: 1.0000 - val_loss: 5.8693 - val_accuracy: 0.4640\n",
      "Epoch 621/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 8.1249e-05 - accuracy: 1.0000 - val_loss: 5.8986 - val_accuracy: 0.4640\n",
      "Epoch 622/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 6.4487e-05 - accuracy: 1.0000 - val_loss: 5.8958 - val_accuracy: 0.4640\n",
      "Epoch 623/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 7.7326e-05 - accuracy: 1.0000 - val_loss: 5.9036 - val_accuracy: 0.4640\n",
      "Epoch 624/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 6.7296e-05 - accuracy: 1.0000 - val_loss: 5.9251 - val_accuracy: 0.4640\n",
      "Epoch 625/1000\n",
      "1991/1991 [==============================] - 0s 85us/step - loss: 6.1149e-05 - accuracy: 1.0000 - val_loss: 5.9270 - val_accuracy: 0.4640\n",
      "Epoch 626/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 6.4890e-05 - accuracy: 1.0000 - val_loss: 5.9289 - val_accuracy: 0.4595\n",
      "Epoch 627/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 6.7871e-05 - accuracy: 1.0000 - val_loss: 5.9449 - val_accuracy: 0.4640\n",
      "Epoch 628/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 6.8152e-05 - accuracy: 1.0000 - val_loss: 5.9368 - val_accuracy: 0.4640\n",
      "Epoch 629/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 5.8929e-05 - accuracy: 1.0000 - val_loss: 5.9330 - val_accuracy: 0.4685\n",
      "Epoch 630/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 5.1491e-05 - accuracy: 1.0000 - val_loss: 5.9436 - val_accuracy: 0.4685\n",
      "Epoch 631/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 6.9927e-05 - accuracy: 1.0000 - val_loss: 5.9276 - val_accuracy: 0.4640\n",
      "Epoch 632/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.1770e-05 - accuracy: 1.0000 - val_loss: 5.9589 - val_accuracy: 0.4640\n",
      "Epoch 633/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 5.1565e-05 - accuracy: 1.0000 - val_loss: 5.9673 - val_accuracy: 0.4640\n",
      "Epoch 634/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 6.1477e-05 - accuracy: 1.0000 - val_loss: 5.9610 - val_accuracy: 0.4640\n",
      "Epoch 635/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 6.4142e-05 - accuracy: 1.0000 - val_loss: 5.9542 - val_accuracy: 0.4640\n",
      "Epoch 636/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 5.1492e-05 - accuracy: 1.0000 - val_loss: 5.9588 - val_accuracy: 0.4685\n",
      "Epoch 637/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 5.2088e-05 - accuracy: 1.0000 - val_loss: 5.9576 - val_accuracy: 0.4640\n",
      "Epoch 638/1000\n",
      "1991/1991 [==============================] - 0s 114us/step - loss: 5.5906e-05 - accuracy: 1.0000 - val_loss: 5.9728 - val_accuracy: 0.4640\n",
      "Epoch 639/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 5.2988e-05 - accuracy: 1.0000 - val_loss: 5.9813 - val_accuracy: 0.4640\n",
      "Epoch 640/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 4.9930e-05 - accuracy: 1.0000 - val_loss: 5.9782 - val_accuracy: 0.4640\n",
      "Epoch 641/1000\n",
      "1991/1991 [==============================] - 0s 89us/step - loss: 5.6129e-05 - accuracy: 1.0000 - val_loss: 5.9806 - val_accuracy: 0.4595\n",
      "Epoch 642/1000\n",
      "1991/1991 [==============================] - 0s 96us/step - loss: 5.2296e-05 - accuracy: 1.0000 - val_loss: 5.9809 - val_accuracy: 0.4640\n",
      "Epoch 643/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 5.2649e-05 - accuracy: 1.0000 - val_loss: 5.9843 - val_accuracy: 0.4640\n",
      "Epoch 644/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 6.0930e-05 - accuracy: 1.0000 - val_loss: 5.9757 - val_accuracy: 0.4595\n",
      "Epoch 645/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 5.7938e-05 - accuracy: 1.0000 - val_loss: 5.9778 - val_accuracy: 0.4595\n",
      "Epoch 646/1000\n",
      "1991/1991 [==============================] - 0s 87us/step - loss: 4.7190e-05 - accuracy: 1.0000 - val_loss: 5.9966 - val_accuracy: 0.4640\n",
      "Epoch 647/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 4.9600e-05 - accuracy: 1.0000 - val_loss: 6.0276 - val_accuracy: 0.4640\n",
      "Epoch 648/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 4.7981e-05 - accuracy: 1.0000 - val_loss: 6.0337 - val_accuracy: 0.4640\n",
      "Epoch 649/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 4.4904e-05 - accuracy: 1.0000 - val_loss: 6.0156 - val_accuracy: 0.4595\n",
      "Epoch 650/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 4.7873e-05 - accuracy: 1.0000 - val_loss: 6.0422 - val_accuracy: 0.4640\n",
      "Epoch 651/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 4.9780e-05 - accuracy: 1.0000 - val_loss: 6.0384 - val_accuracy: 0.4640\n",
      "Epoch 652/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 5.2261e-05 - accuracy: 1.0000 - val_loss: 6.0372 - val_accuracy: 0.4640\n",
      "Epoch 653/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 4.6594e-05 - accuracy: 1.0000 - val_loss: 6.0505 - val_accuracy: 0.4595\n",
      "Epoch 654/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 4.5635e-05 - accuracy: 1.0000 - val_loss: 6.0453 - val_accuracy: 0.4595\n",
      "Epoch 655/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 4.3778e-05 - accuracy: 1.0000 - val_loss: 6.0385 - val_accuracy: 0.4595\n",
      "Epoch 656/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 4.7551e-05 - accuracy: 1.0000 - val_loss: 6.0465 - val_accuracy: 0.4595\n",
      "Epoch 657/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.9188e-05 - accuracy: 1.0000 - val_loss: 6.0588 - val_accuracy: 0.4640\n",
      "Epoch 658/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 4.3868e-05 - accuracy: 1.0000 - val_loss: 6.0609 - val_accuracy: 0.4640\n",
      "Epoch 659/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 1.2190e-04 - accuracy: 1.0000 - val_loss: 6.0722 - val_accuracy: 0.4595\n",
      "Epoch 660/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.8128 - accuracy: 0.8910 - val_loss: 3.7803 - val_accuracy: 0.4189\n",
      "Epoch 661/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.8077 - accuracy: 0.5610 - val_loss: 0.7183 - val_accuracy: 0.4550\n",
      "Epoch 662/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.6878 - accuracy: 0.5153 - val_loss: 0.6996 - val_accuracy: 0.4685\n",
      "Epoch 663/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.6851 - accuracy: 0.5655 - val_loss: 0.7000 - val_accuracy: 0.4910\n",
      "Epoch 664/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.6798 - accuracy: 0.5706 - val_loss: 0.7040 - val_accuracy: 0.5135\n",
      "Epoch 665/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.6742 - accuracy: 0.5876 - val_loss: 0.7104 - val_accuracy: 0.5135\n",
      "Epoch 666/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.6658 - accuracy: 0.5851 - val_loss: 0.7213 - val_accuracy: 0.4955\n",
      "Epoch 667/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.6607 - accuracy: 0.5917 - val_loss: 0.7474 - val_accuracy: 0.4910\n",
      "Epoch 668/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.6492 - accuracy: 0.6042 - val_loss: 0.7837 - val_accuracy: 0.4595\n",
      "Epoch 669/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.6363 - accuracy: 0.6253 - val_loss: 0.8032 - val_accuracy: 0.4820\n",
      "Epoch 670/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.6236 - accuracy: 0.6288 - val_loss: 0.8305 - val_accuracy: 0.5090\n",
      "Epoch 671/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.6074 - accuracy: 0.6434 - val_loss: 0.8460 - val_accuracy: 0.4820\n",
      "Epoch 672/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.5902 - accuracy: 0.6645 - val_loss: 0.9242 - val_accuracy: 0.4820\n",
      "Epoch 673/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.5774 - accuracy: 0.6816 - val_loss: 0.9956 - val_accuracy: 0.4324\n",
      "Epoch 674/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.5753 - accuracy: 0.6700 - val_loss: 0.8118 - val_accuracy: 0.4775\n",
      "Epoch 675/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.5672 - accuracy: 0.6956 - val_loss: 0.9546 - val_accuracy: 0.4910\n",
      "Epoch 676/1000\n",
      "1991/1991 [==============================] - 0s 82us/step - loss: 0.5310 - accuracy: 0.7167 - val_loss: 1.1302 - val_accuracy: 0.4865\n",
      "Epoch 677/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.5185 - accuracy: 0.7258 - val_loss: 1.0254 - val_accuracy: 0.5315\n",
      "Epoch 678/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.4933 - accuracy: 0.7343 - val_loss: 1.0871 - val_accuracy: 0.4775\n",
      "Epoch 679/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.4950 - accuracy: 0.7338 - val_loss: 1.1122 - val_accuracy: 0.4730\n",
      "Epoch 680/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.4628 - accuracy: 0.7604 - val_loss: 1.1751 - val_accuracy: 0.5000\n",
      "Epoch 681/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.4546 - accuracy: 0.7680 - val_loss: 1.2639 - val_accuracy: 0.4595\n",
      "Epoch 682/1000\n",
      "1991/1991 [==============================] - 0s 81us/step - loss: 0.4552 - accuracy: 0.7715 - val_loss: 1.5049 - val_accuracy: 0.4459\n",
      "Epoch 683/1000\n",
      "1991/1991 [==============================] - 0s 78us/step - loss: 0.4593 - accuracy: 0.7785 - val_loss: 1.3152 - val_accuracy: 0.4414\n",
      "Epoch 684/1000\n",
      "1991/1991 [==============================] - 0s 79us/step - loss: 0.4417 - accuracy: 0.7830 - val_loss: 1.2197 - val_accuracy: 0.4865\n",
      "Epoch 685/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.3933 - accuracy: 0.8086 - val_loss: 1.4393 - val_accuracy: 0.4865\n",
      "Epoch 686/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.3744 - accuracy: 0.8192 - val_loss: 1.6223 - val_accuracy: 0.4550\n",
      "Epoch 687/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 0.3822 - accuracy: 0.8127 - val_loss: 1.5045 - val_accuracy: 0.4505\n",
      "Epoch 688/1000\n",
      "1991/1991 [==============================] - 0s 80us/step - loss: 0.3692 - accuracy: 0.8222 - val_loss: 1.3837 - val_accuracy: 0.4775\n",
      "Epoch 689/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.3644 - accuracy: 0.8272 - val_loss: 1.2891 - val_accuracy: 0.5360\n",
      "Epoch 690/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.3647 - accuracy: 0.8257 - val_loss: 1.3868 - val_accuracy: 0.4640\n",
      "Epoch 691/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.2920 - accuracy: 0.8654 - val_loss: 1.8350 - val_accuracy: 0.4685\n",
      "Epoch 692/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.2606 - accuracy: 0.8835 - val_loss: 1.6944 - val_accuracy: 0.4820\n",
      "Epoch 693/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.2336 - accuracy: 0.8995 - val_loss: 1.9862 - val_accuracy: 0.4910\n",
      "Epoch 694/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.2537 - accuracy: 0.8880 - val_loss: 1.9454 - val_accuracy: 0.4910\n",
      "Epoch 695/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.2131 - accuracy: 0.9076 - val_loss: 1.9120 - val_accuracy: 0.5090\n",
      "Epoch 696/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.2139 - accuracy: 0.9071 - val_loss: 2.1277 - val_accuracy: 0.4595\n",
      "Epoch 697/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.2264 - accuracy: 0.9006 - val_loss: 2.0040 - val_accuracy: 0.5000\n",
      "Epoch 698/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1952 - accuracy: 0.9161 - val_loss: 2.2572 - val_accuracy: 0.5000\n",
      "Epoch 699/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1893 - accuracy: 0.9191 - val_loss: 2.2387 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.1615 - accuracy: 0.9332 - val_loss: 2.3648 - val_accuracy: 0.5135\n",
      "Epoch 701/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.1589 - accuracy: 0.9302 - val_loss: 2.2881 - val_accuracy: 0.4955\n",
      "Epoch 702/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1460 - accuracy: 0.9382 - val_loss: 2.3032 - val_accuracy: 0.5270\n",
      "Epoch 703/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.1519 - accuracy: 0.9367 - val_loss: 2.3519 - val_accuracy: 0.5090\n",
      "Epoch 704/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.1912 - accuracy: 0.9272 - val_loss: 2.4324 - val_accuracy: 0.5090\n",
      "Epoch 705/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.1918 - accuracy: 0.9191 - val_loss: 2.2419 - val_accuracy: 0.4955\n",
      "Epoch 706/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.1480 - accuracy: 0.9377 - val_loss: 2.2802 - val_accuracy: 0.5315\n",
      "Epoch 707/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.1394 - accuracy: 0.9468 - val_loss: 2.3225 - val_accuracy: 0.5270\n",
      "Epoch 708/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.1144 - accuracy: 0.9593 - val_loss: 2.3208 - val_accuracy: 0.4955\n",
      "Epoch 709/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0756 - accuracy: 0.9809 - val_loss: 2.4230 - val_accuracy: 0.5225\n",
      "Epoch 710/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0656 - accuracy: 0.9769 - val_loss: 2.6344 - val_accuracy: 0.5360\n",
      "Epoch 711/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0653 - accuracy: 0.9774 - val_loss: 2.5385 - val_accuracy: 0.5225\n",
      "Epoch 712/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0800 - accuracy: 0.9749 - val_loss: 2.8036 - val_accuracy: 0.5135\n",
      "Epoch 713/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0822 - accuracy: 0.9704 - val_loss: 2.7537 - val_accuracy: 0.4955\n",
      "Epoch 714/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0847 - accuracy: 0.9663 - val_loss: 2.7818 - val_accuracy: 0.4910\n",
      "Epoch 715/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.1002 - accuracy: 0.9633 - val_loss: 2.6647 - val_accuracy: 0.5135\n",
      "Epoch 716/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0701 - accuracy: 0.9749 - val_loss: 2.8182 - val_accuracy: 0.5135\n",
      "Epoch 717/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0674 - accuracy: 0.9764 - val_loss: 2.7059 - val_accuracy: 0.5360\n",
      "Epoch 718/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0516 - accuracy: 0.9849 - val_loss: 2.9571 - val_accuracy: 0.5045\n",
      "Epoch 719/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0583 - accuracy: 0.9794 - val_loss: 2.8570 - val_accuracy: 0.5270\n",
      "Epoch 720/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0448 - accuracy: 0.9859 - val_loss: 2.9468 - val_accuracy: 0.5225\n",
      "Epoch 721/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0884 - accuracy: 0.9689 - val_loss: 2.8851 - val_accuracy: 0.5360\n",
      "Epoch 722/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0673 - accuracy: 0.9774 - val_loss: 3.2221 - val_accuracy: 0.5180\n",
      "Epoch 723/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0652 - accuracy: 0.9779 - val_loss: 3.1776 - val_accuracy: 0.5180\n",
      "Epoch 724/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0690 - accuracy: 0.9744 - val_loss: 3.1277 - val_accuracy: 0.4775\n",
      "Epoch 725/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0688 - accuracy: 0.9724 - val_loss: 2.9386 - val_accuracy: 0.5090\n",
      "Epoch 726/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0490 - accuracy: 0.9854 - val_loss: 2.8678 - val_accuracy: 0.5315\n",
      "Epoch 727/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0322 - accuracy: 0.9925 - val_loss: 3.2285 - val_accuracy: 0.5180\n",
      "Epoch 728/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0334 - accuracy: 0.9920 - val_loss: 3.0509 - val_accuracy: 0.5135\n",
      "Epoch 729/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0311 - accuracy: 0.9930 - val_loss: 3.2979 - val_accuracy: 0.5135\n",
      "Epoch 730/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0239 - accuracy: 0.9930 - val_loss: 3.3214 - val_accuracy: 0.4910\n",
      "Epoch 731/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0348 - accuracy: 0.9895 - val_loss: 3.3924 - val_accuracy: 0.5090\n",
      "Epoch 732/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0391 - accuracy: 0.9844 - val_loss: 3.3963 - val_accuracy: 0.5000\n",
      "Epoch 733/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0255 - accuracy: 0.9935 - val_loss: 3.2929 - val_accuracy: 0.5090\n",
      "Epoch 734/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 3.4215 - val_accuracy: 0.5000\n",
      "Epoch 735/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0157 - accuracy: 0.9965 - val_loss: 3.3573 - val_accuracy: 0.5090\n",
      "Epoch 736/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0127 - accuracy: 0.9980 - val_loss: 3.3632 - val_accuracy: 0.5045\n",
      "Epoch 737/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0127 - accuracy: 0.9970 - val_loss: 3.5011 - val_accuracy: 0.4865\n",
      "Epoch 738/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0156 - accuracy: 0.9955 - val_loss: 3.4805 - val_accuracy: 0.5225\n",
      "Epoch 739/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0498 - accuracy: 0.9819 - val_loss: 3.4851 - val_accuracy: 0.4955\n",
      "Epoch 740/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 0.0467 - accuracy: 0.9844 - val_loss: 3.3115 - val_accuracy: 0.5225\n",
      "Epoch 741/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0546 - accuracy: 0.9844 - val_loss: 3.4711 - val_accuracy: 0.5000\n",
      "Epoch 742/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0612 - accuracy: 0.9794 - val_loss: 3.2615 - val_accuracy: 0.5405\n",
      "Epoch 743/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0657 - accuracy: 0.9754 - val_loss: 3.2450 - val_accuracy: 0.5045\n",
      "Epoch 744/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0358 - accuracy: 0.9869 - val_loss: 3.4152 - val_accuracy: 0.4775\n",
      "Epoch 745/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 3.3224 - val_accuracy: 0.5090\n",
      "Epoch 746/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0193 - accuracy: 0.9955 - val_loss: 3.5359 - val_accuracy: 0.4910\n",
      "Epoch 747/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0335 - accuracy: 0.9900 - val_loss: 3.4399 - val_accuracy: 0.5000\n",
      "Epoch 748/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0741 - accuracy: 0.9819 - val_loss: 3.3500 - val_accuracy: 0.5090\n",
      "Epoch 749/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0425 - accuracy: 0.9859 - val_loss: 3.3316 - val_accuracy: 0.5315\n",
      "Epoch 750/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 3.5384 - val_accuracy: 0.5090\n",
      "Epoch 751/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0144 - accuracy: 0.9975 - val_loss: 3.7037 - val_accuracy: 0.4955\n",
      "Epoch 752/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 3.5457 - val_accuracy: 0.5090\n",
      "Epoch 753/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 3.6682 - val_accuracy: 0.5090\n",
      "Epoch 754/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 3.6895 - val_accuracy: 0.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 755/1000\n",
      "1991/1991 [==============================] - 0s 84us/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 3.7188 - val_accuracy: 0.5090\n",
      "Epoch 756/1000\n",
      "1991/1991 [==============================] - 0s 85us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.7011 - val_accuracy: 0.5090\n",
      "Epoch 757/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.7191 - val_accuracy: 0.5045\n",
      "Epoch 758/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 3.7580 - val_accuracy: 0.5045\n",
      "Epoch 759/1000\n",
      "1991/1991 [==============================] - 0s 77us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.7524 - val_accuracy: 0.4955\n",
      "Epoch 760/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.7954 - val_accuracy: 0.5135\n",
      "Epoch 761/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.8124 - val_accuracy: 0.5090\n",
      "Epoch 762/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.7987 - val_accuracy: 0.5045\n",
      "Epoch 763/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.8066 - val_accuracy: 0.4910\n",
      "Epoch 764/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.8283 - val_accuracy: 0.5000\n",
      "Epoch 765/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.8731 - val_accuracy: 0.5000\n",
      "Epoch 766/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.8455 - val_accuracy: 0.5135\n",
      "Epoch 767/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.8492 - val_accuracy: 0.4955\n",
      "Epoch 768/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.8614 - val_accuracy: 0.4955\n",
      "Epoch 769/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.8923 - val_accuracy: 0.4955\n",
      "Epoch 770/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.9138 - val_accuracy: 0.5045\n",
      "Epoch 771/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9290 - val_accuracy: 0.5000\n",
      "Epoch 772/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.9169 - val_accuracy: 0.5045\n",
      "Epoch 773/1000\n",
      "1991/1991 [==============================] - 0s 72us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9378 - val_accuracy: 0.5090\n",
      "Epoch 774/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9440 - val_accuracy: 0.5045\n",
      "Epoch 775/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9375 - val_accuracy: 0.4910\n",
      "Epoch 776/1000\n",
      "1991/1991 [==============================] - 0s 75us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.9783 - val_accuracy: 0.5000\n",
      "Epoch 777/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9630 - val_accuracy: 0.4910\n",
      "Epoch 778/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.9916 - val_accuracy: 0.5000\n",
      "Epoch 779/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.9891 - val_accuracy: 0.5135\n",
      "Epoch 780/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0076 - val_accuracy: 0.4910\n",
      "Epoch 781/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0183 - val_accuracy: 0.4910\n",
      "Epoch 782/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.9968 - val_accuracy: 0.4955\n",
      "Epoch 783/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0123 - val_accuracy: 0.4820\n",
      "Epoch 784/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.0312 - val_accuracy: 0.5000\n",
      "Epoch 785/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0174 - val_accuracy: 0.4955\n",
      "Epoch 786/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.0052 - val_accuracy: 0.5045\n",
      "Epoch 787/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 9.3910e-04 - accuracy: 1.0000 - val_loss: 4.0391 - val_accuracy: 0.4955\n",
      "Epoch 788/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.2605e-04 - accuracy: 1.0000 - val_loss: 4.0517 - val_accuracy: 0.4910\n",
      "Epoch 789/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 8.7870e-04 - accuracy: 1.0000 - val_loss: 4.0523 - val_accuracy: 0.5000\n",
      "Epoch 790/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.0857 - val_accuracy: 0.4865\n",
      "Epoch 791/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.7797e-04 - accuracy: 1.0000 - val_loss: 4.0656 - val_accuracy: 0.5000\n",
      "Epoch 792/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.3725e-04 - accuracy: 1.0000 - val_loss: 4.0644 - val_accuracy: 0.5000\n",
      "Epoch 793/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 8.6771e-04 - accuracy: 1.0000 - val_loss: 4.0903 - val_accuracy: 0.4955\n",
      "Epoch 794/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 8.5558e-04 - accuracy: 1.0000 - val_loss: 4.0948 - val_accuracy: 0.5000\n",
      "Epoch 795/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 7.9153e-04 - accuracy: 1.0000 - val_loss: 4.0962 - val_accuracy: 0.4955\n",
      "Epoch 796/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 7.3749e-04 - accuracy: 1.0000 - val_loss: 4.1236 - val_accuracy: 0.4955\n",
      "Epoch 797/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 8.7190e-04 - accuracy: 1.0000 - val_loss: 4.1104 - val_accuracy: 0.4865\n",
      "Epoch 798/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.6804e-04 - accuracy: 1.0000 - val_loss: 4.1206 - val_accuracy: 0.4820\n",
      "Epoch 799/1000\n",
      "1991/1991 [==============================] - 0s 76us/step - loss: 8.8106e-04 - accuracy: 1.0000 - val_loss: 4.1452 - val_accuracy: 0.4820\n",
      "Epoch 800/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 8.2152e-04 - accuracy: 1.0000 - val_loss: 4.1581 - val_accuracy: 0.4955\n",
      "Epoch 801/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 6.9201e-04 - accuracy: 1.0000 - val_loss: 4.1461 - val_accuracy: 0.5090\n",
      "Epoch 802/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 6.6533e-04 - accuracy: 1.0000 - val_loss: 4.1687 - val_accuracy: 0.5045\n",
      "Epoch 803/1000\n",
      "1991/1991 [==============================] - 0s 73us/step - loss: 6.8528e-04 - accuracy: 1.0000 - val_loss: 4.1701 - val_accuracy: 0.4955\n",
      "Epoch 804/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 7.8518e-04 - accuracy: 1.0000 - val_loss: 4.1805 - val_accuracy: 0.5000\n",
      "Epoch 805/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 7.9469e-04 - accuracy: 1.0000 - val_loss: 4.1719 - val_accuracy: 0.4865\n",
      "Epoch 806/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 8.0644e-04 - accuracy: 1.0000 - val_loss: 4.1821 - val_accuracy: 0.4865\n",
      "Epoch 807/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 5.8201e-04 - accuracy: 1.0000 - val_loss: 4.1971 - val_accuracy: 0.4910\n",
      "Epoch 808/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 5.8613e-04 - accuracy: 1.0000 - val_loss: 4.1826 - val_accuracy: 0.4955\n",
      "Epoch 809/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.3725e-04 - accuracy: 1.0000 - val_loss: 4.2091 - val_accuracy: 0.4955\n",
      "Epoch 810/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.4810e-04 - accuracy: 1.0000 - val_loss: 4.2025 - val_accuracy: 0.4955\n",
      "Epoch 811/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.6738e-04 - accuracy: 1.0000 - val_loss: 4.2187 - val_accuracy: 0.5000\n",
      "Epoch 812/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.6165e-04 - accuracy: 1.0000 - val_loss: 4.2160 - val_accuracy: 0.5090\n",
      "Epoch 813/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.3050e-04 - accuracy: 1.0000 - val_loss: 4.2153 - val_accuracy: 0.5135\n",
      "Epoch 814/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 5.4436e-04 - accuracy: 1.0000 - val_loss: 4.2562 - val_accuracy: 0.5045\n",
      "Epoch 815/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.8442e-04 - accuracy: 1.0000 - val_loss: 4.2478 - val_accuracy: 0.4955\n",
      "Epoch 816/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.7284e-04 - accuracy: 1.0000 - val_loss: 4.2395 - val_accuracy: 0.5000\n",
      "Epoch 817/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.1781e-04 - accuracy: 1.0000 - val_loss: 4.2391 - val_accuracy: 0.4955\n",
      "Epoch 818/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 5.4448e-04 - accuracy: 1.0000 - val_loss: 4.2622 - val_accuracy: 0.5000\n",
      "Epoch 819/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 5.0592e-04 - accuracy: 1.0000 - val_loss: 4.2717 - val_accuracy: 0.4955\n",
      "Epoch 820/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 8.9524e-04 - accuracy: 1.0000 - val_loss: 4.2739 - val_accuracy: 0.4820\n",
      "Epoch 821/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 4.4611 - val_accuracy: 0.4865\n",
      "Epoch 822/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0819 - accuracy: 0.9754 - val_loss: 4.0932 - val_accuracy: 0.5000\n",
      "Epoch 823/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.3750 - accuracy: 0.9086 - val_loss: 4.1601 - val_accuracy: 0.4955\n",
      "Epoch 824/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.4047 - accuracy: 0.8639 - val_loss: 2.2572 - val_accuracy: 0.5045\n",
      "Epoch 825/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.2643 - accuracy: 0.9006 - val_loss: 2.3128 - val_accuracy: 0.4640\n",
      "Epoch 826/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.1161 - accuracy: 0.9618 - val_loss: 2.2803 - val_accuracy: 0.5135\n",
      "Epoch 827/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0470 - accuracy: 0.9869 - val_loss: 2.8330 - val_accuracy: 0.4865\n",
      "Epoch 828/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0241 - accuracy: 0.9950 - val_loss: 3.0670 - val_accuracy: 0.5090\n",
      "Epoch 829/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0122 - accuracy: 0.9980 - val_loss: 3.3145 - val_accuracy: 0.4820\n",
      "Epoch 830/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0091 - accuracy: 0.9990 - val_loss: 3.2749 - val_accuracy: 0.5000\n",
      "Epoch 831/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.3548 - val_accuracy: 0.4775\n",
      "Epoch 832/1000\n",
      "1991/1991 [==============================] - 0s 71us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.3963 - val_accuracy: 0.4820\n",
      "Epoch 833/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.4361 - val_accuracy: 0.4955\n",
      "Epoch 834/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.4256 - val_accuracy: 0.5000\n",
      "Epoch 835/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.5020 - val_accuracy: 0.4910\n",
      "Epoch 836/1000\n",
      "1991/1991 [==============================] - 0s 68us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.5292 - val_accuracy: 0.4865\n",
      "Epoch 837/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.5507 - val_accuracy: 0.4820\n",
      "Epoch 838/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.5695 - val_accuracy: 0.4865\n",
      "Epoch 839/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.5992 - val_accuracy: 0.4820\n",
      "Epoch 840/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.6111 - val_accuracy: 0.4865\n",
      "Epoch 841/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.6270 - val_accuracy: 0.4865\n",
      "Epoch 842/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.6965 - val_accuracy: 0.4865\n",
      "Epoch 843/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.7027 - val_accuracy: 0.4865\n",
      "Epoch 844/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.7196 - val_accuracy: 0.4820\n",
      "Epoch 845/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.7056 - val_accuracy: 0.4865\n",
      "Epoch 846/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.7310 - val_accuracy: 0.4865\n",
      "Epoch 847/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.7499 - val_accuracy: 0.4910\n",
      "Epoch 848/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.7669 - val_accuracy: 0.4865\n",
      "Epoch 849/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.7920 - val_accuracy: 0.4820\n",
      "Epoch 850/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.7898 - val_accuracy: 0.4775\n",
      "Epoch 851/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.8216 - val_accuracy: 0.4865\n",
      "Epoch 852/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8217 - val_accuracy: 0.4865\n",
      "Epoch 853/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.8251 - val_accuracy: 0.4865\n",
      "Epoch 854/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.8459 - val_accuracy: 0.4910\n",
      "Epoch 855/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8736 - val_accuracy: 0.4910\n",
      "Epoch 856/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 9.9846e-04 - accuracy: 1.0000 - val_loss: 3.8697 - val_accuracy: 0.4865\n",
      "Epoch 857/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.8747 - val_accuracy: 0.4865\n",
      "Epoch 858/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 9.9728e-04 - accuracy: 1.0000 - val_loss: 3.8823 - val_accuracy: 0.4820\n",
      "Epoch 859/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.9041 - val_accuracy: 0.4820\n",
      "Epoch 860/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 9.4789e-04 - accuracy: 1.0000 - val_loss: 3.9076 - val_accuracy: 0.4865\n",
      "Epoch 861/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 9.1337e-04 - accuracy: 1.0000 - val_loss: 3.9083 - val_accuracy: 0.4865\n",
      "Epoch 862/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 8.4523e-04 - accuracy: 1.0000 - val_loss: 3.9362 - val_accuracy: 0.4820\n",
      "Epoch 863/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 62us/step - loss: 7.8004e-04 - accuracy: 1.0000 - val_loss: 3.9388 - val_accuracy: 0.4775\n",
      "Epoch 864/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 8.4641e-04 - accuracy: 1.0000 - val_loss: 3.9540 - val_accuracy: 0.4820\n",
      "Epoch 865/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 8.2318e-04 - accuracy: 1.0000 - val_loss: 3.9666 - val_accuracy: 0.4865\n",
      "Epoch 866/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 7.1634e-04 - accuracy: 1.0000 - val_loss: 3.9678 - val_accuracy: 0.4820\n",
      "Epoch 867/1000\n",
      "1991/1991 [==============================] - 0s 61us/step - loss: 7.5930e-04 - accuracy: 1.0000 - val_loss: 3.9852 - val_accuracy: 0.4865\n",
      "Epoch 868/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 8.0680e-04 - accuracy: 1.0000 - val_loss: 3.9803 - val_accuracy: 0.4865\n",
      "Epoch 869/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 6.9070e-04 - accuracy: 1.0000 - val_loss: 3.9925 - val_accuracy: 0.4865\n",
      "Epoch 870/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.7263e-04 - accuracy: 1.0000 - val_loss: 4.0165 - val_accuracy: 0.4865\n",
      "Epoch 871/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 6.3983e-04 - accuracy: 1.0000 - val_loss: 4.0196 - val_accuracy: 0.4820\n",
      "Epoch 872/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.5808e-04 - accuracy: 1.0000 - val_loss: 4.0255 - val_accuracy: 0.4820\n",
      "Epoch 873/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 7.1819e-04 - accuracy: 1.0000 - val_loss: 4.0340 - val_accuracy: 0.4865\n",
      "Epoch 874/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 6.1366e-04 - accuracy: 1.0000 - val_loss: 4.0473 - val_accuracy: 0.4820\n",
      "Epoch 875/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.8421e-04 - accuracy: 1.0000 - val_loss: 4.0650 - val_accuracy: 0.4775\n",
      "Epoch 876/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.8510e-04 - accuracy: 1.0000 - val_loss: 4.0595 - val_accuracy: 0.4820\n",
      "Epoch 877/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.4006e-04 - accuracy: 1.0000 - val_loss: 4.0688 - val_accuracy: 0.4865\n",
      "Epoch 878/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 6.1598e-04 - accuracy: 1.0000 - val_loss: 4.0690 - val_accuracy: 0.4865\n",
      "Epoch 879/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 5.9418e-04 - accuracy: 1.0000 - val_loss: 4.0844 - val_accuracy: 0.4820\n",
      "Epoch 880/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 5.9628e-04 - accuracy: 1.0000 - val_loss: 4.0829 - val_accuracy: 0.4910\n",
      "Epoch 881/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.7570e-04 - accuracy: 1.0000 - val_loss: 4.0859 - val_accuracy: 0.4865\n",
      "Epoch 882/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.3291e-04 - accuracy: 1.0000 - val_loss: 4.1000 - val_accuracy: 0.4775\n",
      "Epoch 883/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.5086e-04 - accuracy: 1.0000 - val_loss: 4.0826 - val_accuracy: 0.4910\n",
      "Epoch 884/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.2154e-04 - accuracy: 1.0000 - val_loss: 4.1064 - val_accuracy: 0.4820\n",
      "Epoch 885/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.9557e-04 - accuracy: 1.0000 - val_loss: 4.1071 - val_accuracy: 0.4820\n",
      "Epoch 886/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 5.2659e-04 - accuracy: 1.0000 - val_loss: 4.1177 - val_accuracy: 0.4910\n",
      "Epoch 887/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.8205e-04 - accuracy: 1.0000 - val_loss: 4.1346 - val_accuracy: 0.4865\n",
      "Epoch 888/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.7355e-04 - accuracy: 1.0000 - val_loss: 4.1584 - val_accuracy: 0.4820\n",
      "Epoch 889/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 5.4174e-04 - accuracy: 1.0000 - val_loss: 4.1656 - val_accuracy: 0.4865\n",
      "Epoch 890/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.8602e-04 - accuracy: 1.0000 - val_loss: 4.1603 - val_accuracy: 0.4865\n",
      "Epoch 891/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 4.3912e-04 - accuracy: 1.0000 - val_loss: 4.1579 - val_accuracy: 0.4865\n",
      "Epoch 892/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.9397e-04 - accuracy: 1.0000 - val_loss: 4.1819 - val_accuracy: 0.4685\n",
      "Epoch 893/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.4089e-04 - accuracy: 1.0000 - val_loss: 4.1768 - val_accuracy: 0.4775\n",
      "Epoch 894/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 3.7723e-04 - accuracy: 1.0000 - val_loss: 4.1877 - val_accuracy: 0.4820\n",
      "Epoch 895/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.6260e-04 - accuracy: 1.0000 - val_loss: 4.1982 - val_accuracy: 0.4775\n",
      "Epoch 896/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.7240e-04 - accuracy: 1.0000 - val_loss: 4.1914 - val_accuracy: 0.4730\n",
      "Epoch 897/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.4840e-04 - accuracy: 1.0000 - val_loss: 4.2049 - val_accuracy: 0.4775\n",
      "Epoch 898/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.0083e-04 - accuracy: 1.0000 - val_loss: 4.2143 - val_accuracy: 0.4820\n",
      "Epoch 899/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.7074e-04 - accuracy: 1.0000 - val_loss: 4.2565 - val_accuracy: 0.4775\n",
      "Epoch 900/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 4.0499e-04 - accuracy: 1.0000 - val_loss: 4.2411 - val_accuracy: 0.4775\n",
      "Epoch 901/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.2547e-04 - accuracy: 1.0000 - val_loss: 4.2292 - val_accuracy: 0.4865\n",
      "Epoch 902/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.1248e-04 - accuracy: 1.0000 - val_loss: 4.2382 - val_accuracy: 0.4730\n",
      "Epoch 903/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 4.1751e-04 - accuracy: 1.0000 - val_loss: 4.2454 - val_accuracy: 0.4820\n",
      "Epoch 904/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 4.0966e-04 - accuracy: 1.0000 - val_loss: 4.2666 - val_accuracy: 0.4820\n",
      "Epoch 905/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.4674e-04 - accuracy: 1.0000 - val_loss: 4.2792 - val_accuracy: 0.4775\n",
      "Epoch 906/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.5241e-04 - accuracy: 1.0000 - val_loss: 4.2918 - val_accuracy: 0.4820\n",
      "Epoch 907/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.6177e-04 - accuracy: 1.0000 - val_loss: 4.2792 - val_accuracy: 0.4730\n",
      "Epoch 908/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.5241e-04 - accuracy: 1.0000 - val_loss: 4.2880 - val_accuracy: 0.4730\n",
      "Epoch 909/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 3.3591e-04 - accuracy: 1.0000 - val_loss: 4.2895 - val_accuracy: 0.4730\n",
      "Epoch 910/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.5396e-04 - accuracy: 1.0000 - val_loss: 4.2957 - val_accuracy: 0.4730\n",
      "Epoch 911/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 3.3235e-04 - accuracy: 1.0000 - val_loss: 4.2939 - val_accuracy: 0.4685\n",
      "Epoch 912/1000\n",
      "1991/1991 [==============================] - 0s 74us/step - loss: 3.6836e-04 - accuracy: 1.0000 - val_loss: 4.2958 - val_accuracy: 0.4820\n",
      "Epoch 913/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 3.3366e-04 - accuracy: 1.0000 - val_loss: 4.3152 - val_accuracy: 0.4730\n",
      "Epoch 914/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 4.0202e-04 - accuracy: 1.0000 - val_loss: 4.3319 - val_accuracy: 0.4820\n",
      "Epoch 915/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.1873e-04 - accuracy: 1.0000 - val_loss: 4.3287 - val_accuracy: 0.4775\n",
      "Epoch 916/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.4480e-04 - accuracy: 1.0000 - val_loss: 4.3214 - val_accuracy: 0.4820\n",
      "Epoch 917/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.9918e-04 - accuracy: 1.0000 - val_loss: 4.3225 - val_accuracy: 0.4775\n",
      "Epoch 918/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 3.2247e-04 - accuracy: 1.0000 - val_loss: 4.3368 - val_accuracy: 0.4820\n",
      "Epoch 919/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.0701e-04 - accuracy: 1.0000 - val_loss: 4.3521 - val_accuracy: 0.4820\n",
      "Epoch 920/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 3.0623e-04 - accuracy: 1.0000 - val_loss: 4.3512 - val_accuracy: 0.4820\n",
      "Epoch 921/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 2.7226e-04 - accuracy: 1.0000 - val_loss: 4.3715 - val_accuracy: 0.4865\n",
      "Epoch 922/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 3.0750e-04 - accuracy: 1.0000 - val_loss: 4.3660 - val_accuracy: 0.4775\n",
      "Epoch 923/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 3.0266e-04 - accuracy: 1.0000 - val_loss: 4.3604 - val_accuracy: 0.4730\n",
      "Epoch 924/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.8603e-04 - accuracy: 1.0000 - val_loss: 4.3477 - val_accuracy: 0.4775\n",
      "Epoch 925/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.4932e-04 - accuracy: 1.0000 - val_loss: 4.3720 - val_accuracy: 0.4775\n",
      "Epoch 926/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 3.2085e-04 - accuracy: 1.0000 - val_loss: 4.3733 - val_accuracy: 0.4775\n",
      "Epoch 927/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.9243e-04 - accuracy: 1.0000 - val_loss: 4.3796 - val_accuracy: 0.4685\n",
      "Epoch 928/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.3702e-04 - accuracy: 1.0000 - val_loss: 4.3871 - val_accuracy: 0.4775\n",
      "Epoch 929/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.5553e-04 - accuracy: 1.0000 - val_loss: 4.4125 - val_accuracy: 0.4730\n",
      "Epoch 930/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.7785e-04 - accuracy: 1.0000 - val_loss: 4.4097 - val_accuracy: 0.4730\n",
      "Epoch 931/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.6281e-04 - accuracy: 1.0000 - val_loss: 4.3924 - val_accuracy: 0.4730\n",
      "Epoch 932/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.6601e-04 - accuracy: 1.0000 - val_loss: 4.4031 - val_accuracy: 0.4730\n",
      "Epoch 933/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.2444e-04 - accuracy: 1.0000 - val_loss: 4.4034 - val_accuracy: 0.4730\n",
      "Epoch 934/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.6025e-04 - accuracy: 1.0000 - val_loss: 4.4109 - val_accuracy: 0.4730\n",
      "Epoch 935/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.6038e-04 - accuracy: 1.0000 - val_loss: 4.4199 - val_accuracy: 0.4685\n",
      "Epoch 936/1000\n",
      "1991/1991 [==============================] - 0s 70us/step - loss: 2.3865e-04 - accuracy: 1.0000 - val_loss: 4.4378 - val_accuracy: 0.4685\n",
      "Epoch 937/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.6881e-04 - accuracy: 1.0000 - val_loss: 4.4410 - val_accuracy: 0.4685\n",
      "Epoch 938/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1599e-04 - accuracy: 1.0000 - val_loss: 4.4466 - val_accuracy: 0.4685\n",
      "Epoch 939/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.8323e-04 - accuracy: 1.0000 - val_loss: 4.4550 - val_accuracy: 0.4775\n",
      "Epoch 940/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.1251e-04 - accuracy: 1.0000 - val_loss: 4.4613 - val_accuracy: 0.4775\n",
      "Epoch 941/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.3847e-04 - accuracy: 1.0000 - val_loss: 4.4558 - val_accuracy: 0.4730\n",
      "Epoch 942/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.1161e-04 - accuracy: 1.0000 - val_loss: 4.4654 - val_accuracy: 0.4730\n",
      "Epoch 943/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1961e-04 - accuracy: 1.0000 - val_loss: 4.4709 - val_accuracy: 0.4775\n",
      "Epoch 944/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1867e-04 - accuracy: 1.0000 - val_loss: 4.4728 - val_accuracy: 0.4685\n",
      "Epoch 945/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1009e-04 - accuracy: 1.0000 - val_loss: 4.4762 - val_accuracy: 0.4820\n",
      "Epoch 946/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.1464e-04 - accuracy: 1.0000 - val_loss: 4.4860 - val_accuracy: 0.4775\n",
      "Epoch 947/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.4494e-04 - accuracy: 1.0000 - val_loss: 4.4858 - val_accuracy: 0.4685\n",
      "Epoch 948/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 2.0278e-04 - accuracy: 1.0000 - val_loss: 4.4953 - val_accuracy: 0.4820\n",
      "Epoch 949/1000\n",
      "1991/1991 [==============================] - 0s 66us/step - loss: 1.8309e-04 - accuracy: 1.0000 - val_loss: 4.5129 - val_accuracy: 0.4820\n",
      "Epoch 950/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.0258e-04 - accuracy: 1.0000 - val_loss: 4.5192 - val_accuracy: 0.4730\n",
      "Epoch 951/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.2968e-04 - accuracy: 1.0000 - val_loss: 4.5015 - val_accuracy: 0.4865\n",
      "Epoch 952/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 2.1974e-04 - accuracy: 1.0000 - val_loss: 4.5108 - val_accuracy: 0.4730\n",
      "Epoch 953/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9663e-04 - accuracy: 1.0000 - val_loss: 4.5199 - val_accuracy: 0.4730\n",
      "Epoch 954/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.8730e-04 - accuracy: 1.0000 - val_loss: 4.5230 - val_accuracy: 0.4730\n",
      "Epoch 955/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.9804e-04 - accuracy: 1.0000 - val_loss: 4.5429 - val_accuracy: 0.4730\n",
      "Epoch 956/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.8458e-04 - accuracy: 1.0000 - val_loss: 4.5287 - val_accuracy: 0.4820\n",
      "Epoch 957/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.7715e-04 - accuracy: 1.0000 - val_loss: 4.5399 - val_accuracy: 0.4730\n",
      "Epoch 958/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.7956e-04 - accuracy: 1.0000 - val_loss: 4.5553 - val_accuracy: 0.4730\n",
      "Epoch 959/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.7983e-04 - accuracy: 1.0000 - val_loss: 4.5523 - val_accuracy: 0.4730\n",
      "Epoch 960/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.8151e-04 - accuracy: 1.0000 - val_loss: 4.5740 - val_accuracy: 0.4775\n",
      "Epoch 961/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9655e-04 - accuracy: 1.0000 - val_loss: 4.5663 - val_accuracy: 0.4730\n",
      "Epoch 962/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.6892e-04 - accuracy: 1.0000 - val_loss: 4.5806 - val_accuracy: 0.4730\n",
      "Epoch 963/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.7444e-04 - accuracy: 1.0000 - val_loss: 4.5804 - val_accuracy: 0.4730\n",
      "Epoch 964/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.8399e-04 - accuracy: 1.0000 - val_loss: 4.5780 - val_accuracy: 0.4730\n",
      "Epoch 965/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 2.2553e-04 - accuracy: 1.0000 - val_loss: 4.5834 - val_accuracy: 0.4730\n",
      "Epoch 966/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9212e-04 - accuracy: 1.0000 - val_loss: 4.5849 - val_accuracy: 0.4730\n",
      "Epoch 967/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 2.0119e-04 - accuracy: 1.0000 - val_loss: 4.5894 - val_accuracy: 0.4730\n",
      "Epoch 968/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.6128e-04 - accuracy: 1.0000 - val_loss: 4.6000 - val_accuracy: 0.4730\n",
      "Epoch 969/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.6898e-04 - accuracy: 1.0000 - val_loss: 4.6057 - val_accuracy: 0.4730\n",
      "Epoch 970/1000\n",
      "1991/1991 [==============================] - 0s 67us/step - loss: 1.5494e-04 - accuracy: 1.0000 - val_loss: 4.5998 - val_accuracy: 0.4730\n",
      "Epoch 971/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 1.7211e-04 - accuracy: 1.0000 - val_loss: 4.6042 - val_accuracy: 0.4820\n",
      "Epoch 972/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.9832e-04 - accuracy: 1.0000 - val_loss: 4.6233 - val_accuracy: 0.4730\n",
      "Epoch 973/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.6200e-04 - accuracy: 1.0000 - val_loss: 4.6487 - val_accuracy: 0.4775\n",
      "Epoch 974/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.4685e-04 - accuracy: 1.0000 - val_loss: 4.6410 - val_accuracy: 0.4730\n",
      "Epoch 975/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.3731e-04 - accuracy: 1.0000 - val_loss: 4.6343 - val_accuracy: 0.4775\n",
      "Epoch 976/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.5053e-04 - accuracy: 1.0000 - val_loss: 4.6356 - val_accuracy: 0.4730\n",
      "Epoch 977/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.8311e-04 - accuracy: 1.0000 - val_loss: 4.6430 - val_accuracy: 0.4730\n",
      "Epoch 978/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.7269e-04 - accuracy: 1.0000 - val_loss: 4.6518 - val_accuracy: 0.4730\n",
      "Epoch 979/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.7627e-04 - accuracy: 1.0000 - val_loss: 4.6666 - val_accuracy: 0.4775\n",
      "Epoch 980/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.2929e-04 - accuracy: 1.0000 - val_loss: 4.6587 - val_accuracy: 0.4820\n",
      "Epoch 981/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.5153e-04 - accuracy: 1.0000 - val_loss: 4.6750 - val_accuracy: 0.4775\n",
      "Epoch 982/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2589e-04 - accuracy: 1.0000 - val_loss: 4.6738 - val_accuracy: 0.4685\n",
      "Epoch 983/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.3110e-04 - accuracy: 1.0000 - val_loss: 4.6793 - val_accuracy: 0.4775\n",
      "Epoch 984/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.4028e-04 - accuracy: 1.0000 - val_loss: 4.6826 - val_accuracy: 0.4730\n",
      "Epoch 985/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2172e-04 - accuracy: 1.0000 - val_loss: 4.6902 - val_accuracy: 0.4685\n",
      "Epoch 986/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.3399e-04 - accuracy: 1.0000 - val_loss: 4.6839 - val_accuracy: 0.4820\n",
      "Epoch 987/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2840e-04 - accuracy: 1.0000 - val_loss: 4.6840 - val_accuracy: 0.4775\n",
      "Epoch 988/1000\n",
      "1991/1991 [==============================] - 0s 62us/step - loss: 1.3923e-04 - accuracy: 1.0000 - val_loss: 4.6909 - val_accuracy: 0.4730\n",
      "Epoch 989/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.3954e-04 - accuracy: 1.0000 - val_loss: 4.7048 - val_accuracy: 0.4775\n",
      "Epoch 990/1000\n",
      "1991/1991 [==============================] - 0s 65us/step - loss: 1.3932e-04 - accuracy: 1.0000 - val_loss: 4.7049 - val_accuracy: 0.4730\n",
      "Epoch 991/1000\n",
      "1991/1991 [==============================] - 0s 69us/step - loss: 1.3503e-04 - accuracy: 1.0000 - val_loss: 4.6944 - val_accuracy: 0.4730\n",
      "Epoch 992/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.3071e-04 - accuracy: 1.0000 - val_loss: 4.7109 - val_accuracy: 0.4730\n",
      "Epoch 993/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.2281e-04 - accuracy: 1.0000 - val_loss: 4.7234 - val_accuracy: 0.4775\n",
      "Epoch 994/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.0739e-04 - accuracy: 1.0000 - val_loss: 4.7288 - val_accuracy: 0.4775\n",
      "Epoch 995/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.4688e-04 - accuracy: 1.0000 - val_loss: 4.7255 - val_accuracy: 0.4685\n",
      "Epoch 996/1000\n",
      "1991/1991 [==============================] - 0s 63us/step - loss: 1.1691e-04 - accuracy: 1.0000 - val_loss: 4.7199 - val_accuracy: 0.4820\n",
      "Epoch 997/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.1754e-04 - accuracy: 1.0000 - val_loss: 4.7145 - val_accuracy: 0.4730\n",
      "Epoch 998/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.3745e-04 - accuracy: 1.0000 - val_loss: 4.7393 - val_accuracy: 0.4685\n",
      "Epoch 999/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.3337e-04 - accuracy: 1.0000 - val_loss: 4.7466 - val_accuracy: 0.4820\n",
      "Epoch 1000/1000\n",
      "1991/1991 [==============================] - 0s 64us/step - loss: 1.0747e-04 - accuracy: 1.0000 - val_loss: 4.7363 - val_accuracy: 0.4820\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "246/246 [==============================] - 0s 23us/step\n",
      "test loss, test acc: [4.854146275093885, 0.5487805008888245]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (246, 1)\n",
      "rmse: 0.6615410340599218\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 2\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=500, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, input_shape=(3, 92), kernel_initializer=\"glorot_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, return_sequences=True, kernel_initializer=\"glorot_normal\")`\n",
      "  \"\"\"\n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, kernel_initializer=\"glorot_normal\")`\n",
      "  \n",
      "/Users/chinkashiwakin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_normal\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_76 (LSTM)               (None, 3, 128)            113152    \n",
      "_________________________________________________________________\n",
      "lstm_77 (LSTM)               (None, 3, 64)             49408     \n",
      "_________________________________________________________________\n",
      "lstm_78 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 175,009\n",
      "Trainable params: 175,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1990 samples, validate on 222 samples\n",
      "Epoch 1/1000\n",
      "1990/1990 [==============================] - 1s 702us/step - loss: 0.6922 - accuracy: 0.5286 - val_loss: 0.6867 - val_accuracy: 0.5766\n",
      "Epoch 2/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6920 - accuracy: 0.5276 - val_loss: 0.6901 - val_accuracy: 0.5766\n",
      "Epoch 3/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinkashiwakin/anaconda3/lib/python3.6/site-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6922 - accuracy: 0.5191 - val_loss: 0.6885 - val_accuracy: 0.5766\n",
      "Epoch 4/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6915 - accuracy: 0.5281 - val_loss: 0.6868 - val_accuracy: 0.5766\n",
      "Epoch 5/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6912 - accuracy: 0.5296 - val_loss: 0.6881 - val_accuracy: 0.5766\n",
      "Epoch 6/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6904 - accuracy: 0.5286 - val_loss: 0.6892 - val_accuracy: 0.5631\n",
      "Epoch 7/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6909 - accuracy: 0.5422 - val_loss: 0.6909 - val_accuracy: 0.5676\n",
      "Epoch 8/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6899 - accuracy: 0.5382 - val_loss: 0.6942 - val_accuracy: 0.5225\n",
      "Epoch 9/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6907 - accuracy: 0.5427 - val_loss: 0.6901 - val_accuracy: 0.5631\n",
      "Epoch 10/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6894 - accuracy: 0.5412 - val_loss: 0.6942 - val_accuracy: 0.5225\n",
      "Epoch 11/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6901 - accuracy: 0.5387 - val_loss: 0.6988 - val_accuracy: 0.5090\n",
      "Epoch 12/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6872 - accuracy: 0.5513 - val_loss: 0.6887 - val_accuracy: 0.5676\n",
      "Epoch 13/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6899 - accuracy: 0.5317 - val_loss: 0.6895 - val_accuracy: 0.5631\n",
      "Epoch 14/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6894 - accuracy: 0.5377 - val_loss: 0.6891 - val_accuracy: 0.5586\n",
      "Epoch 15/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6884 - accuracy: 0.5492 - val_loss: 0.7029 - val_accuracy: 0.5270\n",
      "Epoch 16/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.6901 - accuracy: 0.5397 - val_loss: 0.6835 - val_accuracy: 0.5721\n",
      "Epoch 17/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6901 - accuracy: 0.5302 - val_loss: 0.6890 - val_accuracy: 0.5676\n",
      "Epoch 18/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.6903 - accuracy: 0.5452 - val_loss: 0.6902 - val_accuracy: 0.5676\n",
      "Epoch 19/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6887 - accuracy: 0.5462 - val_loss: 0.6970 - val_accuracy: 0.5315\n",
      "Epoch 20/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6873 - accuracy: 0.5538 - val_loss: 0.7000 - val_accuracy: 0.5360\n",
      "Epoch 21/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6866 - accuracy: 0.5513 - val_loss: 0.7159 - val_accuracy: 0.4685\n",
      "Epoch 22/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6901 - accuracy: 0.5337 - val_loss: 0.6856 - val_accuracy: 0.5631\n",
      "Epoch 23/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6890 - accuracy: 0.5392 - val_loss: 0.6918 - val_accuracy: 0.5586\n",
      "Epoch 24/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6877 - accuracy: 0.5553 - val_loss: 0.6966 - val_accuracy: 0.5405\n",
      "Epoch 25/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6862 - accuracy: 0.5523 - val_loss: 0.7176 - val_accuracy: 0.4955\n",
      "Epoch 26/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6865 - accuracy: 0.5372 - val_loss: 0.6895 - val_accuracy: 0.5586\n",
      "Epoch 27/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6868 - accuracy: 0.5437 - val_loss: 0.7132 - val_accuracy: 0.4685\n",
      "Epoch 28/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6871 - accuracy: 0.5407 - val_loss: 0.6976 - val_accuracy: 0.5405\n",
      "Epoch 29/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6874 - accuracy: 0.5603 - val_loss: 0.6957 - val_accuracy: 0.5495\n",
      "Epoch 30/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6861 - accuracy: 0.5477 - val_loss: 0.7012 - val_accuracy: 0.5541\n",
      "Epoch 31/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 0.6873 - accuracy: 0.5538 - val_loss: 0.6942 - val_accuracy: 0.5495\n",
      "Epoch 32/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6841 - accuracy: 0.5563 - val_loss: 0.7010 - val_accuracy: 0.5541\n",
      "Epoch 33/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6853 - accuracy: 0.5513 - val_loss: 0.7063 - val_accuracy: 0.5270\n",
      "Epoch 34/1000\n",
      "1990/1990 [==============================] - 0s 86us/step - loss: 0.6843 - accuracy: 0.5528 - val_loss: 0.7035 - val_accuracy: 0.5360\n",
      "Epoch 35/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6843 - accuracy: 0.5472 - val_loss: 0.6912 - val_accuracy: 0.5450\n",
      "Epoch 36/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6838 - accuracy: 0.5553 - val_loss: 0.7335 - val_accuracy: 0.4234\n",
      "Epoch 37/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6851 - accuracy: 0.5518 - val_loss: 0.6970 - val_accuracy: 0.5586\n",
      "Epoch 38/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6863 - accuracy: 0.5593 - val_loss: 0.6898 - val_accuracy: 0.5586\n",
      "Epoch 39/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6855 - accuracy: 0.5558 - val_loss: 0.7053 - val_accuracy: 0.5225\n",
      "Epoch 40/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6840 - accuracy: 0.5533 - val_loss: 0.7054 - val_accuracy: 0.5495\n",
      "Epoch 41/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6864 - accuracy: 0.5578 - val_loss: 0.6867 - val_accuracy: 0.5631\n",
      "Epoch 42/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6854 - accuracy: 0.5538 - val_loss: 0.7011 - val_accuracy: 0.5315\n",
      "Epoch 43/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6824 - accuracy: 0.5653 - val_loss: 0.7100 - val_accuracy: 0.5405\n",
      "Epoch 44/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.6828 - accuracy: 0.5618 - val_loss: 0.7007 - val_accuracy: 0.5450\n",
      "Epoch 45/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6816 - accuracy: 0.5693 - val_loss: 0.7209 - val_accuracy: 0.4955\n",
      "Epoch 46/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6844 - accuracy: 0.5568 - val_loss: 0.7142 - val_accuracy: 0.5270\n",
      "Epoch 47/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6828 - accuracy: 0.5678 - val_loss: 0.7023 - val_accuracy: 0.5225\n",
      "Epoch 48/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6830 - accuracy: 0.5573 - val_loss: 0.7038 - val_accuracy: 0.5405\n",
      "Epoch 49/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6821 - accuracy: 0.5613 - val_loss: 0.6990 - val_accuracy: 0.5450\n",
      "Epoch 50/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6811 - accuracy: 0.5593 - val_loss: 0.7011 - val_accuracy: 0.5405\n",
      "Epoch 51/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6826 - accuracy: 0.5553 - val_loss: 0.7068 - val_accuracy: 0.5045\n",
      "Epoch 52/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6824 - accuracy: 0.5613 - val_loss: 0.6973 - val_accuracy: 0.5450\n",
      "Epoch 53/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6790 - accuracy: 0.5643 - val_loss: 0.7116 - val_accuracy: 0.5360\n",
      "Epoch 54/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6833 - accuracy: 0.5583 - val_loss: 0.6963 - val_accuracy: 0.5450\n",
      "Epoch 55/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6815 - accuracy: 0.5683 - val_loss: 0.7033 - val_accuracy: 0.5495\n",
      "Epoch 56/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6782 - accuracy: 0.5719 - val_loss: 0.7143 - val_accuracy: 0.5360\n",
      "Epoch 57/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6791 - accuracy: 0.5653 - val_loss: 0.6977 - val_accuracy: 0.5495\n",
      "Epoch 58/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6853 - accuracy: 0.5513 - val_loss: 0.6942 - val_accuracy: 0.5405\n",
      "Epoch 59/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6810 - accuracy: 0.5683 - val_loss: 0.7104 - val_accuracy: 0.5135\n",
      "Epoch 60/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6799 - accuracy: 0.5709 - val_loss: 0.7033 - val_accuracy: 0.5450\n",
      "Epoch 61/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6761 - accuracy: 0.5764 - val_loss: 0.7379 - val_accuracy: 0.4550\n",
      "Epoch 62/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6832 - accuracy: 0.5548 - val_loss: 0.7194 - val_accuracy: 0.4685\n",
      "Epoch 63/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6830 - accuracy: 0.5704 - val_loss: 0.7022 - val_accuracy: 0.5405\n",
      "Epoch 64/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6822 - accuracy: 0.5603 - val_loss: 0.7085 - val_accuracy: 0.5135\n",
      "Epoch 65/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6793 - accuracy: 0.5673 - val_loss: 0.7151 - val_accuracy: 0.4955\n",
      "Epoch 66/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6782 - accuracy: 0.5714 - val_loss: 0.7145 - val_accuracy: 0.5090\n",
      "Epoch 67/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6767 - accuracy: 0.5739 - val_loss: 0.6934 - val_accuracy: 0.5405\n",
      "Epoch 68/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6750 - accuracy: 0.5744 - val_loss: 0.7068 - val_accuracy: 0.5090\n",
      "Epoch 69/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6811 - accuracy: 0.5668 - val_loss: 0.7346 - val_accuracy: 0.4324\n",
      "Epoch 70/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6810 - accuracy: 0.5588 - val_loss: 0.6914 - val_accuracy: 0.5541\n",
      "Epoch 71/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6771 - accuracy: 0.5698 - val_loss: 0.7176 - val_accuracy: 0.4820\n",
      "Epoch 72/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6758 - accuracy: 0.5784 - val_loss: 0.7176 - val_accuracy: 0.4640\n",
      "Epoch 73/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6768 - accuracy: 0.5784 - val_loss: 0.7197 - val_accuracy: 0.5135\n",
      "Epoch 74/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6758 - accuracy: 0.5653 - val_loss: 0.7209 - val_accuracy: 0.4910\n",
      "Epoch 75/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6759 - accuracy: 0.5698 - val_loss: 0.7131 - val_accuracy: 0.5135\n",
      "Epoch 76/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6748 - accuracy: 0.5769 - val_loss: 0.6959 - val_accuracy: 0.5450\n",
      "Epoch 77/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6865 - accuracy: 0.5518 - val_loss: 0.7191 - val_accuracy: 0.4865\n",
      "Epoch 78/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.6780 - accuracy: 0.5698 - val_loss: 0.7119 - val_accuracy: 0.5180\n",
      "Epoch 79/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6746 - accuracy: 0.5819 - val_loss: 0.7186 - val_accuracy: 0.5045\n",
      "Epoch 80/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6715 - accuracy: 0.5789 - val_loss: 0.7226 - val_accuracy: 0.5180\n",
      "Epoch 81/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6720 - accuracy: 0.5739 - val_loss: 0.7376 - val_accuracy: 0.4775\n",
      "Epoch 82/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6791 - accuracy: 0.5608 - val_loss: 0.7196 - val_accuracy: 0.4910\n",
      "Epoch 83/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.6724 - accuracy: 0.5824 - val_loss: 0.7251 - val_accuracy: 0.5045\n",
      "Epoch 84/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.6705 - accuracy: 0.5809 - val_loss: 0.7300 - val_accuracy: 0.4910\n",
      "Epoch 85/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6733 - accuracy: 0.5719 - val_loss: 0.7196 - val_accuracy: 0.4865\n",
      "Epoch 86/1000\n",
      "1990/1990 [==============================] - 0s 126us/step - loss: 0.6677 - accuracy: 0.5844 - val_loss: 0.7441 - val_accuracy: 0.4955\n",
      "Epoch 87/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 0.6746 - accuracy: 0.5719 - val_loss: 0.7271 - val_accuracy: 0.4550\n",
      "Epoch 88/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 0.6738 - accuracy: 0.5729 - val_loss: 0.7334 - val_accuracy: 0.4910\n",
      "Epoch 89/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6706 - accuracy: 0.5864 - val_loss: 0.7253 - val_accuracy: 0.4775\n",
      "Epoch 90/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6688 - accuracy: 0.5834 - val_loss: 0.7199 - val_accuracy: 0.5135\n",
      "Epoch 91/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6749 - accuracy: 0.5744 - val_loss: 0.6897 - val_accuracy: 0.5450\n",
      "Epoch 92/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6726 - accuracy: 0.5533 - val_loss: 0.7048 - val_accuracy: 0.5270\n",
      "Epoch 93/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.6705 - accuracy: 0.5779 - val_loss: 0.7240 - val_accuracy: 0.4865\n",
      "Epoch 94/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6668 - accuracy: 0.5819 - val_loss: 0.7169 - val_accuracy: 0.5090\n",
      "Epoch 95/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6683 - accuracy: 0.5844 - val_loss: 0.7250 - val_accuracy: 0.4550\n",
      "Epoch 96/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6674 - accuracy: 0.5889 - val_loss: 0.7547 - val_accuracy: 0.4865\n",
      "Epoch 97/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6671 - accuracy: 0.5950 - val_loss: 0.7328 - val_accuracy: 0.4595\n",
      "Epoch 98/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.6593 - accuracy: 0.5935 - val_loss: 0.7584 - val_accuracy: 0.5000\n",
      "Epoch 99/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6595 - accuracy: 0.5970 - val_loss: 0.7656 - val_accuracy: 0.4640\n",
      "Epoch 100/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 0.6647 - accuracy: 0.5819 - val_loss: 0.7128 - val_accuracy: 0.5135\n",
      "Epoch 101/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6625 - accuracy: 0.5864 - val_loss: 0.7395 - val_accuracy: 0.4910\n",
      "Epoch 102/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 0.6599 - accuracy: 0.5935 - val_loss: 0.7134 - val_accuracy: 0.4910\n",
      "Epoch 103/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6603 - accuracy: 0.5975 - val_loss: 0.7491 - val_accuracy: 0.4820\n",
      "Epoch 104/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.6660 - accuracy: 0.5960 - val_loss: 0.7401 - val_accuracy: 0.4955\n",
      "Epoch 105/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6643 - accuracy: 0.5925 - val_loss: 0.7333 - val_accuracy: 0.4685\n",
      "Epoch 106/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.6665 - accuracy: 0.5819 - val_loss: 0.7350 - val_accuracy: 0.4910\n",
      "Epoch 107/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 0.6566 - accuracy: 0.5995 - val_loss: 0.7543 - val_accuracy: 0.4459\n",
      "Epoch 108/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6560 - accuracy: 0.6005 - val_loss: 0.7288 - val_accuracy: 0.4595\n",
      "Epoch 109/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.6593 - accuracy: 0.5995 - val_loss: 0.7648 - val_accuracy: 0.4730\n",
      "Epoch 110/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.6579 - accuracy: 0.5859 - val_loss: 0.7481 - val_accuracy: 0.4595\n",
      "Epoch 111/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.6535 - accuracy: 0.5975 - val_loss: 0.7598 - val_accuracy: 0.4775\n",
      "Epoch 112/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.6469 - accuracy: 0.6040 - val_loss: 0.7899 - val_accuracy: 0.4279\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6512 - accuracy: 0.5985 - val_loss: 0.7459 - val_accuracy: 0.5270\n",
      "Epoch 114/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.6482 - accuracy: 0.6020 - val_loss: 0.7523 - val_accuracy: 0.4730\n",
      "Epoch 115/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.6499 - accuracy: 0.6090 - val_loss: 0.7682 - val_accuracy: 0.4550\n",
      "Epoch 116/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6450 - accuracy: 0.6085 - val_loss: 0.7819 - val_accuracy: 0.4595\n",
      "Epoch 117/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6475 - accuracy: 0.6196 - val_loss: 0.7843 - val_accuracy: 0.4459\n",
      "Epoch 118/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.6339 - accuracy: 0.6201 - val_loss: 0.7875 - val_accuracy: 0.4685\n",
      "Epoch 119/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6347 - accuracy: 0.6151 - val_loss: 0.8081 - val_accuracy: 0.4324\n",
      "Epoch 120/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6370 - accuracy: 0.6106 - val_loss: 0.7705 - val_accuracy: 0.4595\n",
      "Epoch 121/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.6356 - accuracy: 0.6151 - val_loss: 0.8092 - val_accuracy: 0.4595\n",
      "Epoch 122/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6267 - accuracy: 0.6146 - val_loss: 0.8094 - val_accuracy: 0.4550\n",
      "Epoch 123/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6307 - accuracy: 0.6206 - val_loss: 0.8042 - val_accuracy: 0.4865\n",
      "Epoch 124/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.6269 - accuracy: 0.6246 - val_loss: 0.8020 - val_accuracy: 0.4730\n",
      "Epoch 125/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6262 - accuracy: 0.6402 - val_loss: 0.7769 - val_accuracy: 0.4730\n",
      "Epoch 126/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.6254 - accuracy: 0.6266 - val_loss: 0.8442 - val_accuracy: 0.4730\n",
      "Epoch 127/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.6235 - accuracy: 0.6317 - val_loss: 0.8341 - val_accuracy: 0.4595\n",
      "Epoch 128/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6161 - accuracy: 0.6457 - val_loss: 0.8192 - val_accuracy: 0.4550\n",
      "Epoch 129/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.6184 - accuracy: 0.6317 - val_loss: 0.8439 - val_accuracy: 0.4414\n",
      "Epoch 130/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.6079 - accuracy: 0.6402 - val_loss: 0.8796 - val_accuracy: 0.4414\n",
      "Epoch 131/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.6061 - accuracy: 0.6503 - val_loss: 0.8792 - val_accuracy: 0.4685\n",
      "Epoch 132/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.5980 - accuracy: 0.6467 - val_loss: 0.9028 - val_accuracy: 0.4910\n",
      "Epoch 133/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.5978 - accuracy: 0.6603 - val_loss: 0.8628 - val_accuracy: 0.4505\n",
      "Epoch 134/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.5980 - accuracy: 0.6583 - val_loss: 0.8497 - val_accuracy: 0.4640\n",
      "Epoch 135/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.5941 - accuracy: 0.6593 - val_loss: 0.9377 - val_accuracy: 0.4820\n",
      "Epoch 136/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.5798 - accuracy: 0.6663 - val_loss: 0.9004 - val_accuracy: 0.4550\n",
      "Epoch 137/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.5714 - accuracy: 0.6789 - val_loss: 0.9579 - val_accuracy: 0.4955\n",
      "Epoch 138/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.5900 - accuracy: 0.6543 - val_loss: 0.8998 - val_accuracy: 0.4414\n",
      "Epoch 139/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 0.5647 - accuracy: 0.6744 - val_loss: 0.9630 - val_accuracy: 0.4640\n",
      "Epoch 140/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.5718 - accuracy: 0.6729 - val_loss: 0.9894 - val_accuracy: 0.4505\n",
      "Epoch 141/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.5682 - accuracy: 0.6764 - val_loss: 0.8618 - val_accuracy: 0.4865\n",
      "Epoch 142/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.5685 - accuracy: 0.6663 - val_loss: 0.9684 - val_accuracy: 0.4685\n",
      "Epoch 143/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.5645 - accuracy: 0.6749 - val_loss: 1.0286 - val_accuracy: 0.4414\n",
      "Epoch 144/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.5418 - accuracy: 0.6925 - val_loss: 1.0184 - val_accuracy: 0.4414\n",
      "Epoch 145/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.5274 - accuracy: 0.7141 - val_loss: 1.0611 - val_accuracy: 0.4685\n",
      "Epoch 146/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.5395 - accuracy: 0.7030 - val_loss: 0.9474 - val_accuracy: 0.5360\n",
      "Epoch 147/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.5418 - accuracy: 0.6925 - val_loss: 1.1308 - val_accuracy: 0.4730\n",
      "Epoch 148/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.5208 - accuracy: 0.7111 - val_loss: 1.1187 - val_accuracy: 0.4685\n",
      "Epoch 149/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.5235 - accuracy: 0.7065 - val_loss: 1.0654 - val_accuracy: 0.4820\n",
      "Epoch 150/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.5141 - accuracy: 0.7116 - val_loss: 1.0761 - val_accuracy: 0.4459\n",
      "Epoch 151/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.4963 - accuracy: 0.7296 - val_loss: 1.1753 - val_accuracy: 0.4640\n",
      "Epoch 152/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.5002 - accuracy: 0.7337 - val_loss: 1.1576 - val_accuracy: 0.4595\n",
      "Epoch 153/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.4905 - accuracy: 0.7362 - val_loss: 1.2019 - val_accuracy: 0.4910\n",
      "Epoch 154/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4750 - accuracy: 0.7538 - val_loss: 1.2110 - val_accuracy: 0.4865\n",
      "Epoch 155/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.4680 - accuracy: 0.7588 - val_loss: 1.1722 - val_accuracy: 0.4730\n",
      "Epoch 156/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.4720 - accuracy: 0.7457 - val_loss: 1.1327 - val_accuracy: 0.4955\n",
      "Epoch 157/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4694 - accuracy: 0.7533 - val_loss: 1.1826 - val_accuracy: 0.4595\n",
      "Epoch 158/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.4576 - accuracy: 0.7623 - val_loss: 1.4520 - val_accuracy: 0.4279\n",
      "Epoch 159/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.4409 - accuracy: 0.7709 - val_loss: 1.3463 - val_accuracy: 0.4685\n",
      "Epoch 160/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4267 - accuracy: 0.7794 - val_loss: 1.3526 - val_accuracy: 0.4505\n",
      "Epoch 161/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.4267 - accuracy: 0.7814 - val_loss: 1.3637 - val_accuracy: 0.4505\n",
      "Epoch 162/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4167 - accuracy: 0.7920 - val_loss: 1.3198 - val_accuracy: 0.4550\n",
      "Epoch 163/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.4202 - accuracy: 0.7905 - val_loss: 1.4339 - val_accuracy: 0.4955\n",
      "Epoch 164/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.4036 - accuracy: 0.7990 - val_loss: 1.4015 - val_accuracy: 0.4685\n",
      "Epoch 165/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.4072 - accuracy: 0.7915 - val_loss: 1.5174 - val_accuracy: 0.4595\n",
      "Epoch 166/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.3904 - accuracy: 0.8020 - val_loss: 1.4252 - val_accuracy: 0.4685\n",
      "Epoch 167/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.3706 - accuracy: 0.8261 - val_loss: 1.4898 - val_accuracy: 0.5000\n",
      "Epoch 168/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.3692 - accuracy: 0.8151 - val_loss: 1.4527 - val_accuracy: 0.4550\n",
      "Epoch 169/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.3728 - accuracy: 0.8286 - val_loss: 1.6456 - val_accuracy: 0.4459\n",
      "Epoch 170/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.3568 - accuracy: 0.8241 - val_loss: 1.6076 - val_accuracy: 0.4595\n",
      "Epoch 171/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.3477 - accuracy: 0.8266 - val_loss: 1.5056 - val_accuracy: 0.4595\n",
      "Epoch 172/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 0.3475 - accuracy: 0.8357 - val_loss: 1.5863 - val_accuracy: 0.4730\n",
      "Epoch 173/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 0.3446 - accuracy: 0.8322 - val_loss: 1.6658 - val_accuracy: 0.4550\n",
      "Epoch 174/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.3169 - accuracy: 0.8462 - val_loss: 1.6536 - val_accuracy: 0.4955\n",
      "Epoch 175/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.3014 - accuracy: 0.8563 - val_loss: 1.7772 - val_accuracy: 0.4820\n",
      "Epoch 176/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.2961 - accuracy: 0.8603 - val_loss: 1.6557 - val_accuracy: 0.4820\n",
      "Epoch 177/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.2878 - accuracy: 0.8633 - val_loss: 1.9159 - val_accuracy: 0.4550\n",
      "Epoch 178/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.2840 - accuracy: 0.8769 - val_loss: 1.7792 - val_accuracy: 0.4865\n",
      "Epoch 179/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2947 - accuracy: 0.8638 - val_loss: 1.8041 - val_accuracy: 0.4685\n",
      "Epoch 180/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2722 - accuracy: 0.8739 - val_loss: 1.7839 - val_accuracy: 0.4865\n",
      "Epoch 181/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.2801 - accuracy: 0.8678 - val_loss: 1.8630 - val_accuracy: 0.4820\n",
      "Epoch 182/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2666 - accuracy: 0.8789 - val_loss: 1.9274 - val_accuracy: 0.4550\n",
      "Epoch 183/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.2499 - accuracy: 0.8869 - val_loss: 2.0262 - val_accuracy: 0.4730\n",
      "Epoch 184/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.2442 - accuracy: 0.8829 - val_loss: 2.0631 - val_accuracy: 0.4640\n",
      "Epoch 185/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.2230 - accuracy: 0.8955 - val_loss: 2.0401 - val_accuracy: 0.4775\n",
      "Epoch 186/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.2255 - accuracy: 0.9030 - val_loss: 2.1304 - val_accuracy: 0.4550\n",
      "Epoch 187/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2027 - accuracy: 0.9106 - val_loss: 2.1124 - val_accuracy: 0.4685\n",
      "Epoch 188/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.2069 - accuracy: 0.9101 - val_loss: 2.3011 - val_accuracy: 0.4820\n",
      "Epoch 189/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.1923 - accuracy: 0.9161 - val_loss: 2.3558 - val_accuracy: 0.4595\n",
      "Epoch 190/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.1945 - accuracy: 0.9181 - val_loss: 2.3019 - val_accuracy: 0.4685\n",
      "Epoch 191/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.2008 - accuracy: 0.9121 - val_loss: 2.2740 - val_accuracy: 0.4685\n",
      "Epoch 192/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.1879 - accuracy: 0.9186 - val_loss: 2.4165 - val_accuracy: 0.4640\n",
      "Epoch 193/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 0.1788 - accuracy: 0.9261 - val_loss: 2.5076 - val_accuracy: 0.4685\n",
      "Epoch 194/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.1645 - accuracy: 0.9296 - val_loss: 2.4701 - val_accuracy: 0.4865\n",
      "Epoch 195/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.1531 - accuracy: 0.9352 - val_loss: 2.4771 - val_accuracy: 0.5000\n",
      "Epoch 196/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.1492 - accuracy: 0.9372 - val_loss: 2.7046 - val_accuracy: 0.4414\n",
      "Epoch 197/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.1737 - accuracy: 0.9211 - val_loss: 2.6853 - val_accuracy: 0.4685\n",
      "Epoch 198/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.1777 - accuracy: 0.9256 - val_loss: 2.4866 - val_accuracy: 0.4865\n",
      "Epoch 199/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.1754 - accuracy: 0.9236 - val_loss: 2.3268 - val_accuracy: 0.4775\n",
      "Epoch 200/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.2358 - accuracy: 0.8985 - val_loss: 2.3834 - val_accuracy: 0.4730\n",
      "Epoch 201/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.1487 - accuracy: 0.9377 - val_loss: 2.6309 - val_accuracy: 0.4775\n",
      "Epoch 202/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.1231 - accuracy: 0.9553 - val_loss: 2.6867 - val_accuracy: 0.4595\n",
      "Epoch 203/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.1155 - accuracy: 0.9508 - val_loss: 2.6858 - val_accuracy: 0.4955\n",
      "Epoch 204/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.1176 - accuracy: 0.9563 - val_loss: 2.7692 - val_accuracy: 0.4865\n",
      "Epoch 205/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0994 - accuracy: 0.9593 - val_loss: 2.8518 - val_accuracy: 0.4955\n",
      "Epoch 206/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.1171 - accuracy: 0.9508 - val_loss: 2.9593 - val_accuracy: 0.4414\n",
      "Epoch 207/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.1281 - accuracy: 0.9568 - val_loss: 3.0122 - val_accuracy: 0.4414\n",
      "Epoch 208/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.1148 - accuracy: 0.9578 - val_loss: 2.8815 - val_accuracy: 0.4955\n",
      "Epoch 209/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0944 - accuracy: 0.9643 - val_loss: 2.9828 - val_accuracy: 0.4550\n",
      "Epoch 210/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0801 - accuracy: 0.9709 - val_loss: 3.1625 - val_accuracy: 0.4955\n",
      "Epoch 211/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0822 - accuracy: 0.9658 - val_loss: 3.0168 - val_accuracy: 0.4775\n",
      "Epoch 212/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0955 - accuracy: 0.9658 - val_loss: 3.1456 - val_accuracy: 0.4775\n",
      "Epoch 213/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0739 - accuracy: 0.9749 - val_loss: 3.0052 - val_accuracy: 0.4955\n",
      "Epoch 214/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0874 - accuracy: 0.9633 - val_loss: 3.2103 - val_accuracy: 0.4640\n",
      "Epoch 215/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.1007 - accuracy: 0.9598 - val_loss: 3.0499 - val_accuracy: 0.4730\n",
      "Epoch 216/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.1177 - accuracy: 0.9528 - val_loss: 3.1158 - val_accuracy: 0.4820\n",
      "Epoch 217/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0887 - accuracy: 0.9623 - val_loss: 3.2189 - val_accuracy: 0.4550\n",
      "Epoch 218/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.0859 - accuracy: 0.9658 - val_loss: 3.1855 - val_accuracy: 0.4910\n",
      "Epoch 219/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0742 - accuracy: 0.9744 - val_loss: 3.2223 - val_accuracy: 0.4730\n",
      "Epoch 220/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0884 - accuracy: 0.9678 - val_loss: 3.5224 - val_accuracy: 0.4775\n",
      "Epoch 221/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0689 - accuracy: 0.9774 - val_loss: 3.2929 - val_accuracy: 0.4730\n",
      "Epoch 222/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.0698 - accuracy: 0.9714 - val_loss: 3.3984 - val_accuracy: 0.4775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0620 - accuracy: 0.9774 - val_loss: 3.1714 - val_accuracy: 0.5000\n",
      "Epoch 224/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.0528 - accuracy: 0.9814 - val_loss: 3.4007 - val_accuracy: 0.4865\n",
      "Epoch 225/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0446 - accuracy: 0.9859 - val_loss: 3.5156 - val_accuracy: 0.4910\n",
      "Epoch 226/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0379 - accuracy: 0.9910 - val_loss: 3.5253 - val_accuracy: 0.4685\n",
      "Epoch 227/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0312 - accuracy: 0.9915 - val_loss: 3.4614 - val_accuracy: 0.4730\n",
      "Epoch 228/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.0274 - accuracy: 0.9940 - val_loss: 3.3898 - val_accuracy: 0.4865\n",
      "Epoch 229/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.0321 - accuracy: 0.9925 - val_loss: 3.6957 - val_accuracy: 0.4505\n",
      "Epoch 230/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0311 - accuracy: 0.9910 - val_loss: 3.4622 - val_accuracy: 0.4820\n",
      "Epoch 231/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.0222 - accuracy: 0.9945 - val_loss: 3.7273 - val_accuracy: 0.4865\n",
      "Epoch 232/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.0222 - accuracy: 0.9950 - val_loss: 3.5711 - val_accuracy: 0.4910\n",
      "Epoch 233/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.0196 - accuracy: 0.9950 - val_loss: 3.7494 - val_accuracy: 0.4775\n",
      "Epoch 234/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 0.0232 - accuracy: 0.9945 - val_loss: 3.6694 - val_accuracy: 0.4775\n",
      "Epoch 235/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0346 - accuracy: 0.9859 - val_loss: 3.7181 - val_accuracy: 0.4820\n",
      "Epoch 236/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.0396 - accuracy: 0.9854 - val_loss: 3.8542 - val_accuracy: 0.4685\n",
      "Epoch 237/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0526 - accuracy: 0.9839 - val_loss: 3.7096 - val_accuracy: 0.4550\n",
      "Epoch 238/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.0383 - accuracy: 0.9879 - val_loss: 3.7065 - val_accuracy: 0.4865\n",
      "Epoch 239/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 0.0319 - accuracy: 0.9899 - val_loss: 3.8491 - val_accuracy: 0.4865\n",
      "Epoch 240/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0349 - accuracy: 0.9899 - val_loss: 3.8125 - val_accuracy: 0.4820\n",
      "Epoch 241/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 0.0395 - accuracy: 0.9834 - val_loss: 3.7933 - val_accuracy: 0.5000\n",
      "Epoch 242/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.0302 - accuracy: 0.9889 - val_loss: 3.9163 - val_accuracy: 0.4955\n",
      "Epoch 243/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0271 - accuracy: 0.9925 - val_loss: 3.7228 - val_accuracy: 0.5000\n",
      "Epoch 244/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.0288 - accuracy: 0.9910 - val_loss: 4.0175 - val_accuracy: 0.4820\n",
      "Epoch 245/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.0245 - accuracy: 0.9935 - val_loss: 3.9452 - val_accuracy: 0.5000\n",
      "Epoch 246/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0338 - accuracy: 0.9935 - val_loss: 3.9187 - val_accuracy: 0.4910\n",
      "Epoch 247/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0278 - accuracy: 0.9905 - val_loss: 3.9500 - val_accuracy: 0.4910\n",
      "Epoch 248/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.0562 - accuracy: 0.9859 - val_loss: 3.9910 - val_accuracy: 0.4955\n",
      "Epoch 249/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 0.1151 - accuracy: 0.9643 - val_loss: 3.6621 - val_accuracy: 0.5225\n",
      "Epoch 250/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 0.2319 - accuracy: 0.9241 - val_loss: 3.7612 - val_accuracy: 0.4955\n",
      "Epoch 251/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.2129 - accuracy: 0.9271 - val_loss: 3.4293 - val_accuracy: 0.4820\n",
      "Epoch 252/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.1512 - accuracy: 0.9382 - val_loss: 3.0004 - val_accuracy: 0.5045\n",
      "Epoch 253/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0739 - accuracy: 0.9719 - val_loss: 2.9218 - val_accuracy: 0.4910\n",
      "Epoch 254/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.0465 - accuracy: 0.9834 - val_loss: 3.1764 - val_accuracy: 0.4775\n",
      "Epoch 255/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0259 - accuracy: 0.9945 - val_loss: 3.4646 - val_accuracy: 0.4730\n",
      "Epoch 256/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 0.0197 - accuracy: 0.9975 - val_loss: 3.3005 - val_accuracy: 0.4685\n",
      "Epoch 257/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 0.0117 - accuracy: 0.9990 - val_loss: 3.3910 - val_accuracy: 0.4820\n",
      "Epoch 258/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 3.4332 - val_accuracy: 0.4820\n",
      "Epoch 259/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.5578 - val_accuracy: 0.4640\n",
      "Epoch 260/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 3.6062 - val_accuracy: 0.4685\n",
      "Epoch 261/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.6639 - val_accuracy: 0.4730\n",
      "Epoch 262/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.6583 - val_accuracy: 0.4730\n",
      "Epoch 263/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 3.7060 - val_accuracy: 0.4730\n",
      "Epoch 264/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 0.0067 - accuracy: 0.9995 - val_loss: 3.6962 - val_accuracy: 0.4865\n",
      "Epoch 265/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 3.7981 - val_accuracy: 0.4775\n",
      "Epoch 266/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.7969 - val_accuracy: 0.4775\n",
      "Epoch 267/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.8325 - val_accuracy: 0.4730\n",
      "Epoch 268/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.8601 - val_accuracy: 0.4775\n",
      "Epoch 269/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.9044 - val_accuracy: 0.4730\n",
      "Epoch 270/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.8870 - val_accuracy: 0.4730\n",
      "Epoch 271/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.9251 - val_accuracy: 0.4730\n",
      "Epoch 272/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9465 - val_accuracy: 0.4775\n",
      "Epoch 273/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.9532 - val_accuracy: 0.4730\n",
      "Epoch 274/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.9651 - val_accuracy: 0.4730\n",
      "Epoch 275/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.9845 - val_accuracy: 0.4730\n",
      "Epoch 276/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.9963 - val_accuracy: 0.4730\n",
      "Epoch 277/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.0065 - val_accuracy: 0.4775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.0368 - val_accuracy: 0.4775\n",
      "Epoch 279/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0409 - val_accuracy: 0.4730\n",
      "Epoch 280/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0467 - val_accuracy: 0.4730\n",
      "Epoch 281/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0764 - val_accuracy: 0.4775\n",
      "Epoch 282/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0924 - val_accuracy: 0.4730\n",
      "Epoch 283/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.0990 - val_accuracy: 0.4730\n",
      "Epoch 284/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1083 - val_accuracy: 0.4730\n",
      "Epoch 285/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1213 - val_accuracy: 0.4730\n",
      "Epoch 286/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.1319 - val_accuracy: 0.4730\n",
      "Epoch 287/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1328 - val_accuracy: 0.4730\n",
      "Epoch 288/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1628 - val_accuracy: 0.4775\n",
      "Epoch 289/1000\n",
      "1990/1990 [==============================] - 0s 116us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1656 - val_accuracy: 0.4820\n",
      "Epoch 290/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1676 - val_accuracy: 0.4730\n",
      "Epoch 291/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.1836 - val_accuracy: 0.4730\n",
      "Epoch 292/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1983 - val_accuracy: 0.4730\n",
      "Epoch 293/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2093 - val_accuracy: 0.4730\n",
      "Epoch 294/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.2151 - val_accuracy: 0.4730\n",
      "Epoch 295/1000\n",
      "1990/1990 [==============================] - 0s 132us/step - loss: 9.7641e-04 - accuracy: 1.0000 - val_loss: 4.2301 - val_accuracy: 0.4730\n",
      "Epoch 296/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 9.5048e-04 - accuracy: 1.0000 - val_loss: 4.2366 - val_accuracy: 0.4730\n",
      "Epoch 297/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 9.6787e-04 - accuracy: 1.0000 - val_loss: 4.2438 - val_accuracy: 0.4775\n",
      "Epoch 298/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 9.1847e-04 - accuracy: 1.0000 - val_loss: 4.2579 - val_accuracy: 0.4820\n",
      "Epoch 299/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 8.7240e-04 - accuracy: 1.0000 - val_loss: 4.2659 - val_accuracy: 0.4775\n",
      "Epoch 300/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 8.1701e-04 - accuracy: 1.0000 - val_loss: 4.2787 - val_accuracy: 0.4775\n",
      "Epoch 301/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 8.2179e-04 - accuracy: 1.0000 - val_loss: 4.2991 - val_accuracy: 0.4775\n",
      "Epoch 302/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 8.2143e-04 - accuracy: 1.0000 - val_loss: 4.3047 - val_accuracy: 0.4775\n",
      "Epoch 303/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 7.9452e-04 - accuracy: 1.0000 - val_loss: 4.3081 - val_accuracy: 0.4730\n",
      "Epoch 304/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 7.9430e-04 - accuracy: 1.0000 - val_loss: 4.3179 - val_accuracy: 0.4730\n",
      "Epoch 305/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 7.9148e-04 - accuracy: 1.0000 - val_loss: 4.3324 - val_accuracy: 0.4730\n",
      "Epoch 306/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 7.7981e-04 - accuracy: 1.0000 - val_loss: 4.3397 - val_accuracy: 0.4730\n",
      "Epoch 307/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 7.3421e-04 - accuracy: 1.0000 - val_loss: 4.3483 - val_accuracy: 0.4775\n",
      "Epoch 308/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 7.2464e-04 - accuracy: 1.0000 - val_loss: 4.3559 - val_accuracy: 0.4775\n",
      "Epoch 309/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 7.0982e-04 - accuracy: 1.0000 - val_loss: 4.3584 - val_accuracy: 0.4820\n",
      "Epoch 310/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 6.9027e-04 - accuracy: 1.0000 - val_loss: 4.3655 - val_accuracy: 0.4775\n",
      "Epoch 311/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 6.5628e-04 - accuracy: 1.0000 - val_loss: 4.3763 - val_accuracy: 0.4775\n",
      "Epoch 312/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.6808e-04 - accuracy: 1.0000 - val_loss: 4.3840 - val_accuracy: 0.4820\n",
      "Epoch 313/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.5562e-04 - accuracy: 1.0000 - val_loss: 4.3826 - val_accuracy: 0.4865\n",
      "Epoch 314/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.8369e-04 - accuracy: 1.0000 - val_loss: 4.3918 - val_accuracy: 0.4820\n",
      "Epoch 315/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.2654e-04 - accuracy: 1.0000 - val_loss: 4.4005 - val_accuracy: 0.4820\n",
      "Epoch 316/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 5.9235e-04 - accuracy: 1.0000 - val_loss: 4.4149 - val_accuracy: 0.4865\n",
      "Epoch 317/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 5.7421e-04 - accuracy: 1.0000 - val_loss: 4.4283 - val_accuracy: 0.4820\n",
      "Epoch 318/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.5796e-04 - accuracy: 1.0000 - val_loss: 4.4361 - val_accuracy: 0.4820\n",
      "Epoch 319/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.2794e-04 - accuracy: 1.0000 - val_loss: 4.4309 - val_accuracy: 0.4820\n",
      "Epoch 320/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 5.5433e-04 - accuracy: 1.0000 - val_loss: 4.4476 - val_accuracy: 0.4865\n",
      "Epoch 321/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.1772e-04 - accuracy: 1.0000 - val_loss: 4.4566 - val_accuracy: 0.4910\n",
      "Epoch 322/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.5924e-04 - accuracy: 1.0000 - val_loss: 4.4633 - val_accuracy: 0.4865\n",
      "Epoch 323/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.4030e-04 - accuracy: 1.0000 - val_loss: 4.4769 - val_accuracy: 0.4910\n",
      "Epoch 324/1000\n",
      "1990/1990 [==============================] - 0s 87us/step - loss: 5.2165e-04 - accuracy: 1.0000 - val_loss: 4.4630 - val_accuracy: 0.4775\n",
      "Epoch 325/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 5.1659e-04 - accuracy: 1.0000 - val_loss: 4.4651 - val_accuracy: 0.4865\n",
      "Epoch 326/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.0466e-04 - accuracy: 1.0000 - val_loss: 4.4837 - val_accuracy: 0.4865\n",
      "Epoch 327/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 5.0614e-04 - accuracy: 1.0000 - val_loss: 4.4922 - val_accuracy: 0.4865\n",
      "Epoch 328/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 4.7692e-04 - accuracy: 1.0000 - val_loss: 4.4979 - val_accuracy: 0.4820\n",
      "Epoch 329/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 4.6550e-04 - accuracy: 1.0000 - val_loss: 4.5122 - val_accuracy: 0.4910\n",
      "Epoch 330/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 4.6811e-04 - accuracy: 1.0000 - val_loss: 4.5104 - val_accuracy: 0.4955\n",
      "Epoch 331/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 4.5234e-04 - accuracy: 1.0000 - val_loss: 4.5210 - val_accuracy: 0.4865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.3693e-04 - accuracy: 1.0000 - val_loss: 4.5375 - val_accuracy: 0.4865\n",
      "Epoch 333/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.3417e-04 - accuracy: 1.0000 - val_loss: 4.5434 - val_accuracy: 0.4910\n",
      "Epoch 334/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 4.4036e-04 - accuracy: 1.0000 - val_loss: 4.5483 - val_accuracy: 0.4865\n",
      "Epoch 335/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 4.1160e-04 - accuracy: 1.0000 - val_loss: 4.5515 - val_accuracy: 0.4910\n",
      "Epoch 336/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 4.1954e-04 - accuracy: 1.0000 - val_loss: 4.5668 - val_accuracy: 0.4865\n",
      "Epoch 337/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 4.0923e-04 - accuracy: 1.0000 - val_loss: 4.5784 - val_accuracy: 0.4910\n",
      "Epoch 338/1000\n",
      "1990/1990 [==============================] - 0s 86us/step - loss: 4.0947e-04 - accuracy: 1.0000 - val_loss: 4.5837 - val_accuracy: 0.4865\n",
      "Epoch 339/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 3.8391e-04 - accuracy: 1.0000 - val_loss: 4.5809 - val_accuracy: 0.4865\n",
      "Epoch 340/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 4.0077e-04 - accuracy: 1.0000 - val_loss: 4.5872 - val_accuracy: 0.4910\n",
      "Epoch 341/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 3.9692e-04 - accuracy: 1.0000 - val_loss: 4.5960 - val_accuracy: 0.4910\n",
      "Epoch 342/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 3.8702e-04 - accuracy: 1.0000 - val_loss: 4.5965 - val_accuracy: 0.4910\n",
      "Epoch 343/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 3.5996e-04 - accuracy: 1.0000 - val_loss: 4.6064 - val_accuracy: 0.4865\n",
      "Epoch 344/1000\n",
      "1990/1990 [==============================] - 0s 130us/step - loss: 3.8531e-04 - accuracy: 1.0000 - val_loss: 4.6187 - val_accuracy: 0.4910\n",
      "Epoch 345/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 3.6159e-04 - accuracy: 1.0000 - val_loss: 4.6293 - val_accuracy: 0.4910\n",
      "Epoch 346/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 3.6288e-04 - accuracy: 1.0000 - val_loss: 4.6256 - val_accuracy: 0.4955\n",
      "Epoch 347/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 3.5099e-04 - accuracy: 1.0000 - val_loss: 4.6309 - val_accuracy: 0.4955\n",
      "Epoch 348/1000\n",
      "1990/1990 [==============================] - 0s 156us/step - loss: 3.4839e-04 - accuracy: 1.0000 - val_loss: 4.6444 - val_accuracy: 0.4865\n",
      "Epoch 349/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 3.4492e-04 - accuracy: 1.0000 - val_loss: 4.6473 - val_accuracy: 0.4910\n",
      "Epoch 350/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 3.2922e-04 - accuracy: 1.0000 - val_loss: 4.6501 - val_accuracy: 0.4910\n",
      "Epoch 351/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 3.2485e-04 - accuracy: 1.0000 - val_loss: 4.6556 - val_accuracy: 0.4910\n",
      "Epoch 352/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.2587e-04 - accuracy: 1.0000 - val_loss: 4.6719 - val_accuracy: 0.4865\n",
      "Epoch 353/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 3.0848e-04 - accuracy: 1.0000 - val_loss: 4.6652 - val_accuracy: 0.4910\n",
      "Epoch 354/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 3.1164e-04 - accuracy: 1.0000 - val_loss: 4.6760 - val_accuracy: 0.4910\n",
      "Epoch 355/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 3.1815e-04 - accuracy: 1.0000 - val_loss: 4.6825 - val_accuracy: 0.4910\n",
      "Epoch 356/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 3.0567e-04 - accuracy: 1.0000 - val_loss: 4.6894 - val_accuracy: 0.4910\n",
      "Epoch 357/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 2.7877e-04 - accuracy: 1.0000 - val_loss: 4.6926 - val_accuracy: 0.4910\n",
      "Epoch 358/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 2.8914e-04 - accuracy: 1.0000 - val_loss: 4.6937 - val_accuracy: 0.4910\n",
      "Epoch 359/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 2.8830e-04 - accuracy: 1.0000 - val_loss: 4.6973 - val_accuracy: 0.4955\n",
      "Epoch 360/1000\n",
      "1990/1990 [==============================] - 0s 87us/step - loss: 2.8832e-04 - accuracy: 1.0000 - val_loss: 4.7147 - val_accuracy: 0.4910\n",
      "Epoch 361/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 2.8016e-04 - accuracy: 1.0000 - val_loss: 4.7181 - val_accuracy: 0.4910\n",
      "Epoch 362/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 2.5803e-04 - accuracy: 1.0000 - val_loss: 4.7221 - val_accuracy: 0.4955\n",
      "Epoch 363/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 2.5849e-04 - accuracy: 1.0000 - val_loss: 4.7303 - val_accuracy: 0.4910\n",
      "Epoch 364/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 2.7780e-04 - accuracy: 1.0000 - val_loss: 4.7311 - val_accuracy: 0.4910\n",
      "Epoch 365/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 2.5572e-04 - accuracy: 1.0000 - val_loss: 4.7397 - val_accuracy: 0.4910\n",
      "Epoch 366/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 2.5787e-04 - accuracy: 1.0000 - val_loss: 4.7441 - val_accuracy: 0.4910\n",
      "Epoch 367/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 2.5554e-04 - accuracy: 1.0000 - val_loss: 4.7482 - val_accuracy: 0.4910\n",
      "Epoch 368/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 2.5661e-04 - accuracy: 1.0000 - val_loss: 4.7608 - val_accuracy: 0.4910\n",
      "Epoch 369/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.4560e-04 - accuracy: 1.0000 - val_loss: 4.7672 - val_accuracy: 0.4910\n",
      "Epoch 370/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 2.4376e-04 - accuracy: 1.0000 - val_loss: 4.7654 - val_accuracy: 0.4910\n",
      "Epoch 371/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.4439e-04 - accuracy: 1.0000 - val_loss: 4.7703 - val_accuracy: 0.4910\n",
      "Epoch 372/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 2.2267e-04 - accuracy: 1.0000 - val_loss: 4.7835 - val_accuracy: 0.4865\n",
      "Epoch 373/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 2.1846e-04 - accuracy: 1.0000 - val_loss: 4.7877 - val_accuracy: 0.4865\n",
      "Epoch 374/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.2741e-04 - accuracy: 1.0000 - val_loss: 4.7924 - val_accuracy: 0.4910\n",
      "Epoch 375/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 2.2176e-04 - accuracy: 1.0000 - val_loss: 4.7885 - val_accuracy: 0.4910\n",
      "Epoch 376/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 2.2169e-04 - accuracy: 1.0000 - val_loss: 4.7964 - val_accuracy: 0.4910\n",
      "Epoch 377/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.2109e-04 - accuracy: 1.0000 - val_loss: 4.8080 - val_accuracy: 0.4910\n",
      "Epoch 378/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.0939e-04 - accuracy: 1.0000 - val_loss: 4.8110 - val_accuracy: 0.4910\n",
      "Epoch 379/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.2118e-04 - accuracy: 1.0000 - val_loss: 4.8131 - val_accuracy: 0.4910\n",
      "Epoch 380/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 2.1352e-04 - accuracy: 1.0000 - val_loss: 4.8202 - val_accuracy: 0.4910\n",
      "Epoch 381/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 2.1356e-04 - accuracy: 1.0000 - val_loss: 4.8296 - val_accuracy: 0.4910\n",
      "Epoch 382/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 2.0075e-04 - accuracy: 1.0000 - val_loss: 4.8319 - val_accuracy: 0.4910\n",
      "Epoch 383/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.8989e-04 - accuracy: 1.0000 - val_loss: 4.8311 - val_accuracy: 0.4910\n",
      "Epoch 384/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 1.9882e-04 - accuracy: 1.0000 - val_loss: 4.8415 - val_accuracy: 0.4910\n",
      "Epoch 385/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.9325e-04 - accuracy: 1.0000 - val_loss: 4.8494 - val_accuracy: 0.4910\n",
      "Epoch 386/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 1.8855e-04 - accuracy: 1.0000 - val_loss: 4.8465 - val_accuracy: 0.4910\n",
      "Epoch 387/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 1.8903e-04 - accuracy: 1.0000 - val_loss: 4.8580 - val_accuracy: 0.4910\n",
      "Epoch 388/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 1.8901e-04 - accuracy: 1.0000 - val_loss: 4.8688 - val_accuracy: 0.4910\n",
      "Epoch 389/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 1.8790e-04 - accuracy: 1.0000 - val_loss: 4.8699 - val_accuracy: 0.4910\n",
      "Epoch 390/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.0039e-04 - accuracy: 1.0000 - val_loss: 4.8629 - val_accuracy: 0.4910\n",
      "Epoch 391/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.8903e-04 - accuracy: 1.0000 - val_loss: 4.8656 - val_accuracy: 0.4910\n",
      "Epoch 392/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.8528e-04 - accuracy: 1.0000 - val_loss: 4.8930 - val_accuracy: 0.4910\n",
      "Epoch 393/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 1.7990e-04 - accuracy: 1.0000 - val_loss: 4.8863 - val_accuracy: 0.4910\n",
      "Epoch 394/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 1.8386e-04 - accuracy: 1.0000 - val_loss: 4.8922 - val_accuracy: 0.4865\n",
      "Epoch 395/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.6810e-04 - accuracy: 1.0000 - val_loss: 4.9002 - val_accuracy: 0.4910\n",
      "Epoch 396/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7126e-04 - accuracy: 1.0000 - val_loss: 4.9058 - val_accuracy: 0.4910\n",
      "Epoch 397/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.6036e-04 - accuracy: 1.0000 - val_loss: 4.9074 - val_accuracy: 0.4910\n",
      "Epoch 398/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.7164e-04 - accuracy: 1.0000 - val_loss: 4.9193 - val_accuracy: 0.4910\n",
      "Epoch 399/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 1.6615e-04 - accuracy: 1.0000 - val_loss: 4.9112 - val_accuracy: 0.4910\n",
      "Epoch 400/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 1.6374e-04 - accuracy: 1.0000 - val_loss: 4.9110 - val_accuracy: 0.4910\n",
      "Epoch 401/1000\n",
      "1990/1990 [==============================] - 0s 131us/step - loss: 1.6309e-04 - accuracy: 1.0000 - val_loss: 4.9313 - val_accuracy: 0.4910\n",
      "Epoch 402/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.6410e-04 - accuracy: 1.0000 - val_loss: 4.9326 - val_accuracy: 0.4910\n",
      "Epoch 403/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.4641e-04 - accuracy: 1.0000 - val_loss: 4.9346 - val_accuracy: 0.4910\n",
      "Epoch 404/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 1.6110e-04 - accuracy: 1.0000 - val_loss: 4.9442 - val_accuracy: 0.4910\n",
      "Epoch 405/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 1.6054e-04 - accuracy: 1.0000 - val_loss: 4.9508 - val_accuracy: 0.4910\n",
      "Epoch 406/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 1.5457e-04 - accuracy: 1.0000 - val_loss: 4.9607 - val_accuracy: 0.4910\n",
      "Epoch 407/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 1.4264e-04 - accuracy: 1.0000 - val_loss: 4.9638 - val_accuracy: 0.4910\n",
      "Epoch 408/1000\n",
      "1990/1990 [==============================] - 0s 139us/step - loss: 1.4391e-04 - accuracy: 1.0000 - val_loss: 4.9668 - val_accuracy: 0.4910\n",
      "Epoch 409/1000\n",
      "1990/1990 [==============================] - 0s 157us/step - loss: 1.5436e-04 - accuracy: 1.0000 - val_loss: 4.9665 - val_accuracy: 0.4910\n",
      "Epoch 410/1000\n",
      "1990/1990 [==============================] - 0s 137us/step - loss: 1.4470e-04 - accuracy: 1.0000 - val_loss: 4.9688 - val_accuracy: 0.4910\n",
      "Epoch 411/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 1.4749e-04 - accuracy: 1.0000 - val_loss: 4.9848 - val_accuracy: 0.4910\n",
      "Epoch 412/1000\n",
      "1990/1990 [==============================] - 0s 125us/step - loss: 1.5016e-04 - accuracy: 1.0000 - val_loss: 4.9823 - val_accuracy: 0.4910\n",
      "Epoch 413/1000\n",
      "1990/1990 [==============================] - 0s 136us/step - loss: 1.3885e-04 - accuracy: 1.0000 - val_loss: 4.9864 - val_accuracy: 0.4910\n",
      "Epoch 414/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 1.3748e-04 - accuracy: 1.0000 - val_loss: 4.9960 - val_accuracy: 0.4865\n",
      "Epoch 415/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 1.4117e-04 - accuracy: 1.0000 - val_loss: 4.9934 - val_accuracy: 0.4865\n",
      "Epoch 416/1000\n",
      "1990/1990 [==============================] - 0s 126us/step - loss: 1.3092e-04 - accuracy: 1.0000 - val_loss: 4.9971 - val_accuracy: 0.4910\n",
      "Epoch 417/1000\n",
      "1990/1990 [==============================] - 0s 158us/step - loss: 1.2880e-04 - accuracy: 1.0000 - val_loss: 4.9986 - val_accuracy: 0.4865\n",
      "Epoch 418/1000\n",
      "1990/1990 [==============================] - 0s 127us/step - loss: 1.2617e-04 - accuracy: 1.0000 - val_loss: 5.0014 - val_accuracy: 0.4820\n",
      "Epoch 419/1000\n",
      "1990/1990 [==============================] - 0s 143us/step - loss: 1.2658e-04 - accuracy: 1.0000 - val_loss: 5.0071 - val_accuracy: 0.4865\n",
      "Epoch 420/1000\n",
      "1990/1990 [==============================] - 0s 135us/step - loss: 1.2396e-04 - accuracy: 1.0000 - val_loss: 5.0131 - val_accuracy: 0.4910\n",
      "Epoch 421/1000\n",
      "1990/1990 [==============================] - 0s 153us/step - loss: 1.2414e-04 - accuracy: 1.0000 - val_loss: 5.0269 - val_accuracy: 0.4865\n",
      "Epoch 422/1000\n",
      "1990/1990 [==============================] - 0s 144us/step - loss: 1.2387e-04 - accuracy: 1.0000 - val_loss: 5.0326 - val_accuracy: 0.4865\n",
      "Epoch 423/1000\n",
      "1990/1990 [==============================] - 0s 131us/step - loss: 1.2522e-04 - accuracy: 1.0000 - val_loss: 5.0378 - val_accuracy: 0.4910\n",
      "Epoch 424/1000\n",
      "1990/1990 [==============================] - 0s 138us/step - loss: 1.1654e-04 - accuracy: 1.0000 - val_loss: 5.0413 - val_accuracy: 0.4910\n",
      "Epoch 425/1000\n",
      "1990/1990 [==============================] - 0s 138us/step - loss: 1.1729e-04 - accuracy: 1.0000 - val_loss: 5.0467 - val_accuracy: 0.4910\n",
      "Epoch 426/1000\n",
      "1990/1990 [==============================] - 0s 159us/step - loss: 1.2015e-04 - accuracy: 1.0000 - val_loss: 5.0458 - val_accuracy: 0.4955\n",
      "Epoch 427/1000\n",
      "1990/1990 [==============================] - 0s 143us/step - loss: 1.1001e-04 - accuracy: 1.0000 - val_loss: 5.0451 - val_accuracy: 0.4955\n",
      "Epoch 428/1000\n",
      "1990/1990 [==============================] - 0s 136us/step - loss: 1.1686e-04 - accuracy: 1.0000 - val_loss: 5.0587 - val_accuracy: 0.4955\n",
      "Epoch 429/1000\n",
      "1990/1990 [==============================] - 0s 150us/step - loss: 1.1310e-04 - accuracy: 1.0000 - val_loss: 5.0598 - val_accuracy: 0.4910\n",
      "Epoch 430/1000\n",
      "1990/1990 [==============================] - 0s 159us/step - loss: 1.1206e-04 - accuracy: 1.0000 - val_loss: 5.0673 - val_accuracy: 0.4910\n",
      "Epoch 431/1000\n",
      "1990/1990 [==============================] - 0s 146us/step - loss: 1.1084e-04 - accuracy: 1.0000 - val_loss: 5.0729 - val_accuracy: 0.4865\n",
      "Epoch 432/1000\n",
      "1990/1990 [==============================] - 0s 155us/step - loss: 1.0658e-04 - accuracy: 1.0000 - val_loss: 5.0674 - val_accuracy: 0.4910\n",
      "Epoch 433/1000\n",
      "1990/1990 [==============================] - 0s 141us/step - loss: 1.0518e-04 - accuracy: 1.0000 - val_loss: 5.0724 - val_accuracy: 0.4955\n",
      "Epoch 434/1000\n",
      "1990/1990 [==============================] - 0s 156us/step - loss: 1.0544e-04 - accuracy: 1.0000 - val_loss: 5.0789 - val_accuracy: 0.4910\n",
      "Epoch 435/1000\n",
      "1990/1990 [==============================] - 0s 135us/step - loss: 1.1260e-04 - accuracy: 1.0000 - val_loss: 5.0910 - val_accuracy: 0.4955\n",
      "Epoch 436/1000\n",
      "1990/1990 [==============================] - 0s 161us/step - loss: 1.0209e-04 - accuracy: 1.0000 - val_loss: 5.0936 - val_accuracy: 0.4865\n",
      "Epoch 437/1000\n",
      "1990/1990 [==============================] - 0s 141us/step - loss: 1.0711e-04 - accuracy: 1.0000 - val_loss: 5.0953 - val_accuracy: 0.4910\n",
      "Epoch 438/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 190us/step - loss: 9.9100e-05 - accuracy: 1.0000 - val_loss: 5.0880 - val_accuracy: 0.4910\n",
      "Epoch 439/1000\n",
      "1990/1990 [==============================] - 0s 189us/step - loss: 9.9842e-05 - accuracy: 1.0000 - val_loss: 5.1033 - val_accuracy: 0.4910\n",
      "Epoch 440/1000\n",
      "1990/1990 [==============================] - 0s 158us/step - loss: 1.0230e-04 - accuracy: 1.0000 - val_loss: 5.1076 - val_accuracy: 0.4865\n",
      "Epoch 441/1000\n",
      "1990/1990 [==============================] - 0s 141us/step - loss: 1.0246e-04 - accuracy: 1.0000 - val_loss: 5.1152 - val_accuracy: 0.4910\n",
      "Epoch 442/1000\n",
      "1990/1990 [==============================] - 0s 137us/step - loss: 9.3670e-05 - accuracy: 1.0000 - val_loss: 5.1174 - val_accuracy: 0.4910\n",
      "Epoch 443/1000\n",
      "1990/1990 [==============================] - 0s 130us/step - loss: 1.0104e-04 - accuracy: 1.0000 - val_loss: 5.1112 - val_accuracy: 0.4955\n",
      "Epoch 444/1000\n",
      "1990/1990 [==============================] - 0s 138us/step - loss: 9.4277e-05 - accuracy: 1.0000 - val_loss: 5.1175 - val_accuracy: 0.4955\n",
      "Epoch 445/1000\n",
      "1990/1990 [==============================] - 0s 144us/step - loss: 9.8694e-05 - accuracy: 1.0000 - val_loss: 5.1289 - val_accuracy: 0.4910\n",
      "Epoch 446/1000\n",
      "1990/1990 [==============================] - 0s 132us/step - loss: 9.7013e-05 - accuracy: 1.0000 - val_loss: 5.1330 - val_accuracy: 0.4910\n",
      "Epoch 447/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 8.8923e-05 - accuracy: 1.0000 - val_loss: 5.1405 - val_accuracy: 0.4910\n",
      "Epoch 448/1000\n",
      "1990/1990 [==============================] - 0s 124us/step - loss: 9.2653e-05 - accuracy: 1.0000 - val_loss: 5.1413 - val_accuracy: 0.4865\n",
      "Epoch 449/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 8.7326e-05 - accuracy: 1.0000 - val_loss: 5.1429 - val_accuracy: 0.4865\n",
      "Epoch 450/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 9.3006e-05 - accuracy: 1.0000 - val_loss: 5.1493 - val_accuracy: 0.4910\n",
      "Epoch 451/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 9.1383e-05 - accuracy: 1.0000 - val_loss: 5.1510 - val_accuracy: 0.4865\n",
      "Epoch 452/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 8.7571e-05 - accuracy: 1.0000 - val_loss: 5.1528 - val_accuracy: 0.4910\n",
      "Epoch 453/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 8.8591e-05 - accuracy: 1.0000 - val_loss: 5.1536 - val_accuracy: 0.4910\n",
      "Epoch 454/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 8.3502e-05 - accuracy: 1.0000 - val_loss: 5.1635 - val_accuracy: 0.4910\n",
      "Epoch 455/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 8.0654e-05 - accuracy: 1.0000 - val_loss: 5.1613 - val_accuracy: 0.4910\n",
      "Epoch 456/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 7.9218e-05 - accuracy: 1.0000 - val_loss: 5.1679 - val_accuracy: 0.4955\n",
      "Epoch 457/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 8.1045e-05 - accuracy: 1.0000 - val_loss: 5.1736 - val_accuracy: 0.4955\n",
      "Epoch 458/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 8.4792e-05 - accuracy: 1.0000 - val_loss: 5.1808 - val_accuracy: 0.4865\n",
      "Epoch 459/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 8.3857e-05 - accuracy: 1.0000 - val_loss: 5.1855 - val_accuracy: 0.4910\n",
      "Epoch 460/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 8.6729e-05 - accuracy: 1.0000 - val_loss: 5.1881 - val_accuracy: 0.4865\n",
      "Epoch 461/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 8.2971e-05 - accuracy: 1.0000 - val_loss: 5.1928 - val_accuracy: 0.4865\n",
      "Epoch 462/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 7.8725e-05 - accuracy: 1.0000 - val_loss: 5.1956 - val_accuracy: 0.4910\n",
      "Epoch 463/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 7.8893e-05 - accuracy: 1.0000 - val_loss: 5.2044 - val_accuracy: 0.4910\n",
      "Epoch 464/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 7.7524e-05 - accuracy: 1.0000 - val_loss: 5.2046 - val_accuracy: 0.4910\n",
      "Epoch 465/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 8.0814e-05 - accuracy: 1.0000 - val_loss: 5.2121 - val_accuracy: 0.4820\n",
      "Epoch 466/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 7.6624e-05 - accuracy: 1.0000 - val_loss: 5.2112 - val_accuracy: 0.4865\n",
      "Epoch 467/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 7.1420e-05 - accuracy: 1.0000 - val_loss: 5.2177 - val_accuracy: 0.4910\n",
      "Epoch 468/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 7.4014e-05 - accuracy: 1.0000 - val_loss: 5.2291 - val_accuracy: 0.4955\n",
      "Epoch 469/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 7.1745e-05 - accuracy: 1.0000 - val_loss: 5.2344 - val_accuracy: 0.4955\n",
      "Epoch 470/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 7.9410e-05 - accuracy: 1.0000 - val_loss: 5.2334 - val_accuracy: 0.4955\n",
      "Epoch 471/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 7.3653e-05 - accuracy: 1.0000 - val_loss: 5.2435 - val_accuracy: 0.4865\n",
      "Epoch 472/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 7.8827e-05 - accuracy: 1.0000 - val_loss: 5.2472 - val_accuracy: 0.4865\n",
      "Epoch 473/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.8203e-05 - accuracy: 1.0000 - val_loss: 5.2561 - val_accuracy: 0.4865\n",
      "Epoch 474/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.4740e-05 - accuracy: 1.0000 - val_loss: 5.2460 - val_accuracy: 0.4910\n",
      "Epoch 475/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 7.1892e-05 - accuracy: 1.0000 - val_loss: 5.2510 - val_accuracy: 0.4910\n",
      "Epoch 476/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 6.8199e-05 - accuracy: 1.0000 - val_loss: 5.2624 - val_accuracy: 0.4910\n",
      "Epoch 477/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 6.6015e-05 - accuracy: 1.0000 - val_loss: 5.2724 - val_accuracy: 0.4910\n",
      "Epoch 478/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.9991e-05 - accuracy: 1.0000 - val_loss: 5.2766 - val_accuracy: 0.4910\n",
      "Epoch 479/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 6.2725e-05 - accuracy: 1.0000 - val_loss: 5.2741 - val_accuracy: 0.4910\n",
      "Epoch 480/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 6.5721e-05 - accuracy: 1.0000 - val_loss: 5.2763 - val_accuracy: 0.4910\n",
      "Epoch 481/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.4196e-05 - accuracy: 1.0000 - val_loss: 5.2801 - val_accuracy: 0.4865\n",
      "Epoch 482/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 6.3802e-05 - accuracy: 1.0000 - val_loss: 5.2833 - val_accuracy: 0.4910\n",
      "Epoch 483/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 6.2715e-05 - accuracy: 1.0000 - val_loss: 5.2886 - val_accuracy: 0.4910\n",
      "Epoch 484/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 5.7222e-05 - accuracy: 1.0000 - val_loss: 5.2853 - val_accuracy: 0.4955\n",
      "Epoch 485/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.2669e-05 - accuracy: 1.0000 - val_loss: 5.2925 - val_accuracy: 0.4955\n",
      "Epoch 486/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.0483e-05 - accuracy: 1.0000 - val_loss: 5.2879 - val_accuracy: 0.4955\n",
      "Epoch 487/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 5.7788e-05 - accuracy: 1.0000 - val_loss: 5.2914 - val_accuracy: 0.4910\n",
      "Epoch 488/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 5.6656e-05 - accuracy: 1.0000 - val_loss: 5.2916 - val_accuracy: 0.4955\n",
      "Epoch 489/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 5.6608e-05 - accuracy: 1.0000 - val_loss: 5.2964 - val_accuracy: 0.4955\n",
      "Epoch 490/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 5.9707e-05 - accuracy: 1.0000 - val_loss: 5.3067 - val_accuracy: 0.4910\n",
      "Epoch 491/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 5.4804e-05 - accuracy: 1.0000 - val_loss: 5.3102 - val_accuracy: 0.4955\n",
      "Epoch 492/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 5.6624e-05 - accuracy: 1.0000 - val_loss: 5.3114 - val_accuracy: 0.4910\n",
      "Epoch 493/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 6.1599e-05 - accuracy: 1.0000 - val_loss: 5.3180 - val_accuracy: 0.4865\n",
      "Epoch 494/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 5.6297e-05 - accuracy: 1.0000 - val_loss: 5.3314 - val_accuracy: 0.4910\n",
      "Epoch 495/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 5.5868e-05 - accuracy: 1.0000 - val_loss: 5.3297 - val_accuracy: 0.4955\n",
      "Epoch 496/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 5.6084e-05 - accuracy: 1.0000 - val_loss: 5.3276 - val_accuracy: 0.4955\n",
      "Epoch 497/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 5.4866e-05 - accuracy: 1.0000 - val_loss: 5.3388 - val_accuracy: 0.4910\n",
      "Epoch 498/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 5.0748e-05 - accuracy: 1.0000 - val_loss: 5.3455 - val_accuracy: 0.4955\n",
      "Epoch 499/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 5.1625e-05 - accuracy: 1.0000 - val_loss: 5.3483 - val_accuracy: 0.4955\n",
      "Epoch 500/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 5.5415e-05 - accuracy: 1.0000 - val_loss: 5.3425 - val_accuracy: 0.4955\n",
      "Epoch 501/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 5.1946e-05 - accuracy: 1.0000 - val_loss: 5.3545 - val_accuracy: 0.4910\n",
      "Epoch 502/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.9580e-05 - accuracy: 1.0000 - val_loss: 5.3651 - val_accuracy: 0.4910\n",
      "Epoch 503/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 5.0331e-05 - accuracy: 1.0000 - val_loss: 5.3718 - val_accuracy: 0.4910\n",
      "Epoch 504/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 5.0224e-05 - accuracy: 1.0000 - val_loss: 5.3726 - val_accuracy: 0.4910\n",
      "Epoch 505/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 5.0632e-05 - accuracy: 1.0000 - val_loss: 5.3721 - val_accuracy: 0.4910\n",
      "Epoch 506/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 5.2333e-05 - accuracy: 1.0000 - val_loss: 5.3776 - val_accuracy: 0.4865\n",
      "Epoch 507/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 4.7939e-05 - accuracy: 1.0000 - val_loss: 5.3913 - val_accuracy: 0.4865\n",
      "Epoch 508/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 4.5973e-05 - accuracy: 1.0000 - val_loss: 5.3963 - val_accuracy: 0.4865\n",
      "Epoch 509/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 4.9205e-05 - accuracy: 1.0000 - val_loss: 5.3994 - val_accuracy: 0.4865\n",
      "Epoch 510/1000\n",
      "1990/1990 [==============================] - 0s 135us/step - loss: 4.8500e-05 - accuracy: 1.0000 - val_loss: 5.4065 - val_accuracy: 0.4910\n",
      "Epoch 511/1000\n",
      "1990/1990 [==============================] - 0s 129us/step - loss: 4.6144e-05 - accuracy: 1.0000 - val_loss: 5.4098 - val_accuracy: 0.4910\n",
      "Epoch 512/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 4.1815e-05 - accuracy: 1.0000 - val_loss: 5.4055 - val_accuracy: 0.4910\n",
      "Epoch 513/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 4.5065e-05 - accuracy: 1.0000 - val_loss: 5.4096 - val_accuracy: 0.4955\n",
      "Epoch 514/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 5.0618e-05 - accuracy: 1.0000 - val_loss: 5.4078 - val_accuracy: 0.4865\n",
      "Epoch 515/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 4.3961e-05 - accuracy: 1.0000 - val_loss: 5.4090 - val_accuracy: 0.4910\n",
      "Epoch 516/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 4.8089e-05 - accuracy: 1.0000 - val_loss: 5.4079 - val_accuracy: 0.4865\n",
      "Epoch 517/1000\n",
      "1990/1990 [==============================] - 0s 125us/step - loss: 4.4348e-05 - accuracy: 1.0000 - val_loss: 5.4079 - val_accuracy: 0.4865\n",
      "Epoch 518/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 4.4316e-05 - accuracy: 1.0000 - val_loss: 5.4108 - val_accuracy: 0.4865\n",
      "Epoch 519/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 4.2566e-05 - accuracy: 1.0000 - val_loss: 5.4165 - val_accuracy: 0.4865\n",
      "Epoch 520/1000\n",
      "1990/1990 [==============================] - 0s 134us/step - loss: 4.3012e-05 - accuracy: 1.0000 - val_loss: 5.4305 - val_accuracy: 0.4910\n",
      "Epoch 521/1000\n",
      "1990/1990 [==============================] - 0s 127us/step - loss: 4.3534e-05 - accuracy: 1.0000 - val_loss: 5.4331 - val_accuracy: 0.4910\n",
      "Epoch 522/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 4.0443e-05 - accuracy: 1.0000 - val_loss: 5.4456 - val_accuracy: 0.4820\n",
      "Epoch 523/1000\n",
      "1990/1990 [==============================] - 0s 150us/step - loss: 4.3158e-05 - accuracy: 1.0000 - val_loss: 5.4479 - val_accuracy: 0.4820\n",
      "Epoch 524/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 3.8592e-05 - accuracy: 1.0000 - val_loss: 5.4525 - val_accuracy: 0.4865\n",
      "Epoch 525/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 4.2812e-05 - accuracy: 1.0000 - val_loss: 5.4589 - val_accuracy: 0.4865\n",
      "Epoch 526/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 4.0070e-05 - accuracy: 1.0000 - val_loss: 5.4645 - val_accuracy: 0.4820\n",
      "Epoch 527/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 4.0503e-05 - accuracy: 1.0000 - val_loss: 5.4632 - val_accuracy: 0.4910\n",
      "Epoch 528/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 3.9998e-05 - accuracy: 1.0000 - val_loss: 5.4726 - val_accuracy: 0.4865\n",
      "Epoch 529/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 4.2449e-05 - accuracy: 1.0000 - val_loss: 5.4770 - val_accuracy: 0.4820\n",
      "Epoch 530/1000\n",
      "1990/1990 [==============================] - 0s 123us/step - loss: 4.1026e-05 - accuracy: 1.0000 - val_loss: 5.4804 - val_accuracy: 0.4820\n",
      "Epoch 531/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 4.1960e-05 - accuracy: 1.0000 - val_loss: 5.4736 - val_accuracy: 0.4820\n",
      "Epoch 532/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 3.8356e-05 - accuracy: 1.0000 - val_loss: 5.4927 - val_accuracy: 0.4820\n",
      "Epoch 533/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 3.6292e-05 - accuracy: 1.0000 - val_loss: 5.4960 - val_accuracy: 0.4865\n",
      "Epoch 534/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 3.7695e-05 - accuracy: 1.0000 - val_loss: 5.5085 - val_accuracy: 0.4775\n",
      "Epoch 535/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 3.6857e-05 - accuracy: 1.0000 - val_loss: 5.5088 - val_accuracy: 0.4775\n",
      "Epoch 536/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 3.3615e-05 - accuracy: 1.0000 - val_loss: 5.5046 - val_accuracy: 0.4820\n",
      "Epoch 537/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 3.7189e-05 - accuracy: 1.0000 - val_loss: 5.5097 - val_accuracy: 0.4865\n",
      "Epoch 538/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 3.5257e-05 - accuracy: 1.0000 - val_loss: 5.5096 - val_accuracy: 0.4910\n",
      "Epoch 539/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 3.3235e-05 - accuracy: 1.0000 - val_loss: 5.5108 - val_accuracy: 0.4910\n",
      "Epoch 540/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 3.4874e-05 - accuracy: 1.0000 - val_loss: 5.5225 - val_accuracy: 0.4910\n",
      "Epoch 541/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 3.4715e-05 - accuracy: 1.0000 - val_loss: 5.5269 - val_accuracy: 0.4865\n",
      "Epoch 542/1000\n",
      "1990/1990 [==============================] - 0s 130us/step - loss: 3.3523e-05 - accuracy: 1.0000 - val_loss: 5.5302 - val_accuracy: 0.4865\n",
      "Epoch 543/1000\n",
      "1990/1990 [==============================] - 0s 148us/step - loss: 3.2717e-05 - accuracy: 1.0000 - val_loss: 5.5298 - val_accuracy: 0.4910\n",
      "Epoch 544/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 133us/step - loss: 3.5697e-05 - accuracy: 1.0000 - val_loss: 5.5247 - val_accuracy: 0.4865\n",
      "Epoch 545/1000\n",
      "1990/1990 [==============================] - 0s 137us/step - loss: 3.0795e-05 - accuracy: 1.0000 - val_loss: 5.5322 - val_accuracy: 0.4865\n",
      "Epoch 546/1000\n",
      "1990/1990 [==============================] - 0s 137us/step - loss: 3.1295e-05 - accuracy: 1.0000 - val_loss: 5.5400 - val_accuracy: 0.4865\n",
      "Epoch 547/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 3.3507e-05 - accuracy: 1.0000 - val_loss: 5.5441 - val_accuracy: 0.4910\n",
      "Epoch 548/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 3.3142e-05 - accuracy: 1.0000 - val_loss: 5.5429 - val_accuracy: 0.4865\n",
      "Epoch 549/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 3.1489e-05 - accuracy: 1.0000 - val_loss: 5.5560 - val_accuracy: 0.4910\n",
      "Epoch 550/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 3.0863e-05 - accuracy: 1.0000 - val_loss: 5.5577 - val_accuracy: 0.4865\n",
      "Epoch 551/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 3.0221e-05 - accuracy: 1.0000 - val_loss: 5.5621 - val_accuracy: 0.4865\n",
      "Epoch 552/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 3.0231e-05 - accuracy: 1.0000 - val_loss: 5.5585 - val_accuracy: 0.4910\n",
      "Epoch 553/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 3.2377e-05 - accuracy: 1.0000 - val_loss: 5.5658 - val_accuracy: 0.4910\n",
      "Epoch 554/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 3.3059e-05 - accuracy: 1.0000 - val_loss: 5.5705 - val_accuracy: 0.4910\n",
      "Epoch 555/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 3.1243e-05 - accuracy: 1.0000 - val_loss: 5.5840 - val_accuracy: 0.4910\n",
      "Epoch 556/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 2.9768e-05 - accuracy: 1.0000 - val_loss: 5.5768 - val_accuracy: 0.4910\n",
      "Epoch 557/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 3.0971e-05 - accuracy: 1.0000 - val_loss: 5.5695 - val_accuracy: 0.4910\n",
      "Epoch 558/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 2.9186e-05 - accuracy: 1.0000 - val_loss: 5.5733 - val_accuracy: 0.4910\n",
      "Epoch 559/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 3.2789e-05 - accuracy: 1.0000 - val_loss: 5.5768 - val_accuracy: 0.4865\n",
      "Epoch 560/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 2.9810e-05 - accuracy: 1.0000 - val_loss: 5.5800 - val_accuracy: 0.4865\n",
      "Epoch 561/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 3.0663e-05 - accuracy: 1.0000 - val_loss: 5.5987 - val_accuracy: 0.4865\n",
      "Epoch 562/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 2.9048e-05 - accuracy: 1.0000 - val_loss: 5.5998 - val_accuracy: 0.4820\n",
      "Epoch 563/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.6839e-05 - accuracy: 1.0000 - val_loss: 5.6012 - val_accuracy: 0.4910\n",
      "Epoch 564/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 2.7792e-05 - accuracy: 1.0000 - val_loss: 5.6023 - val_accuracy: 0.4865\n",
      "Epoch 565/1000\n",
      "1990/1990 [==============================] - 0s 126us/step - loss: 2.7815e-05 - accuracy: 1.0000 - val_loss: 5.6126 - val_accuracy: 0.4910\n",
      "Epoch 566/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.7417e-05 - accuracy: 1.0000 - val_loss: 5.6235 - val_accuracy: 0.4865\n",
      "Epoch 567/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 2.9106e-05 - accuracy: 1.0000 - val_loss: 5.6211 - val_accuracy: 0.4820\n",
      "Epoch 568/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 2.6823e-05 - accuracy: 1.0000 - val_loss: 5.6266 - val_accuracy: 0.4820\n",
      "Epoch 569/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 2.6074e-05 - accuracy: 1.0000 - val_loss: 5.6250 - val_accuracy: 0.4865\n",
      "Epoch 570/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.5410e-05 - accuracy: 1.0000 - val_loss: 5.6221 - val_accuracy: 0.4820\n",
      "Epoch 571/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 2.5700e-05 - accuracy: 1.0000 - val_loss: 5.6249 - val_accuracy: 0.4910\n",
      "Epoch 572/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.3499e-05 - accuracy: 1.0000 - val_loss: 5.6366 - val_accuracy: 0.4865\n",
      "Epoch 573/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 2.4116e-05 - accuracy: 1.0000 - val_loss: 5.6361 - val_accuracy: 0.4910\n",
      "Epoch 574/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.5541e-05 - accuracy: 1.0000 - val_loss: 5.6435 - val_accuracy: 0.4910\n",
      "Epoch 575/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 2.4708e-05 - accuracy: 1.0000 - val_loss: 5.6397 - val_accuracy: 0.4910\n",
      "Epoch 576/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.4414e-05 - accuracy: 1.0000 - val_loss: 5.6452 - val_accuracy: 0.4910\n",
      "Epoch 577/1000\n",
      "1990/1990 [==============================] - 0s 138us/step - loss: 2.4563e-05 - accuracy: 1.0000 - val_loss: 5.6542 - val_accuracy: 0.4910\n",
      "Epoch 578/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 2.2416e-05 - accuracy: 1.0000 - val_loss: 5.6481 - val_accuracy: 0.4865\n",
      "Epoch 579/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 2.3868e-05 - accuracy: 1.0000 - val_loss: 5.6577 - val_accuracy: 0.4910\n",
      "Epoch 580/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.4547e-05 - accuracy: 1.0000 - val_loss: 5.6606 - val_accuracy: 0.4910\n",
      "Epoch 581/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 2.2774e-05 - accuracy: 1.0000 - val_loss: 5.6731 - val_accuracy: 0.4865\n",
      "Epoch 582/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.2936e-05 - accuracy: 1.0000 - val_loss: 5.6714 - val_accuracy: 0.4865\n",
      "Epoch 583/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.2723e-05 - accuracy: 1.0000 - val_loss: 5.6851 - val_accuracy: 0.4865\n",
      "Epoch 584/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 2.3795e-05 - accuracy: 1.0000 - val_loss: 5.6876 - val_accuracy: 0.4865\n",
      "Epoch 585/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.2885e-05 - accuracy: 1.0000 - val_loss: 5.6867 - val_accuracy: 0.4820\n",
      "Epoch 586/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 2.3382e-05 - accuracy: 1.0000 - val_loss: 5.6812 - val_accuracy: 0.4865\n",
      "Epoch 587/1000\n",
      "1990/1990 [==============================] - 0s 120us/step - loss: 2.1232e-05 - accuracy: 1.0000 - val_loss: 5.6919 - val_accuracy: 0.4865\n",
      "Epoch 588/1000\n",
      "1990/1990 [==============================] - 0s 124us/step - loss: 2.2311e-05 - accuracy: 1.0000 - val_loss: 5.6989 - val_accuracy: 0.4910\n",
      "Epoch 589/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 2.3005e-05 - accuracy: 1.0000 - val_loss: 5.7089 - val_accuracy: 0.4865\n",
      "Epoch 590/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.0644e-05 - accuracy: 1.0000 - val_loss: 5.7033 - val_accuracy: 0.4910\n",
      "Epoch 591/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 2.0346e-05 - accuracy: 1.0000 - val_loss: 5.7094 - val_accuracy: 0.4865\n",
      "Epoch 592/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 2.1714e-05 - accuracy: 1.0000 - val_loss: 5.7194 - val_accuracy: 0.4865\n",
      "Epoch 593/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 2.0209e-05 - accuracy: 1.0000 - val_loss: 5.7178 - val_accuracy: 0.4910\n",
      "Epoch 594/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.1807e-05 - accuracy: 1.0000 - val_loss: 5.7196 - val_accuracy: 0.4910\n",
      "Epoch 595/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.9544e-05 - accuracy: 1.0000 - val_loss: 5.7189 - val_accuracy: 0.4820\n",
      "Epoch 596/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.9244e-05 - accuracy: 1.0000 - val_loss: 5.7217 - val_accuracy: 0.4865\n",
      "Epoch 597/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 2.0558e-05 - accuracy: 1.0000 - val_loss: 5.7370 - val_accuracy: 0.4865\n",
      "Epoch 598/1000\n",
      "1990/1990 [==============================] - 0s 116us/step - loss: 2.1113e-05 - accuracy: 1.0000 - val_loss: 5.7349 - val_accuracy: 0.4820\n",
      "Epoch 599/1000\n",
      "1990/1990 [==============================] - 0s 129us/step - loss: 1.9884e-05 - accuracy: 1.0000 - val_loss: 5.7345 - val_accuracy: 0.4910\n",
      "Epoch 600/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 2.0814e-05 - accuracy: 1.0000 - val_loss: 5.7480 - val_accuracy: 0.4910\n",
      "Epoch 601/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 2.1037e-05 - accuracy: 1.0000 - val_loss: 5.7457 - val_accuracy: 0.4910\n",
      "Epoch 602/1000\n",
      "1990/1990 [==============================] - 0s 126us/step - loss: 1.9733e-05 - accuracy: 1.0000 - val_loss: 5.7353 - val_accuracy: 0.4865\n",
      "Epoch 603/1000\n",
      "1990/1990 [==============================] - 0s 124us/step - loss: 1.9210e-05 - accuracy: 1.0000 - val_loss: 5.7478 - val_accuracy: 0.4865\n",
      "Epoch 604/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.9021e-05 - accuracy: 1.0000 - val_loss: 5.7701 - val_accuracy: 0.4820\n",
      "Epoch 605/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.7884e-05 - accuracy: 1.0000 - val_loss: 5.7761 - val_accuracy: 0.4820\n",
      "Epoch 606/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.9107e-05 - accuracy: 1.0000 - val_loss: 5.7645 - val_accuracy: 0.4865\n",
      "Epoch 607/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.8238e-05 - accuracy: 1.0000 - val_loss: 5.7597 - val_accuracy: 0.4865\n",
      "Epoch 608/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.7318e-05 - accuracy: 1.0000 - val_loss: 5.7641 - val_accuracy: 0.4865\n",
      "Epoch 609/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7877e-05 - accuracy: 1.0000 - val_loss: 5.7683 - val_accuracy: 0.4910\n",
      "Epoch 610/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.7902e-05 - accuracy: 1.0000 - val_loss: 5.7760 - val_accuracy: 0.4910\n",
      "Epoch 611/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.7386e-05 - accuracy: 1.0000 - val_loss: 5.7745 - val_accuracy: 0.4865\n",
      "Epoch 612/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.7846e-05 - accuracy: 1.0000 - val_loss: 5.7680 - val_accuracy: 0.4865\n",
      "Epoch 613/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.6606e-05 - accuracy: 1.0000 - val_loss: 5.7768 - val_accuracy: 0.4865\n",
      "Epoch 614/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.7448e-05 - accuracy: 1.0000 - val_loss: 5.7862 - val_accuracy: 0.4910\n",
      "Epoch 615/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.6224e-05 - accuracy: 1.0000 - val_loss: 5.7917 - val_accuracy: 0.4910\n",
      "Epoch 616/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.5779e-05 - accuracy: 1.0000 - val_loss: 5.7960 - val_accuracy: 0.4865\n",
      "Epoch 617/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.6908e-05 - accuracy: 1.0000 - val_loss: 5.7873 - val_accuracy: 0.4865\n",
      "Epoch 618/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.7078e-05 - accuracy: 1.0000 - val_loss: 5.7945 - val_accuracy: 0.4865\n",
      "Epoch 619/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.6854e-05 - accuracy: 1.0000 - val_loss: 5.8090 - val_accuracy: 0.4820\n",
      "Epoch 620/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.6600e-05 - accuracy: 1.0000 - val_loss: 5.8198 - val_accuracy: 0.4865\n",
      "Epoch 621/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7285e-05 - accuracy: 1.0000 - val_loss: 5.8113 - val_accuracy: 0.4865\n",
      "Epoch 622/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.6471e-05 - accuracy: 1.0000 - val_loss: 5.8179 - val_accuracy: 0.4865\n",
      "Epoch 623/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 1.4990e-05 - accuracy: 1.0000 - val_loss: 5.8164 - val_accuracy: 0.4865\n",
      "Epoch 624/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.4055e-05 - accuracy: 1.0000 - val_loss: 5.8128 - val_accuracy: 0.4865\n",
      "Epoch 625/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.5219e-05 - accuracy: 1.0000 - val_loss: 5.8270 - val_accuracy: 0.4865\n",
      "Epoch 626/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.3891e-05 - accuracy: 1.0000 - val_loss: 5.8327 - val_accuracy: 0.4910\n",
      "Epoch 627/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.4746e-05 - accuracy: 1.0000 - val_loss: 5.8380 - val_accuracy: 0.4910\n",
      "Epoch 628/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.4265e-05 - accuracy: 1.0000 - val_loss: 5.8466 - val_accuracy: 0.4865\n",
      "Epoch 629/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.5257e-05 - accuracy: 1.0000 - val_loss: 5.8519 - val_accuracy: 0.4865\n",
      "Epoch 630/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.4496e-05 - accuracy: 1.0000 - val_loss: 5.8481 - val_accuracy: 0.4865\n",
      "Epoch 631/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.4123e-05 - accuracy: 1.0000 - val_loss: 5.8600 - val_accuracy: 0.4865\n",
      "Epoch 632/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.6364e-05 - accuracy: 1.0000 - val_loss: 5.8656 - val_accuracy: 0.4865\n",
      "Epoch 633/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.4266e-05 - accuracy: 1.0000 - val_loss: 5.8499 - val_accuracy: 0.4865\n",
      "Epoch 634/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.4775e-05 - accuracy: 1.0000 - val_loss: 5.8578 - val_accuracy: 0.4865\n",
      "Epoch 635/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.2859e-05 - accuracy: 1.0000 - val_loss: 5.8758 - val_accuracy: 0.4910\n",
      "Epoch 636/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.3595e-05 - accuracy: 1.0000 - val_loss: 5.8767 - val_accuracy: 0.4865\n",
      "Epoch 637/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.4408e-05 - accuracy: 1.0000 - val_loss: 5.8721 - val_accuracy: 0.4910\n",
      "Epoch 638/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.3250e-05 - accuracy: 1.0000 - val_loss: 5.8745 - val_accuracy: 0.4910\n",
      "Epoch 639/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.3893e-05 - accuracy: 1.0000 - val_loss: 5.8803 - val_accuracy: 0.4865\n",
      "Epoch 640/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.3822e-05 - accuracy: 1.0000 - val_loss: 5.8800 - val_accuracy: 0.4865\n",
      "Epoch 641/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.4108e-05 - accuracy: 1.0000 - val_loss: 5.8924 - val_accuracy: 0.4865\n",
      "Epoch 642/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.2580e-05 - accuracy: 1.0000 - val_loss: 5.8870 - val_accuracy: 0.4865\n",
      "Epoch 643/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.2950e-05 - accuracy: 1.0000 - val_loss: 5.8848 - val_accuracy: 0.4865\n",
      "Epoch 644/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.1889e-05 - accuracy: 1.0000 - val_loss: 5.8885 - val_accuracy: 0.4865\n",
      "Epoch 645/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.2145e-05 - accuracy: 1.0000 - val_loss: 5.8981 - val_accuracy: 0.4865\n",
      "Epoch 646/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.1531e-05 - accuracy: 1.0000 - val_loss: 5.9072 - val_accuracy: 0.4820\n",
      "Epoch 647/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.2332e-05 - accuracy: 1.0000 - val_loss: 5.9231 - val_accuracy: 0.4820\n",
      "Epoch 648/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.2051e-05 - accuracy: 1.0000 - val_loss: 5.9279 - val_accuracy: 0.4820\n",
      "Epoch 649/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.3279e-05 - accuracy: 1.0000 - val_loss: 5.9246 - val_accuracy: 0.4820\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.3427e-05 - accuracy: 1.0000 - val_loss: 5.9160 - val_accuracy: 0.4820\n",
      "Epoch 651/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.2152e-05 - accuracy: 1.0000 - val_loss: 5.9184 - val_accuracy: 0.4865\n",
      "Epoch 652/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.1742e-05 - accuracy: 1.0000 - val_loss: 5.9136 - val_accuracy: 0.4865\n",
      "Epoch 653/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.1889e-05 - accuracy: 1.0000 - val_loss: 5.9207 - val_accuracy: 0.4865\n",
      "Epoch 654/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.1384e-05 - accuracy: 1.0000 - val_loss: 5.9284 - val_accuracy: 0.4865\n",
      "Epoch 655/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.0876e-05 - accuracy: 1.0000 - val_loss: 5.9326 - val_accuracy: 0.4865\n",
      "Epoch 656/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.2021e-05 - accuracy: 1.0000 - val_loss: 5.9318 - val_accuracy: 0.4865\n",
      "Epoch 657/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 1.1909e-05 - accuracy: 1.0000 - val_loss: 5.9410 - val_accuracy: 0.4820\n",
      "Epoch 658/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 1.2820e-05 - accuracy: 1.0000 - val_loss: 5.9388 - val_accuracy: 0.4820\n",
      "Epoch 659/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 1.0977e-05 - accuracy: 1.0000 - val_loss: 5.9488 - val_accuracy: 0.4820\n",
      "Epoch 660/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0698e-05 - accuracy: 1.0000 - val_loss: 5.9499 - val_accuracy: 0.4820\n",
      "Epoch 661/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.1012e-05 - accuracy: 1.0000 - val_loss: 5.9586 - val_accuracy: 0.4820\n",
      "Epoch 662/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.1059e-05 - accuracy: 1.0000 - val_loss: 5.9605 - val_accuracy: 0.4820\n",
      "Epoch 663/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0780e-05 - accuracy: 1.0000 - val_loss: 5.9636 - val_accuracy: 0.4820\n",
      "Epoch 664/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0192e-05 - accuracy: 1.0000 - val_loss: 5.9648 - val_accuracy: 0.4820\n",
      "Epoch 665/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.0783e-05 - accuracy: 1.0000 - val_loss: 5.9774 - val_accuracy: 0.4820\n",
      "Epoch 666/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.0949e-05 - accuracy: 1.0000 - val_loss: 5.9886 - val_accuracy: 0.4820\n",
      "Epoch 667/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.1839e-05 - accuracy: 1.0000 - val_loss: 5.9880 - val_accuracy: 0.4865\n",
      "Epoch 668/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.1128e-05 - accuracy: 1.0000 - val_loss: 5.9814 - val_accuracy: 0.4865\n",
      "Epoch 669/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.1195e-05 - accuracy: 1.0000 - val_loss: 5.9955 - val_accuracy: 0.4820\n",
      "Epoch 670/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.0239e-05 - accuracy: 1.0000 - val_loss: 6.0007 - val_accuracy: 0.4865\n",
      "Epoch 671/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 1.0664e-05 - accuracy: 1.0000 - val_loss: 6.0062 - val_accuracy: 0.4865\n",
      "Epoch 672/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.0333e-05 - accuracy: 1.0000 - val_loss: 6.0104 - val_accuracy: 0.4820\n",
      "Epoch 673/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 9.8581e-06 - accuracy: 1.0000 - val_loss: 6.0080 - val_accuracy: 0.4775\n",
      "Epoch 674/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 9.7721e-06 - accuracy: 1.0000 - val_loss: 6.0184 - val_accuracy: 0.4910\n",
      "Epoch 675/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.0605e-05 - accuracy: 1.0000 - val_loss: 6.0392 - val_accuracy: 0.4865\n",
      "Epoch 676/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 9.6847e-06 - accuracy: 1.0000 - val_loss: 6.0237 - val_accuracy: 0.4820\n",
      "Epoch 677/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 9.3741e-06 - accuracy: 1.0000 - val_loss: 6.0185 - val_accuracy: 0.4865\n",
      "Epoch 678/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 1.0666e-05 - accuracy: 1.0000 - val_loss: 6.0170 - val_accuracy: 0.4865\n",
      "Epoch 679/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 9.4877e-06 - accuracy: 1.0000 - val_loss: 6.0288 - val_accuracy: 0.4910\n",
      "Epoch 680/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 9.0673e-06 - accuracy: 1.0000 - val_loss: 6.0321 - val_accuracy: 0.4865\n",
      "Epoch 681/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.0020e-05 - accuracy: 1.0000 - val_loss: 6.0378 - val_accuracy: 0.4820\n",
      "Epoch 682/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 9.3903e-06 - accuracy: 1.0000 - val_loss: 6.0369 - val_accuracy: 0.4820\n",
      "Epoch 683/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 8.8005e-06 - accuracy: 1.0000 - val_loss: 6.0344 - val_accuracy: 0.4865\n",
      "Epoch 684/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 8.6924e-06 - accuracy: 1.0000 - val_loss: 6.0373 - val_accuracy: 0.4865\n",
      "Epoch 685/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 9.1393e-06 - accuracy: 1.0000 - val_loss: 6.0430 - val_accuracy: 0.4820\n",
      "Epoch 686/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 8.4604e-06 - accuracy: 1.0000 - val_loss: 6.0507 - val_accuracy: 0.4820\n",
      "Epoch 687/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 9.3789e-06 - accuracy: 1.0000 - val_loss: 6.0607 - val_accuracy: 0.4865\n",
      "Epoch 688/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 7.9066e-06 - accuracy: 1.0000 - val_loss: 6.0610 - val_accuracy: 0.4820\n",
      "Epoch 689/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 8.5807e-06 - accuracy: 1.0000 - val_loss: 6.0719 - val_accuracy: 0.4865\n",
      "Epoch 690/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 8.8669e-06 - accuracy: 1.0000 - val_loss: 6.0813 - val_accuracy: 0.4820\n",
      "Epoch 691/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 8.7242e-06 - accuracy: 1.0000 - val_loss: 6.0888 - val_accuracy: 0.4820\n",
      "Epoch 692/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 8.2173e-06 - accuracy: 1.0000 - val_loss: 6.0744 - val_accuracy: 0.4775\n",
      "Epoch 693/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 8.1883e-06 - accuracy: 1.0000 - val_loss: 6.0852 - val_accuracy: 0.4865\n",
      "Epoch 694/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 8.7884e-06 - accuracy: 1.0000 - val_loss: 6.1000 - val_accuracy: 0.4865\n",
      "Epoch 695/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 8.3259e-06 - accuracy: 1.0000 - val_loss: 6.1017 - val_accuracy: 0.4820\n",
      "Epoch 696/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 7.9245e-06 - accuracy: 1.0000 - val_loss: 6.1022 - val_accuracy: 0.4865\n",
      "Epoch 697/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 8.2176e-06 - accuracy: 1.0000 - val_loss: 6.1056 - val_accuracy: 0.4865\n",
      "Epoch 698/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 8.6251e-06 - accuracy: 1.0000 - val_loss: 6.0901 - val_accuracy: 0.4820\n",
      "Epoch 699/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 8.3401e-06 - accuracy: 1.0000 - val_loss: 6.0982 - val_accuracy: 0.4865\n",
      "Epoch 700/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 7.6266e-06 - accuracy: 1.0000 - val_loss: 6.0999 - val_accuracy: 0.4910\n",
      "Epoch 701/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 7.1481e-06 - accuracy: 1.0000 - val_loss: 6.1125 - val_accuracy: 0.4865\n",
      "Epoch 702/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 7.7866e-06 - accuracy: 1.0000 - val_loss: 6.1149 - val_accuracy: 0.4820\n",
      "Epoch 703/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 6.9463e-06 - accuracy: 1.0000 - val_loss: 6.1180 - val_accuracy: 0.4865\n",
      "Epoch 704/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 7.8300e-06 - accuracy: 1.0000 - val_loss: 6.1255 - val_accuracy: 0.4820\n",
      "Epoch 705/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 7.3653e-06 - accuracy: 1.0000 - val_loss: 6.1289 - val_accuracy: 0.4775\n",
      "Epoch 706/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 7.0371e-06 - accuracy: 1.0000 - val_loss: 6.1312 - val_accuracy: 0.4775\n",
      "Epoch 707/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 8.1317e-06 - accuracy: 1.0000 - val_loss: 6.1379 - val_accuracy: 0.4820\n",
      "Epoch 708/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 6.8111e-06 - accuracy: 1.0000 - val_loss: 6.1399 - val_accuracy: 0.4820\n",
      "Epoch 709/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.9976e-06 - accuracy: 1.0000 - val_loss: 6.1256 - val_accuracy: 0.4865\n",
      "Epoch 710/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 7.2902e-06 - accuracy: 1.0000 - val_loss: 6.1335 - val_accuracy: 0.4910\n",
      "Epoch 711/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.3920e-06 - accuracy: 1.0000 - val_loss: 6.1422 - val_accuracy: 0.4910\n",
      "Epoch 712/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 7.2681e-06 - accuracy: 1.0000 - val_loss: 6.1386 - val_accuracy: 0.4910\n",
      "Epoch 713/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 7.3311e-06 - accuracy: 1.0000 - val_loss: 6.1587 - val_accuracy: 0.4865\n",
      "Epoch 714/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 7.2860e-06 - accuracy: 1.0000 - val_loss: 6.1638 - val_accuracy: 0.4865\n",
      "Epoch 715/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 6.3717e-06 - accuracy: 1.0000 - val_loss: 6.1542 - val_accuracy: 0.4820\n",
      "Epoch 716/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 6.4047e-06 - accuracy: 1.0000 - val_loss: 6.1483 - val_accuracy: 0.4865\n",
      "Epoch 717/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 6.4379e-06 - accuracy: 1.0000 - val_loss: 6.1625 - val_accuracy: 0.4865\n",
      "Epoch 718/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 6.2090e-06 - accuracy: 1.0000 - val_loss: 6.1694 - val_accuracy: 0.4820\n",
      "Epoch 719/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 6.0410e-06 - accuracy: 1.0000 - val_loss: 6.1837 - val_accuracy: 0.4865\n",
      "Epoch 720/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 6.2734e-06 - accuracy: 1.0000 - val_loss: 6.1774 - val_accuracy: 0.4910\n",
      "Epoch 721/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 6.3607e-06 - accuracy: 1.0000 - val_loss: 6.1781 - val_accuracy: 0.4865\n",
      "Epoch 722/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 6.0689e-06 - accuracy: 1.0000 - val_loss: 6.1995 - val_accuracy: 0.4865\n",
      "Epoch 723/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 5.8709e-06 - accuracy: 1.0000 - val_loss: 6.1979 - val_accuracy: 0.4820\n",
      "Epoch 724/1000\n",
      "1990/1990 [==============================] - 0s 123us/step - loss: 6.2785e-06 - accuracy: 1.0000 - val_loss: 6.1874 - val_accuracy: 0.4820\n",
      "Epoch 725/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 5.7363e-06 - accuracy: 1.0000 - val_loss: 6.1912 - val_accuracy: 0.4865\n",
      "Epoch 726/1000\n",
      "1990/1990 [==============================] - 0s 133us/step - loss: 6.0849e-06 - accuracy: 1.0000 - val_loss: 6.2005 - val_accuracy: 0.4865\n",
      "Epoch 727/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 6.1927e-06 - accuracy: 1.0000 - val_loss: 6.1988 - val_accuracy: 0.4910\n",
      "Epoch 728/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 5.9000e-06 - accuracy: 1.0000 - val_loss: 6.1976 - val_accuracy: 0.4910\n",
      "Epoch 729/1000\n",
      "1990/1990 [==============================] - 0s 118us/step - loss: 5.6770e-06 - accuracy: 1.0000 - val_loss: 6.1991 - val_accuracy: 0.4865\n",
      "Epoch 730/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 5.8367e-06 - accuracy: 1.0000 - val_loss: 6.2076 - val_accuracy: 0.4865\n",
      "Epoch 731/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 5.9268e-06 - accuracy: 1.0000 - val_loss: 6.2105 - val_accuracy: 0.4865\n",
      "Epoch 732/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 5.8121e-06 - accuracy: 1.0000 - val_loss: 6.2213 - val_accuracy: 0.4865\n",
      "Epoch 733/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 5.7581e-06 - accuracy: 1.0000 - val_loss: 6.2157 - val_accuracy: 0.4910\n",
      "Epoch 734/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 5.9987e-06 - accuracy: 1.0000 - val_loss: 6.2230 - val_accuracy: 0.4865\n",
      "Epoch 735/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 5.6543e-06 - accuracy: 1.0000 - val_loss: 6.2235 - val_accuracy: 0.4775\n",
      "Epoch 736/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 5.2537e-06 - accuracy: 1.0000 - val_loss: 6.2304 - val_accuracy: 0.4865\n",
      "Epoch 737/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 5.8845e-06 - accuracy: 1.0000 - val_loss: 6.2369 - val_accuracy: 0.4865\n",
      "Epoch 738/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 5.7079e-06 - accuracy: 1.0000 - val_loss: 6.2278 - val_accuracy: 0.4910\n",
      "Epoch 739/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 5.7144e-06 - accuracy: 1.0000 - val_loss: 6.2350 - val_accuracy: 0.4865\n",
      "Epoch 740/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 5.5338e-06 - accuracy: 1.0000 - val_loss: 6.2375 - val_accuracy: 0.4865\n",
      "Epoch 741/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 5.1141e-06 - accuracy: 1.0000 - val_loss: 6.2541 - val_accuracy: 0.4865\n",
      "Epoch 742/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 5.1032e-06 - accuracy: 1.0000 - val_loss: 6.2508 - val_accuracy: 0.4820\n",
      "Epoch 743/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 5.1255e-06 - accuracy: 1.0000 - val_loss: 6.2537 - val_accuracy: 0.4865\n",
      "Epoch 744/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 4.9121e-06 - accuracy: 1.0000 - val_loss: 6.2518 - val_accuracy: 0.4865\n",
      "Epoch 745/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.8606e-06 - accuracy: 1.0000 - val_loss: 6.2451 - val_accuracy: 0.4865\n",
      "Epoch 746/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.9154e-06 - accuracy: 1.0000 - val_loss: 6.2502 - val_accuracy: 0.4820\n",
      "Epoch 747/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 5.2042e-06 - accuracy: 1.0000 - val_loss: 6.2519 - val_accuracy: 0.4865\n",
      "Epoch 748/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 5.4619e-06 - accuracy: 1.0000 - val_loss: 6.2679 - val_accuracy: 0.4865\n",
      "Epoch 749/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 5.3816e-06 - accuracy: 1.0000 - val_loss: 6.2629 - val_accuracy: 0.4820\n",
      "Epoch 750/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 5.0737e-06 - accuracy: 1.0000 - val_loss: 6.2680 - val_accuracy: 0.4910\n",
      "Epoch 751/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 4.8973e-06 - accuracy: 1.0000 - val_loss: 6.2671 - val_accuracy: 0.4910\n",
      "Epoch 752/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 4.8303e-06 - accuracy: 1.0000 - val_loss: 6.2804 - val_accuracy: 0.4865\n",
      "Epoch 753/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 4.7590e-06 - accuracy: 1.0000 - val_loss: 6.2737 - val_accuracy: 0.4865\n",
      "Epoch 754/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 4.9966e-06 - accuracy: 1.0000 - val_loss: 6.2768 - val_accuracy: 0.4865\n",
      "Epoch 755/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 4.5404e-06 - accuracy: 1.0000 - val_loss: 6.2888 - val_accuracy: 0.4865\n",
      "Epoch 756/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 104us/step - loss: 4.8151e-06 - accuracy: 1.0000 - val_loss: 6.2829 - val_accuracy: 0.4865\n",
      "Epoch 757/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 4.9266e-06 - accuracy: 1.0000 - val_loss: 6.2885 - val_accuracy: 0.4865\n",
      "Epoch 758/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 4.8132e-06 - accuracy: 1.0000 - val_loss: 6.2968 - val_accuracy: 0.4865\n",
      "Epoch 759/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 4.3529e-06 - accuracy: 1.0000 - val_loss: 6.3104 - val_accuracy: 0.4820\n",
      "Epoch 760/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.6639e-06 - accuracy: 1.0000 - val_loss: 6.3041 - val_accuracy: 0.4820\n",
      "Epoch 761/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 4.4783e-06 - accuracy: 1.0000 - val_loss: 6.3164 - val_accuracy: 0.4820\n",
      "Epoch 762/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.0819e-06 - accuracy: 1.0000 - val_loss: 6.3265 - val_accuracy: 0.4865\n",
      "Epoch 763/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 4.6383e-06 - accuracy: 1.0000 - val_loss: 6.3083 - val_accuracy: 0.4865\n",
      "Epoch 764/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 4.0433e-06 - accuracy: 1.0000 - val_loss: 6.3190 - val_accuracy: 0.4865\n",
      "Epoch 765/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 4.0805e-06 - accuracy: 1.0000 - val_loss: 6.3258 - val_accuracy: 0.4865\n",
      "Epoch 766/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 4.4045e-06 - accuracy: 1.0000 - val_loss: 6.3377 - val_accuracy: 0.4865\n",
      "Epoch 767/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 4.6565e-06 - accuracy: 1.0000 - val_loss: 6.3334 - val_accuracy: 0.4865\n",
      "Epoch 768/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 4.8285e-06 - accuracy: 1.0000 - val_loss: 6.3498 - val_accuracy: 0.4865\n",
      "Epoch 769/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 3.9803e-06 - accuracy: 1.0000 - val_loss: 6.3754 - val_accuracy: 0.4865\n",
      "Epoch 770/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 3.9832e-06 - accuracy: 1.0000 - val_loss: 6.3590 - val_accuracy: 0.4865\n",
      "Epoch 771/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 4.4417e-06 - accuracy: 1.0000 - val_loss: 6.3510 - val_accuracy: 0.4820\n",
      "Epoch 772/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 4.1538e-06 - accuracy: 1.0000 - val_loss: 6.3568 - val_accuracy: 0.4820\n",
      "Epoch 773/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.8459e-06 - accuracy: 1.0000 - val_loss: 6.3643 - val_accuracy: 0.4820\n",
      "Epoch 774/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 4.0079e-06 - accuracy: 1.0000 - val_loss: 6.3768 - val_accuracy: 0.4865\n",
      "Epoch 775/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 4.2316e-06 - accuracy: 1.0000 - val_loss: 6.3667 - val_accuracy: 0.4865\n",
      "Epoch 776/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 4.0987e-06 - accuracy: 1.0000 - val_loss: 6.3692 - val_accuracy: 0.4865\n",
      "Epoch 777/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 3.7349e-06 - accuracy: 1.0000 - val_loss: 6.3806 - val_accuracy: 0.4865\n",
      "Epoch 778/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.7513e-06 - accuracy: 1.0000 - val_loss: 6.3849 - val_accuracy: 0.4865\n",
      "Epoch 779/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 3.7873e-06 - accuracy: 1.0000 - val_loss: 6.3695 - val_accuracy: 0.4865\n",
      "Epoch 780/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 3.9928e-06 - accuracy: 1.0000 - val_loss: 6.3769 - val_accuracy: 0.4865\n",
      "Epoch 781/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.6471e-06 - accuracy: 1.0000 - val_loss: 6.3760 - val_accuracy: 0.4865\n",
      "Epoch 782/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 4.0061e-06 - accuracy: 1.0000 - val_loss: 6.3883 - val_accuracy: 0.4865\n",
      "Epoch 783/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 3.8821e-06 - accuracy: 1.0000 - val_loss: 6.3820 - val_accuracy: 0.4865\n",
      "Epoch 784/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.6370e-06 - accuracy: 1.0000 - val_loss: 6.3882 - val_accuracy: 0.4865\n",
      "Epoch 785/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.4656e-06 - accuracy: 1.0000 - val_loss: 6.3891 - val_accuracy: 0.4865\n",
      "Epoch 786/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 3.5318e-06 - accuracy: 1.0000 - val_loss: 6.3943 - val_accuracy: 0.4865\n",
      "Epoch 787/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 3.5120e-06 - accuracy: 1.0000 - val_loss: 6.3960 - val_accuracy: 0.4865\n",
      "Epoch 788/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 3.3815e-06 - accuracy: 1.0000 - val_loss: 6.4007 - val_accuracy: 0.4865\n",
      "Epoch 789/1000\n",
      "1990/1990 [==============================] - 0s 117us/step - loss: 3.8715e-06 - accuracy: 1.0000 - val_loss: 6.3996 - val_accuracy: 0.4865\n",
      "Epoch 790/1000\n",
      "1990/1990 [==============================] - 0s 125us/step - loss: 3.5625e-06 - accuracy: 1.0000 - val_loss: 6.4058 - val_accuracy: 0.4865\n",
      "Epoch 791/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 3.7064e-06 - accuracy: 1.0000 - val_loss: 6.4153 - val_accuracy: 0.4865\n",
      "Epoch 792/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 3.6538e-06 - accuracy: 1.0000 - val_loss: 6.4191 - val_accuracy: 0.4865\n",
      "Epoch 793/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.6712e-06 - accuracy: 1.0000 - val_loss: 6.4281 - val_accuracy: 0.4820\n",
      "Epoch 794/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 3.4337e-06 - accuracy: 1.0000 - val_loss: 6.4167 - val_accuracy: 0.4865\n",
      "Epoch 795/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.9046e-06 - accuracy: 1.0000 - val_loss: 6.4223 - val_accuracy: 0.4865\n",
      "Epoch 796/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.2871e-06 - accuracy: 1.0000 - val_loss: 6.4423 - val_accuracy: 0.4820\n",
      "Epoch 797/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 3.0558e-06 - accuracy: 1.0000 - val_loss: 6.4537 - val_accuracy: 0.4820\n",
      "Epoch 798/1000\n",
      "1990/1990 [==============================] - 0s 90us/step - loss: 3.1030e-06 - accuracy: 1.0000 - val_loss: 6.4567 - val_accuracy: 0.4820\n",
      "Epoch 799/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 3.0308e-06 - accuracy: 1.0000 - val_loss: 6.4513 - val_accuracy: 0.4820\n",
      "Epoch 800/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 2.9854e-06 - accuracy: 1.0000 - val_loss: 6.4445 - val_accuracy: 0.4820\n",
      "Epoch 801/1000\n",
      "1990/1990 [==============================] - 0s 87us/step - loss: 3.2290e-06 - accuracy: 1.0000 - val_loss: 6.4651 - val_accuracy: 0.4820\n",
      "Epoch 802/1000\n",
      "1990/1990 [==============================] - 0s 89us/step - loss: 3.1235e-06 - accuracy: 1.0000 - val_loss: 6.4501 - val_accuracy: 0.4820\n",
      "Epoch 803/1000\n",
      "1990/1990 [==============================] - 0s 88us/step - loss: 3.0622e-06 - accuracy: 1.0000 - val_loss: 6.4696 - val_accuracy: 0.4820\n",
      "Epoch 804/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 3.0315e-06 - accuracy: 1.0000 - val_loss: 6.4722 - val_accuracy: 0.4775\n",
      "Epoch 805/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 2.9416e-06 - accuracy: 1.0000 - val_loss: 6.4644 - val_accuracy: 0.4820\n",
      "Epoch 806/1000\n",
      "1990/1990 [==============================] - 0s 91us/step - loss: 3.1562e-06 - accuracy: 1.0000 - val_loss: 6.4730 - val_accuracy: 0.4820\n",
      "Epoch 807/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 2.9143e-06 - accuracy: 1.0000 - val_loss: 6.4672 - val_accuracy: 0.4865\n",
      "Epoch 808/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 3.1563e-06 - accuracy: 1.0000 - val_loss: 6.4731 - val_accuracy: 0.4865\n",
      "Epoch 809/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 3.1654e-06 - accuracy: 1.0000 - val_loss: 6.4826 - val_accuracy: 0.4865\n",
      "Epoch 810/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 3.0435e-06 - accuracy: 1.0000 - val_loss: 6.4642 - val_accuracy: 0.4820\n",
      "Epoch 811/1000\n",
      "1990/1990 [==============================] - 0s 128us/step - loss: 2.7845e-06 - accuracy: 1.0000 - val_loss: 6.4679 - val_accuracy: 0.4910\n",
      "Epoch 812/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 2.6116e-06 - accuracy: 1.0000 - val_loss: 6.4868 - val_accuracy: 0.4865\n",
      "Epoch 813/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 2.7268e-06 - accuracy: 1.0000 - val_loss: 6.4894 - val_accuracy: 0.4865\n",
      "Epoch 814/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 2.8967e-06 - accuracy: 1.0000 - val_loss: 6.4856 - val_accuracy: 0.4865\n",
      "Epoch 815/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 2.8679e-06 - accuracy: 1.0000 - val_loss: 6.4868 - val_accuracy: 0.4820\n",
      "Epoch 816/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 2.8176e-06 - accuracy: 1.0000 - val_loss: 6.4911 - val_accuracy: 0.4910\n",
      "Epoch 817/1000\n",
      "1990/1990 [==============================] - 0s 132us/step - loss: 2.9845e-06 - accuracy: 1.0000 - val_loss: 6.4952 - val_accuracy: 0.4865\n",
      "Epoch 818/1000\n",
      "1990/1990 [==============================] - 0s 116us/step - loss: 2.5936e-06 - accuracy: 1.0000 - val_loss: 6.5117 - val_accuracy: 0.4910\n",
      "Epoch 819/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.4962e-06 - accuracy: 1.0000 - val_loss: 6.5066 - val_accuracy: 0.4910\n",
      "Epoch 820/1000\n",
      "1990/1990 [==============================] - 0s 109us/step - loss: 2.8779e-06 - accuracy: 1.0000 - val_loss: 6.5158 - val_accuracy: 0.4865\n",
      "Epoch 821/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 2.5243e-06 - accuracy: 1.0000 - val_loss: 6.5294 - val_accuracy: 0.4865\n",
      "Epoch 822/1000\n",
      "1990/1990 [==============================] - 0s 116us/step - loss: 2.5357e-06 - accuracy: 1.0000 - val_loss: 6.5321 - val_accuracy: 0.4865\n",
      "Epoch 823/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.5409e-06 - accuracy: 1.0000 - val_loss: 6.5252 - val_accuracy: 0.4910\n",
      "Epoch 824/1000\n",
      "1990/1990 [==============================] - 0s 136us/step - loss: 2.5146e-06 - accuracy: 1.0000 - val_loss: 6.5182 - val_accuracy: 0.4910\n",
      "Epoch 825/1000\n",
      "1990/1990 [==============================] - 0s 115us/step - loss: 2.4869e-06 - accuracy: 1.0000 - val_loss: 6.5341 - val_accuracy: 0.4910\n",
      "Epoch 826/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.6104e-06 - accuracy: 1.0000 - val_loss: 6.5445 - val_accuracy: 0.4865\n",
      "Epoch 827/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 2.3443e-06 - accuracy: 1.0000 - val_loss: 6.5493 - val_accuracy: 0.4865\n",
      "Epoch 828/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.4700e-06 - accuracy: 1.0000 - val_loss: 6.5526 - val_accuracy: 0.4865\n",
      "Epoch 829/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.4818e-06 - accuracy: 1.0000 - val_loss: 6.5412 - val_accuracy: 0.4910\n",
      "Epoch 830/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 2.5117e-06 - accuracy: 1.0000 - val_loss: 6.5505 - val_accuracy: 0.4910\n",
      "Epoch 831/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 2.4506e-06 - accuracy: 1.0000 - val_loss: 6.5564 - val_accuracy: 0.4865\n",
      "Epoch 832/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 2.3246e-06 - accuracy: 1.0000 - val_loss: 6.5626 - val_accuracy: 0.4820\n",
      "Epoch 833/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.4592e-06 - accuracy: 1.0000 - val_loss: 6.5618 - val_accuracy: 0.4865\n",
      "Epoch 834/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 2.4349e-06 - accuracy: 1.0000 - val_loss: 6.5642 - val_accuracy: 0.4865\n",
      "Epoch 835/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 2.3248e-06 - accuracy: 1.0000 - val_loss: 6.5679 - val_accuracy: 0.4865\n",
      "Epoch 836/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.5333e-06 - accuracy: 1.0000 - val_loss: 6.5707 - val_accuracy: 0.4865\n",
      "Epoch 837/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 2.2082e-06 - accuracy: 1.0000 - val_loss: 6.5724 - val_accuracy: 0.4820\n",
      "Epoch 838/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 2.4940e-06 - accuracy: 1.0000 - val_loss: 6.5810 - val_accuracy: 0.4865\n",
      "Epoch 839/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 2.2541e-06 - accuracy: 1.0000 - val_loss: 6.5855 - val_accuracy: 0.4865\n",
      "Epoch 840/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 2.4961e-06 - accuracy: 1.0000 - val_loss: 6.5827 - val_accuracy: 0.4865\n",
      "Epoch 841/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.3010e-06 - accuracy: 1.0000 - val_loss: 6.5877 - val_accuracy: 0.4865\n",
      "Epoch 842/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.1678e-06 - accuracy: 1.0000 - val_loss: 6.5881 - val_accuracy: 0.4865\n",
      "Epoch 843/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 2.0768e-06 - accuracy: 1.0000 - val_loss: 6.5848 - val_accuracy: 0.4910\n",
      "Epoch 844/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.2600e-06 - accuracy: 1.0000 - val_loss: 6.5862 - val_accuracy: 0.4910\n",
      "Epoch 845/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 2.1305e-06 - accuracy: 1.0000 - val_loss: 6.6053 - val_accuracy: 0.4865\n",
      "Epoch 846/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.9900e-06 - accuracy: 1.0000 - val_loss: 6.6047 - val_accuracy: 0.4865\n",
      "Epoch 847/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 2.0953e-06 - accuracy: 1.0000 - val_loss: 6.5977 - val_accuracy: 0.4865\n",
      "Epoch 848/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.9157e-06 - accuracy: 1.0000 - val_loss: 6.5994 - val_accuracy: 0.4865\n",
      "Epoch 849/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.9330e-06 - accuracy: 1.0000 - val_loss: 6.5968 - val_accuracy: 0.4865\n",
      "Epoch 850/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 2.1653e-06 - accuracy: 1.0000 - val_loss: 6.6155 - val_accuracy: 0.4865\n",
      "Epoch 851/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.8776e-06 - accuracy: 1.0000 - val_loss: 6.6305 - val_accuracy: 0.4865\n",
      "Epoch 852/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 2.1557e-06 - accuracy: 1.0000 - val_loss: 6.6346 - val_accuracy: 0.4865\n",
      "Epoch 853/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 2.0357e-06 - accuracy: 1.0000 - val_loss: 6.6240 - val_accuracy: 0.4910\n",
      "Epoch 854/1000\n",
      "1990/1990 [==============================] - 0s 113us/step - loss: 1.9727e-06 - accuracy: 1.0000 - val_loss: 6.6263 - val_accuracy: 0.4910\n",
      "Epoch 855/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.8916e-06 - accuracy: 1.0000 - val_loss: 6.6189 - val_accuracy: 0.4910\n",
      "Epoch 856/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.9073e-06 - accuracy: 1.0000 - val_loss: 6.6195 - val_accuracy: 0.4865\n",
      "Epoch 857/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.8667e-06 - accuracy: 1.0000 - val_loss: 6.6290 - val_accuracy: 0.4865\n",
      "Epoch 858/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.8955e-06 - accuracy: 1.0000 - val_loss: 6.6455 - val_accuracy: 0.4865\n",
      "Epoch 859/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.9898e-06 - accuracy: 1.0000 - val_loss: 6.6535 - val_accuracy: 0.4865\n",
      "Epoch 860/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.8483e-06 - accuracy: 1.0000 - val_loss: 6.6508 - val_accuracy: 0.4865\n",
      "Epoch 861/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.9140e-06 - accuracy: 1.0000 - val_loss: 6.6673 - val_accuracy: 0.4865\n",
      "Epoch 862/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.8936e-06 - accuracy: 1.0000 - val_loss: 6.6504 - val_accuracy: 0.4865\n",
      "Epoch 863/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 1.8422e-06 - accuracy: 1.0000 - val_loss: 6.6635 - val_accuracy: 0.4865\n",
      "Epoch 864/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.8043e-06 - accuracy: 1.0000 - val_loss: 6.6651 - val_accuracy: 0.4865\n",
      "Epoch 865/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.6745e-06 - accuracy: 1.0000 - val_loss: 6.6800 - val_accuracy: 0.4865\n",
      "Epoch 866/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.8744e-06 - accuracy: 1.0000 - val_loss: 6.6847 - val_accuracy: 0.4865\n",
      "Epoch 867/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.6095e-06 - accuracy: 1.0000 - val_loss: 6.6855 - val_accuracy: 0.4865\n",
      "Epoch 868/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.7635e-06 - accuracy: 1.0000 - val_loss: 6.6815 - val_accuracy: 0.4865\n",
      "Epoch 869/1000\n",
      "1990/1990 [==============================] - 0s 122us/step - loss: 1.8619e-06 - accuracy: 1.0000 - val_loss: 6.6800 - val_accuracy: 0.4865\n",
      "Epoch 870/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 1.7187e-06 - accuracy: 1.0000 - val_loss: 6.6856 - val_accuracy: 0.4865\n",
      "Epoch 871/1000\n",
      "1990/1990 [==============================] - 0s 121us/step - loss: 1.7028e-06 - accuracy: 1.0000 - val_loss: 6.6861 - val_accuracy: 0.4865\n",
      "Epoch 872/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 1.5585e-06 - accuracy: 1.0000 - val_loss: 6.7029 - val_accuracy: 0.4865\n",
      "Epoch 873/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7222e-06 - accuracy: 1.0000 - val_loss: 6.7068 - val_accuracy: 0.4865\n",
      "Epoch 874/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 1.9560e-06 - accuracy: 1.0000 - val_loss: 6.6862 - val_accuracy: 0.4865\n",
      "Epoch 875/1000\n",
      "1990/1990 [==============================] - 0s 119us/step - loss: 1.7586e-06 - accuracy: 1.0000 - val_loss: 6.7024 - val_accuracy: 0.4865\n",
      "Epoch 876/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.7518e-06 - accuracy: 1.0000 - val_loss: 6.7039 - val_accuracy: 0.4865\n",
      "Epoch 877/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.7748e-06 - accuracy: 1.0000 - val_loss: 6.6980 - val_accuracy: 0.4865\n",
      "Epoch 878/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.6985e-06 - accuracy: 1.0000 - val_loss: 6.7124 - val_accuracy: 0.4820\n",
      "Epoch 879/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.6336e-06 - accuracy: 1.0000 - val_loss: 6.7140 - val_accuracy: 0.4820\n",
      "Epoch 880/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.7060e-06 - accuracy: 1.0000 - val_loss: 6.7198 - val_accuracy: 0.4865\n",
      "Epoch 881/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.6546e-06 - accuracy: 1.0000 - val_loss: 6.7139 - val_accuracy: 0.4865\n",
      "Epoch 882/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.6857e-06 - accuracy: 1.0000 - val_loss: 6.7207 - val_accuracy: 0.4865\n",
      "Epoch 883/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.4931e-06 - accuracy: 1.0000 - val_loss: 6.7356 - val_accuracy: 0.4865\n",
      "Epoch 884/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.3397e-06 - accuracy: 1.0000 - val_loss: 6.7384 - val_accuracy: 0.4865\n",
      "Epoch 885/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.5492e-06 - accuracy: 1.0000 - val_loss: 6.7387 - val_accuracy: 0.4865\n",
      "Epoch 886/1000\n",
      "1990/1990 [==============================] - 0s 107us/step - loss: 1.5671e-06 - accuracy: 1.0000 - val_loss: 6.7454 - val_accuracy: 0.4865\n",
      "Epoch 887/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.3981e-06 - accuracy: 1.0000 - val_loss: 6.7616 - val_accuracy: 0.4865\n",
      "Epoch 888/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.4601e-06 - accuracy: 1.0000 - val_loss: 6.7630 - val_accuracy: 0.4865\n",
      "Epoch 889/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.4262e-06 - accuracy: 1.0000 - val_loss: 6.7582 - val_accuracy: 0.4865\n",
      "Epoch 890/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.3796e-06 - accuracy: 1.0000 - val_loss: 6.7604 - val_accuracy: 0.4865\n",
      "Epoch 891/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.4299e-06 - accuracy: 1.0000 - val_loss: 6.7630 - val_accuracy: 0.4865\n",
      "Epoch 892/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.3317e-06 - accuracy: 1.0000 - val_loss: 6.7611 - val_accuracy: 0.4865\n",
      "Epoch 893/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.3229e-06 - accuracy: 1.0000 - val_loss: 6.7693 - val_accuracy: 0.4865\n",
      "Epoch 894/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.3315e-06 - accuracy: 1.0000 - val_loss: 6.7865 - val_accuracy: 0.4865\n",
      "Epoch 895/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.4310e-06 - accuracy: 1.0000 - val_loss: 6.7782 - val_accuracy: 0.4820\n",
      "Epoch 896/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.3513e-06 - accuracy: 1.0000 - val_loss: 6.7931 - val_accuracy: 0.4865\n",
      "Epoch 897/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.2905e-06 - accuracy: 1.0000 - val_loss: 6.7893 - val_accuracy: 0.4865\n",
      "Epoch 898/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.3517e-06 - accuracy: 1.0000 - val_loss: 6.8065 - val_accuracy: 0.4865\n",
      "Epoch 899/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.2985e-06 - accuracy: 1.0000 - val_loss: 6.8034 - val_accuracy: 0.4865\n",
      "Epoch 900/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.2775e-06 - accuracy: 1.0000 - val_loss: 6.8235 - val_accuracy: 0.4865\n",
      "Epoch 901/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.4005e-06 - accuracy: 1.0000 - val_loss: 6.8120 - val_accuracy: 0.4865\n",
      "Epoch 902/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.4260e-06 - accuracy: 1.0000 - val_loss: 6.8045 - val_accuracy: 0.4910\n",
      "Epoch 903/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 1.2903e-06 - accuracy: 1.0000 - val_loss: 6.8128 - val_accuracy: 0.4820\n",
      "Epoch 904/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 1.2833e-06 - accuracy: 1.0000 - val_loss: 6.8229 - val_accuracy: 0.4820\n",
      "Epoch 905/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 1.2431e-06 - accuracy: 1.0000 - val_loss: 6.8338 - val_accuracy: 0.4910\n",
      "Epoch 906/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.2312e-06 - accuracy: 1.0000 - val_loss: 6.8386 - val_accuracy: 0.4865\n",
      "Epoch 907/1000\n",
      "1990/1990 [==============================] - 0s 111us/step - loss: 1.3347e-06 - accuracy: 1.0000 - val_loss: 6.8396 - val_accuracy: 0.4865\n",
      "Epoch 908/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 1.2532e-06 - accuracy: 1.0000 - val_loss: 6.8376 - val_accuracy: 0.4820\n",
      "Epoch 909/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 1.3337e-06 - accuracy: 1.0000 - val_loss: 6.8240 - val_accuracy: 0.4775\n",
      "Epoch 910/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 1.2874e-06 - accuracy: 1.0000 - val_loss: 6.8431 - val_accuracy: 0.4820\n",
      "Epoch 911/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.1196e-06 - accuracy: 1.0000 - val_loss: 6.8540 - val_accuracy: 0.4775\n",
      "Epoch 912/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.2362e-06 - accuracy: 1.0000 - val_loss: 6.8475 - val_accuracy: 0.4820\n",
      "Epoch 913/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.2411e-06 - accuracy: 1.0000 - val_loss: 6.8566 - val_accuracy: 0.4865\n",
      "Epoch 914/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.2425e-06 - accuracy: 1.0000 - val_loss: 6.8506 - val_accuracy: 0.4865\n",
      "Epoch 915/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.3014e-06 - accuracy: 1.0000 - val_loss: 6.8691 - val_accuracy: 0.4865\n",
      "Epoch 916/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.1585e-06 - accuracy: 1.0000 - val_loss: 6.8626 - val_accuracy: 0.4865\n",
      "Epoch 917/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0587e-06 - accuracy: 1.0000 - val_loss: 6.8661 - val_accuracy: 0.4865\n",
      "Epoch 918/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.2095e-06 - accuracy: 1.0000 - val_loss: 6.8619 - val_accuracy: 0.4910\n",
      "Epoch 919/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.2281e-06 - accuracy: 1.0000 - val_loss: 6.8626 - val_accuracy: 0.4910\n",
      "Epoch 920/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0349e-06 - accuracy: 1.0000 - val_loss: 6.8665 - val_accuracy: 0.4910\n",
      "Epoch 921/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 1.0755e-06 - accuracy: 1.0000 - val_loss: 6.8758 - val_accuracy: 0.4910\n",
      "Epoch 922/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.0524e-06 - accuracy: 1.0000 - val_loss: 6.8815 - val_accuracy: 0.4820\n",
      "Epoch 923/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 1.0867e-06 - accuracy: 1.0000 - val_loss: 6.8876 - val_accuracy: 0.4910\n",
      "Epoch 924/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 9.9644e-07 - accuracy: 1.0000 - val_loss: 6.8780 - val_accuracy: 0.4910\n",
      "Epoch 925/1000\n",
      "1990/1990 [==============================] - 0s 142us/step - loss: 1.1605e-06 - accuracy: 1.0000 - val_loss: 6.8769 - val_accuracy: 0.4910\n",
      "Epoch 926/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.0802e-06 - accuracy: 1.0000 - val_loss: 6.8919 - val_accuracy: 0.4865\n",
      "Epoch 927/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.0552e-06 - accuracy: 1.0000 - val_loss: 6.9056 - val_accuracy: 0.4910\n",
      "Epoch 928/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 1.0065e-06 - accuracy: 1.0000 - val_loss: 6.9043 - val_accuracy: 0.4910\n",
      "Epoch 929/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 1.1213e-06 - accuracy: 1.0000 - val_loss: 6.9121 - val_accuracy: 0.4910\n",
      "Epoch 930/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.0675e-06 - accuracy: 1.0000 - val_loss: 6.8894 - val_accuracy: 0.4865\n",
      "Epoch 931/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 9.5108e-07 - accuracy: 1.0000 - val_loss: 6.9138 - val_accuracy: 0.4910\n",
      "Epoch 932/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 9.4725e-07 - accuracy: 1.0000 - val_loss: 6.9192 - val_accuracy: 0.4910\n",
      "Epoch 933/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 1.0410e-06 - accuracy: 1.0000 - val_loss: 6.9149 - val_accuracy: 0.4865\n",
      "Epoch 934/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 9.4936e-07 - accuracy: 1.0000 - val_loss: 6.9225 - val_accuracy: 0.4865\n",
      "Epoch 935/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 9.6804e-07 - accuracy: 1.0000 - val_loss: 6.9158 - val_accuracy: 0.4865\n",
      "Epoch 936/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 1.0581e-06 - accuracy: 1.0000 - val_loss: 6.9186 - val_accuracy: 0.4865\n",
      "Epoch 937/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 9.0588e-07 - accuracy: 1.0000 - val_loss: 6.9374 - val_accuracy: 0.4865\n",
      "Epoch 938/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.0048e-06 - accuracy: 1.0000 - val_loss: 6.9233 - val_accuracy: 0.4865\n",
      "Epoch 939/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 8.9959e-07 - accuracy: 1.0000 - val_loss: 6.9004 - val_accuracy: 0.4910\n",
      "Epoch 940/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 9.6387e-07 - accuracy: 1.0000 - val_loss: 6.9083 - val_accuracy: 0.4910\n",
      "Epoch 941/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 8.8059e-07 - accuracy: 1.0000 - val_loss: 6.9365 - val_accuracy: 0.4910\n",
      "Epoch 942/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 9.6848e-07 - accuracy: 1.0000 - val_loss: 6.9371 - val_accuracy: 0.4910\n",
      "Epoch 943/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 8.9963e-07 - accuracy: 1.0000 - val_loss: 6.9381 - val_accuracy: 0.4910\n",
      "Epoch 944/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 9.1159e-07 - accuracy: 1.0000 - val_loss: 6.9366 - val_accuracy: 0.4865\n",
      "Epoch 945/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 1.0069e-06 - accuracy: 1.0000 - val_loss: 6.9349 - val_accuracy: 0.4910\n",
      "Epoch 946/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 9.4204e-07 - accuracy: 1.0000 - val_loss: 6.9467 - val_accuracy: 0.4910\n",
      "Epoch 947/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 1.0120e-06 - accuracy: 1.0000 - val_loss: 6.9371 - val_accuracy: 0.4865\n",
      "Epoch 948/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 9.2595e-07 - accuracy: 1.0000 - val_loss: 6.9302 - val_accuracy: 0.4910\n",
      "Epoch 949/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 8.3841e-07 - accuracy: 1.0000 - val_loss: 6.9345 - val_accuracy: 0.4910\n",
      "Epoch 950/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 9.8524e-07 - accuracy: 1.0000 - val_loss: 6.9504 - val_accuracy: 0.4820\n",
      "Epoch 951/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 1.0460e-06 - accuracy: 1.0000 - val_loss: 6.9509 - val_accuracy: 0.4910\n",
      "Epoch 952/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 8.9544e-07 - accuracy: 1.0000 - val_loss: 6.9413 - val_accuracy: 0.4910\n",
      "Epoch 953/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 7.7678e-07 - accuracy: 1.0000 - val_loss: 6.9535 - val_accuracy: 0.4910\n",
      "Epoch 954/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 7.7483e-07 - accuracy: 1.0000 - val_loss: 6.9717 - val_accuracy: 0.4910\n",
      "Epoch 955/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 8.0313e-07 - accuracy: 1.0000 - val_loss: 6.9795 - val_accuracy: 0.4865\n",
      "Epoch 956/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 8.3655e-07 - accuracy: 1.0000 - val_loss: 6.9838 - val_accuracy: 0.4910\n",
      "Epoch 957/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 9.2335e-07 - accuracy: 1.0000 - val_loss: 6.9819 - val_accuracy: 0.4820\n",
      "Epoch 958/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 7.5616e-07 - accuracy: 1.0000 - val_loss: 6.9781 - val_accuracy: 0.4910\n",
      "Epoch 959/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 8.0663e-07 - accuracy: 1.0000 - val_loss: 6.9933 - val_accuracy: 0.4865\n",
      "Epoch 960/1000\n",
      "1990/1990 [==============================] - 0s 114us/step - loss: 7.5134e-07 - accuracy: 1.0000 - val_loss: 6.9966 - val_accuracy: 0.4865\n",
      "Epoch 961/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 7.8474e-07 - accuracy: 1.0000 - val_loss: 7.0144 - val_accuracy: 0.4865\n",
      "Epoch 962/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 7.3750e-07 - accuracy: 1.0000 - val_loss: 7.0119 - val_accuracy: 0.4865\n",
      "Epoch 963/1000\n",
      "1990/1990 [==============================] - 0s 106us/step - loss: 7.6060e-07 - accuracy: 1.0000 - val_loss: 7.0332 - val_accuracy: 0.4910\n",
      "Epoch 964/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 7.8424e-07 - accuracy: 1.0000 - val_loss: 7.0380 - val_accuracy: 0.4865\n",
      "Epoch 965/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 7.2746e-07 - accuracy: 1.0000 - val_loss: 7.0248 - val_accuracy: 0.4820\n",
      "Epoch 966/1000\n",
      "1990/1990 [==============================] - 0s 108us/step - loss: 7.1927e-07 - accuracy: 1.0000 - val_loss: 7.0208 - val_accuracy: 0.4820\n",
      "Epoch 967/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 7.7209e-07 - accuracy: 1.0000 - val_loss: 7.0348 - val_accuracy: 0.4865\n",
      "Epoch 968/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990/1990 [==============================] - 0s 101us/step - loss: 7.0168e-07 - accuracy: 1.0000 - val_loss: 7.0510 - val_accuracy: 0.4865\n",
      "Epoch 969/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 6.7221e-07 - accuracy: 1.0000 - val_loss: 7.0437 - val_accuracy: 0.4820\n",
      "Epoch 970/1000\n",
      "1990/1990 [==============================] - 0s 96us/step - loss: 7.0285e-07 - accuracy: 1.0000 - val_loss: 7.0563 - val_accuracy: 0.4865\n",
      "Epoch 971/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 7.2667e-07 - accuracy: 1.0000 - val_loss: 7.0619 - val_accuracy: 0.4865\n",
      "Epoch 972/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 8.1890e-07 - accuracy: 1.0000 - val_loss: 7.0409 - val_accuracy: 0.4910\n",
      "Epoch 973/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 7.5879e-07 - accuracy: 1.0000 - val_loss: 7.0534 - val_accuracy: 0.4865\n",
      "Epoch 974/1000\n",
      "1990/1990 [==============================] - 0s 97us/step - loss: 7.2283e-07 - accuracy: 1.0000 - val_loss: 7.0389 - val_accuracy: 0.4775\n",
      "Epoch 975/1000\n",
      "1990/1990 [==============================] - 0s 103us/step - loss: 8.4370e-07 - accuracy: 1.0000 - val_loss: 7.0603 - val_accuracy: 0.4865\n",
      "Epoch 976/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 6.6467e-07 - accuracy: 1.0000 - val_loss: 7.0638 - val_accuracy: 0.4820\n",
      "Epoch 977/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 6.5251e-07 - accuracy: 1.0000 - val_loss: 7.0594 - val_accuracy: 0.4910\n",
      "Epoch 978/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 7.5873e-07 - accuracy: 1.0000 - val_loss: 7.0547 - val_accuracy: 0.4865\n",
      "Epoch 979/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 7.5596e-07 - accuracy: 1.0000 - val_loss: 7.0635 - val_accuracy: 0.4820\n",
      "Epoch 980/1000\n",
      "1990/1990 [==============================] - 0s 99us/step - loss: 6.5534e-07 - accuracy: 1.0000 - val_loss: 7.0763 - val_accuracy: 0.4820\n",
      "Epoch 981/1000\n",
      "1990/1990 [==============================] - 0s 101us/step - loss: 6.4759e-07 - accuracy: 1.0000 - val_loss: 7.0857 - val_accuracy: 0.4820\n",
      "Epoch 982/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.7184e-07 - accuracy: 1.0000 - val_loss: 7.0895 - val_accuracy: 0.4865\n",
      "Epoch 983/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 6.5416e-07 - accuracy: 1.0000 - val_loss: 7.0819 - val_accuracy: 0.4730\n",
      "Epoch 984/1000\n",
      "1990/1990 [==============================] - 0s 104us/step - loss: 6.9154e-07 - accuracy: 1.0000 - val_loss: 7.0868 - val_accuracy: 0.4820\n",
      "Epoch 985/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 6.8621e-07 - accuracy: 1.0000 - val_loss: 7.0802 - val_accuracy: 0.4730\n",
      "Epoch 986/1000\n",
      "1990/1990 [==============================] - 0s 93us/step - loss: 6.2914e-07 - accuracy: 1.0000 - val_loss: 7.0928 - val_accuracy: 0.4775\n",
      "Epoch 987/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 6.3497e-07 - accuracy: 1.0000 - val_loss: 7.0879 - val_accuracy: 0.4730\n",
      "Epoch 988/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 6.4463e-07 - accuracy: 1.0000 - val_loss: 7.0900 - val_accuracy: 0.4820\n",
      "Epoch 989/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 6.1439e-07 - accuracy: 1.0000 - val_loss: 7.0880 - val_accuracy: 0.4775\n",
      "Epoch 990/1000\n",
      "1990/1990 [==============================] - 0s 92us/step - loss: 6.3522e-07 - accuracy: 1.0000 - val_loss: 7.0923 - val_accuracy: 0.4730\n",
      "Epoch 991/1000\n",
      "1990/1990 [==============================] - 0s 94us/step - loss: 6.8665e-07 - accuracy: 1.0000 - val_loss: 7.0987 - val_accuracy: 0.4775\n",
      "Epoch 992/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 6.7404e-07 - accuracy: 1.0000 - val_loss: 7.1202 - val_accuracy: 0.4775\n",
      "Epoch 993/1000\n",
      "1990/1990 [==============================] - 0s 95us/step - loss: 6.6030e-07 - accuracy: 1.0000 - val_loss: 7.1015 - val_accuracy: 0.4775\n",
      "Epoch 994/1000\n",
      "1990/1990 [==============================] - 0s 105us/step - loss: 6.6993e-07 - accuracy: 1.0000 - val_loss: 7.1299 - val_accuracy: 0.4775\n",
      "Epoch 995/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 6.7462e-07 - accuracy: 1.0000 - val_loss: 7.1205 - val_accuracy: 0.4820\n",
      "Epoch 996/1000\n",
      "1990/1990 [==============================] - 0s 112us/step - loss: 5.9327e-07 - accuracy: 1.0000 - val_loss: 7.1329 - val_accuracy: 0.4820\n",
      "Epoch 997/1000\n",
      "1990/1990 [==============================] - 0s 110us/step - loss: 5.6023e-07 - accuracy: 1.0000 - val_loss: 7.1341 - val_accuracy: 0.4820\n",
      "Epoch 998/1000\n",
      "1990/1990 [==============================] - 0s 102us/step - loss: 5.8307e-07 - accuracy: 1.0000 - val_loss: 7.1514 - val_accuracy: 0.4865\n",
      "Epoch 999/1000\n",
      "1990/1990 [==============================] - 0s 98us/step - loss: 6.0042e-07 - accuracy: 1.0000 - val_loss: 7.1580 - val_accuracy: 0.4865\n",
      "Epoch 1000/1000\n",
      "1990/1990 [==============================] - 0s 100us/step - loss: 5.4652e-07 - accuracy: 1.0000 - val_loss: 7.1423 - val_accuracy: 0.4820\n",
      "15 day\n",
      "\n",
      "# Evaluate on test data\n",
      "246/246 [==============================] - 0s 39us/step\n",
      "test loss, test acc: [6.191910716576305, 0.5243902206420898]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (246, 1)\n",
      "rmse: 0.6759263456327976\n"
     ]
    }
   ],
   "source": [
    "PAST_DAYS = 3\n",
    "X_train_batches, y_train_batches = build_batch(stock_with_abs_norm, label_abs_1d, PAST_DAYS, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_batches, y_train_batches, test_size=0.1, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "model = buildTrendModel_4stacks(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"val_accuracy\", patience=500, verbose=1, mode=\"max\")\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_valid, y_valid), callbacks=[callback])\n",
    "\n",
    "print(\"15 day\")\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions shape:', predictions.shape)\n",
    "\n",
    "\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "print('rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99999881e-01],\n",
       "       [3.36779465e-07],\n",
       "       [7.94687629e-01],\n",
       "       [2.50774921e-08],\n",
       "       [9.89200885e-07],\n",
       "       [9.99990463e-01],\n",
       "       [1.30112655e-02],\n",
       "       [9.74454358e-03],\n",
       "       [1.75809117e-10],\n",
       "       [4.09911166e-07],\n",
       "       [9.15058862e-10],\n",
       "       [1.02268594e-07],\n",
       "       [9.99974370e-01],\n",
       "       [9.99982238e-01],\n",
       "       [1.09167904e-05],\n",
       "       [9.98772681e-01],\n",
       "       [4.71672484e-12],\n",
       "       [2.11470180e-07],\n",
       "       [2.02098349e-03],\n",
       "       [4.77092783e-07],\n",
       "       [3.28242777e-07],\n",
       "       [2.09092743e-08],\n",
       "       [2.28779186e-07],\n",
       "       [1.83533153e-08],\n",
       "       [9.99721348e-01],\n",
       "       [1.47447432e-03],\n",
       "       [1.49149025e-06],\n",
       "       [7.03328021e-11],\n",
       "       [9.99997497e-01],\n",
       "       [7.24537134e-01],\n",
       "       [1.04117580e-02],\n",
       "       [6.79251621e-04],\n",
       "       [9.99718010e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.99595940e-01],\n",
       "       [9.99999523e-01],\n",
       "       [9.99999523e-01],\n",
       "       [9.95771825e-01],\n",
       "       [1.00000000e+00],\n",
       "       [2.35642117e-10],\n",
       "       [9.56349075e-01],\n",
       "       [1.61956921e-01],\n",
       "       [9.99250352e-01],\n",
       "       [1.88362435e-07],\n",
       "       [3.73131499e-08],\n",
       "       [8.43410313e-01],\n",
       "       [2.21974074e-06],\n",
       "       [1.00000000e+00],\n",
       "       [3.42562645e-09],\n",
       "       [1.99752799e-06],\n",
       "       [9.99998093e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.98138666e-01],\n",
       "       [1.45587313e-03],\n",
       "       [6.07999482e-08],\n",
       "       [1.00000000e+00],\n",
       "       [7.25050853e-11],\n",
       "       [9.96990561e-01],\n",
       "       [5.78786603e-05],\n",
       "       [2.35773027e-01],\n",
       "       [2.35697182e-12],\n",
       "       [1.00000000e+00],\n",
       "       [9.99999523e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.97114897e-01],\n",
       "       [9.97755587e-01],\n",
       "       [5.30216028e-07],\n",
       "       [1.00000000e+00],\n",
       "       [2.18733476e-04],\n",
       "       [2.95684976e-03],\n",
       "       [9.99999285e-01],\n",
       "       [9.99999523e-01],\n",
       "       [1.84736162e-01],\n",
       "       [1.23446986e-08],\n",
       "       [9.99999881e-01],\n",
       "       [9.97935176e-01],\n",
       "       [3.69486344e-08],\n",
       "       [1.22529462e-01],\n",
       "       [1.00000000e+00],\n",
       "       [3.85177756e-09],\n",
       "       [2.47656385e-06],\n",
       "       [1.00000000e+00],\n",
       "       [9.48547240e-05],\n",
       "       [9.99733388e-01],\n",
       "       [1.00000000e+00],\n",
       "       [3.18315180e-10],\n",
       "       [9.99900579e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.57430912e-10],\n",
       "       [4.20036940e-06],\n",
       "       [9.99996543e-01],\n",
       "       [9.99883652e-01],\n",
       "       [5.25691985e-06],\n",
       "       [4.36228449e-11],\n",
       "       [9.99997258e-01],\n",
       "       [7.31205273e-06],\n",
       "       [9.99982357e-01],\n",
       "       [6.56363000e-07],\n",
       "       [1.41548276e-06],\n",
       "       [8.68151009e-01],\n",
       "       [4.90426771e-07],\n",
       "       [2.13089556e-01],\n",
       "       [1.00000000e+00],\n",
       "       [1.48206392e-09],\n",
       "       [9.99990702e-01],\n",
       "       [3.48084654e-06],\n",
       "       [2.37109598e-05],\n",
       "       [8.18135619e-01],\n",
       "       [2.16886656e-06],\n",
       "       [4.12362903e-01],\n",
       "       [9.99999881e-01],\n",
       "       [9.99944806e-01],\n",
       "       [1.00000000e+00],\n",
       "       [8.25106446e-03],\n",
       "       [3.90727967e-01],\n",
       "       [7.45881815e-04],\n",
       "       [4.85295750e-04],\n",
       "       [1.56301051e-01],\n",
       "       [2.88253116e-10],\n",
       "       [2.29266710e-11],\n",
       "       [1.00000000e+00],\n",
       "       [9.99962687e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.96474206e-01],\n",
       "       [4.17348572e-10],\n",
       "       [9.99992132e-01],\n",
       "       [1.20534338e-02],\n",
       "       [1.91952521e-09],\n",
       "       [2.78337513e-08],\n",
       "       [1.00000000e+00],\n",
       "       [9.99784172e-01],\n",
       "       [6.98520908e-10],\n",
       "       [8.57730483e-05],\n",
       "       [9.99999762e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.70011950e-01],\n",
       "       [9.99989867e-01],\n",
       "       [2.34028508e-12],\n",
       "       [1.60002146e-05],\n",
       "       [9.77491260e-01],\n",
       "       [9.99999881e-01],\n",
       "       [2.70684774e-04],\n",
       "       [7.00632441e-08],\n",
       "       [3.42429371e-06],\n",
       "       [1.35736977e-09],\n",
       "       [1.60650526e-09],\n",
       "       [3.89836088e-04],\n",
       "       [2.12798223e-01],\n",
       "       [9.67670739e-01],\n",
       "       [4.52289358e-03],\n",
       "       [9.99908209e-01],\n",
       "       [9.99996543e-01],\n",
       "       [2.24049668e-08],\n",
       "       [3.30218002e-02],\n",
       "       [1.00000000e+00],\n",
       "       [9.99999881e-01],\n",
       "       [1.38297798e-06],\n",
       "       [6.78252343e-10],\n",
       "       [1.20860700e-11],\n",
       "       [2.47961424e-07],\n",
       "       [9.94566679e-01],\n",
       "       [9.68289077e-01],\n",
       "       [1.87214762e-12],\n",
       "       [3.33909497e-07],\n",
       "       [9.18827772e-01],\n",
       "       [9.99998450e-01],\n",
       "       [9.87309158e-01],\n",
       "       [1.96871100e-04],\n",
       "       [9.99996066e-01],\n",
       "       [9.06078945e-12],\n",
       "       [1.18881684e-11],\n",
       "       [4.37960116e-05],\n",
       "       [1.00000000e+00],\n",
       "       [3.41856753e-06],\n",
       "       [9.99295831e-01],\n",
       "       [3.98765609e-04],\n",
       "       [9.99999762e-01],\n",
       "       [6.88446977e-04],\n",
       "       [6.06661979e-06],\n",
       "       [2.27309942e-11],\n",
       "       [3.35893291e-09],\n",
       "       [1.90818287e-06],\n",
       "       [5.60323713e-07],\n",
       "       [2.39945939e-06],\n",
       "       [1.87805802e-01],\n",
       "       [2.85501068e-04],\n",
       "       [7.44731587e-05],\n",
       "       [1.79096445e-01],\n",
       "       [1.59466329e-09],\n",
       "       [1.00000000e+00],\n",
       "       [3.55249408e-09],\n",
       "       [1.00000000e+00],\n",
       "       [1.80677995e-09],\n",
       "       [4.78096626e-06],\n",
       "       [1.33553657e-08],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [1.23114505e-08],\n",
       "       [3.34960640e-12],\n",
       "       [2.52382897e-08],\n",
       "       [1.00000000e+00],\n",
       "       [5.51169387e-06],\n",
       "       [1.00000000e+00],\n",
       "       [1.16733611e-01],\n",
       "       [1.00000000e+00],\n",
       "       [5.30466608e-11],\n",
       "       [9.99067366e-01],\n",
       "       [7.16575002e-03],\n",
       "       [9.99981880e-01],\n",
       "       [4.39635173e-09],\n",
       "       [3.54684886e-08],\n",
       "       [1.30886810e-05],\n",
       "       [9.99999881e-01],\n",
       "       [2.53696214e-07],\n",
       "       [9.99994755e-01],\n",
       "       [9.97847080e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.99985695e-01],\n",
       "       [5.50208324e-06],\n",
       "       [5.46276115e-11],\n",
       "       [2.26636554e-09],\n",
       "       [4.14194554e-01],\n",
       "       [7.13889960e-08],\n",
       "       [1.39186467e-08],\n",
       "       [9.10679591e-06],\n",
       "       [5.76785445e-01],\n",
       "       [6.18324518e-01],\n",
       "       [1.25910628e-05],\n",
       "       [1.75236516e-06],\n",
       "       [4.18880445e-05],\n",
       "       [1.00000000e+00],\n",
       "       [1.34778329e-05],\n",
       "       [3.68557274e-08],\n",
       "       [1.77066090e-06],\n",
       "       [9.99249756e-01],\n",
       "       [1.20994227e-03],\n",
       "       [9.74571347e-01],\n",
       "       [9.99419451e-01],\n",
       "       [9.99792278e-01],\n",
       "       [9.98692334e-01],\n",
       "       [9.97542381e-01],\n",
       "       [4.29325625e-02],\n",
       "       [9.41573363e-03],\n",
       "       [1.05733890e-12],\n",
       "       [3.82211208e-01],\n",
       "       [1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('X_train shape: ', X_train.shape)\n",
    "# print('X_valid shape: ', X_valid.shape)\n",
    "# print('y_train shape: ', y_train.shape)\n",
    "# print('y_valid shape: ', y_valid.shape)\n",
    "\n",
    "# print('X_test shape: ', X_test.shape)\n",
    "# print('y_test shape: ', y_test.shape)\n",
    "\n",
    "# print('X_test_batches shape: ', X_train_batches.shape)\n",
    "# print('Y_test_batches shape: ', y_train_batches.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
